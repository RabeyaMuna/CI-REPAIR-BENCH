{
    "sha_fail": "424e2bec035589620a6ae25f51d4f389adc3a12e",
    "changed_files": [
        {
            "commit": "424e2bec035589620a6ae25f51d4f389adc3a12e",
            "file_path": "baselines/statavg/.gitignore",
            "diff": "diff --git a/baselines/statavg/.gitignore b/baselines/statavg/.gitignore\nnew file mode 100644\nindex 000000000..165ade32d\n--- /dev/null\n+++ b/baselines/statavg/.gitignore\n@@ -0,0 +1,166 @@\n+# Byte-compiled / optimized / DLL files\n+__pycache__/\n+*.py[cod]\n+*$py.class\n+\n+# Flower directory\n+.flwr\n+\n+# C extensions\n+*.so\n+\n+# Distribution / packaging\n+.Python\n+build/\n+develop-eggs/\n+dist/\n+downloads/\n+eggs/\n+.eggs/\n+lib/\n+lib64/\n+parts/\n+sdist/\n+var/\n+wheels/\n+share/python-wheels/\n+*.egg-info/\n+.installed.cfg\n+*.egg\n+MANIFEST\n+\n+# PyInstaller\n+#  Usually these files are written by a python script from a template\n+#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n+*.manifest\n+*.spec\n+\n+# Installer logs\n+pip-log.txt\n+pip-delete-this-directory.txt\n+\n+# Unit test / coverage reports\n+htmlcov/\n+.tox/\n+.nox/\n+.coverage\n+.coverage.*\n+.cache\n+nosetests.xml\n+coverage.xml\n+*.cover\n+*.py,cover\n+.hypothesis/\n+.pytest_cache/\n+cover/\n+\n+# Translations\n+*.mo\n+*.pot\n+\n+# Django stuff:\n+*.log\n+local_settings.py\n+db.sqlite3\n+db.sqlite3-journal\n+\n+# Flask stuff:\n+instance/\n+.webassets-cache\n+\n+# Scrapy stuff:\n+.scrapy\n+\n+# Sphinx documentation\n+docs/_build/\n+\n+# PyBuilder\n+.pybuilder/\n+target/\n+\n+# Jupyter Notebook\n+.ipynb_checkpoints\n+\n+# IPython\n+profile_default/\n+ipython_config.py\n+\n+# pyenv\n+#   For a library or package, you might want to ignore these files since the code is\n+#   intended to run in multiple environments; otherwise, check them in:\n+# .python-version\n+\n+# pipenv\n+#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n+#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n+#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n+#   install all needed dependencies.\n+#Pipfile.lock\n+\n+# poetry\n+#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n+#   This is especially recommended for binary packages to ensure reproducibility, and is more\n+#   commonly ignored for libraries.\n+#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n+#poetry.lock\n+\n+# pdm\n+#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n+#pdm.lock\n+#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n+#   in version control.\n+#   https://pdm.fming.dev/#use-with-ide\n+.pdm.toml\n+\n+# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n+__pypackages__/\n+\n+# Celery stuff\n+celerybeat-schedule\n+celerybeat.pid\n+\n+# SageMath parsed files\n+*.sage.py\n+\n+# Environments\n+.env\n+.venv\n+env/\n+venv/\n+ENV/\n+env.bak/\n+venv.bak/\n+\n+# Spyder project settings\n+.spyderproject\n+.spyproject\n+\n+# Rope project settings\n+.ropeproject\n+\n+# mkdocs documentation\n+/site\n+\n+# mypy\n+.mypy_cache/\n+.dmypy.json\n+dmypy.json\n+\n+# Pyre type checker\n+.pyre/\n+\n+# pytype static type analyzer\n+.pytype/\n+\n+# Cython debug symbols\n+cython_debug/\n+\n+# PyCharm\n+#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n+#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n+#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n+#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n+#.idea/\n+\n+results/\n+dataset/\n\\ No newline at end of file\n"
        },
        {
            "commit": "424e2bec035589620a6ae25f51d4f389adc3a12e",
            "file_path": "baselines/statavg/LICENSE",
            "diff": "diff --git a/baselines/statavg/LICENSE b/baselines/statavg/LICENSE\nindex d64569567..7a4a3ea24 100644\n--- a/baselines/statavg/LICENSE\n+++ b/baselines/statavg/LICENSE\n@@ -199,4 +199,4 @@\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n-   limitations under the License.\n+   limitations under the License.\n\\ No newline at end of file\n"
        },
        {
            "commit": "424e2bec035589620a6ae25f51d4f389adc3a12e",
            "file_path": "baselines/statavg/_static/fig.png",
            "diff": "diff --git a/baselines/statavg/_static/fig.png b/baselines/statavg/_static/fig.png\ndeleted file mode 100644\nindex 4c9b1dc7c..000000000\nBinary files a/baselines/statavg/_static/fig.png and /dev/null differ\n"
        },
        {
            "commit": "424e2bec035589620a6ae25f51d4f389adc3a12e",
            "file_path": "baselines/statavg/_static/results.png",
            "diff": "diff --git a/baselines/statavg/_static/results.png b/baselines/statavg/_static/results.png\nnew file mode 100644\nindex 000000000..7e94d8227\nBinary files /dev/null and b/baselines/statavg/_static/results.png differ\n"
        },
        {
            "commit": "424e2bec035589620a6ae25f51d4f389adc3a12e",
            "file_path": "baselines/statavg/docs/plot_and_viz_results.ipynb",
            "diff": "diff --git a/baselines/statavg/docs/plot_and_viz_results.ipynb b/baselines/statavg/docs/plot_and_viz_results.ipynb\nnew file mode 100644\nindex 000000000..c1a6edf01\n--- /dev/null\n+++ b/baselines/statavg/docs/plot_and_viz_results.ipynb\n@@ -0,0 +1,106 @@\n+{\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import matplotlib.pyplot as plt\\n\",\n+    \"import pickle\\n\",\n+    \"from pathlib import Path\\n\",\n+    \"import os\\n\",\n+    \"\\n\",\n+    \"SAVE_PATH = Path(os.path.abspath(\\\"\\\")).parent / \\\"_static\\\"\\n\",\n+    \"SAVE_PATH.mkdir(exist_ok=True, parents=True)\\n\",\n+    \"RESULTS_PATH =  Path(os.path.abspath(\\\"\\\")).parent / \\\"results\\\"\\n\",\n+    \"FEDAVG_PATH = RESULTS_PATH / \\\"fedavg\\\" / \\\"results.pickle\\\"\\n\",\n+    \"STATAVG_PATH = RESULTS_PATH / \\\"statavg\\\" / \\\"results.pickle\\\"\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"def plot_accuracy(stat_avg_path: Path, fedavg_path: Path) -> None:\\n\",\n+    \"    \\\"\\\"\\\"Plot the accuracy.\\\"\\\"\\\"\\n\",\n+    \"    with open(stat_avg_path, \\\"rb\\\") as file:\\n\",\n+    \"        statavg_results = pickle.load(file)\\n\",\n+    \"\\n\",\n+    \"    with open(fedavg_path, \\\"rb\\\") as file:\\n\",\n+    \"        fedavg_results = pickle.load(file)\\n\",\n+    \"\\n\",\n+    \"    fig, ax = plt.subplots(1,1, figsize=(12,8))\\n\",\n+    \"    for results, label in [(statavg_results, \\\"StatAvg\\\"), (fedavg_results, \\\"FedAvg\\\")]:\\n\",\n+    \"        accuracy_dict = results[\\\"history\\\"].metrics_distributed\\n\",\n+    \"        accuracy_lst = accuracy_dict[\\\"accuracy\\\"]\\n\",\n+    \"\\n\",\n+    \"        rounds = [p[0] for p in accuracy_lst]\\n\",\n+    \"        acc = [p[1] for p in accuracy_lst]\\n\",\n+    \"\\n\",\n+    \"        ax.plot(rounds, acc, marker=\\\"o\\\", linestyle=\\\"-\\\", label=label)\\n\",\n+    \"    ax.legend(fontsize=14)\\n\",\n+    \"    ax.set_xlabel(\\\"Rounds\\\", fontsize=14)\\n\",\n+    \"    ax.set_ylabel(\\\"Testing Accuracy\\\", fontsize=14)\\n\",\n+    \"    ax.tick_params(axis='both', labelsize=14)\\n\",\n+    \"\\n\",\n+    \"    ax.grid(True)\\n\",\n+    \"    fig.show()\\n\",\n+    \"    return fig\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"fig = plot_accuracy(stat_avg_path=STATAVG_PATH, fedavg_path=FEDAVG_PATH)\\n\",\n+    \"def saveFig(name, fig):\\n\",\n+    \"    fig.savefig(\\n\",\n+    \"        name,\\n\",\n+    \"        dpi=None,\\n\",\n+    \"        facecolor=fig.get_facecolor(),\\n\",\n+    \"        edgecolor=\\\"none\\\",\\n\",\n+    \"        orientation=\\\"portrait\\\",\\n\",\n+    \"        format=\\\"png\\\",\\n\",\n+    \"        transparent=False,\\n\",\n+    \"        bbox_inches=\\\"tight\\\",\\n\",\n+    \"        pad_inches=0.2,\\n\",\n+    \"        metadata=None,\\n\",\n+    \"    )\\n\",\n+    \"saveFig(SAVE_PATH/\\\"results.png\\\", fig)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": []\n+  }\n+ ],\n+ \"metadata\": {\n+  \"kernelspec\": {\n+   \"display_name\": \"statavg\",\n+   \"language\": \"python\",\n+   \"name\": \"python3\"\n+  },\n+  \"language_info\": {\n+   \"codemirror_mode\": {\n+    \"name\": \"ipython\",\n+    \"version\": 3\n+   },\n+   \"file_extension\": \".py\",\n+   \"mimetype\": \"text/x-python\",\n+   \"name\": \"python\",\n+   \"nbconvert_exporter\": \"python\",\n+   \"pygments_lexer\": \"ipython3\",\n+   \"version\": \"3.11.10\"\n+  }\n+ },\n+ \"nbformat\": 4,\n+ \"nbformat_minor\": 2\n+}\n"
        },
        {
            "commit": "424e2bec035589620a6ae25f51d4f389adc3a12e",
            "file_path": "baselines/statavg/run_experiments.sh",
            "diff": "diff --git a/baselines/statavg/run_experiments.sh b/baselines/statavg/run_experiments.sh\nnew file mode 100644\nindex 000000000..733f2a90d\n--- /dev/null\n+++ b/baselines/statavg/run_experiments.sh\n@@ -0,0 +1,4 @@\n+export TF_FORCE_GPU_ALLOW_GROWTH=\"1\"\n+export TF_CPP_MIN_LOG_LEVEL=\"3\"\n+flwr run . \n+flwr run . --run-config conf/fedavg.toml \n\\ No newline at end of file\n"
        },
        {
            "commit": "424e2bec035589620a6ae25f51d4f389adc3a12e",
            "file_path": "baselines/statavg/statavg/__init__.py",
            "diff": "diff --git a/baselines/statavg/statavg/__init__.py b/baselines/statavg/statavg/__init__.py\nindex 02a650fb4..b406565b8 100644\n--- a/baselines/statavg/statavg/__init__.py\n+++ b/baselines/statavg/statavg/__init__.py\n@@ -1 +1 @@\n-\"\"\"Template baseline package.\"\"\"\n+\"\"\"statavg: A Flower Baseline.\"\"\"\n"
        },
        {
            "commit": "424e2bec035589620a6ae25f51d4f389adc3a12e",
            "file_path": "baselines/statavg/statavg/client_app.py",
            "diff": "diff --git a/baselines/statavg/statavg/client_app.py b/baselines/statavg/statavg/client_app.py\nnew file mode 100644\nindex 000000000..dfbf4cc9e\n--- /dev/null\n+++ b/baselines/statavg/statavg/client_app.py\n@@ -0,0 +1,250 @@\n+\"\"\"statavg: A Flower Baseline.\"\"\"\n+\n+import json\n+import pickle\n+from typing import Dict, Tuple\n+\n+import numpy as np\n+import tensorflow as tf\n+from imblearn.over_sampling import SMOTE\n+from omegaconf import OmegaConf\n+from pandas import DataFrame\n+from sklearn.model_selection import train_test_split\n+from sklearn.preprocessing import LabelEncoder, StandardScaler\n+\n+from flwr.client import ClientApp, NumPyClient\n+from flwr.common import ConfigRecord, Context\n+from flwr.common.typing import NDArrays, Scalar\n+from statavg.dataset import prepare_dataset\n+from statavg.model import get_model\n+\n+\n+# Define Flower client\n+# pylint: disable=too-many-instance-attributes\n+class FedAvgClient(NumPyClient):\n+    \"\"\"Client class that will implement FedAvg.\"\"\"\n+\n+    # pylint: disable=too-many-arguments\n+    def __init__(\n+        self,\n+        trainset: DataFrame,\n+        val_ratio: float,\n+        client_id: int,\n+        learning_rate: float,\n+        strategy_name: str,\n+        local_epochs: int,\n+        batch_size: int,\n+        client_state,\n+    ) -> None:\n+\n+        # load trainset\n+        self.data = trainset\n+\n+        # client id\n+        self.client_id = client_id\n+        self.val_ratio = val_ratio\n+        self.learning_rate = learning_rate\n+        self._strategy_name = strategy_name\n+        self.local_epochs = local_epochs\n+        self.batch_size = batch_size\n+        self.client_state = client_state\n+\n+        # preprocess and split the dataset\n+        self.x_train, self.y_train, self.x_val, self.y_val = preprocess_and_split(\n+            self.data, val_ratio\n+        )\n+\n+        # get the model\n+        self.model = get_model(self.x_train.shape[1], len(self.y_train.value_counts()))\n+\n+        opt = tf.keras.optimizers.Adam(\n+            learning_rate=self.learning_rate, beta_1=0.99, beta_2=0.999, epsilon=1e-08\n+        )\n+        self.model.compile(\n+            loss=\"sparse_categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"]\n+        )\n+\n+        # encode labels\n+        self.encode_labels()\n+\n+        # scale and resample\n+        self.scaler_first_r = None  # will be used only at the 1st round\n+        scaler, use_transform_only = self.determine_scaler()\n+        self.normalize_data(scaler, use_transform_only)\n+        self.resample()\n+\n+    def determine_scaler(self):\n+        \"\"\"Determine data scaler.\"\"\"\n+        return StandardScaler(), False\n+\n+    def resample(self) -> None:\n+        \"\"\"Perform resampling using SMOTE.\"\"\"\n+        smo = SMOTE(random_state=42)\n+        self.x_train, self.y_train = smo.fit_resample(self.x_train, self.y_train)\n+\n+    def normalize_data(self, scaler, use_transform_only) -> None:\n+        \"\"\"Normalise data.\"\"\"\n+        if use_transform_only:\n+            self.x_train = scaler.transform(self.x_train)\n+        else:\n+            self.x_train = scaler.fit_transform(self.x_train)\n+        self.x_val = scaler.transform(self.x_val)\n+        self.scaler_first_r = scaler\n+\n+    def encode_labels(self) -> None:\n+        \"\"\"Encode the labels.\"\"\"\n+        enc_y = LabelEncoder()\n+        self.y_train = enc_y.fit_transform(self.y_train.to_numpy().reshape(-1))\n+        self.y_val = enc_y.transform(self.y_val.to_numpy().reshape(-1))\n+\n+    def get_parameters(self, config):\n+        \"\"\"Get the training parameters.\"\"\"\n+        return self.model.get_weights()\n+\n+    def fit(\n+        self, parameters: NDArrays, config: Dict[str, Scalar]\n+    ) -> Tuple[NDArrays, int, Dict[str, Scalar]]:\n+        \"\"\"Create a Dict with local statistics and sends it to the server.\"\"\"\n+        # set weights and fit\n+        self.model.set_weights(parameters)\n+        self.model.fit(\n+            self.x_train,\n+            self.y_train,\n+            epochs=self.local_epochs,\n+            batch_size=self.batch_size,\n+            verbose=0,\n+        )\n+\n+        weights = self.model.get_weights()  # type: ignore\n+        return weights, len(self.x_train), {}  # type: ignore\n+\n+    def evaluate(\n+        self, parameters: NDArrays, config: Dict[str, Scalar]\n+    ) -> Tuple[float, int, Dict]:\n+        \"\"\"Evaluate using the validation set: x_val.\"\"\"\n+        self.model.set_weights(parameters)\n+        loss, accuracy = self.model.evaluate(self.x_val, self.y_val, verbose=0)\n+\n+        return loss, len(self.x_val), {\"accuracy\": accuracy}\n+\n+\n+class StatAvgClient(FedAvgClient):\n+    \"\"\"Client class that will implement StatAvg.\"\"\"\n+\n+    def determine_scaler(self):\n+        \"\"\"Determine scaler with client context.\"\"\"\n+        if \"scaler\" not in self.client_state.config_records.keys():\n+            scaler = StandardScaler()\n+            use_transform_only = False\n+        else:\n+            scaler = pickle.loads(\n+                self.client_state.config_records[\"scaler\"][\"serialized_obj\"]\n+            )\n+            use_transform_only = True\n+        return scaler, use_transform_only\n+\n+    def fit(self, parameters, config):\n+        \"\"\"Create a Dict with local statistics and sends it to the server.\n+\n+        At the 1st round: Receives aggregated statistics from the server.\n+        At the 2nd round. Performs conventional local training.\n+        \"\"\"\n+        # the client statistics are saved as a Dict[str, int] only at 1st round\n+        metrics = {}\n+        if config[\"current_round\"] == 1:\n+            metrics = configure_metrics(self.scaler_first_r)\n+\n+        # read the global statistics only at 2nd round\n+        if config[\"current_round\"] == 2:\n+            for key, val in config.items():\n+                if val == 0:\n+                    stats_global = json.loads(key)\n+                else:\n+                    stats_global = None\n+\n+            mean_global = np.array(stats_global[\"mean_global\"])\n+            var_global = np.array(stats_global[\"var_global\"])\n+            std_global = np.sqrt(var_global)\n+\n+            scaler = StandardScaler()\n+            scaler.mean_ = mean_global\n+            scaler.var_ = var_global\n+            scaler.scale_ = std_global\n+            #             Save the scaler/statistics\n+            # -------------------------------------------------\n+            # In subsequent rounds (>2), clients load the saved scalers and retrieve\n+            # the global statistics.\n+            # Note that scaling/normalization happens in __init()__\n+            # by invoking the method normalize_data().\n+            serialized_scaler = pickle.dumps(scaler)\n+            self.client_state.config_records[\"scaler\"] = ConfigRecord(\n+                {\"serialized_obj\": serialized_scaler}\n+            )\n+            # Sent to the server so that the\n+            # server can access the local scalers for eval.\n+            metrics = {\"scaler\": serialized_scaler}\n+            # ---------------------------------------------------\n+\n+        weights, train_len, _ = super().fit(parameters, config)\n+        return weights, train_len, metrics\n+\n+\n+def preprocess_and_split(\n+    data: DataFrame, val_ratio: float\n+) -> Tuple[np.ndarray, DataFrame, np.ndarray, DataFrame]:\n+    \"\"\"Preprocess and split data into train and val sets.\"\"\"\n+    # keep label('type')\n+    Y = data[[\"type\"]]\n+\n+    # remove the label('type') from data\n+    data = data.drop([\"type\"], axis=1)\n+\n+    # train-test splitting\n+    x_train, x_val, y_train, y_val = train_test_split(\n+        data, Y, test_size=val_ratio, stratify=Y, random_state=42\n+    )\n+    x_train = x_train.to_numpy()\n+    x_val = x_val.to_numpy()\n+    return x_train, y_train, x_val, y_val\n+\n+\n+def configure_metrics(scaler: StandardScaler) -> Dict[str, int]:\n+    \"\"\"Transform the client's statistics to a Dict.\"\"\"\n+    # insert statistical metrics into a dict\n+    stats = {\"mean\": scaler.mean_.tolist(), \"var\": scaler.var_.tolist()}\n+\n+    # convert to json\n+    json_stats = json.dumps(stats)\n+\n+    # 0 is a random int, will not be used anywhere.\n+    # Used just for consistency with Flower documentation.\n+    metrics = {json_stats: 0}\n+\n+    return metrics\n+\n+\n+def client_fn(context: Context):\n+    \"\"\"Construct a Client that will be run in a ClientApp.\"\"\"\n+    # Load model and data\n+    cfg = OmegaConf.create(context.run_config)\n+    partition_id = int(context.node_config[\"partition-id\"])\n+    trainset, _ = prepare_dataset(\n+        cfg.num_clients, cfg.path_to_dataset, cfg.include_test, cfg.testset_ratio\n+    )\n+    client_state = context.state\n+    client_type = FedAvgClient if cfg.strategy_name == \"fedavg\" else StatAvgClient\n+    # Return Client instance\n+    return client_type(\n+        trainset=trainset[partition_id],\n+        val_ratio=cfg.val_ratio,\n+        client_id=partition_id,\n+        learning_rate=cfg.learning_rate,\n+        strategy_name=cfg.strategy_name,\n+        local_epochs=cfg.local_epochs,\n+        batch_size=cfg.batch_size,\n+        client_state=client_state,\n+    ).to_client()\n+\n+\n+# Flower ClientApp\n+app = ClientApp(client_fn)\n"
        },
        {
            "commit": "424e2bec035589620a6ae25f51d4f389adc3a12e",
            "file_path": "baselines/statavg/statavg/dataset.py",
            "diff": "diff --git a/baselines/statavg/statavg/dataset.py b/baselines/statavg/statavg/dataset.py\nindex fa9093fd1..b1e6527b6 100644\n--- a/baselines/statavg/statavg/dataset.py\n+++ b/baselines/statavg/statavg/dataset.py\n@@ -1,5 +1,80 @@\n-\"\"\"Place the dataset in the 'dataset' directory.\n+\"\"\"statavg: A Flower Baseline.\"\"\"\n \n-The dataset is loaded through 'dataset_preperation.py', where the appropriate pre-\n-processing is performed. Note, that the dataset should be stored in .csv file.\n-\"\"\"\n+import os\n+from pathlib import Path\n+from typing import List, Tuple\n+\n+import pandas as pd\n+from pandas import DataFrame\n+from sklearn.model_selection import train_test_split\n+\n+\n+def get_split_dataset(\n+    path_to_dataset: str, include_testset: bool, testset_ratio: float\n+) -> Tuple[DataFrame, DataFrame]:\n+    \"\"\"Load and split datsaet to train (client) and test (server).\"\"\"\n+    script_dir = Path(os.path.abspath(__file__)).parent.parent\n+    dataset_path = os.path.join(script_dir, path_to_dataset, \"dataset.csv\")\n+    dataset = pd.read_csv(dataset_path)\n+\n+    # remove NaN\n+    dataset = dataset.dropna(axis=0, how=\"any\")\n+\n+    # keep label\n+    Y = dataset[[\"type\"]]\n+\n+    # remove irrelevant features\n+    dataset = dataset.drop([\"PID\", \"CMD\", \"label\"], axis=1)\n+\n+    # stratified split\n+    if include_testset:\n+        trainset, testset, _, _ = train_test_split(\n+            dataset, Y, test_size=testset_ratio, stratify=Y, random_state=41\n+        )\n+        res = trainset, testset\n+    else:\n+        # if include_test is false, return an empty DataFrame for the testset\n+        res = dataset, pd.DataFrame()\n+\n+    return res\n+\n+\n+def split_clients_data(\n+    X: DataFrame, Y: DataFrame, num_partitions: int\n+) -> List[DataFrame]:\n+    \"\"\"Perform stratified split based on labels.\"\"\"\n+    x_curr = X\n+    y_curr = Y\n+    partition_dataset = []\n+    for _ in range(num_partitions - 1):\n+        x_curr, x_temp, y_curr, _ = train_test_split(\n+            x_curr,\n+            y_curr,\n+            test_size=1 / num_partitions,\n+            stratify=y_curr,\n+            random_state=41,\n+        )\n+        partition_dataset.append(x_temp)\n+        num_partitions = num_partitions - 1\n+    partition_dataset.append(x_curr)\n+    return partition_dataset\n+\n+\n+def prepare_dataset(\n+    num_partitions: int,\n+    path_to_dataset: str,\n+    include_testset: bool,\n+    testset_ratio: float,\n+) -> Tuple[DataFrame, List[DataFrame]]:\n+    \"\"\"Create the following partitions.\n+\n+    train_partitions: trainsets for clients.\n+    testset: testset for server (if server-side evaluation is needed).\n+    \"\"\"\n+    trainset, testset = get_split_dataset(\n+        path_to_dataset, include_testset, testset_ratio\n+    )\n+    y_trainset = trainset[\"type\"]\n+    train_partitions = split_clients_data(trainset, y_trainset, num_partitions)\n+\n+    return train_partitions, testset\n"
        },
        {
            "commit": "424e2bec035589620a6ae25f51d4f389adc3a12e",
            "file_path": "baselines/statavg/statavg/dataset_preparation.py",
            "diff": "diff --git a/baselines/statavg/statavg/dataset_preparation.py b/baselines/statavg/statavg/dataset_preparation.py\ndeleted file mode 100644\nindex 41868109e..000000000\n--- a/baselines/statavg/statavg/dataset_preparation.py\n+++ /dev/null\n@@ -1,75 +0,0 @@\n-\"\"\"Prepare the dataset.\"\"\"\n-\n-import os\n-from typing import List, Tuple\n-\n-import pandas as pd\n-from omegaconf import DictConfig\n-from pandas import DataFrame\n-from sklearn.model_selection import train_test_split\n-\n-\n-def get_split_dataset(\n-    path_to_dataset: str, include_testset: DictConfig\n-) -> Tuple[DataFrame, DataFrame]:\n-    \"\"\"Load and split datsaet to train (client) and test (server).\"\"\"\n-    script_dir = os.path.dirname(os.path.abspath(__file__))\n-    dataset_path = os.path.join(script_dir, path_to_dataset, \"dataset.csv\")\n-    dataset = pd.read_csv(dataset_path)\n-\n-    # remove NaN\n-    dataset = dataset.dropna(axis=0, how=\"any\")\n-\n-    # keep label\n-    Y = dataset[[\"type\"]]\n-\n-    # remove irrelevant features\n-    dataset = dataset.drop([\"PID\", \"CMD\", \"label\"], axis=1)\n-\n-    # stratified split\n-    if include_testset.flag:\n-        trainset, testset, _, _ = train_test_split(\n-            dataset, Y, test_size=include_testset.ratio, stratify=Y, random_state=41\n-        )\n-        res = trainset, testset\n-    else:\n-        # if include_test is false, return an empty DataFrame for the testset\n-        res = dataset, pd.DataFrame()\n-\n-    return res\n-\n-\n-def split_clients_data(\n-    X: DataFrame, Y: DataFrame, num_partitions: int\n-) -> List[DataFrame]:\n-    \"\"\"Perform stratified split based on labels.\"\"\"\n-    x_curr = X\n-    y_curr = Y\n-    partition_dataset = []\n-    for _ in range(num_partitions - 1):\n-        x_curr, x_temp, y_curr, _ = train_test_split(\n-            x_curr,\n-            y_curr,\n-            test_size=1 / num_partitions,\n-            stratify=y_curr,\n-            random_state=41,\n-        )\n-        partition_dataset.append(x_temp)\n-        num_partitions = num_partitions - 1\n-    partition_dataset.append(x_curr)\n-    return partition_dataset\n-\n-\n-def prepare_dataset(\n-    num_partitions: int, path_to_dataset: str, include_testset: DictConfig\n-) -> Tuple[DataFrame, List[DataFrame]]:\n-    \"\"\"Create the following partitions.\n-\n-    train_partitions: trainsets for clients.\n-    testset: testset for server (if server-side evaluation is needed).\n-    \"\"\"\n-    trainset, testset = get_split_dataset(path_to_dataset, include_testset)\n-    y_trainset = trainset[\"type\"]\n-    train_partitions = split_clients_data(trainset, y_trainset, num_partitions)\n-\n-    return train_partitions, testset\n"
        },
        {
            "commit": "424e2bec035589620a6ae25f51d4f389adc3a12e",
            "file_path": "baselines/statavg/statavg/main.py",
            "diff": "diff --git a/baselines/statavg/statavg/main.py b/baselines/statavg/statavg/main.py\ndeleted file mode 100644\nindex 147b08104..000000000\n--- a/baselines/statavg/statavg/main.py\n+++ /dev/null\n@@ -1,96 +0,0 @@\n-\"\"\"Run the simulation.\"\"\"\n-\n-import os\n-import pickle\n-import shutil\n-from pathlib import Path\n-\n-import flwr as fl\n-import hydra\n-from flwr.simulation.ray_transport.utils import enable_tf_gpu_growth\n-from hydra.core.hydra_config import HydraConfig\n-from hydra.utils import instantiate\n-from omegaconf import DictConfig, OmegaConf\n-\n-from .client import get_client_fn\n-from .dataset_preparation import prepare_dataset\n-from .server import get_evaluate_fn, get_on_fit_config_fn\n-from .utils import plot_accuracy\n-\n-# Optional: Force TensorFlow to use the CPU only\n-# tf.config.set_visible_devices([], 'GPU')\n-\n-# Optional: Make TensorFlow logs less verbose\n-os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n-\n-# If a GPU is used, enable GPU growth for TensorFlow\n-enable_tf_gpu_growth()\n-\n-\n-@hydra.main(config_path=\"conf\", config_name=\"base\", version_base=None)\n-def main(cfg: DictConfig):\n-    \"\"\"Define standards for simulation.\"\"\"\n-    # 1. Print parsed config\n-    print(OmegaConf.to_yaml(cfg))\n-\n-    # Hydra automatically creates an output directory\n-    # Let's retrieve it and save some results there\n-    save_path = Path(HydraConfig.get().runtime.output_dir)\n-\n-    # 2. Prepare your dataset\n-    trainset, testset = prepare_dataset(\n-        cfg.num_clients, cfg.path_to_dataset, cfg.include_testset\n-    )\n-\n-    # 3. Define your clients\n-    client_fn = get_client_fn(\n-        trainset,\n-        cfg.scaler_save_path,\n-        cfg.val_ratio,\n-        cfg.strategy_name,\n-        cfg.learning_rate,\n-    )\n-\n-    # 4. Define your strategy\n-    strategy = instantiate(\n-        cfg.strategy,\n-        on_fit_config_fn=get_on_fit_config_fn(cfg.config_fit),\n-        evaluate_fn=get_evaluate_fn(\n-            testset, cfg.input_shape, cfg.num_classes, cfg.scaler_save_path\n-        ),\n-    )\n-\n-    # 5. Start Simulation\n-\n-    history = fl.simulation.start_simulation(\n-        client_fn=client_fn,\n-        num_clients=cfg.num_clients,\n-        config=fl.server.ServerConfig(cfg.num_rounds),\n-        strategy=strategy,\n-        client_resources={\n-            \"num_cpus\": cfg.client_resources.num_cpus,\n-            \"num_gpus\": cfg.client_resources.num_gpus,\n-        },\n-        ray_init_args={},\n-        actor_kwargs={\"on_actor_init_fn\": enable_tf_gpu_growth},\n-    )\n-\n-    # 6. Save your results\n-    results = {\"history\": history}\n-    results_path = f\"{str(save_path)}/results.pickle\"\n-    with open(results_path, \"wb\") as file:\n-        pickle.dump(results, file)\n-\n-    # Plot averaged accuracy\n-    plot_accuracy(results_path)\n-\n-    # (Optional): Delete scalers directory for future experiments\n-    script_dir = os.path.dirname(os.path.abspath(__file__))\n-    scaler_save_path = os.path.join(script_dir, cfg.scaler_save_path)\n-    if cfg.delete_scaler_dir:\n-        if os.path.exists(scaler_save_path):\n-            shutil.rmtree(scaler_save_path)\n-\n-\n-if __name__ == \"__main__\":\n-    main()\n"
        },
        {
            "commit": "424e2bec035589620a6ae25f51d4f389adc3a12e",
            "file_path": "baselines/statavg/statavg/model.py",
            "diff": "diff --git a/baselines/statavg/statavg/model.py b/baselines/statavg/statavg/model.py\nnew file mode 100644\nindex 000000000..64fa54a76\n--- /dev/null\n+++ b/baselines/statavg/statavg/model.py\n@@ -0,0 +1,23 @@\n+\"\"\"statavg: A Flower Baseline.\"\"\"\n+\n+import tensorflow as tf\n+\n+\n+def get_model(input_shape: int, num_classes: int):\n+    \"\"\"Return the model.\"\"\"\n+    # creates a model with the given input shape and output: num_classes\n+    model = tf.keras.Sequential(\n+        [\n+            tf.keras.layers.Dense(128, activation=\"relu\", input_dim=input_shape),\n+            tf.keras.layers.Dense(128, activation=\"relu\"),\n+            tf.keras.layers.Dense(128, activation=\"relu\"),\n+            tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n+        ]\n+    )\n+    opt = tf.keras.optimizers.Adam(\n+        learning_rate=0.002, beta_1=0.99, beta_2=0.999, epsilon=1e-08\n+    )\n+    model.compile(\n+        loss=\"sparse_categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"]\n+    )\n+    return model\n"
        },
        {
            "commit": "424e2bec035589620a6ae25f51d4f389adc3a12e",
            "file_path": "baselines/statavg/statavg/server.py",
            "diff": "diff --git a/baselines/statavg/statavg/server.py b/baselines/statavg/statavg/server.py\nindex 82ea9df59..fc2065496 100644\n--- a/baselines/statavg/statavg/server.py\n+++ b/baselines/statavg/statavg/server.py\n@@ -1,64 +1,44 @@\n-\"\"\"Flower Server.\"\"\"\n+\"\"\"statavg: A Flower Baseline.\"\"\"\n \n import os\n-\n-import joblib\n-from omegaconf import DictConfig\n-from pandas import DataFrame\n-from sklearn.preprocessing import LabelEncoder, StandardScaler\n-\n-from .models import get_model\n-\n-\n-def get_on_fit_config_fn(conf: DictConfig):\n-    \"\"\"Return fit_config_fn used in strategy.\"\"\"\n-\n-    def fit_config_fn(server_round: int):\n-        \"\"\"Return the server's config file.\"\"\"\n-        config = {\n-            \"batch_size\": conf.batch_size,\n-            \"current_round\": server_round,\n-            \"local_epochs\": conf.local_epochs,\n-            \"total_rounds\": conf.total_rounds,\n-        }\n-\n-        return config\n-\n-    return fit_config_fn\n-\n-\n-def get_evaluate_fn(\n-    testset: DataFrame, input_shape: int, num_classes: int, scaler_path: str\n-):\n-    \"\"\"Return evaluate_fn used in strategy.\"\"\"\n-\n-    def evalaute_fn(server_round, parameters, config):\n-        \"\"\"Evaluate the test set (if provided).\"\"\"\n-        _, _ = server_round, config\n-\n-        if testset.empty:\n-            # this implies that testset is not used\n-            # and thus, included_testset from config file is False\n-            return None, {\"accuracy\": None}\n-\n-        y_test = testset[[\"type\"]]\n-        enc_y = LabelEncoder()\n-        y_test = enc_y.fit_transform(y_test.to_numpy().reshape(-1))\n-        x_test = testset.drop([\"type\"], axis=1).to_numpy()\n-\n-        # normalization\n-        # Check if the directory of the scaler exists and pick a scaler\n-        # of an arbitrary user. It's the same for all users.\n-        if not os.path.exists(scaler_path):\n-            scaler = StandardScaler()\n-            x_test = scaler.fit_transform(x_test)\n-        else:\n-            scaler = joblib.load(f\"{scaler_path}/client_1/scaler.joblib\")\n-            x_test = scaler.transform(x_test)\n-\n-        model = get_model(input_shape, num_classes)\n-        model.set_weights(parameters)\n-        loss, accuracy = model.evaluate(x_test, y_test)\n-        return loss, {\"accuracy\": accuracy}\n-\n-    return evalaute_fn\n+import pickle\n+from logging import INFO\n+from pathlib import Path\n+\n+from flwr.common import log\n+from flwr.server import Server\n+\n+PROJECT_DIR = Path(os.path.abspath(__file__)).parent.parent\n+\n+\n+class ResultsSaverServer(Server):\n+    \"\"\"Server to save history to disk.\"\"\"\n+\n+    def __init__(\n+        self,\n+        *,\n+        client_manager,\n+        strategy=None,\n+        results_saver_fn=None,\n+        run_config=None,\n+    ):\n+        super().__init__(client_manager=client_manager, strategy=strategy)\n+        self.results_saver_fn = results_saver_fn\n+        self.run_config = run_config\n+\n+    def fit(self, num_rounds, timeout):\n+        \"\"\"Run federated averaging for a number of rounds.\"\"\"\n+        history, elapsed = super().fit(num_rounds, timeout)\n+        if self.results_saver_fn:\n+            log(INFO, \"Results saver function provided. Executing\")\n+            self.results_saver_fn(history, self.run_config)\n+        return history, elapsed\n+\n+\n+def save_results_and_clean_dir(history, run_config):\n+    \"\"\"Save history and clean scaler dir.\"\"\"\n+    results = {\"history\": history}\n+    results_path = PROJECT_DIR / run_config.results_save_dir / run_config.strategy_name\n+    results_path.mkdir(exist_ok=True, parents=True)\n+    with open(results_path / \"results.pickle\", \"wb\") as file:\n+        pickle.dump(results, file)\n"
        },
        {
            "commit": "424e2bec035589620a6ae25f51d4f389adc3a12e",
            "file_path": "baselines/statavg/statavg/server_app.py",
            "diff": "diff --git a/baselines/statavg/statavg/server_app.py b/baselines/statavg/statavg/server_app.py\nnew file mode 100644\nindex 000000000..6da7c0d7f\n--- /dev/null\n+++ b/baselines/statavg/statavg/server_app.py\n@@ -0,0 +1,87 @@\n+\"\"\"statavg: A Flower Baseline.\"\"\"\n+\n+from omegaconf import DictConfig, OmegaConf\n+from sklearn.preprocessing import LabelEncoder, StandardScaler\n+\n+from flwr.common import Context\n+from flwr.server import (\n+    ServerApp,\n+    ServerAppComponents,\n+    ServerConfig,\n+    SimpleClientManager,\n+)\n+from statavg.dataset import prepare_dataset\n+from statavg.model import get_model\n+from statavg.server import ResultsSaverServer, save_results_and_clean_dir\n+from statavg.strategy import define_server_strategy\n+\n+\n+def get_evaluate_fn(cfg: DictConfig):\n+    \"\"\"Return evaluate_fn used in strategy.\"\"\"\n+    _, testset = prepare_dataset(\n+        cfg.num_clients, cfg.path_to_dataset, cfg.include_test, cfg.testset_ratio\n+    )\n+\n+    def evalaute_fn(server_round, parameters, scaler):\n+        \"\"\"Evaluate the test set (if provided).\"\"\"\n+        _ = server_round\n+        if testset.empty:\n+            # this implies that testset is not used\n+            # and thus, included_testset from config file is False\n+            return None, {\"accuracy\": None}\n+\n+        y_test = testset[[\"type\"]]\n+        enc_y = LabelEncoder()\n+        y_test = enc_y.fit_transform(y_test.to_numpy().reshape(-1))\n+        x_test = testset.drop([\"type\"], axis=1).to_numpy()\n+\n+        # normalization\n+        # Check if the directory of the scaler exists and pick a scaler\n+        # of an arbitrary user. It's the same for all users.\n+        if scaler:\n+            scaler = scaler[1]\n+            x_test = scaler.transform(x_test)\n+        else:\n+            scaler = StandardScaler()\n+            x_test = scaler.fit_transform(x_test)\n+\n+        model = get_model(cfg.input_shape, cfg.num_classes)\n+        model.set_weights(parameters)\n+        loss, accuracy = model.evaluate(x_test, y_test)\n+        return loss, {\"accuracy\": accuracy}\n+\n+    return evalaute_fn\n+\n+\n+def server_fn(context: Context):\n+    \"\"\"Define standards for simulation.\"\"\"\n+    cfg = OmegaConf.create(context.run_config)\n+    print(cfg)\n+    strategy_class = define_server_strategy(cfg)\n+\n+    def fit_config_fn(server_round: int):\n+        \"\"\"Return the server's config file.\"\"\"\n+        config = {\n+            \"current_round\": server_round,\n+        }\n+\n+        return config\n+\n+    strategy = strategy_class(\n+        min_fit_clients=cfg.num_clients,\n+        on_fit_config_fn=fit_config_fn,\n+        evaluate_fn=get_evaluate_fn(cfg),\n+    )\n+    config = ServerConfig(num_rounds=int(cfg.num_server_rounds))\n+    client_manager = SimpleClientManager()\n+    server = ResultsSaverServer(\n+        strategy=strategy,\n+        client_manager=client_manager,\n+        results_saver_fn=save_results_and_clean_dir,\n+        run_config=cfg,\n+    )\n+    return ServerAppComponents(server=server, config=config)\n+\n+\n+# Create ServerApp\n+app = ServerApp(server_fn=server_fn)\n"
        },
        {
            "commit": "424e2bec035589620a6ae25f51d4f389adc3a12e",
            "file_path": "baselines/statavg/statavg/strategy.py",
            "diff": "diff --git a/baselines/statavg/statavg/strategy.py b/baselines/statavg/statavg/strategy.py\nindex cd1aabf8a..41a8bccc8 100644\n--- a/baselines/statavg/statavg/strategy.py\n+++ b/baselines/statavg/statavg/strategy.py\n@@ -1,17 +1,48 @@\n-\"\"\"StatAvg strategy.\"\"\"\n+\"\"\"statavg: A Flower Baseline.\"\"\"\n \n import json\n+import pickle\n from typing import Dict, List, Optional, Tuple, Union\n \n-import flwr as fl\n import numpy as np\n-from flwr.common import FitIns, FitRes, Parameters, Scalar\n+\n+from flwr.common import FitIns, FitRes, Parameters, Scalar, parameters_to_ndarrays\n from flwr.server.client_manager import ClientManager\n from flwr.server.client_proxy import ClientProxy\n+from flwr.server.strategy import FedAvg\n+\n+\n+# Class for implementing FedAvg including the method aggregate_evaluate\n+class FedAvgAggrEv(FedAvg):\n+    \"\"\"FedAvg with aggregate_evaluate.\"\"\"\n+\n+    def aggregate_evaluate(\n+        self,\n+        server_round: int,\n+        results,\n+        failures,\n+    ):\n+        \"\"\"Calculate a weighted average of the clients' accuracy.\"\"\"\n+        if not results:\n+            return None, {}\n+\n+        # Call aggregate_evaluate from base class (FedAvg) to aggregate loss and metrics\n+        aggregated_loss, _ = super().aggregate_evaluate(server_round, results, failures)\n+\n+        # Weigh accuracy of each client by number of examples used\n+        accuracies = [r.metrics[\"accuracy\"] * r.num_examples for _, r in results]\n+        examples = [r.num_examples for _, r in results]\n+\n+        # Aggregate and print custom metric\n+        aggregated_accuracy = sum(accuracies) / sum(examples)\n+        print(f\"Round {server_round}, aggr. client accuracy: {aggregated_accuracy}\")\n+\n+        # Return aggregated loss and metrics (i.e., aggregated accuracy)\n+        return aggregated_loss, {\"accuracy\": aggregated_accuracy}\n \n \n # Custom class for implementing StatAvg (inherits properties from FedAvg)\n-class CustomStatAvg(fl.server.strategy.FedAvg):\n+class CustomStatAvg(FedAvgAggrEv):\n     \"\"\"StatAvg.\n \n     The server receives the client local statistics. only at the 1st round, and\n@@ -26,6 +57,8 @@ class CustomStatAvg(fl.server.strategy.FedAvg):\n         # Define the attribute metrics_aggregated and initialize with arbitrary values\n         # this will carry the global aggregated statistics\n         self.metrics_aggregated = {\"initialization\": 0}\n+        self.fit_metrics_aggregation_fn = CustomStatAvg.get_average_statistics\n+        self.client_scalars = None\n \n     def aggregate_fit(\n         self,\n@@ -35,13 +68,20 @@ class CustomStatAvg(fl.server.strategy.FedAvg):\n     ) -> Tuple[Optional[Parameters], Dict[str, Scalar]]:\n         \"\"\"Aggregate both parameters and local statistics.\"\"\"\n         # thats to prevent aggregate_fit from making aggregations\n-        if server_round > 1:\n+        if server_round == 2:\n+            self.fit_metrics_aggregation_fn = CustomStatAvg.get_client_scalers\n+        elif server_round > 2:\n             self.fit_metrics_aggregation_fn = None\n \n-        parameters_aggregated, self.metrics_aggregated = super().aggregate_fit(\n+        parameters_aggregated, metrics_aggregated = super().aggregate_fit(\n             server_round, results, failures\n         )\n-\n+        if isinstance(metrics_aggregated, list):\n+            # In the second round, we return client scalers.\n+            self.client_scalars = metrics_aggregated\n+            self.metrics_aggregated = {}\n+        else:\n+            self.metrics_aggregated = metrics_aggregated\n         return parameters_aggregated, self.metrics_aggregated\n \n     def configure_fit(\n@@ -78,33 +118,24 @@ class CustomStatAvg(fl.server.strategy.FedAvg):\n         # Return client/config pairs\n         return [(client, fit_ins) for client in clients]\n \n-    def aggregate_evaluate(\n-        self,\n-        server_round: int,\n-        results,\n-        failures,\n-    ):\n-        \"\"\"Calculate a weighted average of the clients' accuracy.\"\"\"\n-        if not results:\n-            return None, {}\n-\n-        # Call aggregate_evaluate from base class (FedAvg) to aggregate loss and metrics\n-        aggregated_loss, _ = super().aggregate_evaluate(server_round, results, failures)\n-\n-        # Weigh accuracy of each client by number of examples used\n-        accuracies = [r.metrics[\"accuracy\"] * r.num_examples for _, r in results]\n-        examples = [r.num_examples for _, r in results]\n-\n-        # Aggregate and print custom metric\n-        aggregated_accuracy = sum(accuracies) / sum(examples)\n-        print(f\"Round {server_round}, aggr. client accuracy: {aggregated_accuracy}\")\n-\n-        # Return aggregated loss and metrics (i.e., aggregated accuracy)\n-        return aggregated_loss, {\"accuracy\": aggregated_accuracy}\n+    def evaluate(self, server_round, parameters):\n+        \"\"\"Evaluate function with client scalers.\"\"\"\n+        if self.evaluate_fn is None:\n+            # No evaluation function provided\n+            return None\n+        parameters_ndarrays = parameters_to_ndarrays(parameters)\n+        # Replace empty config dict with client scalars.\n+        eval_res = self.evaluate_fn(\n+            server_round, parameters_ndarrays, self.client_scalars\n+        )\n+        if eval_res is None:\n+            return None\n+        loss, metrics = eval_res\n+        return loss, metrics\n \n     @staticmethod\n     def get_average_statistics(\n-        fit_metrics: List[Tuple[int, Dict[str, int]]]\n+        fit_metrics: List[Tuple[int, Dict[str, int]]],\n     ) -> Dict[str, int]:\n         \"\"\"Return the aggregated metrics.\n \n@@ -146,31 +177,22 @@ class CustomStatAvg(fl.server.strategy.FedAvg):\n \n         return metrics_global\n \n-\n-# Class for implementing FedAvg including the method aggregate_evaluate\n-class FedAvgAggrEv(fl.server.strategy.FedAvg):\n-    \"\"\"FedAvg with aggregate_evaluate.\"\"\"\n-\n-    def aggregate_evaluate(\n-        self,\n-        server_round: int,\n-        results,\n-        failures,\n+    @staticmethod\n+    def get_client_scalers(\n+        fit_metrics,\n     ):\n-        \"\"\"Calculate a weighted average of the clients' accuracy.\"\"\"\n-        if not results:\n-            return None, {}\n-\n-        # Call aggregate_evaluate from base class (FedAvg) to aggregate loss and metrics\n-        aggregated_loss, _ = super().aggregate_evaluate(server_round, results, failures)\n-\n-        # Weigh accuracy of each client by number of examples used\n-        accuracies = [r.metrics[\"accuracy\"] * r.num_examples for _, r in results]\n-        examples = [r.num_examples for _, r in results]\n-\n-        # Aggregate and print custom metric\n-        aggregated_accuracy = sum(accuracies) / sum(examples)\n-        print(f\"Round {server_round}, aggr. client accuracy: {aggregated_accuracy}\")\n-\n-        # Return aggregated loss and metrics (i.e., aggregated accuracy)\n-        return aggregated_loss, {\"accuracy\": aggregated_accuracy}\n+        \"\"\"Extract scalers from metrics.\"\"\"\n+        client_scalars = [\n+            pickle.loads(client_objs[\"scaler\"]) for _, client_objs in fit_metrics\n+        ]\n+        return client_scalars\n+\n+\n+def define_server_strategy(config):\n+    \"\"\"Define the strategy to be used by the simulation.\"\"\"\n+    class_to_return = None\n+    if config.strategy_name.lower() == \"fedavg\":\n+        class_to_return = FedAvgAggrEv\n+    else:\n+        class_to_return = CustomStatAvg\n+    return class_to_return\n"
        },
        {
            "commit": "424e2bec035589620a6ae25f51d4f389adc3a12e",
            "file_path": "baselines/statavg/statavg/utils.py",
            "diff": "diff --git a/baselines/statavg/statavg/utils.py b/baselines/statavg/statavg/utils.py\nindex fc4767066..08f3c5de7 100644\n--- a/baselines/statavg/statavg/utils.py\n+++ b/baselines/statavg/statavg/utils.py\n@@ -1,67 +1,44 @@\n-\"\"\"Contains utility functions.\"\"\"\n-\n-import pickle\n-\n-import matplotlib.pyplot as plt\n-import numpy as np\n-from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score\n-\n-\n-def evaluation_metrics(y_true, classes, predicted_test):\n-    \"\"\"Not used in the current implementation.\n-\n-    Auxiliary for generating additional results if needed.\n-    \"\"\"\n-    # Accuracy\n-    accuracy = accuracy_score(y_true, classes)\n-    print(\"Accuracy: %f\" % accuracy)\n-\n-    cnf_matrix = confusion_matrix(y_true, classes)\n-\n-    false_p = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)\n-    false_n = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n-    true_p = np.diag(cnf_matrix)\n-    true_n = cnf_matrix.sum() - (false_p + false_n + true_p)\n-\n-    false_p = false_p.astype(float)\n-    false_n = false_n.astype(float)\n-    true_p = true_p.astype(float)\n-    true_n = true_n.astype(float)\n-\n-    # true positive rate - TPR\n-    tpr = true_p / (true_p + false_n)\n-    print(\"TPR: \", np.mean(tpr))\n-\n-    # false positive rate - FPR\n-    fpr = false_p / (false_p + true_n)\n-    print(\"FPR: \", np.mean(fpr))\n-\n-    # F1 Score\n-    fsc = f1_score(y_true, classes, average=\"weighted\")\n-    print(\"F1 score: %f\" % fsc)\n-\n-    auc = roc_auc_score(y_true, predicted_test, multi_class=\"ovr\")\n-    print(\"AUC Score: %f\" % auc)\n-    eval_metrics = (accuracy, fsc)\n-\n-    return eval_metrics\n-\n-\n-def plot_accuracy(results_path: str) -> None:\n-    \"\"\"Plot the accuracy.\"\"\"\n-    with open(results_path, \"rb\") as file:\n-        results = pickle.load(file)\n-\n-    accuracy_dict = results[\"history\"].metrics_distributed\n-    accuracy_lst = accuracy_dict[\"accuracy\"]\n-\n-    rounds = [p[0] for p in accuracy_lst]\n-    acc = [p[1] for p in accuracy_lst]\n-\n-    plt.plot(rounds, acc, marker=\"o\", linestyle=\"-\")\n-\n-    plt.xlabel(\"Rounds\")\n-    plt.ylabel(\"Testing Accuracy\")\n-\n-    plt.grid(True)\n-    plt.show()\n+\"\"\"statavg: A Flower Baseline.\"\"\"\n+\n+import numpy as np\n+from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score\n+\n+\n+def evaluation_metrics(y_true, classes, predicted_test):\n+    \"\"\"Not used in the current implementation.\n+\n+    Auxiliary for generating additional results if needed.\n+    \"\"\"\n+    # Accuracy\n+    accuracy = accuracy_score(y_true, classes)\n+    print(f\"Accuracy: {accuracy}\")\n+\n+    cnf_matrix = confusion_matrix(y_true, classes)\n+\n+    false_p = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)\n+    false_n = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n+    true_p = np.diag(cnf_matrix)\n+    true_n = cnf_matrix.sum() - (false_p + false_n + true_p)\n+\n+    false_p = false_p.astype(float)\n+    false_n = false_n.astype(float)\n+    true_p = true_p.astype(float)\n+    true_n = true_n.astype(float)\n+\n+    # true positive rate - TPR\n+    tpr = true_p / (true_p + false_n)\n+    print(\"TPR: \", np.mean(tpr))\n+\n+    # false positive rate - FPR\n+    fpr = false_p / (false_p + true_n)\n+    print(\"FPR: \", np.mean(fpr))\n+\n+    # F1 Score\n+    fsc = f1_score(y_true, classes, average=\"weighted\")\n+    print(f\"F1 score: {fsc}\")\n+\n+    auc = roc_auc_score(y_true, predicted_test, multi_class=\"ovr\")\n+    print(f\"AUC Score: {auc}\")\n+    eval_metrics = (accuracy, fsc)\n+\n+    return eval_metrics\n"
        }
    ]
}