{
    "sha_fail": "4410203c56984c613d23f29a81ecd1b96c57b1ee",
    "changed_files": [
        {
            "commit": "4410203c56984c613d23f29a81ecd1b96c57b1ee",
            "file_path": "composer/callbacks/checkpoint_saver.py",
            "diff": "diff --git a/composer/callbacks/checkpoint_saver.py b/composer/callbacks/checkpoint_saver.py\nindex 34acb04..c876343 100644\n--- a/composer/callbacks/checkpoint_saver.py\n+++ b/composer/callbacks/checkpoint_saver.py\n@@ -382,7 +382,12 @@ class CheckpointSaver(Callback):  # noqa: D101\n                 ).lstrip('/')\n \n             log.debug(f'Uploading checkpoint to {remote_file_name}')\n-            logger.upload_file(remote_file_name=remote_file_name, file_path=saved_path, overwrite=self.overwrite)\n+            try:\n+                logger.upload_file(remote_file_name=remote_file_name, file_path=saved_path, overwrite=self.overwrite)\n+            except FileExistsError as e:\n+                raise FileExistsError(\n+                    f'Uploading checkpoint failed with error: {e}. overwrite was set to {self.overwrite}. To overwrite checkpoints with Trainer, set save_overwrite to True.'\n+                ) from e\n \n             # symlinks stay the same with sharded checkpointing\n             if self.latest_remote_file_name is not None:\n"
        },
        {
            "commit": "4410203c56984c613d23f29a81ecd1b96c57b1ee",
            "file_path": "composer/loggers/mlflow_logger.py",
            "diff": "diff --git a/composer/loggers/mlflow_logger.py b/composer/loggers/mlflow_logger.py\nindex a586a72..c3a2051 100644\n--- a/composer/loggers/mlflow_logger.py\n+++ b/composer/loggers/mlflow_logger.py\n@@ -36,6 +36,7 @@ class MLFlowLogger(LoggerDestination):\n             use the MLflow environment variable or a default value\n         run_name: (str, optional): MLflow run name. If not set it will be the same as the\n             Trainer run name\n+        tags: (dict, optional): MLflow tags to log with the run\n         tracking_uri (str | pathlib.Path, optional): MLflow tracking uri, the URI to the\n             remote or local endpoint where logs are stored (If none it is set to MLflow default)\n         rank_zero_only (bool, optional): Whether to log only on the rank-zero process\n@@ -55,6 +56,7 @@ class MLFlowLogger(LoggerDestination):\n         self,\n         experiment_name: Optional[str] = None,\n         run_name: Optional[str] = None,\n+        tags: Optional[Dict[str, Any]] = None,\n         tracking_uri: Optional[Union[str, pathlib.Path]] = None,\n         rank_zero_only: bool = True,\n         flush_interval: int = 10,\n@@ -71,8 +73,9 @@ class MLFlowLogger(LoggerDestination):\n                                                 conda_channel='conda-forge') from e\n         self._enabled = (not rank_zero_only) or dist.get_global_rank() == 0\n \n-        self.run_name = run_name\n         self.experiment_name = experiment_name\n+        self.run_name = run_name\n+        self.tags = tags\n         self.model_registry_prefix = model_registry_prefix\n         self.model_registry_uri = model_registry_uri\n         if self.model_registry_uri == 'databricks-uc':\n@@ -133,7 +136,7 @@ class MLFlowLogger(LoggerDestination):\n                     run_name=self.run_name,\n                 )\n                 self._run_id = new_run.info.run_id\n-            mlflow.start_run(run_id=self._run_id)\n+            mlflow.start_run(run_id=self._run_id, tags=self.tags)\n \n     def log_table(self, columns: List[str], rows: List[List[Any]], name: str = 'Table') -> None:\n         if self._enabled:\n"
        },
        {
            "commit": "4410203c56984c613d23f29a81ecd1b96c57b1ee",
            "file_path": "composer/loggers/mosaicml_logger.py",
            "diff": "diff --git a/composer/loggers/mosaicml_logger.py b/composer/loggers/mosaicml_logger.py\nindex d14b3d8..bcdc3ce 100644\n--- a/composer/loggers/mosaicml_logger.py\n+++ b/composer/loggers/mosaicml_logger.py\n@@ -7,6 +7,7 @@ from __future__ import annotations\n \n import collections.abc\n import fnmatch\n+import json\n import logging\n import operator\n import os\n@@ -117,6 +118,8 @@ class MosaicMLLogger(LoggerDestination):\n         self._flush_metadata()\n \n     def fit_end(self, state: State, logger: Logger) -> None:\n+        # Log model training finished time for run events\n+        self._log_metadata({'train_finished_time': time.time()})\n         training_progress_data = self._get_training_progress_metrics(state)\n         log.debug(f'\\nLogging FINAL training progress data to metadata:\\n{dict_to_str(training_progress_data)}')\n         self._log_metadata(training_progress_data)\n@@ -218,24 +221,26 @@ def format_data_to_json_serializable(data: Any):\n         str: ``data`` as a string.\n     \"\"\"\n     try:\n+        ret = None\n         if data is None:\n-            return 'None'\n-        if type(data) in (str, int, float, bool):\n-            return data\n-        if isinstance(data, torch.Tensor):\n+            ret = 'None'\n+        elif type(data) in (str, int, float, bool):\n+            ret = data\n+        elif isinstance(data, torch.Tensor):\n             if data.shape == () or reduce(operator.mul, data.shape, 1) == 1:\n-                return format_data_to_json_serializable(data.cpu().item())\n-            return 'Tensor of shape ' + str(data.shape)\n-        if isinstance(data, collections.abc.Mapping):\n-            return {format_data_to_json_serializable(k): format_data_to_json_serializable(v) for k, v in data.items()}\n-        if isinstance(data, collections.abc.Iterable):\n-            return [format_data_to_json_serializable(v) for v in data]\n-\n-        # Unknown format catch-all\n-        return str(data)\n+                ret = format_data_to_json_serializable(data.cpu().item())\n+            ret = 'Tensor of shape ' + str(data.shape)\n+        elif isinstance(data, collections.abc.Mapping):\n+            ret = {format_data_to_json_serializable(k): format_data_to_json_serializable(v) for k, v in data.items()}\n+        elif isinstance(data, collections.abc.Iterable):\n+            ret = [format_data_to_json_serializable(v) for v in data]\n+        else:  # Unknown format catch-all\n+            ret = str(data)\n+        json.dumps(ret)  # Check if ret is JSON serializable\n+        return ret\n     except RuntimeError as e:\n-        warnings.warn('Encountered unexpected error while formatting data to be JSON serializable. '\n-                      f'Returning empty string instead. Error: {str(e)}')\n+        warnings.warn(f'Encountered unexpected error while formatting data of type {type(data)} to '\n+                      f'be JSON serializable. Returning empty string instead. Error: {str(e)}')\n         return ''\n \n \n"
        },
        {
            "commit": "4410203c56984c613d23f29a81ecd1b96c57b1ee",
            "file_path": "composer/loggers/remote_uploader_downloader.py",
            "diff": "diff --git a/composer/loggers/remote_uploader_downloader.py b/composer/loggers/remote_uploader_downloader.py\nindex 0319e29..a3f9698 100644\n--- a/composer/loggers/remote_uploader_downloader.py\n+++ b/composer/loggers/remote_uploader_downloader.py\n@@ -356,7 +356,7 @@ class RemoteUploaderDownloader(LoggerDestination):\n         # Periodically check to see if any of the upload workers crashed\n         # They would crash if:\n         #   a) A file could not be uploaded, and the retry counter failed, or\n-        #   b) allow_overwrite=False, but the file already exists,\n+        #   b) overwrite=False, but the file already exists,\n         if not self._all_workers_alive:\n             if not self._exception_queue.empty():\n                 exception = self._exception_queue.get_nowait()\n@@ -615,7 +615,7 @@ def _upload_worker(\n                     pass\n                 else:\n                     # Exceptions will be detected on the next batch_end or epoch_end event\n-                    e = FileExistsError(f'Object {uri} already exists, but allow_overwrite was set to False.')\n+                    e = FileExistsError(f'Object {uri} already exists, but overwrite was set to False.')\n                     exception_queue.put_nowait(e)\n                     raise e\n             log.info('Uploading file %s to %s', file_path_to_upload, uri)\n"
        },
        {
            "commit": "4410203c56984c613d23f29a81ecd1b96c57b1ee",
            "file_path": "composer/profiler/torch_profiler.py",
            "diff": "diff --git a/composer/profiler/torch_profiler.py b/composer/profiler/torch_profiler.py\nindex bfff9c6..dc33d82 100644\n--- a/composer/profiler/torch_profiler.py\n+++ b/composer/profiler/torch_profiler.py\n@@ -256,9 +256,11 @@ class TorchProfiler(Callback):  # noqa: D101\n         del state, logger  # unused\n         if self.profiler is not None:\n             log.info(self.profiler.key_averages().table(sort_by='cpu_time_total', row_limit=20))\n-            log.info(self.profiler.key_averages().table(sort_by='self_cpu_memory_usage', row_limit=20))\n+            if self.profile_memory:\n+                log.info(self.profiler.key_averages().table(sort_by='self_cpu_memory_usage', row_limit=20))\n             if torch.profiler.ProfilerActivity.CUDA in self.profiler.activities:\n                 log.info(self.profiler.key_averages().table(sort_by='cuda_time_total', row_limit=20))\n-                log.info(self.profiler.key_averages().table(sort_by='self_cuda_memory_usage', row_limit=20))\n+                if self.profile_memory:\n+                    log.info(self.profiler.key_averages().table(sort_by='self_cuda_memory_usage', row_limit=20))\n             self.profiler.__exit__(None, None, None)\n             self.profiler = None\n"
        },
        {
            "commit": "4410203c56984c613d23f29a81ecd1b96c57b1ee",
            "file_path": "composer/trainer/dist_strategy.py",
            "diff": "diff --git a/composer/trainer/dist_strategy.py b/composer/trainer/dist_strategy.py\nindex 768089e..70afe26 100644\n--- a/composer/trainer/dist_strategy.py\n+++ b/composer/trainer/dist_strategy.py\n@@ -530,21 +530,40 @@ def prepare_fsdp_module(\n \n             # Activation Checkpointing\n             if activation_checkpointing or activation_cpu_offload:\n-                if not activation_checkpointing_reentrant:\n-                    first_wrap_fn = lambda m: checkpoint_wrapper(m, checkpoint_impl=CheckpointImpl.NO_REENTRANT\n-                                                                ) if activation_checkpointing else (lambda module:\n-                                                                                                    module)\n-                    second_wrap_fn = (\n-                        lambda module: checkpoint_wrapper(\n-                            first_wrap_fn(module),  # type: ignore reportGeneralTypeIssues\n-                            checkpoint_impl=CheckpointImpl.NO_REENTRANT,\n-                            offload_to_cpu=True)) if activation_cpu_offload else first_wrap_fn\n+                if version.parse(torch.__version__) > version.parse('2.1.0.dev'):\n+                    from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import offload_wrapper\n+                    if not activation_checkpointing_reentrant:\n+                        first_wrap_fn = lambda m: checkpoint_wrapper(m, checkpoint_impl=CheckpointImpl.NO_REENTRANT\n+                                                                    ) if activation_checkpointing else (lambda module:\n+                                                                                                        module)\n+                        second_wrap_fn = (\n+                            lambda module: offload_wrapper(\n+                                first_wrap_fn(module)\n+                                if activation_checkpointing else module,  # type: ignore reportGeneralTypeIssues\n+                            )) if activation_cpu_offload else first_wrap_fn\n+                    else:\n+                        first_wrap_fn = checkpoint_wrapper if activation_checkpointing else (lambda module: module)\n+                        second_wrap_fn = (\n+                            lambda module: offload_wrapper(\n+                                first_wrap_fn(module)\n+                                if activation_checkpointing else module)  # type: ignore reportGeneralTypeIssues\n+                        ) if activation_cpu_offload else first_wrap_fn\n                 else:\n-                    first_wrap_fn = checkpoint_wrapper if activation_checkpointing else (lambda module: module)\n-                    second_wrap_fn = (\n-                        lambda module: checkpoint_wrapper(\n-                            first_wrap_fn(module),  # type: ignore reportGeneralTypeIssues\n-                            offload_to_cpu=True)) if activation_cpu_offload else first_wrap_fn\n+                    if not activation_checkpointing_reentrant:\n+                        first_wrap_fn = lambda m: checkpoint_wrapper(m, checkpoint_impl=CheckpointImpl.NO_REENTRANT\n+                                                                    ) if activation_checkpointing else (lambda module:\n+                                                                                                        module)\n+                        second_wrap_fn = (\n+                            lambda module: checkpoint_wrapper(\n+                                first_wrap_fn(module),  # type: ignore reportGeneralTypeIssues\n+                                checkpoint_impl=CheckpointImpl.NO_REENTRANT,\n+                                offload_to_cpu=True)) if activation_cpu_offload else first_wrap_fn\n+                    else:\n+                        first_wrap_fn = checkpoint_wrapper if activation_checkpointing else (lambda module: module)\n+                        second_wrap_fn = (\n+                            lambda module: checkpoint_wrapper(\n+                                first_wrap_fn(module),  # type: ignore reportGeneralTypeIssues\n+                                offload_to_cpu=True)) if activation_cpu_offload else first_wrap_fn\n \n                 # Choose which modules to activation checkpoint according to the following priority:\n                 # If module has attribute `module._activation_checkpointing = ...`, always respect it\n"
        },
        {
            "commit": "4410203c56984c613d23f29a81ecd1b96c57b1ee",
            "file_path": "composer/trainer/trainer.py",
            "diff": "diff --git a/composer/trainer/trainer.py b/composer/trainer/trainer.py\nindex b0fae5b..081fe68 100644\n--- a/composer/trainer/trainer.py\n+++ b/composer/trainer/trainer.py\n@@ -1078,18 +1078,19 @@ class Trainer:\n                 loggers.append(\n                     ConsoleLogger(stream=console_stream, log_interval=console_log_interval, log_traces=log_traces))\n \n-        if save_folder is not None:\n-            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)\n-            if remote_ud is not None:\n-                loggers.append(remote_ud)\n-\n         # MosaicML Logger\n+        # Keep MosaicML logger above the RemoteUploaderDownloader so that fit end is reported before the final checkpoint begins uploading\n         if os.environ.get(MOSAICML_PLATFORM_ENV_VAR, 'false').lower() == 'true' and os.environ.get(\n                 MOSAICML_ACCESS_TOKEN_ENV_VAR) is not None and not any(isinstance(x, MosaicMLLogger) for x in loggers):\n             log.info('Detected run on MosaicML platform. Adding MosaicMLLogger to loggers.')\n             mosaicml_logger = MosaicMLLogger()\n             loggers.append(mosaicml_logger)\n \n+        if save_folder is not None:\n+            remote_ud = maybe_create_remote_uploader_downloader_from_uri(save_folder, loggers)\n+            if remote_ud is not None:\n+                loggers.append(remote_ud)\n+\n         # Logger\n         self.logger = Logger(state=self.state, destinations=loggers)\n \n"
        },
        {
            "commit": "4410203c56984c613d23f29a81ecd1b96c57b1ee",
            "file_path": "composer/utils/object_store/s3_object_store.py",
            "diff": "diff --git a/composer/utils/object_store/s3_object_store.py b/composer/utils/object_store/s3_object_store.py\nindex fec0a91..854d447 100644\n--- a/composer/utils/object_store/s3_object_store.py\n+++ b/composer/utils/object_store/s3_object_store.py\n@@ -135,13 +135,18 @@ class S3ObjectStore(ObjectStore):\n         file_size = os.path.getsize(filename)\n         cb_wrapper = None if callback is None else lambda bytes_transferred: callback(bytes_transferred, file_size)\n \n-        # Validate kwargs\n-        if len(kwargs) != 0:\n+        # Validate kwargs. Use env var for Canned ACL if present and one has not been passed in.\n+        if len(kwargs) == 0:\n+            if 'S3_CANNED_ACL' in os.environ:\n+                kwargs['ExtraArgs'] = {'ACL': os.environ['S3_CANNED_ACL']}\n+        else:\n             if len(kwargs) > 1 or 'ExtraArgs' not in kwargs or not isinstance(kwargs['ExtraArgs'], dict):\n                 raise ValueError('S3ObjectStore.upload_object only supports an additional ExtraArgs dictionary.')\n             for key in kwargs['ExtraArgs']:\n                 if key not in S3Transfer.ALLOWED_UPLOAD_ARGS:\n                     raise ValueError(f'{key} is not an allowed upload argument.')\n+            if 'S3_CANNED_ACL' in os.environ and 'ACL' not in kwargs['ExtraArgs']:\n+                kwargs['ExtraArgs']['ACL'] = os.environ['S3_CANNED_ACL']\n \n         self.client.upload_file(Bucket=self.bucket,\n                                 Key=self.get_key(object_name),\n"
        },
        {
            "commit": "4410203c56984c613d23f29a81ecd1b96c57b1ee",
            "file_path": "docker/Dockerfile",
            "diff": "diff --git a/docker/Dockerfile b/docker/Dockerfile\nindex 70747e0..38633ee 100644\n--- a/docker/Dockerfile\n+++ b/docker/Dockerfile\n@@ -354,6 +354,11 @@ RUN pip install --no-cache-dir --upgrade \\\n         ipython${IPYTHON_VERSION} \\\n         urllib3${URLLIB3_VERSION}\n \n+##################################################\n+# Override NVIDIA mistaken env var for 11.8 images\n+##################################################\n+ARG NVIDIA_REQUIRE_CUDA_OVERRIDE\n+ENV NVIDIA_REQUIRE_CUDA=${NVIDIA_REQUIRE_CUDA_OVERRIDE:-$NVIDIA_REQUIRE_CUDA}\n \n ################\n # Composer Image\n"
        },
        {
            "commit": "4410203c56984c613d23f29a81ecd1b96c57b1ee",
            "file_path": "docker/generate_build_matrix.py",
            "diff": "diff --git a/docker/generate_build_matrix.py b/docker/generate_build_matrix.py\nindex ee22211..627bb14 100644\n--- a/docker/generate_build_matrix.py\n+++ b/docker/generate_build_matrix.py\n@@ -136,6 +136,17 @@ def _main():\n \n         cuda_version = _get_cuda_version(pytorch_version=pytorch_version, use_cuda=use_cuda)\n \n+        override_string = ('cuda>=11.8 brand=tesla,driver>=470,driver<471 '\n+                           'brand=tesla,driver>=515,driver<516 brand=unknown,driver>=470,driver<471 '\n+                           'brand=unknown,driver>=515,driver<516 brand=nvidia,driver>=470,driver<471 '\n+                           'brand=nvidia,driver>=515,driver<516 brand=nvidiartx,driver>=470,driver<471 '\n+                           'brand=nvidiartx,driver>=515,driver<516 brand=geforce,driver>=470,driver<471 '\n+                           'brand=geforce,driver>=515,driver<516 brand=quadro,driver>=470,driver<471 '\n+                           'brand=quadro,driver>=515,driver<516 brand=titan,driver>=470,driver<471 '\n+                           'brand=titan,driver>=515,driver<516 brand=titanrtx,driver>=470,driver<471 '\n+                           'brand=titanrtx,driver>=515,driver<516')\n+        nvidia_require_cuda_override = '' if cuda_version != '11.8.0' else override_string\n+\n         entry = {\n             'IMAGE_NAME':\n                 _get_image_name(pytorch_version, cuda_version, stage, interconnect),\n@@ -163,6 +174,8 @@ def _main():\n                 '',\n             'PYTORCH_NIGHTLY_VERSION':\n                 '',\n+            'NVIDIA_REQUIRE_CUDA_OVERRIDE':\n+                nvidia_require_cuda_override,\n         }\n \n         # Only build EFA image on latest python with cuda on pytorch_stage\n"
        },
        {
            "commit": "4410203c56984c613d23f29a81ecd1b96c57b1ee",
            "file_path": "tests/loggers/test_mosaicml_logger.py",
            "diff": "diff --git a/tests/loggers/test_mosaicml_logger.py b/tests/loggers/test_mosaicml_logger.py\nindex 5461829..c02ae15 100644\n--- a/tests/loggers/test_mosaicml_logger.py\n+++ b/tests/loggers/test_mosaicml_logger.py\n@@ -279,6 +279,7 @@ def test_run_events_logged(monkeypatch):\n     assert 'mosaicml/training_progress' in metadata\n     assert metadata['mosaicml/training_progress'] == '[batch=4/4]'\n     assert 'mosaicml/training_sub_progress' not in metadata\n+    assert isinstance(metadata['mosaicml/train_finished_time'], float)\n \n \n def test_token_training_progress_metrics():\n"
        },
        {
            "commit": "4410203c56984c613d23f29a81ecd1b96c57b1ee",
            "file_path": "tests/loggers/test_remote_uploader_downloader.py",
            "diff": "diff --git a/tests/loggers/test_remote_uploader_downloader.py b/tests/loggers/test_remote_uploader_downloader.py\nindex 5043907..4648a07 100644\n--- a/tests/loggers/test_remote_uploader_downloader.py\n+++ b/tests/loggers/test_remote_uploader_downloader.py\n@@ -125,8 +125,7 @@ def object_store_test_helper(\n                     # the fatal exception that the worker throws.\n                     with pytest.raises(\n                             FileExistsError,\n-                            match=\n-                            f'Object local://{remote_file_name} already exists, but allow_overwrite was set to False.'):\n+                            match=f'Object local://{remote_file_name} already exists, but overwrite was set to False.'):\n                         remote_uploader_downloader.run_event(event_to_test, dummy_state, logger)\n \n                 else:\n"
        },
        {
            "commit": "4410203c56984c613d23f29a81ecd1b96c57b1ee",
            "file_path": "tests/trainer/test_fsdp.py",
            "diff": "diff --git a/tests/trainer/test_fsdp.py b/tests/trainer/test_fsdp.py\nindex ad16905..1cf8861 100644\n--- a/tests/trainer/test_fsdp.py\n+++ b/tests/trainer/test_fsdp.py\n@@ -4,9 +4,10 @@\n import pytest\n import torch\n from packaging import version\n+from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import CheckpointWrapper\n from torch.utils.data import DataLoader\n \n-from composer.models import ComposerClassifier\n+from composer.models import ComposerClassifier, ComposerModel\n from composer.trainer.trainer import Trainer\n from composer.utils import dist\n from tests.common import (EmbeddedWeightTiedModel, RandomClassificationDataset, SimpleModel, SimpleWeightTiedModel,\n@@ -117,3 +118,61 @@ def test_fsdp_prefetch_limit(forward_prefetch_limit: int, backward_prefetch_limi\n     )\n \n     trainer.fit()\n+\n+\n+class SimpleMLP(ComposerModel):\n+\n+    def __init__(self, num_features: int = 128, device: str = 'cuda'):\n+        super().__init__()\n+        self.fc1 = torch.nn.Linear(num_features, num_features, device=device, bias=False)\n+        self.fc2 = torch.nn.Linear(num_features, num_features, device=device, bias=False)\n+\n+    def forward(self, x):\n+        x = self.fc1(x)\n+        x = torch.nn.ReLU(x)\n+        x = self.fc2(x)\n+        return x\n+\n+    def loss(self, outputs, batch):\n+        pass\n+\n+\n+@world_size(2)\n+@pytest.mark.gpu\n+@pytest.mark.parametrize('activation_checkpointing', [True, False])\n+@pytest.mark.parametrize('activation_cpu_offload', [True, False])\n+def test_fsdp_act_ckpt_offload(\n+    activation_checkpointing: bool,\n+    activation_cpu_offload: bool,\n+    world_size: int,\n+):\n+    model = SimpleMLP()\n+\n+    fsdp_config = {\n+        'activation_checkpointing': activation_checkpointing,\n+        'activation_checkpointing_reentrant': False,\n+        'activation_cpu_offload': activation_cpu_offload,\n+    }\n+\n+    model.fc1._activation_checkpointing = True\n+\n+    trainer = Trainer(\n+        model=model,\n+        device='gpu',\n+        fsdp_config=fsdp_config,\n+    )\n+\n+    assert trainer.state.fsdp_enabled\n+    if version.parse(torch.__version__) > version.parse('2.1.0.dev'):\n+        from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import OffloadWrapper\n+\n+        if activation_checkpointing and activation_cpu_offload:\n+            assert isinstance(trainer.state.model.fc1._fsdp_wrapped_module, OffloadWrapper)\n+            assert isinstance(trainer.state.model.fc1._fsdp_wrapped_module._checkpoint_wrapped_module,\n+                              CheckpointWrapper)\n+        elif activation_checkpointing:\n+            assert isinstance(trainer.state.model.fc1._fsdp_wrapped_module, CheckpointWrapper)\n+        elif activation_cpu_offload:\n+            assert isinstance(trainer.state.model.fc1._fsdp_wrapped_module, OffloadWrapper)\n+        else:\n+            assert not isinstance(trainer.state.model.fc1._fsdp_wrapped_module, CheckpointWrapper)\n"
        },
        {
            "commit": "4410203c56984c613d23f29a81ecd1b96c57b1ee",
            "file_path": "tests/trainer/test_fsdp_checkpoint.py",
            "diff": "diff --git a/tests/trainer/test_fsdp_checkpoint.py b/tests/trainer/test_fsdp_checkpoint.py\nindex 54d668c..44468bd 100644\n--- a/tests/trainer/test_fsdp_checkpoint.py\n+++ b/tests/trainer/test_fsdp_checkpoint.py\n@@ -393,6 +393,11 @@ def test_fsdp_mixed_with_sync(\n         '0.16.0',\n         marks=pytest.mark.filterwarnings((r'ignore:MosaicMLLogger is not in the state_dict. Its '\n                                           r'state will not be restored.:UserWarning')),\n+    ),\n+    pytest.param(\n+        '0.17.0',\n+        marks=pytest.mark.filterwarnings((r'ignore:MosaicMLLogger is not in the state_dict. Its '\n+                                          r'state will not be restored.:UserWarning')),\n     )\n ])\n @pytest.mark.filterwarnings(r'ignore:.*metrics are not saved with sharded state dict.*:UserWarning')\n"
        },
        {
            "commit": "4410203c56984c613d23f29a81ecd1b96c57b1ee",
            "file_path": "tests/utils/eval_client/test_local_eval_client.py",
            "diff": "diff --git a/tests/utils/eval_client/test_local_eval_client.py b/tests/utils/eval_client/test_local_eval_client.py\nindex 90b80e1..8a59860 100644\n--- a/tests/utils/eval_client/test_local_eval_client.py\n+++ b/tests/utils/eval_client/test_local_eval_client.py\n@@ -28,9 +28,11 @@ from tests.common.markers import world_size\n     ],\n )\n @world_size(1, 2)\n-def test_local_invoke(code: str, result: str, language: str, world_size: int):\n+def test_local_invoke(code: str, result: str, language: str, world_size: int, tmp_path: str):\n     \"\"\"Test invocation function for LocalEvalClient with code that succeeds, fails compilation, times out, and is incorrect in C, C++, Python, JS.\n     \"\"\"\n+    import os\n+    os.makedirs(os.path.dirname(tmp_path), exist_ok=True)\n     eval_client = LocalEvalClient()\n     input = '(1,)' if language == 'python' else '1'\n     assert eval_client.invoke([[[{\n"
        },
        {
            "commit": "4410203c56984c613d23f29a81ecd1b96c57b1ee",
            "file_path": "tests/utils/object_store/test_s3_object_store.py",
            "diff": "diff --git a/tests/utils/object_store/test_s3_object_store.py b/tests/utils/object_store/test_s3_object_store.py\nindex 05c515e..eb6b8a0 100644\n--- a/tests/utils/object_store/test_s3_object_store.py\n+++ b/tests/utils/object_store/test_s3_object_store.py\n@@ -4,6 +4,8 @@\n import os\n import pathlib\n import threading\n+from unittest import mock\n+from unittest.mock import ANY, MagicMock\n \n import pytest\n \n@@ -31,3 +33,63 @@ def test_s3_object_store_multi_threads(tmp_path: pathlib.Path, s3_bucket: str):\n         threads.append(t)\n     for t in threads:\n         t.join()\n+\n+\n+def test_s3_upload_object_arguments(tmp_path: pathlib.Path, s3_bucket: str):\n+    filename = tmp_path / 'localfile.txt'\n+    filename.touch()\n+    remote_obj_name = 'remote.txt'\n+\n+    object_store = S3ObjectStore(bucket=s3_bucket)\n+    object_store.client.upload_file = MagicMock()\n+\n+    with mock.patch.dict('os.environ'):\n+        os.environ.pop('S3_CANNED_ACL', None)\n+\n+        object_store.upload_object(object_name=remote_obj_name, filename=filename)\n+        object_store.client.upload_file.assert_called_with(Bucket='my-bucket',\n+                                                           Key=remote_obj_name,\n+                                                           Filename=filename,\n+                                                           Callback=None,\n+                                                           Config=ANY)\n+\n+        object_store.upload_object(object_name=remote_obj_name,\n+                                   filename=filename,\n+                                   ExtraArgs={'ACL': 'authenticated-read'})\n+        object_store.client.upload_file.assert_called_with(Bucket='my-bucket',\n+                                                           Key=remote_obj_name,\n+                                                           Filename=filename,\n+                                                           Callback=None,\n+                                                           Config=ANY,\n+                                                           ExtraArgs={'ACL': 'authenticated-read'})\n+\n+        os.environ['S3_CANNED_ACL'] = 'bucket-owner-full-control'\n+\n+        object_store.upload_object(object_name=remote_obj_name, filename=filename)\n+        object_store.client.upload_file.assert_called_with(Bucket='my-bucket',\n+                                                           Key=remote_obj_name,\n+                                                           Filename=filename,\n+                                                           Callback=None,\n+                                                           Config=ANY,\n+                                                           ExtraArgs={'ACL': 'bucket-owner-full-control'})\n+\n+        object_store.upload_object(object_name=remote_obj_name, filename=filename, ExtraArgs={'Metadata': {}})\n+        object_store.client.upload_file.assert_called_with(Bucket='my-bucket',\n+                                                           Key=remote_obj_name,\n+                                                           Filename=filename,\n+                                                           Callback=None,\n+                                                           Config=ANY,\n+                                                           ExtraArgs={\n+                                                               'ACL': 'bucket-owner-full-control',\n+                                                               'Metadata': {}\n+                                                           })\n+\n+        object_store.upload_object(object_name=remote_obj_name,\n+                                   filename=filename,\n+                                   ExtraArgs={'ACL': 'authenticated-read'})\n+        object_store.client.upload_file.assert_called_with(Bucket='my-bucket',\n+                                                           Key=remote_obj_name,\n+                                                           Filename=filename,\n+                                                           Callback=None,\n+                                                           Config=ANY,\n+                                                           ExtraArgs={'ACL': 'authenticated-read'})\n"
        }
    ]
}