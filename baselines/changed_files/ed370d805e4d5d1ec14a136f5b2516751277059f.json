{
    "sha_fail": "ed370d805e4d5d1ec14a136f5b2516751277059f",
    "changed_files": [
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/lib/_parsing/_completions.py",
            "diff": "diff --git a/src/openai/lib/_parsing/_completions.py b/src/openai/lib/_parsing/_completions.py\nindex c160070..e14c338 100644\n--- a/src/openai/lib/_parsing/_completions.py\n+++ b/src/openai/lib/_parsing/_completions.py\n@@ -1,6 +1,7 @@\n from __future__ import annotations\n \n import json\n+import logging\n from typing import TYPE_CHECKING, Any, Iterable, cast\n from typing_extensions import TypeVar, TypeGuard, assert_never\n \n@@ -19,14 +20,15 @@ from ...types.chat import (\n     ParsedChatCompletion,\n     ChatCompletionMessage,\n     ParsedFunctionToolCall,\n-    ChatCompletionToolParam,\n     ParsedChatCompletionMessage,\n+    ChatCompletionFunctionToolParam,\n     completion_create_params,\n )\n from ..._exceptions import LengthFinishReasonError, ContentFilterFinishReasonError\n from ...types.shared_params import FunctionDefinition\n from ...types.chat.completion_create_params import ResponseFormat as ResponseFormatParam\n-from ...types.chat.chat_completion_message_tool_call import Function\n+from ...types.chat.chat_completion_tool_param import ChatCompletionToolParam\n+from ...types.chat.chat_completion_message_function_tool_call import Function\n \n ResponseFormatT = TypeVar(\n     \"ResponseFormatT\",\n@@ -35,12 +37,36 @@ ResponseFormatT = TypeVar(\n )\n _default_response_format: None = None\n \n+log: logging.Logger = logging.getLogger(\"openai.lib.parsing\")\n+\n+\n+def is_strict_chat_completion_tool_param(\n+    tool: ChatCompletionToolParam,\n+) -> TypeGuard[ChatCompletionFunctionToolParam]:\n+    \"\"\"Check if the given tool is a strict ChatCompletionFunctionToolParam.\"\"\"\n+    if not tool[\"type\"] == \"function\":\n+        return False\n+    if tool[\"function\"].get(\"strict\") is not True:\n+        return False\n+\n+    return True\n+\n+\n+def select_strict_chat_completion_tools(\n+    tools: Iterable[ChatCompletionToolParam] | NotGiven = NOT_GIVEN,\n+) -> Iterable[ChatCompletionFunctionToolParam] | NotGiven:\n+    \"\"\"Select only the strict ChatCompletionFunctionToolParams from the given tools.\"\"\"\n+    if not is_given(tools):\n+        return NOT_GIVEN\n+\n+    return [t for t in tools if is_strict_chat_completion_tool_param(t)]\n+\n \n def validate_input_tools(\n     tools: Iterable[ChatCompletionToolParam] | NotGiven = NOT_GIVEN,\n-) -> None:\n+) -> Iterable[ChatCompletionFunctionToolParam] | NotGiven:\n     if not is_given(tools):\n-        return\n+        return NOT_GIVEN\n \n     for tool in tools:\n         if tool[\"type\"] != \"function\":\n@@ -54,6 +80,8 @@ def validate_input_tools(\n                 f\"`{tool['function']['name']}` is not strict. Only `strict` function tools can be auto-parsed\"\n             )\n \n+    return cast(Iterable[ChatCompletionFunctionToolParam], tools)\n+\n \n def parse_chat_completion(\n     *,\n@@ -95,6 +123,14 @@ def parse_chat_completion(\n                             type_=ParsedFunctionToolCall,\n                         )\n                     )\n+                elif tool_call.type == \"custom\":\n+                    # warn user that custom tool calls are not callable here\n+                    log.warning(\n+                        \"Custom tool calls are not callable. Ignoring tool call: %s - %s\",\n+                        tool_call.id,\n+                        tool_call.custom.name,\n+                        stacklevel=2,\n+                    )\n                 elif TYPE_CHECKING:  # type: ignore[unreachable]\n                     assert_never(tool_call)\n                 else:\n@@ -129,13 +165,15 @@ def parse_chat_completion(\n     )\n \n \n-def get_input_tool_by_name(*, input_tools: list[ChatCompletionToolParam], name: str) -> ChatCompletionToolParam | None:\n-    return next((t for t in input_tools if t.get(\"function\", {}).get(\"name\") == name), None)\n+def get_input_tool_by_name(\n+    *, input_tools: list[ChatCompletionToolParam], name: str\n+) -> ChatCompletionFunctionToolParam | None:\n+    return next((t for t in input_tools if t[\"type\"] == \"function\" and t.get(\"function\", {}).get(\"name\") == name), None)\n \n \n def parse_function_tool_arguments(\n     *, input_tools: list[ChatCompletionToolParam], function: Function | ParsedFunction\n-) -> object:\n+) -> object | None:\n     input_tool = get_input_tool_by_name(input_tools=input_tools, name=function.name)\n     if not input_tool:\n         return None\n@@ -149,7 +187,7 @@ def parse_function_tool_arguments(\n     if not input_fn.get(\"strict\"):\n         return None\n \n-    return json.loads(function.arguments)\n+    return json.loads(function.arguments)  # type: ignore[no-any-return]\n \n \n def maybe_parse_content(\n@@ -209,6 +247,9 @@ def is_response_format_param(response_format: object) -> TypeGuard[ResponseForma\n \n \n def is_parseable_tool(input_tool: ChatCompletionToolParam) -> bool:\n+    if input_tool[\"type\"] != \"function\":\n+        return False\n+\n     input_fn = cast(object, input_tool.get(\"function\"))\n     if isinstance(input_fn, PydanticFunctionTool):\n         return True\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/lib/_parsing/_responses.py",
            "diff": "diff --git a/src/openai/lib/_parsing/_responses.py b/src/openai/lib/_parsing/_responses.py\nindex 41be1d3..2a30ac8 100644\n--- a/src/openai/lib/_parsing/_responses.py\n+++ b/src/openai/lib/_parsing/_responses.py\n@@ -110,6 +110,7 @@ def parse_response(\n             or output.type == \"local_shell_call\"\n             or output.type == \"mcp_list_tools\"\n             or output.type == \"exec\"\n+            or output.type == \"custom_tool_call\"\n         ):\n             output_list.append(output)\n         elif TYPE_CHECKING:  # type: ignore\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/lib/_tools.py",
            "diff": "diff --git a/src/openai/lib/_tools.py b/src/openai/lib/_tools.py\nindex 415d750..4070ad6 100644\n--- a/src/openai/lib/_tools.py\n+++ b/src/openai/lib/_tools.py\n@@ -5,7 +5,7 @@ from typing import Any, Dict, cast\n import pydantic\n \n from ._pydantic import to_strict_json_schema\n-from ..types.chat import ChatCompletionToolParam\n+from ..types.chat import ChatCompletionFunctionToolParam\n from ..types.shared_params import FunctionDefinition\n from ..types.responses.function_tool_param import FunctionToolParam as ResponsesFunctionToolParam\n \n@@ -42,7 +42,7 @@ def pydantic_function_tool(\n     *,\n     name: str | None = None,  # inferred from class name by default\n     description: str | None = None,  # inferred from class docstring by default\n-) -> ChatCompletionToolParam:\n+) -> ChatCompletionFunctionToolParam:\n     if description is None:\n         # note: we intentionally don't use `.getdoc()` to avoid\n         # including pydantic's docstrings\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/lib/streaming/chat/_completions.py",
            "diff": "diff --git a/src/openai/lib/streaming/chat/_completions.py b/src/openai/lib/streaming/chat/_completions.py\nindex 2cf37ef..1dff628 100644\n--- a/src/openai/lib/streaming/chat/_completions.py\n+++ b/src/openai/lib/streaming/chat/_completions.py\n@@ -37,11 +37,12 @@ from ..._parsing import (\n     parse_function_tool_arguments,\n )\n from ...._streaming import Stream, AsyncStream\n-from ....types.chat import ChatCompletionChunk, ParsedChatCompletion, ChatCompletionToolParam\n+from ....types.chat import ChatCompletionChunk, ParsedChatCompletion\n from ...._exceptions import LengthFinishReasonError, ContentFilterFinishReasonError\n from ....types.chat.chat_completion import ChoiceLogprobs\n from ....types.chat.chat_completion_chunk import Choice as ChoiceChunk\n from ....types.chat.completion_create_params import ResponseFormat as ResponseFormatParam\n+from ....types.chat.chat_completion_tool_param import ChatCompletionToolParam\n \n \n class ChatCompletionStream(Generic[ResponseFormatT]):\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/resources/beta/assistants.py",
            "diff": "diff --git a/src/openai/resources/beta/assistants.py b/src/openai/resources/beta/assistants.py\nindex 9059d93..fe0c99c 100644\n--- a/src/openai/resources/beta/assistants.py\n+++ b/src/openai/resources/beta/assistants.py\n@@ -96,12 +96,11 @@ class Assistants(SyncAPIResource):\n \n           name: The name of the assistant. The maximum length is 256 characters.\n \n-          reasoning_effort: **o-series models only**\n-\n-              Constrains effort on reasoning for\n+          reasoning_effort: Constrains effort on reasoning for\n               [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n-              supported values are `low`, `medium`, and `high`. Reducing reasoning effort can\n-              result in faster responses and fewer tokens used on reasoning in a response.\n+              supported values are `minimal`, `low`, `medium`, and `high`. Reducing reasoning\n+              effort can result in faster responses and fewer tokens used on reasoning in a\n+              response.\n \n           response_format: Specifies the format that the model must output. Compatible with\n               [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),\n@@ -220,6 +219,12 @@ class Assistants(SyncAPIResource):\n         model: Union[\n             str,\n             Literal[\n+                \"gpt-5\",\n+                \"gpt-5-mini\",\n+                \"gpt-5-nano\",\n+                \"gpt-5-2025-08-07\",\n+                \"gpt-5-mini-2025-08-07\",\n+                \"gpt-5-nano-2025-08-07\",\n                 \"gpt-4.1\",\n                 \"gpt-4.1-mini\",\n                 \"gpt-4.1-nano\",\n@@ -298,12 +303,11 @@ class Assistants(SyncAPIResource):\n \n           name: The name of the assistant. The maximum length is 256 characters.\n \n-          reasoning_effort: **o-series models only**\n-\n-              Constrains effort on reasoning for\n+          reasoning_effort: Constrains effort on reasoning for\n               [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n-              supported values are `low`, `medium`, and `high`. Reducing reasoning effort can\n-              result in faster responses and fewer tokens used on reasoning in a response.\n+              supported values are `minimal`, `low`, `medium`, and `high`. Reducing reasoning\n+              effort can result in faster responses and fewer tokens used on reasoning in a\n+              response.\n \n           response_format: Specifies the format that the model must output. Compatible with\n               [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),\n@@ -545,12 +549,11 @@ class AsyncAssistants(AsyncAPIResource):\n \n           name: The name of the assistant. The maximum length is 256 characters.\n \n-          reasoning_effort: **o-series models only**\n-\n-              Constrains effort on reasoning for\n+          reasoning_effort: Constrains effort on reasoning for\n               [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n-              supported values are `low`, `medium`, and `high`. Reducing reasoning effort can\n-              result in faster responses and fewer tokens used on reasoning in a response.\n+              supported values are `minimal`, `low`, `medium`, and `high`. Reducing reasoning\n+              effort can result in faster responses and fewer tokens used on reasoning in a\n+              response.\n \n           response_format: Specifies the format that the model must output. Compatible with\n               [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),\n@@ -669,6 +672,12 @@ class AsyncAssistants(AsyncAPIResource):\n         model: Union[\n             str,\n             Literal[\n+                \"gpt-5\",\n+                \"gpt-5-mini\",\n+                \"gpt-5-nano\",\n+                \"gpt-5-2025-08-07\",\n+                \"gpt-5-mini-2025-08-07\",\n+                \"gpt-5-nano-2025-08-07\",\n                 \"gpt-4.1\",\n                 \"gpt-4.1-mini\",\n                 \"gpt-4.1-nano\",\n@@ -747,12 +756,11 @@ class AsyncAssistants(AsyncAPIResource):\n \n           name: The name of the assistant. The maximum length is 256 characters.\n \n-          reasoning_effort: **o-series models only**\n-\n-              Constrains effort on reasoning for\n+          reasoning_effort: Constrains effort on reasoning for\n               [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n-              supported values are `low`, `medium`, and `high`. Reducing reasoning effort can\n-              result in faster responses and fewer tokens used on reasoning in a response.\n+              supported values are `minimal`, `low`, `medium`, and `high`. Reducing reasoning\n+              effort can result in faster responses and fewer tokens used on reasoning in a\n+              response.\n \n           response_format: Specifies the format that the model must output. Compatible with\n               [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/resources/beta/threads/runs/runs.py",
            "diff": "diff --git a/src/openai/resources/beta/threads/runs/runs.py b/src/openai/resources/beta/threads/runs/runs.py\nindex 3d9ae97..01246d7 100644\n--- a/src/openai/resources/beta/threads/runs/runs.py\n+++ b/src/openai/resources/beta/threads/runs/runs.py\n@@ -167,12 +167,11 @@ class Runs(SyncAPIResource):\n               [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)\n               during tool use.\n \n-          reasoning_effort: **o-series models only**\n-\n-              Constrains effort on reasoning for\n+          reasoning_effort: Constrains effort on reasoning for\n               [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n-              supported values are `low`, `medium`, and `high`. Reducing reasoning effort can\n-              result in faster responses and fewer tokens used on reasoning in a response.\n+              supported values are `minimal`, `low`, `medium`, and `high`. Reducing reasoning\n+              effort can result in faster responses and fewer tokens used on reasoning in a\n+              response.\n \n           response_format: Specifies the format that the model must output. Compatible with\n               [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),\n@@ -322,12 +321,11 @@ class Runs(SyncAPIResource):\n               [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)\n               during tool use.\n \n-          reasoning_effort: **o-series models only**\n-\n-              Constrains effort on reasoning for\n+          reasoning_effort: Constrains effort on reasoning for\n               [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n-              supported values are `low`, `medium`, and `high`. Reducing reasoning effort can\n-              result in faster responses and fewer tokens used on reasoning in a response.\n+              supported values are `minimal`, `low`, `medium`, and `high`. Reducing reasoning\n+              effort can result in faster responses and fewer tokens used on reasoning in a\n+              response.\n \n           response_format: Specifies the format that the model must output. Compatible with\n               [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),\n@@ -473,12 +471,11 @@ class Runs(SyncAPIResource):\n               [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)\n               during tool use.\n \n-          reasoning_effort: **o-series models only**\n-\n-              Constrains effort on reasoning for\n+          reasoning_effort: Constrains effort on reasoning for\n               [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n-              supported values are `low`, `medium`, and `high`. Reducing reasoning effort can\n-              result in faster responses and fewer tokens used on reasoning in a response.\n+              supported values are `minimal`, `low`, `medium`, and `high`. Reducing reasoning\n+              effort can result in faster responses and fewer tokens used on reasoning in a\n+              response.\n \n           response_format: Specifies the format that the model must output. Compatible with\n               [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),\n@@ -1600,12 +1597,11 @@ class AsyncRuns(AsyncAPIResource):\n               [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)\n               during tool use.\n \n-          reasoning_effort: **o-series models only**\n-\n-              Constrains effort on reasoning for\n+          reasoning_effort: Constrains effort on reasoning for\n               [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n-              supported values are `low`, `medium`, and `high`. Reducing reasoning effort can\n-              result in faster responses and fewer tokens used on reasoning in a response.\n+              supported values are `minimal`, `low`, `medium`, and `high`. Reducing reasoning\n+              effort can result in faster responses and fewer tokens used on reasoning in a\n+              response.\n \n           response_format: Specifies the format that the model must output. Compatible with\n               [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),\n@@ -1755,12 +1751,11 @@ class AsyncRuns(AsyncAPIResource):\n               [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)\n               during tool use.\n \n-          reasoning_effort: **o-series models only**\n-\n-              Constrains effort on reasoning for\n+          reasoning_effort: Constrains effort on reasoning for\n               [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n-              supported values are `low`, `medium`, and `high`. Reducing reasoning effort can\n-              result in faster responses and fewer tokens used on reasoning in a response.\n+              supported values are `minimal`, `low`, `medium`, and `high`. Reducing reasoning\n+              effort can result in faster responses and fewer tokens used on reasoning in a\n+              response.\n \n           response_format: Specifies the format that the model must output. Compatible with\n               [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),\n@@ -1906,12 +1901,11 @@ class AsyncRuns(AsyncAPIResource):\n               [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)\n               during tool use.\n \n-          reasoning_effort: **o-series models only**\n-\n-              Constrains effort on reasoning for\n+          reasoning_effort: Constrains effort on reasoning for\n               [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n-              supported values are `low`, `medium`, and `high`. Reducing reasoning effort can\n-              result in faster responses and fewer tokens used on reasoning in a response.\n+              supported values are `minimal`, `low`, `medium`, and `high`. Reducing reasoning\n+              effort can result in faster responses and fewer tokens used on reasoning in a\n+              response.\n \n           response_format: Specifies the format that the model must output. Compatible with\n               [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/resources/chat/completions/completions.py",
            "diff": "diff --git a/src/openai/resources/chat/completions/completions.py b/src/openai/resources/chat/completions/completions.py\nindex cd1cb2b..65f9139 100644\n--- a/src/openai/resources/chat/completions/completions.py\n+++ b/src/openai/resources/chat/completions/completions.py\n@@ -115,6 +115,7 @@ class Completions(SyncAPIResource):\n         top_logprobs: Optional[int] | NotGiven = NOT_GIVEN,\n         top_p: Optional[float] | NotGiven = NOT_GIVEN,\n         user: str | NotGiven = NOT_GIVEN,\n+        verbosity: Optional[Literal[\"low\", \"medium\", \"high\"]] | NotGiven = NOT_GIVEN,\n         web_search_options: completion_create_params.WebSearchOptions | NotGiven = NOT_GIVEN,\n         # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n         # The extra values given here take precedence over values defined on the client or passed to this method.\n@@ -165,7 +166,7 @@ class Completions(SyncAPIResource):\n             print(\"answer: \", message.parsed.final_answer)\n         ```\n         \"\"\"\n-        _validate_input_tools(tools)\n+        chat_completion_tools = _validate_input_tools(tools)\n \n         extra_headers = {\n             \"X-Stainless-Helper-Method\": \"chat.completions.parse\",\n@@ -176,7 +177,7 @@ class Completions(SyncAPIResource):\n             return _parse_chat_completion(\n                 response_format=response_format,\n                 chat_completion=raw_completion,\n-                input_tools=tools,\n+                input_tools=chat_completion_tools,\n             )\n \n         return self._post(\n@@ -215,6 +216,7 @@ class Completions(SyncAPIResource):\n                     \"top_logprobs\": top_logprobs,\n                     \"top_p\": top_p,\n                     \"user\": user,\n+                    \"verbosity\": verbosity,\n                     \"web_search_options\": web_search_options,\n                 },\n                 completion_create_params.CompletionCreateParams,\n@@ -268,6 +270,7 @@ class Completions(SyncAPIResource):\n         top_logprobs: Optional[int] | NotGiven = NOT_GIVEN,\n         top_p: Optional[float] | NotGiven = NOT_GIVEN,\n         user: str | NotGiven = NOT_GIVEN,\n+        verbosity: Optional[Literal[\"low\", \"medium\", \"high\"]] | NotGiven = NOT_GIVEN,\n         web_search_options: completion_create_params.WebSearchOptions | NotGiven = NOT_GIVEN,\n         # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n         # The extra values given here take precedence over values defined on the client or passed to this method.\n@@ -398,12 +401,11 @@ class Completions(SyncAPIResource):\n               hit rates. Replaces the `user` field.\n               [Learn more](https://platform.openai.com/docs/guides/prompt-caching).\n \n-          reasoning_effort: **o-series models only**\n-\n-              Constrains effort on reasoning for\n+          reasoning_effort: Constrains effort on reasoning for\n               [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n-              supported values are `low`, `medium`, and `high`. Reducing reasoning effort can\n-              result in faster responses and fewer tokens used on reasoning in a response.\n+              supported values are `minimal`, `low`, `medium`, and `high`. Reducing reasoning\n+              effort can result in faster responses and fewer tokens used on reasoning in a\n+              response.\n \n           response_format: An object specifying the format that the model must output.\n \n@@ -483,9 +485,9 @@ class Completions(SyncAPIResource):\n               `none` is the default when no tools are present. `auto` is the default if tools\n               are present.\n \n-          tools: A list of tools the model may call. Currently, only functions are supported as a\n-              tool. Use this to provide a list of functions the model may generate JSON inputs\n-              for. A max of 128 functions are supported.\n+          tools: A list of tools the model may call. You can provide either\n+              [custom tools](https://platform.openai.com/docs/guides/function-calling#custom-tools)\n+              or [function tools](https://platform.openai.com/docs/guides/function-calling).\n \n           top_logprobs: An integer between 0 and 20 specifying the number of most likely tokens to\n               return at each token position, each with an associated log probability.\n@@ -503,6 +505,10 @@ class Completions(SyncAPIResource):\n               similar requests and to help OpenAI detect and prevent abuse.\n               [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).\n \n+          verbosity: Constrains the verbosity of the model's response. Lower values will result in\n+              more concise responses, while higher values will result in more verbose\n+              responses. Currently supported values are `low`, `medium`, and `high`.\n+\n           web_search_options: This tool searches the web for relevant results to use in a response. Learn more\n               about the\n               [web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).\n@@ -553,6 +559,7 @@ class Completions(SyncAPIResource):\n         top_logprobs: Optional[int] | NotGiven = NOT_GIVEN,\n         top_p: Optional[float] | NotGiven = NOT_GIVEN,\n         user: str | NotGiven = NOT_GIVEN,\n+        verbosity: Optional[Literal[\"low\", \"medium\", \"high\"]] | NotGiven = NOT_GIVEN,\n         web_search_options: completion_create_params.WebSearchOptions | NotGiven = NOT_GIVEN,\n         # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n         # The extra values given here take precedence over values defined on the client or passed to this method.\n@@ -692,12 +699,11 @@ class Completions(SyncAPIResource):\n               hit rates. Replaces the `user` field.\n               [Learn more](https://platform.openai.com/docs/guides/prompt-caching).\n \n-          reasoning_effort: **o-series models only**\n-\n-              Constrains effort on reasoning for\n+          reasoning_effort: Constrains effort on reasoning for\n               [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n-              supported values are `low`, `medium`, and `high`. Reducing reasoning effort can\n-              result in faster responses and fewer tokens used on reasoning in a response.\n+              supported values are `minimal`, `low`, `medium`, and `high`. Reducing reasoning\n+              effort can result in faster responses and fewer tokens used on reasoning in a\n+              response.\n \n           response_format: An object specifying the format that the model must output.\n \n@@ -768,9 +774,9 @@ class Completions(SyncAPIResource):\n               `none` is the default when no tools are present. `auto` is the default if tools\n               are present.\n \n-          tools: A list of tools the model may call. Currently, only functions are supported as a\n-              tool. Use this to provide a list of functions the model may generate JSON inputs\n-              for. A max of 128 functions are supported.\n+          tools: A list of tools the model may call. You can provide either\n+              [custom tools](https://platform.openai.com/docs/guides/function-calling#custom-tools)\n+              or [function tools](https://platform.openai.com/docs/guides/function-calling).\n \n           top_logprobs: An integer between 0 and 20 specifying the number of most likely tokens to\n               return at each token position, each with an associated log probability.\n@@ -788,6 +794,10 @@ class Completions(SyncAPIResource):\n               similar requests and to help OpenAI detect and prevent abuse.\n               [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).\n \n+          verbosity: Constrains the verbosity of the model's response. Lower values will result in\n+              more concise responses, while higher values will result in more verbose\n+              responses. Currently supported values are `low`, `medium`, and `high`.\n+\n           web_search_options: This tool searches the web for relevant results to use in a response. Learn more\n               about the\n               [web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).\n@@ -838,6 +848,7 @@ class Completions(SyncAPIResource):\n         top_logprobs: Optional[int] | NotGiven = NOT_GIVEN,\n         top_p: Optional[float] | NotGiven = NOT_GIVEN,\n         user: str | NotGiven = NOT_GIVEN,\n+        verbosity: Optional[Literal[\"low\", \"medium\", \"high\"]] | NotGiven = NOT_GIVEN,\n         web_search_options: completion_create_params.WebSearchOptions | NotGiven = NOT_GIVEN,\n         # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n         # The extra values given here take precedence over values defined on the client or passed to this method.\n@@ -977,12 +988,11 @@ class Completions(SyncAPIResource):\n               hit rates. Replaces the `user` field.\n               [Learn more](https://platform.openai.com/docs/guides/prompt-caching).\n \n-          reasoning_effort: **o-series models only**\n-\n-              Constrains effort on reasoning for\n+          reasoning_effort: Constrains effort on reasoning for\n               [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n-              supported values are `low`, `medium`, and `high`. Reducing reasoning effort can\n-              result in faster responses and fewer tokens used on reasoning in a response.\n+              supported values are `minimal`, `low`, `medium`, and `high`. Reducing reasoning\n+              effort can result in faster responses and fewer tokens used on reasoning in a\n+              response.\n \n           response_format: An object specifying the format that the model must output.\n \n@@ -1053,9 +1063,9 @@ class Completions(SyncAPIResource):\n               `none` is the default when no tools are present. `auto` is the default if tools\n               are present.\n \n-          tools: A list of tools the model may call. Currently, only functions are supported as a\n-              tool. Use this to provide a list of functions the model may generate JSON inputs\n-              for. A max of 128 functions are supported.\n+          tools: A list of tools the model may call. You can provide either\n+              [custom tools](https://platform.openai.com/docs/guides/function-calling#custom-tools)\n+              or [function tools](https://platform.openai.com/docs/guides/function-calling).\n \n           top_logprobs: An integer between 0 and 20 specifying the number of most likely tokens to\n               return at each token position, each with an associated log probability.\n@@ -1073,6 +1083,10 @@ class Completions(SyncAPIResource):\n               similar requests and to help OpenAI detect and prevent abuse.\n               [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).\n \n+          verbosity: Constrains the verbosity of the model's response. Lower values will result in\n+              more concise responses, while higher values will result in more verbose\n+              responses. Currently supported values are `low`, `medium`, and `high`.\n+\n           web_search_options: This tool searches the web for relevant results to use in a response. Learn more\n               about the\n               [web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).\n@@ -1123,6 +1137,7 @@ class Completions(SyncAPIResource):\n         top_logprobs: Optional[int] | NotGiven = NOT_GIVEN,\n         top_p: Optional[float] | NotGiven = NOT_GIVEN,\n         user: str | NotGiven = NOT_GIVEN,\n+        verbosity: Optional[Literal[\"low\", \"medium\", \"high\"]] | NotGiven = NOT_GIVEN,\n         web_search_options: completion_create_params.WebSearchOptions | NotGiven = NOT_GIVEN,\n         # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n         # The extra values given here take precedence over values defined on the client or passed to this method.\n@@ -1168,6 +1183,7 @@ class Completions(SyncAPIResource):\n                     \"top_logprobs\": top_logprobs,\n                     \"top_p\": top_p,\n                     \"user\": user,\n+                    \"verbosity\": verbosity,\n                     \"web_search_options\": web_search_options,\n                 },\n                 completion_create_params.CompletionCreateParamsStreaming\n@@ -1396,6 +1412,7 @@ class Completions(SyncAPIResource):\n         top_logprobs: Optional[int] | NotGiven = NOT_GIVEN,\n         top_p: Optional[float] | NotGiven = NOT_GIVEN,\n         user: str | NotGiven = NOT_GIVEN,\n+        verbosity: Optional[Literal[\"low\", \"medium\", \"high\"]] | NotGiven = NOT_GIVEN,\n         web_search_options: completion_create_params.WebSearchOptions | NotGiven = NOT_GIVEN,\n         # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n         # The extra values given here take precedence over values defined on the client or passed to this method.\n@@ -1465,6 +1482,7 @@ class Completions(SyncAPIResource):\n             top_logprobs=top_logprobs,\n             top_p=top_p,\n             user=user,\n+            verbosity=verbosity,\n             web_search_options=web_search_options,\n             extra_headers=extra_headers,\n             extra_query=extra_query,\n@@ -1536,6 +1554,7 @@ class AsyncCompletions(AsyncAPIResource):\n         top_logprobs: Optional[int] | NotGiven = NOT_GIVEN,\n         top_p: Optional[float] | NotGiven = NOT_GIVEN,\n         user: str | NotGiven = NOT_GIVEN,\n+        verbosity: Optional[Literal[\"low\", \"medium\", \"high\"]] | NotGiven = NOT_GIVEN,\n         web_search_options: completion_create_params.WebSearchOptions | NotGiven = NOT_GIVEN,\n         # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n         # The extra values given here take precedence over values defined on the client or passed to this method.\n@@ -1636,6 +1655,7 @@ class AsyncCompletions(AsyncAPIResource):\n                     \"top_logprobs\": top_logprobs,\n                     \"top_p\": top_p,\n                     \"user\": user,\n+                    \"verbosity\": verbosity,\n                     \"web_search_options\": web_search_options,\n                 },\n                 completion_create_params.CompletionCreateParams,\n@@ -1689,6 +1709,7 @@ class AsyncCompletions(AsyncAPIResource):\n         top_logprobs: Optional[int] | NotGiven = NOT_GIVEN,\n         top_p: Optional[float] | NotGiven = NOT_GIVEN,\n         user: str | NotGiven = NOT_GIVEN,\n+        verbosity: Optional[Literal[\"low\", \"medium\", \"high\"]] | NotGiven = NOT_GIVEN,\n         web_search_options: completion_create_params.WebSearchOptions | NotGiven = NOT_GIVEN,\n         # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n         # The extra values given here take precedence over values defined on the client or passed to this method.\n@@ -1819,12 +1840,11 @@ class AsyncCompletions(AsyncAPIResource):\n               hit rates. Replaces the `user` field.\n               [Learn more](https://platform.openai.com/docs/guides/prompt-caching).\n \n-          reasoning_effort: **o-series models only**\n-\n-              Constrains effort on reasoning for\n+          reasoning_effort: Constrains effort on reasoning for\n               [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n-              supported values are `low`, `medium`, and `high`. Reducing reasoning effort can\n-              result in faster responses and fewer tokens used on reasoning in a response.\n+              supported values are `minimal`, `low`, `medium`, and `high`. Reducing reasoning\n+              effort can result in faster responses and fewer tokens used on reasoning in a\n+              response.\n \n           response_format: An object specifying the format that the model must output.\n \n@@ -1904,9 +1924,9 @@ class AsyncCompletions(AsyncAPIResource):\n               `none` is the default when no tools are present. `auto` is the default if tools\n               are present.\n \n-          tools: A list of tools the model may call. Currently, only functions are supported as a\n-              tool. Use this to provide a list of functions the model may generate JSON inputs\n-              for. A max of 128 functions are supported.\n+          tools: A list of tools the model may call. You can provide either\n+              [custom tools](https://platform.openai.com/docs/guides/function-calling#custom-tools)\n+              or [function tools](https://platform.openai.com/docs/guides/function-calling).\n \n           top_logprobs: An integer between 0 and 20 specifying the number of most likely tokens to\n               return at each token position, each with an associated log probability.\n@@ -1924,6 +1944,10 @@ class AsyncCompletions(AsyncAPIResource):\n               similar requests and to help OpenAI detect and prevent abuse.\n               [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).\n \n+          verbosity: Constrains the verbosity of the model's response. Lower values will result in\n+              more concise responses, while higher values will result in more verbose\n+              responses. Currently supported values are `low`, `medium`, and `high`.\n+\n           web_search_options: This tool searches the web for relevant results to use in a response. Learn more\n               about the\n               [web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).\n@@ -1974,6 +1998,7 @@ class AsyncCompletions(AsyncAPIResource):\n         top_logprobs: Optional[int] | NotGiven = NOT_GIVEN,\n         top_p: Optional[float] | NotGiven = NOT_GIVEN,\n         user: str | NotGiven = NOT_GIVEN,\n+        verbosity: Optional[Literal[\"low\", \"medium\", \"high\"]] | NotGiven = NOT_GIVEN,\n         web_search_options: completion_create_params.WebSearchOptions | NotGiven = NOT_GIVEN,\n         # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n         # The extra values given here take precedence over values defined on the client or passed to this method.\n@@ -2113,12 +2138,11 @@ class AsyncCompletions(AsyncAPIResource):\n               hit rates. Replaces the `user` field.\n               [Learn more](https://platform.openai.com/docs/guides/prompt-caching).\n \n-          reasoning_effort: **o-series models only**\n-\n-              Constrains effort on reasoning for\n+          reasoning_effort: Constrains effort on reasoning for\n               [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n-              supported values are `low`, `medium`, and `high`. Reducing reasoning effort can\n-              result in faster responses and fewer tokens used on reasoning in a response.\n+              supported values are `minimal`, `low`, `medium`, and `high`. Reducing reasoning\n+              effort can result in faster responses and fewer tokens used on reasoning in a\n+              response.\n \n           response_format: An object specifying the format that the model must output.\n \n@@ -2189,9 +2213,9 @@ class AsyncCompletions(AsyncAPIResource):\n               `none` is the default when no tools are present. `auto` is the default if tools\n               are present.\n \n-          tools: A list of tools the model may call. Currently, only functions are supported as a\n-              tool. Use this to provide a list of functions the model may generate JSON inputs\n-              for. A max of 128 functions are supported.\n+          tools: A list of tools the model may call. You can provide either\n+              [custom tools](https://platform.openai.com/docs/guides/function-calling#custom-tools)\n+              or [function tools](https://platform.openai.com/docs/guides/function-calling).\n \n           top_logprobs: An integer between 0 and 20 specifying the number of most likely tokens to\n               return at each token position, each with an associated log probability.\n@@ -2209,6 +2233,10 @@ class AsyncCompletions(AsyncAPIResource):\n               similar requests and to help OpenAI detect and prevent abuse.\n               [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).\n \n+          verbosity: Constrains the verbosity of the model's response. Lower values will result in\n+              more concise responses, while higher values will result in more verbose\n+              responses. Currently supported values are `low`, `medium`, and `high`.\n+\n           web_search_options: This tool searches the web for relevant results to use in a response. Learn more\n               about the\n               [web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).\n@@ -2259,6 +2287,7 @@ class AsyncCompletions(AsyncAPIResource):\n         top_logprobs: Optional[int] | NotGiven = NOT_GIVEN,\n         top_p: Optional[float] | NotGiven = NOT_GIVEN,\n         user: str | NotGiven = NOT_GIVEN,\n+        verbosity: Optional[Literal[\"low\", \"medium\", \"high\"]] | NotGiven = NOT_GIVEN,\n         web_search_options: completion_create_params.WebSearchOptions | NotGiven = NOT_GIVEN,\n         # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n         # The extra values given here take precedence over values defined on the client or passed to this method.\n@@ -2398,12 +2427,11 @@ class AsyncCompletions(AsyncAPIResource):\n               hit rates. Replaces the `user` field.\n               [Learn more](https://platform.openai.com/docs/guides/prompt-caching).\n \n-          reasoning_effort: **o-series models only**\n-\n-              Constrains effort on reasoning for\n+          reasoning_effort: Constrains effort on reasoning for\n               [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n-              supported values are `low`, `medium`, and `high`. Reducing reasoning effort can\n-              result in faster responses and fewer tokens used on reasoning in a response.\n+              supported values are `minimal`, `low`, `medium`, and `high`. Reducing reasoning\n+              effort can result in faster responses and fewer tokens used on reasoning in a\n+              response.\n \n           response_format: An object specifying the format that the model must output.\n \n@@ -2474,9 +2502,9 @@ class AsyncCompletions(AsyncAPIResource):\n               `none` is the default when no tools are present. `auto` is the default if tools\n               are present.\n \n-          tools: A list of tools the model may call. Currently, only functions are supported as a\n-              tool. Use this to provide a list of functions the model may generate JSON inputs\n-              for. A max of 128 functions are supported.\n+          tools: A list of tools the model may call. You can provide either\n+              [custom tools](https://platform.openai.com/docs/guides/function-calling#custom-tools)\n+              or [function tools](https://platform.openai.com/docs/guides/function-calling).\n \n           top_logprobs: An integer between 0 and 20 specifying the number of most likely tokens to\n               return at each token position, each with an associated log probability.\n@@ -2494,6 +2522,10 @@ class AsyncCompletions(AsyncAPIResource):\n               similar requests and to help OpenAI detect and prevent abuse.\n               [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).\n \n+          verbosity: Constrains the verbosity of the model's response. Lower values will result in\n+              more concise responses, while higher values will result in more verbose\n+              responses. Currently supported values are `low`, `medium`, and `high`.\n+\n           web_search_options: This tool searches the web for relevant results to use in a response. Learn more\n               about the\n               [web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).\n@@ -2544,6 +2576,7 @@ class AsyncCompletions(AsyncAPIResource):\n         top_logprobs: Optional[int] | NotGiven = NOT_GIVEN,\n         top_p: Optional[float] | NotGiven = NOT_GIVEN,\n         user: str | NotGiven = NOT_GIVEN,\n+        verbosity: Optional[Literal[\"low\", \"medium\", \"high\"]] | NotGiven = NOT_GIVEN,\n         web_search_options: completion_create_params.WebSearchOptions | NotGiven = NOT_GIVEN,\n         # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n         # The extra values given here take precedence over values defined on the client or passed to this method.\n@@ -2589,6 +2622,7 @@ class AsyncCompletions(AsyncAPIResource):\n                     \"top_logprobs\": top_logprobs,\n                     \"top_p\": top_p,\n                     \"user\": user,\n+                    \"verbosity\": verbosity,\n                     \"web_search_options\": web_search_options,\n                 },\n                 completion_create_params.CompletionCreateParamsStreaming\n@@ -2817,6 +2851,7 @@ class AsyncCompletions(AsyncAPIResource):\n         top_logprobs: Optional[int] | NotGiven = NOT_GIVEN,\n         top_p: Optional[float] | NotGiven = NOT_GIVEN,\n         user: str | NotGiven = NOT_GIVEN,\n+        verbosity: Optional[Literal[\"low\", \"medium\", \"high\"]] | NotGiven = NOT_GIVEN,\n         web_search_options: completion_create_params.WebSearchOptions | NotGiven = NOT_GIVEN,\n         # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n         # The extra values given here take precedence over values defined on the client or passed to this method.\n@@ -2887,11 +2922,12 @@ class AsyncCompletions(AsyncAPIResource):\n             top_logprobs=top_logprobs,\n             top_p=top_p,\n             user=user,\n+            verbosity=verbosity,\n+            web_search_options=web_search_options,\n             extra_headers=extra_headers,\n             extra_query=extra_query,\n             extra_body=extra_body,\n             timeout=timeout,\n-            web_search_options=web_search_options,\n         )\n         return AsyncChatCompletionStreamManager(\n             api_request,\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/resources/responses/responses.py",
            "diff": "diff --git a/src/openai/resources/responses/responses.py b/src/openai/resources/responses/responses.py\nindex 6d2b133..5ba2241 100644\n--- a/src/openai/resources/responses/responses.py\n+++ b/src/openai/resources/responses/responses.py\n@@ -93,6 +93,7 @@ class Responses(SyncAPIResource):\n         service_tier: Optional[Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"]] | NotGiven = NOT_GIVEN,\n         store: Optional[bool] | NotGiven = NOT_GIVEN,\n         stream: Optional[Literal[False]] | NotGiven = NOT_GIVEN,\n+        stream_options: Optional[response_create_params.StreamOptions] | NotGiven = NOT_GIVEN,\n         temperature: Optional[float] | NotGiven = NOT_GIVEN,\n         text: ResponseTextConfigParam | NotGiven = NOT_GIVEN,\n         tool_choice: response_create_params.ToolChoice | NotGiven = NOT_GIVEN,\n@@ -101,6 +102,7 @@ class Responses(SyncAPIResource):\n         top_p: Optional[float] | NotGiven = NOT_GIVEN,\n         truncation: Optional[Literal[\"auto\", \"disabled\"]] | NotGiven = NOT_GIVEN,\n         user: str | NotGiven = NOT_GIVEN,\n+        verbosity: Optional[Literal[\"low\", \"medium\", \"high\"]] | NotGiven = NOT_GIVEN,\n         # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n         # The extra values given here take precedence over values defined on the client or passed to this method.\n         extra_headers: Headers | None = None,\n@@ -232,6 +234,8 @@ class Responses(SyncAPIResource):\n               [Streaming section below](https://platform.openai.com/docs/api-reference/responses-streaming)\n               for more information.\n \n+          stream_options: Options for streaming responses. Only set this when you set `stream: true`.\n+\n           temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n               make the output more random, while lower values like 0.2 will make it more\n               focused and deterministic. We generally recommend altering this or `top_p` but\n@@ -259,8 +263,10 @@ class Responses(SyncAPIResource):\n                 Learn more about\n                 [built-in tools](https://platform.openai.com/docs/guides/tools).\n               - **Function calls (custom tools)**: Functions that are defined by you, enabling\n-                the model to call your own code. Learn more about\n+                the model to call your own code with strongly typed arguments and outputs.\n+                Learn more about\n                 [function calling](https://platform.openai.com/docs/guides/function-calling).\n+                You can also use custom tools to call your own code.\n \n           top_logprobs: An integer between 0 and 20 specifying the number of most likely tokens to\n               return at each token position, each with an associated log probability.\n@@ -285,6 +291,10 @@ class Responses(SyncAPIResource):\n               similar requests and to help OpenAI detect and prevent abuse.\n               [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).\n \n+          verbosity: Constrains the verbosity of the model's response. Lower values will result in\n+              more concise responses, while higher values will result in more verbose\n+              responses. Currently supported values are `low`, `medium`, and `high`.\n+\n           extra_headers: Send extra headers\n \n           extra_query: Add additional query parameters to the request\n@@ -316,6 +326,7 @@ class Responses(SyncAPIResource):\n         safety_identifier: str | NotGiven = NOT_GIVEN,\n         service_tier: Optional[Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"]] | NotGiven = NOT_GIVEN,\n         store: Optional[bool] | NotGiven = NOT_GIVEN,\n+        stream_options: Optional[response_create_params.StreamOptions] | NotGiven = NOT_GIVEN,\n         temperature: Optional[float] | NotGiven = NOT_GIVEN,\n         text: ResponseTextConfigParam | NotGiven = NOT_GIVEN,\n         tool_choice: response_create_params.ToolChoice | NotGiven = NOT_GIVEN,\n@@ -324,6 +335,7 @@ class Responses(SyncAPIResource):\n         top_p: Optional[float] | NotGiven = NOT_GIVEN,\n         truncation: Optional[Literal[\"auto\", \"disabled\"]] | NotGiven = NOT_GIVEN,\n         user: str | NotGiven = NOT_GIVEN,\n+        verbosity: Optional[Literal[\"low\", \"medium\", \"high\"]] | NotGiven = NOT_GIVEN,\n         # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n         # The extra values given here take precedence over values defined on the client or passed to this method.\n         extra_headers: Headers | None = None,\n@@ -455,6 +467,8 @@ class Responses(SyncAPIResource):\n \n           store: Whether to store the generated model response for later retrieval via API.\n \n+          stream_options: Options for streaming responses. Only set this when you set `stream: true`.\n+\n           temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n               make the output more random, while lower values like 0.2 will make it more\n               focused and deterministic. We generally recommend altering this or `top_p` but\n@@ -482,8 +496,10 @@ class Responses(SyncAPIResource):\n                 Learn more about\n                 [built-in tools](https://platform.openai.com/docs/guides/tools).\n               - **Function calls (custom tools)**: Functions that are defined by you, enabling\n-                the model to call your own code. Learn more about\n+                the model to call your own code with strongly typed arguments and outputs.\n+                Learn more about\n                 [function calling](https://platform.openai.com/docs/guides/function-calling).\n+                You can also use custom tools to call your own code.\n \n           top_logprobs: An integer between 0 and 20 specifying the number of most likely tokens to\n               return at each token position, each with an associated log probability.\n@@ -508,6 +524,10 @@ class Responses(SyncAPIResource):\n               similar requests and to help OpenAI detect and prevent abuse.\n               [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).\n \n+          verbosity: Constrains the verbosity of the model's response. Lower values will result in\n+              more concise responses, while higher values will result in more verbose\n+              responses. Currently supported values are `low`, `medium`, and `high`.\n+\n           extra_headers: Send extra headers\n \n           extra_query: Add additional query parameters to the request\n@@ -539,6 +559,7 @@ class Responses(SyncAPIResource):\n         safety_identifier: str | NotGiven = NOT_GIVEN,\n         service_tier: Optional[Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"]] | NotGiven = NOT_GIVEN,\n         store: Optional[bool] | NotGiven = NOT_GIVEN,\n+        stream_options: Optional[response_create_params.StreamOptions] | NotGiven = NOT_GIVEN,\n         temperature: Optional[float] | NotGiven = NOT_GIVEN,\n         text: ResponseTextConfigParam | NotGiven = NOT_GIVEN,\n         tool_choice: response_create_params.ToolChoice | NotGiven = NOT_GIVEN,\n@@ -547,6 +568,7 @@ class Responses(SyncAPIResource):\n         top_p: Optional[float] | NotGiven = NOT_GIVEN,\n         truncation: Optional[Literal[\"auto\", \"disabled\"]] | NotGiven = NOT_GIVEN,\n         user: str | NotGiven = NOT_GIVEN,\n+        verbosity: Optional[Literal[\"low\", \"medium\", \"high\"]] | NotGiven = NOT_GIVEN,\n         # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n         # The extra values given here take precedence over values defined on the client or passed to this method.\n         extra_headers: Headers | None = None,\n@@ -678,6 +700,8 @@ class Responses(SyncAPIResource):\n \n           store: Whether to store the generated model response for later retrieval via API.\n \n+          stream_options: Options for streaming responses. Only set this when you set `stream: true`.\n+\n           temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n               make the output more random, while lower values like 0.2 will make it more\n               focused and deterministic. We generally recommend altering this or `top_p` but\n@@ -705,8 +729,10 @@ class Responses(SyncAPIResource):\n                 Learn more about\n                 [built-in tools](https://platform.openai.com/docs/guides/tools).\n               - **Function calls (custom tools)**: Functions that are defined by you, enabling\n-                the model to call your own code. Learn more about\n+                the model to call your own code with strongly typed arguments and outputs.\n+                Learn more about\n                 [function calling](https://platform.openai.com/docs/guides/function-calling).\n+                You can also use custom tools to call your own code.\n \n           top_logprobs: An integer between 0 and 20 specifying the number of most likely tokens to\n               return at each token position, each with an associated log probability.\n@@ -731,6 +757,10 @@ class Responses(SyncAPIResource):\n               similar requests and to help OpenAI detect and prevent abuse.\n               [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).\n \n+          verbosity: Constrains the verbosity of the model's response. Lower values will result in\n+              more concise responses, while higher values will result in more verbose\n+              responses. Currently supported values are `low`, `medium`, and `high`.\n+\n           extra_headers: Send extra headers\n \n           extra_query: Add additional query parameters to the request\n@@ -761,6 +791,7 @@ class Responses(SyncAPIResource):\n         service_tier: Optional[Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"]] | NotGiven = NOT_GIVEN,\n         store: Optional[bool] | NotGiven = NOT_GIVEN,\n         stream: Optional[Literal[False]] | Literal[True] | NotGiven = NOT_GIVEN,\n+        stream_options: Optional[response_create_params.StreamOptions] | NotGiven = NOT_GIVEN,\n         temperature: Optional[float] | NotGiven = NOT_GIVEN,\n         text: ResponseTextConfigParam | NotGiven = NOT_GIVEN,\n         tool_choice: response_create_params.ToolChoice | NotGiven = NOT_GIVEN,\n@@ -769,6 +800,7 @@ class Responses(SyncAPIResource):\n         top_p: Optional[float] | NotGiven = NOT_GIVEN,\n         truncation: Optional[Literal[\"auto\", \"disabled\"]] | NotGiven = NOT_GIVEN,\n         user: str | NotGiven = NOT_GIVEN,\n+        verbosity: Optional[Literal[\"low\", \"medium\", \"high\"]] | NotGiven = NOT_GIVEN,\n         # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n         # The extra values given here take precedence over values defined on the client or passed to this method.\n         extra_headers: Headers | None = None,\n@@ -797,6 +829,7 @@ class Responses(SyncAPIResource):\n                     \"service_tier\": service_tier,\n                     \"store\": store,\n                     \"stream\": stream,\n+                    \"stream_options\": stream_options,\n                     \"temperature\": temperature,\n                     \"text\": text,\n                     \"tool_choice\": tool_choice,\n@@ -805,6 +838,7 @@ class Responses(SyncAPIResource):\n                     \"top_p\": top_p,\n                     \"truncation\": truncation,\n                     \"user\": user,\n+                    \"verbosity\": verbosity,\n                 },\n                 response_create_params.ResponseCreateParamsStreaming\n                 if stream\n@@ -850,6 +884,7 @@ class Responses(SyncAPIResource):\n         previous_response_id: Optional[str] | NotGiven = NOT_GIVEN,\n         reasoning: Optional[Reasoning] | NotGiven = NOT_GIVEN,\n         store: Optional[bool] | NotGiven = NOT_GIVEN,\n+        stream_options: Optional[response_create_params.StreamOptions] | NotGiven = NOT_GIVEN,\n         temperature: Optional[float] | NotGiven = NOT_GIVEN,\n         text: ResponseTextConfigParam | NotGiven = NOT_GIVEN,\n         tool_choice: response_create_params.ToolChoice | NotGiven = NOT_GIVEN,\n@@ -881,6 +916,7 @@ class Responses(SyncAPIResource):\n         previous_response_id: Optional[str] | NotGiven = NOT_GIVEN,\n         reasoning: Optional[Reasoning] | NotGiven = NOT_GIVEN,\n         store: Optional[bool] | NotGiven = NOT_GIVEN,\n+        stream_options: Optional[response_create_params.StreamOptions] | NotGiven = NOT_GIVEN,\n         temperature: Optional[float] | NotGiven = NOT_GIVEN,\n         text: ResponseTextConfigParam | NotGiven = NOT_GIVEN,\n         tool_choice: response_create_params.ToolChoice | NotGiven = NOT_GIVEN,\n@@ -906,6 +942,7 @@ class Responses(SyncAPIResource):\n             \"previous_response_id\": previous_response_id,\n             \"reasoning\": reasoning,\n             \"store\": store,\n+            \"stream_options\": stream_options,\n             \"temperature\": temperature,\n             \"text\": text,\n             \"tool_choice\": tool_choice,\n@@ -950,6 +987,7 @@ class Responses(SyncAPIResource):\n                 parallel_tool_calls=parallel_tool_calls,\n                 previous_response_id=previous_response_id,\n                 store=store,\n+                stream_options=stream_options,\n                 stream=True,\n                 temperature=temperature,\n                 text=text,\n@@ -1007,6 +1045,7 @@ class Responses(SyncAPIResource):\n         service_tier: Optional[Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"]] | NotGiven = NOT_GIVEN,\n         store: Optional[bool] | NotGiven = NOT_GIVEN,\n         stream: Optional[Literal[False]] | Literal[True] | NotGiven = NOT_GIVEN,\n+        stream_options: Optional[response_create_params.StreamOptions] | NotGiven = NOT_GIVEN,\n         temperature: Optional[float] | NotGiven = NOT_GIVEN,\n         text: ResponseTextConfigParam | NotGiven = NOT_GIVEN,\n         tool_choice: response_create_params.ToolChoice | NotGiven = NOT_GIVEN,\n@@ -1015,6 +1054,7 @@ class Responses(SyncAPIResource):\n         top_p: Optional[float] | NotGiven = NOT_GIVEN,\n         truncation: Optional[Literal[\"auto\", \"disabled\"]] | NotGiven = NOT_GIVEN,\n         user: str | NotGiven = NOT_GIVEN,\n+        verbosity: Optional[Literal[\"low\", \"medium\", \"high\"]] | NotGiven = NOT_GIVEN,\n         # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n         # The extra values given here take precedence over values defined on the client or passed to this method.\n         extra_headers: Headers | None = None,\n@@ -1061,6 +1101,7 @@ class Responses(SyncAPIResource):\n                     \"service_tier\": service_tier,\n                     \"store\": store,\n                     \"stream\": stream,\n+                    \"stream_options\": stream_options,\n                     \"temperature\": temperature,\n                     \"text\": text,\n                     \"tool_choice\": tool_choice,\n@@ -1069,6 +1110,7 @@ class Responses(SyncAPIResource):\n                     \"top_p\": top_p,\n                     \"truncation\": truncation,\n                     \"user\": user,\n+                    \"verbosity\": verbosity,\n                 },\n                 response_create_params.ResponseCreateParams,\n             ),\n@@ -1090,6 +1132,7 @@ class Responses(SyncAPIResource):\n         response_id: str,\n         *,\n         include: List[ResponseIncludable] | NotGiven = NOT_GIVEN,\n+        include_obfuscation: bool | NotGiven = NOT_GIVEN,\n         starting_after: int | NotGiven = NOT_GIVEN,\n         stream: Literal[False] | NotGiven = NOT_GIVEN,\n         # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n@@ -1154,6 +1197,13 @@ class Responses(SyncAPIResource):\n           include: Additional fields to include in the response. See the `include` parameter for\n               Response creation above for more information.\n \n+          include_obfuscation: When true, stream obfuscation will be enabled. Stream obfuscation adds random\n+              characters to an `obfuscation` field on streaming delta events to normalize\n+              payload sizes as a mitigation to certain side-channel attacks. These obfuscation\n+              fields are included by default, but add a small amount of overhead to the data\n+              stream. You can set `include_obfuscation` to false to optimize for bandwidth if\n+              you trust the network links between your application and the OpenAI API.\n+\n           starting_after: The sequence number of the event after which to start streaming.\n \n           stream: If set to true, the model response data will be streamed to the client as it is\n@@ -1180,6 +1230,7 @@ class Responses(SyncAPIResource):\n         *,\n         stream: Literal[True],\n         include: List[ResponseIncludable] | NotGiven = NOT_GIVEN,\n+        include_obfuscation: bool | NotGiven = NOT_GIVEN,\n         starting_after: int | NotGiven = NOT_GIVEN,\n         # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n         # The extra values given here take precedence over values defined on the client or passed to this method.\n@@ -1202,6 +1253,13 @@ class Responses(SyncAPIResource):\n           include: Additional fields to include in the response. See the `include` parameter for\n               Response creation above for more information.\n \n+          include_obfuscation: When true, stream obfuscation will be enabled. Stream obfuscation adds random\n+              characters to an `obfuscation` field on streaming delta events to normalize\n+              payload sizes as a mitigation to certain side-channel attacks. These obfuscation\n+              fields are included by default, but add a small amount of overhead to the data\n+              stream. You can set `include_obfuscation` to false to optimize for bandwidth if\n+              you trust the network links between your application and the OpenAI API.\n+\n           starting_after: The sequence number of the event after which to start streaming.\n \n           extra_headers: Send extra headers\n@@ -1221,6 +1279,7 @@ class Responses(SyncAPIResource):\n         *,\n         stream: bool,\n         include: List[ResponseIncludable] | NotGiven = NOT_GIVEN,\n+        include_obfuscation: bool | NotGiven = NOT_GIVEN,\n         starting_after: int | NotGiven = NOT_GIVEN,\n         # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n         # The extra values given here take precedence over values defined on the client or passed to this method.\n@@ -1243,6 +1302,13 @@ class Responses(SyncAPIResource):\n           include: Additional fields to include in the response. See the `include` parameter for\n               Response creation above for more information.\n \n+          include_obfuscation: When true, stream obfuscation will be enabled. Stream obfuscation adds random\n+              characters to an `obfuscation` field on streaming delta events to normalize\n+              payload sizes as a mitigation to certain side-channel attacks. These obfuscation\n+              fields are included by default, but add a small amount of overhead to the data\n+              stream. You can set `include_obfuscation` to false to optimize for bandwidth if\n+              you trust the network links between your application and the OpenAI API.\n+\n           starting_after: The sequence number of the event after which to start streaming.\n \n           extra_headers: Send extra headers\n@@ -1260,6 +1326,7 @@ class Responses(SyncAPIResource):\n         response_id: str,\n         *,\n         include: List[ResponseIncludable] | NotGiven = NOT_GIVEN,\n+        include_obfuscation: bool | NotGiven = NOT_GIVEN,\n         starting_after: int | NotGiven = NOT_GIVEN,\n         stream: Literal[False] | Literal[True] | NotGiven = NOT_GIVEN,\n         # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n@@ -1281,6 +1348,7 @@ class Responses(SyncAPIResource):\n                 query=maybe_transform(\n                     {\n                         \"include\": include,\n+                        \"include_obfuscation\": include_obfuscation,\n                         \"starting_after\": starting_after,\n                         \"stream\": stream,\n                     },\n@@ -1408,6 +1476,7 @@ class AsyncResponses(AsyncAPIResource):\n         service_tier: Optional[Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"]] | NotGiven = NOT_GIVEN,\n         store: Optional[bool] | NotGiven = NOT_GIVEN,\n         stream: Optional[Literal[False]] | NotGiven = NOT_GIVEN,\n+        stream_options: Optional[response_create_params.StreamOptions] | NotGiven = NOT_GIVEN,\n         temperature: Optional[float] | NotGiven = NOT_GIVEN,\n         text: ResponseTextConfigParam | NotGiven = NOT_GIVEN,\n         tool_choice: response_create_params.ToolChoice | NotGiven = NOT_GIVEN,\n@@ -1416,6 +1485,7 @@ class AsyncResponses(AsyncAPIResource):\n         top_p: Optional[float] | NotGiven = NOT_GIVEN,\n         truncation: Optional[Literal[\"auto\", \"disabled\"]] | NotGiven = NOT_GIVEN,\n         user: str | NotGiven = NOT_GIVEN,\n+        verbosity: Optional[Literal[\"low\", \"medium\", \"high\"]] | NotGiven = NOT_GIVEN,\n         # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n         # The extra values given here take precedence over values defined on the client or passed to this method.\n         extra_headers: Headers | None = None,\n@@ -1547,6 +1617,8 @@ class AsyncResponses(AsyncAPIResource):\n               [Streaming section below](https://platform.openai.com/docs/api-reference/responses-streaming)\n               for more information.\n \n+          stream_options: Options for streaming responses. Only set this when you set `stream: true`.\n+\n           temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n               make the output more random, while lower values like 0.2 will make it more\n               focused and deterministic. We generally recommend altering this or `top_p` but\n@@ -1574,8 +1646,10 @@ class AsyncResponses(AsyncAPIResource):\n                 Learn more about\n                 [built-in tools](https://platform.openai.com/docs/guides/tools).\n               - **Function calls (custom tools)**: Functions that are defined by you, enabling\n-                the model to call your own code. Learn more about\n+                the model to call your own code with strongly typed arguments and outputs.\n+                Learn more about\n                 [function calling](https://platform.openai.com/docs/guides/function-calling).\n+                You can also use custom tools to call your own code.\n \n           top_logprobs: An integer between 0 and 20 specifying the number of most likely tokens to\n               return at each token position, each with an associated log probability.\n@@ -1600,6 +1674,10 @@ class AsyncResponses(AsyncAPIResource):\n               similar requests and to help OpenAI detect and prevent abuse.\n               [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).\n \n+          verbosity: Constrains the verbosity of the model's response. Lower values will result in\n+              more concise responses, while higher values will result in more verbose\n+              responses. Currently supported values are `low`, `medium`, and `high`.\n+\n           extra_headers: Send extra headers\n \n           extra_query: Add additional query parameters to the request\n@@ -1631,6 +1709,7 @@ class AsyncResponses(AsyncAPIResource):\n         safety_identifier: str | NotGiven = NOT_GIVEN,\n         service_tier: Optional[Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"]] | NotGiven = NOT_GIVEN,\n         store: Optional[bool] | NotGiven = NOT_GIVEN,\n+        stream_options: Optional[response_create_params.StreamOptions] | NotGiven = NOT_GIVEN,\n         temperature: Optional[float] | NotGiven = NOT_GIVEN,\n         text: ResponseTextConfigParam | NotGiven = NOT_GIVEN,\n         tool_choice: response_create_params.ToolChoice | NotGiven = NOT_GIVEN,\n@@ -1639,6 +1718,7 @@ class AsyncResponses(AsyncAPIResource):\n         top_p: Optional[float] | NotGiven = NOT_GIVEN,\n         truncation: Optional[Literal[\"auto\", \"disabled\"]] | NotGiven = NOT_GIVEN,\n         user: str | NotGiven = NOT_GIVEN,\n+        verbosity: Optional[Literal[\"low\", \"medium\", \"high\"]] | NotGiven = NOT_GIVEN,\n         # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n         # The extra values given here take precedence over values defined on the client or passed to this method.\n         extra_headers: Headers | None = None,\n@@ -1770,6 +1850,8 @@ class AsyncResponses(AsyncAPIResource):\n \n           store: Whether to store the generated model response for later retrieval via API.\n \n+          stream_options: Options for streaming responses. Only set this when you set `stream: true`.\n+\n           temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n               make the output more random, while lower values like 0.2 will make it more\n               focused and deterministic. We generally recommend altering this or `top_p` but\n@@ -1797,8 +1879,10 @@ class AsyncResponses(AsyncAPIResource):\n                 Learn more about\n                 [built-in tools](https://platform.openai.com/docs/guides/tools).\n               - **Function calls (custom tools)**: Functions that are defined by you, enabling\n-                the model to call your own code. Learn more about\n+                the model to call your own code with strongly typed arguments and outputs.\n+                Learn more about\n                 [function calling](https://platform.openai.com/docs/guides/function-calling).\n+                You can also use custom tools to call your own code.\n \n           top_logprobs: An integer between 0 and 20 specifying the number of most likely tokens to\n               return at each token position, each with an associated log probability.\n@@ -1823,6 +1907,10 @@ class AsyncResponses(AsyncAPIResource):\n               similar requests and to help OpenAI detect and prevent abuse.\n               [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).\n \n+          verbosity: Constrains the verbosity of the model's response. Lower values will result in\n+              more concise responses, while higher values will result in more verbose\n+              responses. Currently supported values are `low`, `medium`, and `high`.\n+\n           extra_headers: Send extra headers\n \n           extra_query: Add additional query parameters to the request\n@@ -1854,6 +1942,7 @@ class AsyncResponses(AsyncAPIResource):\n         safety_identifier: str | NotGiven = NOT_GIVEN,\n         service_tier: Optional[Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"]] | NotGiven = NOT_GIVEN,\n         store: Optional[bool] | NotGiven = NOT_GIVEN,\n+        stream_options: Optional[response_create_params.StreamOptions] | NotGiven = NOT_GIVEN,\n         temperature: Optional[float] | NotGiven = NOT_GIVEN,\n         text: ResponseTextConfigParam | NotGiven = NOT_GIVEN,\n         tool_choice: response_create_params.ToolChoice | NotGiven = NOT_GIVEN,\n@@ -1862,6 +1951,7 @@ class AsyncResponses(AsyncAPIResource):\n         top_p: Optional[float] | NotGiven = NOT_GIVEN,\n         truncation: Optional[Literal[\"auto\", \"disabled\"]] | NotGiven = NOT_GIVEN,\n         user: str | NotGiven = NOT_GIVEN,\n+        verbosity: Optional[Literal[\"low\", \"medium\", \"high\"]] | NotGiven = NOT_GIVEN,\n         # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n         # The extra values given here take precedence over values defined on the client or passed to this method.\n         extra_headers: Headers | None = None,\n@@ -1993,6 +2083,8 @@ class AsyncResponses(AsyncAPIResource):\n \n           store: Whether to store the generated model response for later retrieval via API.\n \n+          stream_options: Options for streaming responses. Only set this when you set `stream: true`.\n+\n           temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n               make the output more random, while lower values like 0.2 will make it more\n               focused and deterministic. We generally recommend altering this or `top_p` but\n@@ -2020,8 +2112,10 @@ class AsyncResponses(AsyncAPIResource):\n                 Learn more about\n                 [built-in tools](https://platform.openai.com/docs/guides/tools).\n               - **Function calls (custom tools)**: Functions that are defined by you, enabling\n-                the model to call your own code. Learn more about\n+                the model to call your own code with strongly typed arguments and outputs.\n+                Learn more about\n                 [function calling](https://platform.openai.com/docs/guides/function-calling).\n+                You can also use custom tools to call your own code.\n \n           top_logprobs: An integer between 0 and 20 specifying the number of most likely tokens to\n               return at each token position, each with an associated log probability.\n@@ -2046,6 +2140,10 @@ class AsyncResponses(AsyncAPIResource):\n               similar requests and to help OpenAI detect and prevent abuse.\n               [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).\n \n+          verbosity: Constrains the verbosity of the model's response. Lower values will result in\n+              more concise responses, while higher values will result in more verbose\n+              responses. Currently supported values are `low`, `medium`, and `high`.\n+\n           extra_headers: Send extra headers\n \n           extra_query: Add additional query parameters to the request\n@@ -2076,6 +2174,7 @@ class AsyncResponses(AsyncAPIResource):\n         service_tier: Optional[Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"]] | NotGiven = NOT_GIVEN,\n         store: Optional[bool] | NotGiven = NOT_GIVEN,\n         stream: Optional[Literal[False]] | Literal[True] | NotGiven = NOT_GIVEN,\n+        stream_options: Optional[response_create_params.StreamOptions] | NotGiven = NOT_GIVEN,\n         temperature: Optional[float] | NotGiven = NOT_GIVEN,\n         text: ResponseTextConfigParam | NotGiven = NOT_GIVEN,\n         tool_choice: response_create_params.ToolChoice | NotGiven = NOT_GIVEN,\n@@ -2084,6 +2183,7 @@ class AsyncResponses(AsyncAPIResource):\n         top_p: Optional[float] | NotGiven = NOT_GIVEN,\n         truncation: Optional[Literal[\"auto\", \"disabled\"]] | NotGiven = NOT_GIVEN,\n         user: str | NotGiven = NOT_GIVEN,\n+        verbosity: Optional[Literal[\"low\", \"medium\", \"high\"]] | NotGiven = NOT_GIVEN,\n         # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n         # The extra values given here take precedence over values defined on the client or passed to this method.\n         extra_headers: Headers | None = None,\n@@ -2112,6 +2212,7 @@ class AsyncResponses(AsyncAPIResource):\n                     \"service_tier\": service_tier,\n                     \"store\": store,\n                     \"stream\": stream,\n+                    \"stream_options\": stream_options,\n                     \"temperature\": temperature,\n                     \"text\": text,\n                     \"tool_choice\": tool_choice,\n@@ -2120,6 +2221,7 @@ class AsyncResponses(AsyncAPIResource):\n                     \"top_p\": top_p,\n                     \"truncation\": truncation,\n                     \"user\": user,\n+                    \"verbosity\": verbosity,\n                 },\n                 response_create_params.ResponseCreateParamsStreaming\n                 if stream\n@@ -2165,6 +2267,7 @@ class AsyncResponses(AsyncAPIResource):\n         previous_response_id: Optional[str] | NotGiven = NOT_GIVEN,\n         reasoning: Optional[Reasoning] | NotGiven = NOT_GIVEN,\n         store: Optional[bool] | NotGiven = NOT_GIVEN,\n+        stream_options: Optional[response_create_params.StreamOptions] | NotGiven = NOT_GIVEN,\n         temperature: Optional[float] | NotGiven = NOT_GIVEN,\n         text: ResponseTextConfigParam | NotGiven = NOT_GIVEN,\n         tool_choice: response_create_params.ToolChoice | NotGiven = NOT_GIVEN,\n@@ -2196,6 +2299,7 @@ class AsyncResponses(AsyncAPIResource):\n         previous_response_id: Optional[str] | NotGiven = NOT_GIVEN,\n         reasoning: Optional[Reasoning] | NotGiven = NOT_GIVEN,\n         store: Optional[bool] | NotGiven = NOT_GIVEN,\n+        stream_options: Optional[response_create_params.StreamOptions] | NotGiven = NOT_GIVEN,\n         temperature: Optional[float] | NotGiven = NOT_GIVEN,\n         text: ResponseTextConfigParam | NotGiven = NOT_GIVEN,\n         tool_choice: response_create_params.ToolChoice | NotGiven = NOT_GIVEN,\n@@ -2221,6 +2325,7 @@ class AsyncResponses(AsyncAPIResource):\n             \"previous_response_id\": previous_response_id,\n             \"reasoning\": reasoning,\n             \"store\": store,\n+            \"stream_options\": stream_options,\n             \"temperature\": temperature,\n             \"text\": text,\n             \"tool_choice\": tool_choice,\n@@ -2266,6 +2371,7 @@ class AsyncResponses(AsyncAPIResource):\n                 parallel_tool_calls=parallel_tool_calls,\n                 previous_response_id=previous_response_id,\n                 store=store,\n+                stream_options=stream_options,\n                 temperature=temperature,\n                 text=text,\n                 tool_choice=tool_choice,\n@@ -2326,6 +2432,7 @@ class AsyncResponses(AsyncAPIResource):\n         service_tier: Optional[Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"]] | NotGiven = NOT_GIVEN,\n         store: Optional[bool] | NotGiven = NOT_GIVEN,\n         stream: Optional[Literal[False]] | Literal[True] | NotGiven = NOT_GIVEN,\n+        stream_options: Optional[response_create_params.StreamOptions] | NotGiven = NOT_GIVEN,\n         temperature: Optional[float] | NotGiven = NOT_GIVEN,\n         text: ResponseTextConfigParam | NotGiven = NOT_GIVEN,\n         tool_choice: response_create_params.ToolChoice | NotGiven = NOT_GIVEN,\n@@ -2334,6 +2441,7 @@ class AsyncResponses(AsyncAPIResource):\n         top_p: Optional[float] | NotGiven = NOT_GIVEN,\n         truncation: Optional[Literal[\"auto\", \"disabled\"]] | NotGiven = NOT_GIVEN,\n         user: str | NotGiven = NOT_GIVEN,\n+        verbosity: Optional[Literal[\"low\", \"medium\", \"high\"]] | NotGiven = NOT_GIVEN,\n         # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n         # The extra values given here take precedence over values defined on the client or passed to this method.\n         extra_headers: Headers | None = None,\n@@ -2380,6 +2488,7 @@ class AsyncResponses(AsyncAPIResource):\n                     \"service_tier\": service_tier,\n                     \"store\": store,\n                     \"stream\": stream,\n+                    \"stream_options\": stream_options,\n                     \"temperature\": temperature,\n                     \"text\": text,\n                     \"tool_choice\": tool_choice,\n@@ -2388,6 +2497,7 @@ class AsyncResponses(AsyncAPIResource):\n                     \"top_p\": top_p,\n                     \"truncation\": truncation,\n                     \"user\": user,\n+                    \"verbosity\": verbosity,\n                 },\n                 response_create_params.ResponseCreateParams,\n             ),\n@@ -2409,6 +2519,7 @@ class AsyncResponses(AsyncAPIResource):\n         response_id: str,\n         *,\n         include: List[ResponseIncludable] | NotGiven = NOT_GIVEN,\n+        include_obfuscation: bool | NotGiven = NOT_GIVEN,\n         starting_after: int | NotGiven = NOT_GIVEN,\n         stream: Literal[False] | NotGiven = NOT_GIVEN,\n         # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n@@ -2473,6 +2584,13 @@ class AsyncResponses(AsyncAPIResource):\n           include: Additional fields to include in the response. See the `include` parameter for\n               Response creation above for more information.\n \n+          include_obfuscation: When true, stream obfuscation will be enabled. Stream obfuscation adds random\n+              characters to an `obfuscation` field on streaming delta events to normalize\n+              payload sizes as a mitigation to certain side-channel attacks. These obfuscation\n+              fields are included by default, but add a small amount of overhead to the data\n+              stream. You can set `include_obfuscation` to false to optimize for bandwidth if\n+              you trust the network links between your application and the OpenAI API.\n+\n           starting_after: The sequence number of the event after which to start streaming.\n \n           stream: If set to true, the model response data will be streamed to the client as it is\n@@ -2499,6 +2617,7 @@ class AsyncResponses(AsyncAPIResource):\n         *,\n         stream: Literal[True],\n         include: List[ResponseIncludable] | NotGiven = NOT_GIVEN,\n+        include_obfuscation: bool | NotGiven = NOT_GIVEN,\n         starting_after: int | NotGiven = NOT_GIVEN,\n         # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n         # The extra values given here take precedence over values defined on the client or passed to this method.\n@@ -2521,6 +2640,13 @@ class AsyncResponses(AsyncAPIResource):\n           include: Additional fields to include in the response. See the `include` parameter for\n               Response creation above for more information.\n \n+          include_obfuscation: When true, stream obfuscation will be enabled. Stream obfuscation adds random\n+              characters to an `obfuscation` field on streaming delta events to normalize\n+              payload sizes as a mitigation to certain side-channel attacks. These obfuscation\n+              fields are included by default, but add a small amount of overhead to the data\n+              stream. You can set `include_obfuscation` to false to optimize for bandwidth if\n+              you trust the network links between your application and the OpenAI API.\n+\n           starting_after: The sequence number of the event after which to start streaming.\n \n           extra_headers: Send extra headers\n@@ -2540,6 +2666,7 @@ class AsyncResponses(AsyncAPIResource):\n         *,\n         stream: bool,\n         include: List[ResponseIncludable] | NotGiven = NOT_GIVEN,\n+        include_obfuscation: bool | NotGiven = NOT_GIVEN,\n         starting_after: int | NotGiven = NOT_GIVEN,\n         # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n         # The extra values given here take precedence over values defined on the client or passed to this method.\n@@ -2562,6 +2689,13 @@ class AsyncResponses(AsyncAPIResource):\n           include: Additional fields to include in the response. See the `include` parameter for\n               Response creation above for more information.\n \n+          include_obfuscation: When true, stream obfuscation will be enabled. Stream obfuscation adds random\n+              characters to an `obfuscation` field on streaming delta events to normalize\n+              payload sizes as a mitigation to certain side-channel attacks. These obfuscation\n+              fields are included by default, but add a small amount of overhead to the data\n+              stream. You can set `include_obfuscation` to false to optimize for bandwidth if\n+              you trust the network links between your application and the OpenAI API.\n+\n           starting_after: The sequence number of the event after which to start streaming.\n \n           extra_headers: Send extra headers\n@@ -2579,6 +2713,7 @@ class AsyncResponses(AsyncAPIResource):\n         response_id: str,\n         *,\n         include: List[ResponseIncludable] | NotGiven = NOT_GIVEN,\n+        include_obfuscation: bool | NotGiven = NOT_GIVEN,\n         starting_after: int | NotGiven = NOT_GIVEN,\n         stream: Literal[False] | Literal[True] | NotGiven = NOT_GIVEN,\n         # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n@@ -2600,6 +2735,7 @@ class AsyncResponses(AsyncAPIResource):\n                 query=await async_maybe_transform(\n                     {\n                         \"include\": include,\n+                        \"include_obfuscation\": include_obfuscation,\n                         \"starting_after\": starting_after,\n                         \"stream\": stream,\n                     },\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/__init__.py",
            "diff": "diff --git a/src/openai/types/__init__.py b/src/openai/types/__init__.py\nindex 51f3ee5..1844f71 100644\n--- a/src/openai/types/__init__.py\n+++ b/src/openai/types/__init__.py\n@@ -18,8 +18,11 @@ from .shared import (\n     FunctionDefinition as FunctionDefinition,\n     FunctionParameters as FunctionParameters,\n     ResponseFormatText as ResponseFormatText,\n+    CustomToolInputFormat as CustomToolInputFormat,\n     ResponseFormatJSONObject as ResponseFormatJSONObject,\n     ResponseFormatJSONSchema as ResponseFormatJSONSchema,\n+    ResponseFormatTextPython as ResponseFormatTextPython,\n+    ResponseFormatTextGrammar as ResponseFormatTextGrammar,\n )\n from .upload import Upload as Upload\n from .embedding import Embedding as Embedding\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/beta/assistant_create_params.py",
            "diff": "diff --git a/src/openai/types/beta/assistant_create_params.py b/src/openai/types/beta/assistant_create_params.py\nindex 8b3c331..4b03dc0 100644\n--- a/src/openai/types/beta/assistant_create_params.py\n+++ b/src/openai/types/beta/assistant_create_params.py\n@@ -58,12 +58,12 @@ class AssistantCreateParams(TypedDict, total=False):\n     \"\"\"The name of the assistant. The maximum length is 256 characters.\"\"\"\n \n     reasoning_effort: Optional[ReasoningEffort]\n-    \"\"\"**o-series models only**\n-\n+    \"\"\"\n     Constrains effort on reasoning for\n     [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n-    supported values are `low`, `medium`, and `high`. Reducing reasoning effort can\n-    result in faster responses and fewer tokens used on reasoning in a response.\n+    supported values are `minimal`, `low`, `medium`, and `high`. Reducing reasoning\n+    effort can result in faster responses and fewer tokens used on reasoning in a\n+    response.\n     \"\"\"\n \n     response_format: Optional[AssistantResponseFormatOptionParam]\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/beta/assistant_update_params.py",
            "diff": "diff --git a/src/openai/types/beta/assistant_update_params.py b/src/openai/types/beta/assistant_update_params.py\nindex b28094a..e032554 100644\n--- a/src/openai/types/beta/assistant_update_params.py\n+++ b/src/openai/types/beta/assistant_update_params.py\n@@ -36,6 +36,12 @@ class AssistantUpdateParams(TypedDict, total=False):\n     model: Union[\n         str,\n         Literal[\n+            \"gpt-5\",\n+            \"gpt-5-mini\",\n+            \"gpt-5-nano\",\n+            \"gpt-5-2025-08-07\",\n+            \"gpt-5-mini-2025-08-07\",\n+            \"gpt-5-nano-2025-08-07\",\n             \"gpt-4.1\",\n             \"gpt-4.1-mini\",\n             \"gpt-4.1-nano\",\n@@ -87,12 +93,12 @@ class AssistantUpdateParams(TypedDict, total=False):\n     \"\"\"The name of the assistant. The maximum length is 256 characters.\"\"\"\n \n     reasoning_effort: Optional[ReasoningEffort]\n-    \"\"\"**o-series models only**\n-\n+    \"\"\"\n     Constrains effort on reasoning for\n     [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n-    supported values are `low`, `medium`, and `high`. Reducing reasoning effort can\n-    result in faster responses and fewer tokens used on reasoning in a response.\n+    supported values are `minimal`, `low`, `medium`, and `high`. Reducing reasoning\n+    effort can result in faster responses and fewer tokens used on reasoning in a\n+    response.\n     \"\"\"\n \n     response_format: Optional[AssistantResponseFormatOptionParam]\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/beta/threads/run_create_params.py",
            "diff": "diff --git a/src/openai/types/beta/threads/run_create_params.py b/src/openai/types/beta/threads/run_create_params.py\nindex fc70227..f9defcb 100644\n--- a/src/openai/types/beta/threads/run_create_params.py\n+++ b/src/openai/types/beta/threads/run_create_params.py\n@@ -108,12 +108,12 @@ class RunCreateParamsBase(TypedDict, total=False):\n     \"\"\"\n \n     reasoning_effort: Optional[ReasoningEffort]\n-    \"\"\"**o-series models only**\n-\n+    \"\"\"\n     Constrains effort on reasoning for\n     [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n-    supported values are `low`, `medium`, and `high`. Reducing reasoning effort can\n-    result in faster responses and fewer tokens used on reasoning in a response.\n+    supported values are `minimal`, `low`, `medium`, and `high`. Reducing reasoning\n+    effort can result in faster responses and fewer tokens used on reasoning in a\n+    response.\n     \"\"\"\n \n     response_format: Optional[AssistantResponseFormatOptionParam]\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/chat/__init__.py",
            "diff": "diff --git a/src/openai/types/chat/__init__.py b/src/openai/types/chat/__init__.py\nindex dc26198..ce1cf45 100644\n--- a/src/openai/types/chat/__init__.py\n+++ b/src/openai/types/chat/__init__.py\n@@ -4,7 +4,6 @@ from __future__ import annotations\n \n from .chat_completion import ChatCompletion as ChatCompletion\n from .chat_completion_role import ChatCompletionRole as ChatCompletionRole\n-from .chat_completion_tool import ChatCompletionTool as ChatCompletionTool\n from .chat_completion_audio import ChatCompletionAudio as ChatCompletionAudio\n from .chat_completion_chunk import ChatCompletionChunk as ChatCompletionChunk\n from .completion_list_params import CompletionListParams as CompletionListParams\n@@ -24,16 +23,20 @@ from .parsed_function_tool_call import (\n )\n from .chat_completion_tool_param import ChatCompletionToolParam as ChatCompletionToolParam\n from .chat_completion_audio_param import ChatCompletionAudioParam as ChatCompletionAudioParam\n+from .chat_completion_function_tool import ChatCompletionFunctionTool as ChatCompletionFunctionTool\n from .chat_completion_message_param import ChatCompletionMessageParam as ChatCompletionMessageParam\n from .chat_completion_store_message import ChatCompletionStoreMessage as ChatCompletionStoreMessage\n from .chat_completion_token_logprob import ChatCompletionTokenLogprob as ChatCompletionTokenLogprob\n from .chat_completion_reasoning_effort import ChatCompletionReasoningEffort as ChatCompletionReasoningEffort\n from .chat_completion_content_part_text import ChatCompletionContentPartText as ChatCompletionContentPartText\n+from .chat_completion_custom_tool_param import ChatCompletionCustomToolParam as ChatCompletionCustomToolParam\n from .chat_completion_message_tool_call import ChatCompletionMessageToolCall as ChatCompletionMessageToolCall\n from .chat_completion_content_part_image import ChatCompletionContentPartImage as ChatCompletionContentPartImage\n from .chat_completion_content_part_param import ChatCompletionContentPartParam as ChatCompletionContentPartParam\n from .chat_completion_tool_message_param import ChatCompletionToolMessageParam as ChatCompletionToolMessageParam\n from .chat_completion_user_message_param import ChatCompletionUserMessageParam as ChatCompletionUserMessageParam\n+from .chat_completion_allowed_tools_param import ChatCompletionAllowedToolsParam as ChatCompletionAllowedToolsParam\n+from .chat_completion_function_tool_param import ChatCompletionFunctionToolParam as ChatCompletionFunctionToolParam\n from .chat_completion_stream_options_param import ChatCompletionStreamOptionsParam as ChatCompletionStreamOptionsParam\n from .chat_completion_system_message_param import ChatCompletionSystemMessageParam as ChatCompletionSystemMessageParam\n from .chat_completion_function_message_param import (\n@@ -57,18 +60,36 @@ from .chat_completion_named_tool_choice_param import (\n from .chat_completion_content_part_image_param import (\n     ChatCompletionContentPartImageParam as ChatCompletionContentPartImageParam,\n )\n+from .chat_completion_message_custom_tool_call import (\n+    ChatCompletionMessageCustomToolCall as ChatCompletionMessageCustomToolCall,\n+)\n from .chat_completion_prediction_content_param import (\n     ChatCompletionPredictionContentParam as ChatCompletionPredictionContentParam,\n )\n from .chat_completion_tool_choice_option_param import (\n     ChatCompletionToolChoiceOptionParam as ChatCompletionToolChoiceOptionParam,\n )\n+from .chat_completion_allowed_tool_choice_param import (\n+    ChatCompletionAllowedToolChoiceParam as ChatCompletionAllowedToolChoiceParam,\n+)\n from .chat_completion_content_part_refusal_param import (\n     ChatCompletionContentPartRefusalParam as ChatCompletionContentPartRefusalParam,\n )\n from .chat_completion_function_call_option_param import (\n     ChatCompletionFunctionCallOptionParam as ChatCompletionFunctionCallOptionParam,\n )\n+from .chat_completion_message_function_tool_call import (\n+    ChatCompletionMessageFunctionToolCall as ChatCompletionMessageFunctionToolCall,\n+)\n from .chat_completion_content_part_input_audio_param import (\n     ChatCompletionContentPartInputAudioParam as ChatCompletionContentPartInputAudioParam,\n )\n+from .chat_completion_message_custom_tool_call_param import (\n+    ChatCompletionMessageCustomToolCallParam as ChatCompletionMessageCustomToolCallParam,\n+)\n+from .chat_completion_named_tool_choice_custom_param import (\n+    ChatCompletionNamedToolChoiceCustomParam as ChatCompletionNamedToolChoiceCustomParam,\n+)\n+from .chat_completion_message_function_tool_call_param import (\n+    ChatCompletionMessageFunctionToolCallParam as ChatCompletionMessageFunctionToolCallParam,\n+)\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/chat/chat_completion_allowed_tool_choice_param.py",
            "diff": "diff --git a/src/openai/types/chat/chat_completion_allowed_tool_choice_param.py b/src/openai/types/chat/chat_completion_allowed_tool_choice_param.py\nnew file mode 100644\nindex 0000000..813e629\n--- /dev/null\n+++ b/src/openai/types/chat/chat_completion_allowed_tool_choice_param.py\n@@ -0,0 +1,17 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing_extensions import Literal, Required, TypedDict\n+\n+from .chat_completion_allowed_tools_param import ChatCompletionAllowedToolsParam\n+\n+__all__ = [\"ChatCompletionAllowedToolChoiceParam\"]\n+\n+\n+class ChatCompletionAllowedToolChoiceParam(TypedDict, total=False):\n+    allowed_tools: Required[ChatCompletionAllowedToolsParam]\n+    \"\"\"Constrains the tools available to the model to a pre-defined set.\"\"\"\n+\n+    type: Required[Literal[\"allowed_tools\"]]\n+    \"\"\"Allowed tool configuration type. Always `allowed_tools`.\"\"\"\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/chat/chat_completion_allowed_tools_param.py",
            "diff": "diff --git a/src/openai/types/chat/chat_completion_allowed_tools_param.py b/src/openai/types/chat/chat_completion_allowed_tools_param.py\nnew file mode 100644\nindex 0000000..d9b72d8\n--- /dev/null\n+++ b/src/openai/types/chat/chat_completion_allowed_tools_param.py\n@@ -0,0 +1,32 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing import Dict, Iterable\n+from typing_extensions import Literal, Required, TypedDict\n+\n+__all__ = [\"ChatCompletionAllowedToolsParam\"]\n+\n+\n+class ChatCompletionAllowedToolsParam(TypedDict, total=False):\n+    mode: Required[Literal[\"auto\", \"required\"]]\n+    \"\"\"Constrains the tools available to the model to a pre-defined set.\n+\n+    `auto` allows the model to pick from among the allowed tools and generate a\n+    message.\n+\n+    `required` requires the model to call one or more of the allowed tools.\n+    \"\"\"\n+\n+    tools: Required[Iterable[Dict[str, object]]]\n+    \"\"\"A list of tool definitions that the model should be allowed to call.\n+\n+    For the Chat Completions API, the list of tool definitions might look like:\n+\n+    ```json\n+    [\n+      { \"type\": \"function\", \"function\": { \"name\": \"get_weather\" } },\n+      { \"type\": \"function\", \"function\": { \"name\": \"get_time\" } }\n+    ]\n+    ```\n+    \"\"\"\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/chat/chat_completion_custom_tool_param.py",
            "diff": "diff --git a/src/openai/types/chat/chat_completion_custom_tool_param.py b/src/openai/types/chat/chat_completion_custom_tool_param.py\nnew file mode 100644\nindex 0000000..14959ee\n--- /dev/null\n+++ b/src/openai/types/chat/chat_completion_custom_tool_param.py\n@@ -0,0 +1,58 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing import Union\n+from typing_extensions import Literal, Required, TypeAlias, TypedDict\n+\n+__all__ = [\n+    \"ChatCompletionCustomToolParam\",\n+    \"Custom\",\n+    \"CustomFormat\",\n+    \"CustomFormatText\",\n+    \"CustomFormatGrammar\",\n+    \"CustomFormatGrammarGrammar\",\n+]\n+\n+\n+class CustomFormatText(TypedDict, total=False):\n+    type: Required[Literal[\"text\"]]\n+    \"\"\"Unconstrained text format. Always `text`.\"\"\"\n+\n+\n+class CustomFormatGrammarGrammar(TypedDict, total=False):\n+    definition: Required[str]\n+    \"\"\"The grammar definition.\"\"\"\n+\n+    syntax: Required[Literal[\"lark\", \"regex\"]]\n+    \"\"\"The syntax of the grammar definition. One of `lark` or `regex`.\"\"\"\n+\n+\n+class CustomFormatGrammar(TypedDict, total=False):\n+    grammar: Required[CustomFormatGrammarGrammar]\n+    \"\"\"Your chosen grammar.\"\"\"\n+\n+    type: Required[Literal[\"grammar\"]]\n+    \"\"\"Grammar format. Always `grammar`.\"\"\"\n+\n+\n+CustomFormat: TypeAlias = Union[CustomFormatText, CustomFormatGrammar]\n+\n+\n+class Custom(TypedDict, total=False):\n+    name: Required[str]\n+    \"\"\"The name of the custom tool, used to identify it in tool calls.\"\"\"\n+\n+    description: str\n+    \"\"\"Optional description of the custom tool, used to provide more context.\"\"\"\n+\n+    format: CustomFormat\n+    \"\"\"The input format for the custom tool. Default is unconstrained text.\"\"\"\n+\n+\n+class ChatCompletionCustomToolParam(TypedDict, total=False):\n+    custom: Required[Custom]\n+    \"\"\"Properties of the custom tool.\"\"\"\n+\n+    type: Required[Literal[\"custom\"]]\n+    \"\"\"The type of the custom tool. Always `custom`.\"\"\"\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/chat/chat_completion_function_tool.py",
            "diff": "diff --git a/src/openai/types/chat/chat_completion_function_tool.py b/src/openai/types/chat/chat_completion_function_tool.py\nnew file mode 100644\nindex 0000000..641568a\n--- /dev/null\n+++ b/src/openai/types/chat/chat_completion_function_tool.py\n@@ -0,0 +1,15 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+from ..shared.function_definition import FunctionDefinition\n+\n+__all__ = [\"ChatCompletionFunctionTool\"]\n+\n+\n+class ChatCompletionFunctionTool(BaseModel):\n+    function: FunctionDefinition\n+\n+    type: Literal[\"function\"]\n+    \"\"\"The type of the tool. Currently, only `function` is supported.\"\"\"\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/chat/chat_completion_function_tool_param.py",
            "diff": "diff --git a/src/openai/types/chat/chat_completion_function_tool_param.py b/src/openai/types/chat/chat_completion_function_tool_param.py\nnew file mode 100644\nindex 0000000..a39feea\n--- /dev/null\n+++ b/src/openai/types/chat/chat_completion_function_tool_param.py\n@@ -0,0 +1,16 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing_extensions import Literal, Required, TypedDict\n+\n+from ..shared_params.function_definition import FunctionDefinition\n+\n+__all__ = [\"ChatCompletionFunctionToolParam\"]\n+\n+\n+class ChatCompletionFunctionToolParam(TypedDict, total=False):\n+    function: Required[FunctionDefinition]\n+\n+    type: Required[Literal[\"function\"]]\n+    \"\"\"The type of the tool. Currently, only `function` is supported.\"\"\"\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/chat/chat_completion_message_custom_tool_call.py",
            "diff": "diff --git a/src/openai/types/chat/chat_completion_message_custom_tool_call.py b/src/openai/types/chat/chat_completion_message_custom_tool_call.py\nnew file mode 100644\nindex 0000000..b13c176\n--- /dev/null\n+++ b/src/openai/types/chat/chat_completion_message_custom_tool_call.py\n@@ -0,0 +1,26 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ChatCompletionMessageCustomToolCall\", \"Custom\"]\n+\n+\n+class Custom(BaseModel):\n+    input: str\n+    \"\"\"The input for the custom tool call generated by the model.\"\"\"\n+\n+    name: str\n+    \"\"\"The name of the custom tool to call.\"\"\"\n+\n+\n+class ChatCompletionMessageCustomToolCall(BaseModel):\n+    id: str\n+    \"\"\"The ID of the tool call.\"\"\"\n+\n+    custom: Custom\n+    \"\"\"The custom tool that the model called.\"\"\"\n+\n+    type: Literal[\"custom\"]\n+    \"\"\"The type of the tool. Always `custom`.\"\"\"\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/chat/chat_completion_message_custom_tool_call_param.py",
            "diff": "diff --git a/src/openai/types/chat/chat_completion_message_custom_tool_call_param.py b/src/openai/types/chat/chat_completion_message_custom_tool_call_param.py\nnew file mode 100644\nindex 0000000..3753e0f\n--- /dev/null\n+++ b/src/openai/types/chat/chat_completion_message_custom_tool_call_param.py\n@@ -0,0 +1,26 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing_extensions import Literal, Required, TypedDict\n+\n+__all__ = [\"ChatCompletionMessageCustomToolCallParam\", \"Custom\"]\n+\n+\n+class Custom(TypedDict, total=False):\n+    input: Required[str]\n+    \"\"\"The input for the custom tool call generated by the model.\"\"\"\n+\n+    name: Required[str]\n+    \"\"\"The name of the custom tool to call.\"\"\"\n+\n+\n+class ChatCompletionMessageCustomToolCallParam(TypedDict, total=False):\n+    id: Required[str]\n+    \"\"\"The ID of the tool call.\"\"\"\n+\n+    custom: Required[Custom]\n+    \"\"\"The custom tool that the model called.\"\"\"\n+\n+    type: Required[Literal[\"custom\"]]\n+    \"\"\"The type of the tool. Always `custom`.\"\"\"\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/chat/chat_completion_message_function_tool_call.py",
            "diff": "diff --git a/src/openai/types/chat/chat_completion_message_function_tool_call.py b/src/openai/types/chat/chat_completion_message_function_tool_call.py\nnew file mode 100644\nindex 0000000..d056d9a\n--- /dev/null\n+++ b/src/openai/types/chat/chat_completion_message_function_tool_call.py\n@@ -0,0 +1,31 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ChatCompletionMessageFunctionToolCall\", \"Function\"]\n+\n+\n+class Function(BaseModel):\n+    arguments: str\n+    \"\"\"\n+    The arguments to call the function with, as generated by the model in JSON\n+    format. Note that the model does not always generate valid JSON, and may\n+    hallucinate parameters not defined by your function schema. Validate the\n+    arguments in your code before calling your function.\n+    \"\"\"\n+\n+    name: str\n+    \"\"\"The name of the function to call.\"\"\"\n+\n+\n+class ChatCompletionMessageFunctionToolCall(BaseModel):\n+    id: str\n+    \"\"\"The ID of the tool call.\"\"\"\n+\n+    function: Function\n+    \"\"\"The function that the model called.\"\"\"\n+\n+    type: Literal[\"function\"]\n+    \"\"\"The type of the tool. Currently, only `function` is supported.\"\"\"\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/chat/chat_completion_message_function_tool_call_param.py",
            "diff": "diff --git a/src/openai/types/chat/chat_completion_message_function_tool_call_param.py b/src/openai/types/chat/chat_completion_message_function_tool_call_param.py\nnew file mode 100644\nindex 0000000..7c827ed\n--- /dev/null\n+++ b/src/openai/types/chat/chat_completion_message_function_tool_call_param.py\n@@ -0,0 +1,31 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing_extensions import Literal, Required, TypedDict\n+\n+__all__ = [\"ChatCompletionMessageFunctionToolCallParam\", \"Function\"]\n+\n+\n+class Function(TypedDict, total=False):\n+    arguments: Required[str]\n+    \"\"\"\n+    The arguments to call the function with, as generated by the model in JSON\n+    format. Note that the model does not always generate valid JSON, and may\n+    hallucinate parameters not defined by your function schema. Validate the\n+    arguments in your code before calling your function.\n+    \"\"\"\n+\n+    name: Required[str]\n+    \"\"\"The name of the function to call.\"\"\"\n+\n+\n+class ChatCompletionMessageFunctionToolCallParam(TypedDict, total=False):\n+    id: Required[str]\n+    \"\"\"The ID of the tool call.\"\"\"\n+\n+    function: Required[Function]\n+    \"\"\"The function that the model called.\"\"\"\n+\n+    type: Required[Literal[\"function\"]]\n+    \"\"\"The type of the tool. Currently, only `function` is supported.\"\"\"\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/chat/chat_completion_message_tool_call.py",
            "diff": "diff --git a/src/openai/types/chat/chat_completion_message_tool_call.py b/src/openai/types/chat/chat_completion_message_tool_call.py\nindex 4fec667..c254774 100644\n--- a/src/openai/types/chat/chat_completion_message_tool_call.py\n+++ b/src/openai/types/chat/chat_completion_message_tool_call.py\n@@ -1,31 +1,15 @@\n # File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n \n-from typing_extensions import Literal\n+from typing import Union\n+from typing_extensions import Annotated, TypeAlias\n \n-from ..._models import BaseModel\n+from ..._utils import PropertyInfo\n+from .chat_completion_message_custom_tool_call import ChatCompletionMessageCustomToolCall\n+from .chat_completion_message_function_tool_call import ChatCompletionMessageFunctionToolCall\n \n-__all__ = [\"ChatCompletionMessageToolCall\", \"Function\"]\n+__all__ = [\"ChatCompletionMessageToolCall\"]\n \n-\n-class Function(BaseModel):\n-    arguments: str\n-    \"\"\"\n-    The arguments to call the function with, as generated by the model in JSON\n-    format. Note that the model does not always generate valid JSON, and may\n-    hallucinate parameters not defined by your function schema. Validate the\n-    arguments in your code before calling your function.\n-    \"\"\"\n-\n-    name: str\n-    \"\"\"The name of the function to call.\"\"\"\n-\n-\n-class ChatCompletionMessageToolCall(BaseModel):\n-    id: str\n-    \"\"\"The ID of the tool call.\"\"\"\n-\n-    function: Function\n-    \"\"\"The function that the model called.\"\"\"\n-\n-    type: Literal[\"function\"]\n-    \"\"\"The type of the tool. Currently, only `function` is supported.\"\"\"\n+ChatCompletionMessageToolCall: TypeAlias = Annotated[\n+    Union[ChatCompletionMessageFunctionToolCall, ChatCompletionMessageCustomToolCall],\n+    PropertyInfo(discriminator=\"type\"),\n+]\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/chat/chat_completion_message_tool_call_param.py",
            "diff": "diff --git a/src/openai/types/chat/chat_completion_message_tool_call_param.py b/src/openai/types/chat/chat_completion_message_tool_call_param.py\nindex f616c36..96ba652 100644\n--- a/src/openai/types/chat/chat_completion_message_tool_call_param.py\n+++ b/src/openai/types/chat/chat_completion_message_tool_call_param.py\n@@ -2,30 +2,14 @@\n \n from __future__ import annotations\n \n-from typing_extensions import Literal, Required, TypedDict\n+from typing import Union\n+from typing_extensions import TypeAlias\n \n-__all__ = [\"ChatCompletionMessageToolCallParam\", \"Function\"]\n+from .chat_completion_message_custom_tool_call_param import ChatCompletionMessageCustomToolCallParam\n+from .chat_completion_message_function_tool_call_param import ChatCompletionMessageFunctionToolCallParam\n \n+__all__ = [\"ChatCompletionMessageToolCallParam\"]\n \n-class Function(TypedDict, total=False):\n-    arguments: Required[str]\n-    \"\"\"\n-    The arguments to call the function with, as generated by the model in JSON\n-    format. Note that the model does not always generate valid JSON, and may\n-    hallucinate parameters not defined by your function schema. Validate the\n-    arguments in your code before calling your function.\n-    \"\"\"\n-\n-    name: Required[str]\n-    \"\"\"The name of the function to call.\"\"\"\n-\n-\n-class ChatCompletionMessageToolCallParam(TypedDict, total=False):\n-    id: Required[str]\n-    \"\"\"The ID of the tool call.\"\"\"\n-\n-    function: Required[Function]\n-    \"\"\"The function that the model called.\"\"\"\n-\n-    type: Required[Literal[\"function\"]]\n-    \"\"\"The type of the tool. Currently, only `function` is supported.\"\"\"\n+ChatCompletionMessageToolCallParam: TypeAlias = Union[\n+    ChatCompletionMessageFunctionToolCallParam, ChatCompletionMessageCustomToolCallParam\n+]\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/chat/chat_completion_named_tool_choice_custom_param.py",
            "diff": "diff --git a/src/openai/types/chat/chat_completion_named_tool_choice_custom_param.py b/src/openai/types/chat/chat_completion_named_tool_choice_custom_param.py\nnew file mode 100644\nindex 0000000..1c123c0\n--- /dev/null\n+++ b/src/openai/types/chat/chat_completion_named_tool_choice_custom_param.py\n@@ -0,0 +1,19 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing_extensions import Literal, Required, TypedDict\n+\n+__all__ = [\"ChatCompletionNamedToolChoiceCustomParam\", \"Custom\"]\n+\n+\n+class Custom(TypedDict, total=False):\n+    name: Required[str]\n+    \"\"\"The name of the custom tool to call.\"\"\"\n+\n+\n+class ChatCompletionNamedToolChoiceCustomParam(TypedDict, total=False):\n+    custom: Required[Custom]\n+\n+    type: Required[Literal[\"custom\"]]\n+    \"\"\"For custom tool calling, the type is always `custom`.\"\"\"\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/chat/chat_completion_named_tool_choice_param.py",
            "diff": "diff --git a/src/openai/types/chat/chat_completion_named_tool_choice_param.py b/src/openai/types/chat/chat_completion_named_tool_choice_param.py\nindex 369f8b4..ae1acfb 100644\n--- a/src/openai/types/chat/chat_completion_named_tool_choice_param.py\n+++ b/src/openai/types/chat/chat_completion_named_tool_choice_param.py\n@@ -16,4 +16,4 @@ class ChatCompletionNamedToolChoiceParam(TypedDict, total=False):\n     function: Required[Function]\n \n     type: Required[Literal[\"function\"]]\n-    \"\"\"The type of the tool. Currently, only `function` is supported.\"\"\"\n+    \"\"\"For function calling, the type is always `function`.\"\"\"\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/chat/chat_completion_stream_options_param.py",
            "diff": "diff --git a/src/openai/types/chat/chat_completion_stream_options_param.py b/src/openai/types/chat/chat_completion_stream_options_param.py\nindex 471e0eb..fc3191d 100644\n--- a/src/openai/types/chat/chat_completion_stream_options_param.py\n+++ b/src/openai/types/chat/chat_completion_stream_options_param.py\n@@ -8,6 +8,17 @@ __all__ = [\"ChatCompletionStreamOptionsParam\"]\n \n \n class ChatCompletionStreamOptionsParam(TypedDict, total=False):\n+    include_obfuscation: bool\n+    \"\"\"When true, stream obfuscation will be enabled.\n+\n+    Stream obfuscation adds random characters to an `obfuscation` field on streaming\n+    delta events to normalize payload sizes as a mitigation to certain side-channel\n+    attacks. These obfuscation fields are included by default, but add a small\n+    amount of overhead to the data stream. You can set `include_obfuscation` to\n+    false to optimize for bandwidth if you trust the network links between your\n+    application and the OpenAI API.\n+    \"\"\"\n+\n     include_usage: bool\n     \"\"\"If set, an additional chunk will be streamed before the `data: [DONE]` message.\n \n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/chat/chat_completion_tool_choice_option_param.py",
            "diff": "diff --git a/src/openai/types/chat/chat_completion_tool_choice_option_param.py b/src/openai/types/chat/chat_completion_tool_choice_option_param.py\nindex 7dedf04..f3bb0a4 100644\n--- a/src/openai/types/chat/chat_completion_tool_choice_option_param.py\n+++ b/src/openai/types/chat/chat_completion_tool_choice_option_param.py\n@@ -6,9 +6,14 @@ from typing import Union\n from typing_extensions import Literal, TypeAlias\n \n from .chat_completion_named_tool_choice_param import ChatCompletionNamedToolChoiceParam\n+from .chat_completion_allowed_tool_choice_param import ChatCompletionAllowedToolChoiceParam\n+from .chat_completion_named_tool_choice_custom_param import ChatCompletionNamedToolChoiceCustomParam\n \n __all__ = [\"ChatCompletionToolChoiceOptionParam\"]\n \n ChatCompletionToolChoiceOptionParam: TypeAlias = Union[\n-    Literal[\"none\", \"auto\", \"required\"], ChatCompletionNamedToolChoiceParam\n+    Literal[\"none\", \"auto\", \"required\"],\n+    ChatCompletionAllowedToolChoiceParam,\n+    ChatCompletionNamedToolChoiceParam,\n+    ChatCompletionNamedToolChoiceCustomParam,\n ]\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/chat/chat_completion_tool_param.py",
            "diff": "diff --git a/src/openai/types/chat/chat_completion_tool_param.py b/src/openai/types/chat/chat_completion_tool_param.py\nindex 6c2b1a3..7cd9743 100644\n--- a/src/openai/types/chat/chat_completion_tool_param.py\n+++ b/src/openai/types/chat/chat_completion_tool_param.py\n@@ -2,15 +2,12 @@\n \n from __future__ import annotations\n \n-from typing_extensions import Literal, Required, TypedDict\n+from typing import Union\n+from typing_extensions import TypeAlias\n \n-from ..shared_params.function_definition import FunctionDefinition\n+from .chat_completion_custom_tool_param import ChatCompletionCustomToolParam\n+from .chat_completion_function_tool_param import ChatCompletionFunctionToolParam\n \n __all__ = [\"ChatCompletionToolParam\"]\n \n-\n-class ChatCompletionToolParam(TypedDict, total=False):\n-    function: Required[FunctionDefinition]\n-\n-    type: Required[Literal[\"function\"]]\n-    \"\"\"The type of the tool. Currently, only `function` is supported.\"\"\"\n+ChatCompletionToolParam: TypeAlias = Union[ChatCompletionFunctionToolParam, ChatCompletionCustomToolParam]\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/chat/completion_create_params.py",
            "diff": "diff --git a/src/openai/types/chat/completion_create_params.py b/src/openai/types/chat/completion_create_params.py\nindex 20d7c18..011067a 100644\n--- a/src/openai/types/chat/completion_create_params.py\n+++ b/src/openai/types/chat/completion_create_params.py\n@@ -185,12 +185,12 @@ class CompletionCreateParamsBase(TypedDict, total=False):\n     \"\"\"\n \n     reasoning_effort: Optional[ReasoningEffort]\n-    \"\"\"**o-series models only**\n-\n+    \"\"\"\n     Constrains effort on reasoning for\n     [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n-    supported values are `low`, `medium`, and `high`. Reducing reasoning effort can\n-    result in faster responses and fewer tokens used on reasoning in a response.\n+    supported values are `minimal`, `low`, `medium`, and `high`. Reducing reasoning\n+    effort can result in faster responses and fewer tokens used on reasoning in a\n+    response.\n     \"\"\"\n \n     response_format: ResponseFormat\n@@ -287,9 +287,9 @@ class CompletionCreateParamsBase(TypedDict, total=False):\n     tools: Iterable[ChatCompletionToolParam]\n     \"\"\"A list of tools the model may call.\n \n-    Currently, only functions are supported as a tool. Use this to provide a list of\n-    functions the model may generate JSON inputs for. A max of 128 functions are\n-    supported.\n+    You can provide either\n+    [custom tools](https://platform.openai.com/docs/guides/function-calling#custom-tools)\n+    or [function tools](https://platform.openai.com/docs/guides/function-calling).\n     \"\"\"\n \n     top_logprobs: Optional[int]\n@@ -317,6 +317,14 @@ class CompletionCreateParamsBase(TypedDict, total=False):\n     [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).\n     \"\"\"\n \n+    verbosity: Optional[Literal[\"low\", \"medium\", \"high\"]]\n+    \"\"\"Constrains the verbosity of the model's response.\n+\n+    Lower values will result in more concise responses, while higher values will\n+    result in more verbose responses. Currently supported values are `low`,\n+    `medium`, and `high`.\n+    \"\"\"\n+\n     web_search_options: WebSearchOptions\n     \"\"\"\n     This tool searches the web for relevant results to use in a response. Learn more\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/chat/parsed_function_tool_call.py",
            "diff": "diff --git a/src/openai/types/chat/parsed_function_tool_call.py b/src/openai/types/chat/parsed_function_tool_call.py\nindex 3e90789..e06b354 100644\n--- a/src/openai/types/chat/parsed_function_tool_call.py\n+++ b/src/openai/types/chat/parsed_function_tool_call.py\n@@ -2,7 +2,7 @@\n \n from typing import Optional\n \n-from .chat_completion_message_tool_call import Function, ChatCompletionMessageToolCall\n+from .chat_completion_message_function_tool_call import Function, ChatCompletionMessageFunctionToolCall\n \n __all__ = [\"ParsedFunctionToolCall\", \"ParsedFunction\"]\n \n@@ -24,6 +24,6 @@ class ParsedFunction(Function):\n     \"\"\"\n \n \n-class ParsedFunctionToolCall(ChatCompletionMessageToolCall):\n+class ParsedFunctionToolCall(ChatCompletionMessageFunctionToolCall):\n     function: ParsedFunction\n     \"\"\"The function that the model called.\"\"\"\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/evals/create_eval_completions_run_data_source.py",
            "diff": "diff --git a/src/openai/types/evals/create_eval_completions_run_data_source.py b/src/openai/types/evals/create_eval_completions_run_data_source.py\nindex a0eaa5a..bb39d1d 100644\n--- a/src/openai/types/evals/create_eval_completions_run_data_source.py\n+++ b/src/openai/types/evals/create_eval_completions_run_data_source.py\n@@ -6,10 +6,10 @@ from typing_extensions import Literal, Annotated, TypeAlias\n from ..._utils import PropertyInfo\n from ..._models import BaseModel\n from ..shared.metadata import Metadata\n-from ..chat.chat_completion_tool import ChatCompletionTool\n from ..shared.response_format_text import ResponseFormatText\n from ..responses.easy_input_message import EasyInputMessage\n from ..responses.response_input_text import ResponseInputText\n+from ..chat.chat_completion_function_tool import ChatCompletionFunctionTool\n from ..shared.response_format_json_object import ResponseFormatJSONObject\n from ..shared.response_format_json_schema import ResponseFormatJSONSchema\n \n@@ -186,7 +186,7 @@ class SamplingParams(BaseModel):\n     temperature: Optional[float] = None\n     \"\"\"A higher temperature increases randomness in the outputs.\"\"\"\n \n-    tools: Optional[List[ChatCompletionTool]] = None\n+    tools: Optional[List[ChatCompletionFunctionTool]] = None\n     \"\"\"A list of tools the model may call.\n \n     Currently, only functions are supported as a tool. Use this to provide a list of\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/evals/create_eval_completions_run_data_source_param.py",
            "diff": "diff --git a/src/openai/types/evals/create_eval_completions_run_data_source_param.py b/src/openai/types/evals/create_eval_completions_run_data_source_param.py\nindex 8892b68..7c71ecb 100644\n--- a/src/openai/types/evals/create_eval_completions_run_data_source_param.py\n+++ b/src/openai/types/evals/create_eval_completions_run_data_source_param.py\n@@ -6,10 +6,10 @@ from typing import Dict, Union, Iterable, Optional\n from typing_extensions import Literal, Required, TypeAlias, TypedDict\n \n from ..shared_params.metadata import Metadata\n-from ..chat.chat_completion_tool_param import ChatCompletionToolParam\n from ..responses.easy_input_message_param import EasyInputMessageParam\n from ..shared_params.response_format_text import ResponseFormatText\n from ..responses.response_input_text_param import ResponseInputTextParam\n+from ..chat.chat_completion_function_tool_param import ChatCompletionFunctionToolParam\n from ..shared_params.response_format_json_object import ResponseFormatJSONObject\n from ..shared_params.response_format_json_schema import ResponseFormatJSONSchema\n \n@@ -180,7 +180,7 @@ class SamplingParams(TypedDict, total=False):\n     temperature: float\n     \"\"\"A higher temperature increases randomness in the outputs.\"\"\"\n \n-    tools: Iterable[ChatCompletionToolParam]\n+    tools: Iterable[ChatCompletionFunctionToolParam]\n     \"\"\"A list of tools the model may call.\n \n     Currently, only functions are supported as a tool. Use this to provide a list of\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/responses/__init__.py",
            "diff": "diff --git a/src/openai/types/responses/__init__.py b/src/openai/types/responses/__init__.py\nindex 2e502ed..74d8688 100644\n--- a/src/openai/types/responses/__init__.py\n+++ b/src/openai/types/responses/__init__.py\n@@ -5,6 +5,7 @@ from __future__ import annotations\n from .tool import Tool as Tool\n from .response import Response as Response\n from .tool_param import ToolParam as ToolParam\n+from .custom_tool import CustomTool as CustomTool\n from .computer_tool import ComputerTool as ComputerTool\n from .function_tool import FunctionTool as FunctionTool\n from .response_item import ResponseItem as ResponseItem\n@@ -23,15 +24,18 @@ from .response_status import ResponseStatus as ResponseStatus\n from .tool_choice_mcp import ToolChoiceMcp as ToolChoiceMcp\n from .web_search_tool import WebSearchTool as WebSearchTool\n from .file_search_tool import FileSearchTool as FileSearchTool\n+from .custom_tool_param import CustomToolParam as CustomToolParam\n from .tool_choice_types import ToolChoiceTypes as ToolChoiceTypes\n from .easy_input_message import EasyInputMessage as EasyInputMessage\n from .response_item_list import ResponseItemList as ResponseItemList\n+from .tool_choice_custom import ToolChoiceCustom as ToolChoiceCustom\n from .computer_tool_param import ComputerToolParam as ComputerToolParam\n from .function_tool_param import FunctionToolParam as FunctionToolParam\n from .response_includable import ResponseIncludable as ResponseIncludable\n from .response_input_file import ResponseInputFile as ResponseInputFile\n from .response_input_item import ResponseInputItem as ResponseInputItem\n from .response_input_text import ResponseInputText as ResponseInputText\n+from .tool_choice_allowed import ToolChoiceAllowed as ToolChoiceAllowed\n from .tool_choice_options import ToolChoiceOptions as ToolChoiceOptions\n from .response_error_event import ResponseErrorEvent as ResponseErrorEvent\n from .response_input_image import ResponseInputImage as ResponseInputImage\n@@ -59,12 +63,15 @@ from .easy_input_message_param import EasyInputMessageParam as EasyInputMessageP\n from .response_completed_event import ResponseCompletedEvent as ResponseCompletedEvent\n from .response_retrieve_params import ResponseRetrieveParams as ResponseRetrieveParams\n from .response_text_done_event import ResponseTextDoneEvent as ResponseTextDoneEvent\n+from .tool_choice_custom_param import ToolChoiceCustomParam as ToolChoiceCustomParam\n from .response_audio_done_event import ResponseAudioDoneEvent as ResponseAudioDoneEvent\n+from .response_custom_tool_call import ResponseCustomToolCall as ResponseCustomToolCall\n from .response_incomplete_event import ResponseIncompleteEvent as ResponseIncompleteEvent\n from .response_input_file_param import ResponseInputFileParam as ResponseInputFileParam\n from .response_input_item_param import ResponseInputItemParam as ResponseInputItemParam\n from .response_input_text_param import ResponseInputTextParam as ResponseInputTextParam\n from .response_text_delta_event import ResponseTextDeltaEvent as ResponseTextDeltaEvent\n+from .tool_choice_allowed_param import ToolChoiceAllowedParam as ToolChoiceAllowedParam\n from .response_audio_delta_event import ResponseAudioDeltaEvent as ResponseAudioDeltaEvent\n from .response_in_progress_event import ResponseInProgressEvent as ResponseInProgressEvent\n from .response_input_image_param import ResponseInputImageParam as ResponseInputImageParam\n@@ -84,8 +91,10 @@ from .response_output_refusal_param import ResponseOutputRefusalParam as Respons\n from .response_reasoning_item_param import ResponseReasoningItemParam as ResponseReasoningItemParam\n from .response_file_search_tool_call import ResponseFileSearchToolCall as ResponseFileSearchToolCall\n from .response_mcp_call_failed_event import ResponseMcpCallFailedEvent as ResponseMcpCallFailedEvent\n+from .response_custom_tool_call_param import ResponseCustomToolCallParam as ResponseCustomToolCallParam\n from .response_output_item_done_event import ResponseOutputItemDoneEvent as ResponseOutputItemDoneEvent\n from .response_content_part_done_event import ResponseContentPartDoneEvent as ResponseContentPartDoneEvent\n+from .response_custom_tool_call_output import ResponseCustomToolCallOutput as ResponseCustomToolCallOutput\n from .response_function_tool_call_item import ResponseFunctionToolCallItem as ResponseFunctionToolCallItem\n from .response_output_item_added_event import ResponseOutputItemAddedEvent as ResponseOutputItemAddedEvent\n from .response_computer_tool_call_param import ResponseComputerToolCallParam as ResponseComputerToolCallParam\n@@ -105,6 +114,9 @@ from .response_mcp_list_tools_failed_event import ResponseMcpListToolsFailedEven\n from .response_audio_transcript_delta_event import (\n     ResponseAudioTranscriptDeltaEvent as ResponseAudioTranscriptDeltaEvent,\n )\n+from .response_custom_tool_call_output_param import (\n+    ResponseCustomToolCallOutputParam as ResponseCustomToolCallOutputParam,\n+)\n from .response_mcp_call_arguments_done_event import (\n     ResponseMcpCallArgumentsDoneEvent as ResponseMcpCallArgumentsDoneEvent,\n )\n@@ -153,6 +165,9 @@ from .response_input_message_content_list_param import (\n from .response_mcp_list_tools_in_progress_event import (\n     ResponseMcpListToolsInProgressEvent as ResponseMcpListToolsInProgressEvent,\n )\n+from .response_custom_tool_call_input_done_event import (\n+    ResponseCustomToolCallInputDoneEvent as ResponseCustomToolCallInputDoneEvent,\n+)\n from .response_reasoning_summary_part_done_event import (\n     ResponseReasoningSummaryPartDoneEvent as ResponseReasoningSummaryPartDoneEvent,\n )\n@@ -162,6 +177,9 @@ from .response_reasoning_summary_text_done_event import (\n from .response_web_search_call_in_progress_event import (\n     ResponseWebSearchCallInProgressEvent as ResponseWebSearchCallInProgressEvent,\n )\n+from .response_custom_tool_call_input_delta_event import (\n+    ResponseCustomToolCallInputDeltaEvent as ResponseCustomToolCallInputDeltaEvent,\n+)\n from .response_file_search_call_in_progress_event import (\n     ResponseFileSearchCallInProgressEvent as ResponseFileSearchCallInProgressEvent,\n )\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/responses/custom_tool.py",
            "diff": "diff --git a/src/openai/types/responses/custom_tool.py b/src/openai/types/responses/custom_tool.py\nnew file mode 100644\nindex 0000000..c16ae71\n--- /dev/null\n+++ b/src/openai/types/responses/custom_tool.py\n@@ -0,0 +1,23 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+from ..shared.custom_tool_input_format import CustomToolInputFormat\n+\n+__all__ = [\"CustomTool\"]\n+\n+\n+class CustomTool(BaseModel):\n+    name: str\n+    \"\"\"The name of the custom tool, used to identify it in tool calls.\"\"\"\n+\n+    type: Literal[\"custom\"]\n+    \"\"\"The type of the custom tool. Always `custom`.\"\"\"\n+\n+    description: Optional[str] = None\n+    \"\"\"Optional description of the custom tool, used to provide more context.\"\"\"\n+\n+    format: Optional[CustomToolInputFormat] = None\n+    \"\"\"The input format for the custom tool. Default is unconstrained text.\"\"\"\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/responses/custom_tool_param.py",
            "diff": "diff --git a/src/openai/types/responses/custom_tool_param.py b/src/openai/types/responses/custom_tool_param.py\nnew file mode 100644\nindex 0000000..2afc8b1\n--- /dev/null\n+++ b/src/openai/types/responses/custom_tool_param.py\n@@ -0,0 +1,23 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing_extensions import Literal, Required, TypedDict\n+\n+from ..shared_params.custom_tool_input_format import CustomToolInputFormat\n+\n+__all__ = [\"CustomToolParam\"]\n+\n+\n+class CustomToolParam(TypedDict, total=False):\n+    name: Required[str]\n+    \"\"\"The name of the custom tool, used to identify it in tool calls.\"\"\"\n+\n+    type: Required[Literal[\"custom\"]]\n+    \"\"\"The type of the custom tool. Always `custom`.\"\"\"\n+\n+    description: str\n+    \"\"\"Optional description of the custom tool, used to provide more context.\"\"\"\n+\n+    format: CustomToolInputFormat\n+    \"\"\"The input format for the custom tool. Default is unconstrained text.\"\"\"\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/responses/parsed_response.py",
            "diff": "diff --git a/src/openai/types/responses/parsed_response.py b/src/openai/types/responses/parsed_response.py\nindex e59e86d..1d9db36 100644\n--- a/src/openai/types/responses/parsed_response.py\n+++ b/src/openai/types/responses/parsed_response.py\n@@ -19,6 +19,7 @@ from .response_output_text import ResponseOutputText\n from .response_output_message import ResponseOutputMessage\n from .response_output_refusal import ResponseOutputRefusal\n from .response_reasoning_item import ResponseReasoningItem\n+from .response_custom_tool_call import ResponseCustomToolCall\n from .response_computer_tool_call import ResponseComputerToolCall\n from .response_function_tool_call import ResponseFunctionToolCall\n from .response_function_web_search import ResponseFunctionWebSearch\n@@ -73,6 +74,7 @@ ParsedResponseOutputItem: TypeAlias = Annotated[\n         LocalShellCallAction,\n         McpListTools,\n         ResponseCodeInterpreterToolCall,\n+        ResponseCustomToolCall,\n     ],\n     PropertyInfo(discriminator=\"type\"),\n ]\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/responses/response.py",
            "diff": "diff --git a/src/openai/types/responses/response.py b/src/openai/types/responses/response.py\nindex 7db466d..07a82cb 100644\n--- a/src/openai/types/responses/response.py\n+++ b/src/openai/types/responses/response.py\n@@ -13,7 +13,9 @@ from .tool_choice_mcp import ToolChoiceMcp\n from ..shared.metadata import Metadata\n from ..shared.reasoning import Reasoning\n from .tool_choice_types import ToolChoiceTypes\n+from .tool_choice_custom import ToolChoiceCustom\n from .response_input_item import ResponseInputItem\n+from .tool_choice_allowed import ToolChoiceAllowed\n from .tool_choice_options import ToolChoiceOptions\n from .response_output_item import ResponseOutputItem\n from .response_text_config import ResponseTextConfig\n@@ -28,7 +30,9 @@ class IncompleteDetails(BaseModel):\n     \"\"\"The reason why the response is incomplete.\"\"\"\n \n \n-ToolChoice: TypeAlias = Union[ToolChoiceOptions, ToolChoiceTypes, ToolChoiceFunction, ToolChoiceMcp]\n+ToolChoice: TypeAlias = Union[\n+    ToolChoiceOptions, ToolChoiceAllowed, ToolChoiceTypes, ToolChoiceFunction, ToolChoiceMcp, ToolChoiceCustom\n+]\n \n \n class Response(BaseModel):\n@@ -116,8 +120,10 @@ class Response(BaseModel):\n       Learn more about\n       [built-in tools](https://platform.openai.com/docs/guides/tools).\n     - **Function calls (custom tools)**: Functions that are defined by you, enabling\n-      the model to call your own code. Learn more about\n+      the model to call your own code with strongly typed arguments and outputs.\n+      Learn more about\n       [function calling](https://platform.openai.com/docs/guides/function-calling).\n+      You can also use custom tools to call your own code.\n     \"\"\"\n \n     top_p: Optional[float] = None\n@@ -130,8 +136,8 @@ class Response(BaseModel):\n     \"\"\"\n \n     background: Optional[bool] = None\n-    \"\"\"Whether to run the model response in the background.\n-\n+    \"\"\"\n+    Whether to run the model response in the background.\n     [Learn more](https://platform.openai.com/docs/guides/background).\n     \"\"\"\n \n@@ -253,18 +259,3 @@ class Response(BaseModel):\n     [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).\n     \"\"\"\n \n-    @property\n-    def output_text(self) -> str:\n-        \"\"\"Convenience property that aggregates all `output_text` items from the `output`\n-        list.\n-\n-        If no `output_text` content blocks exist, then an empty string is returned.\n-        \"\"\"\n-        texts: List[str] = []\n-        for output in self.output:\n-            if output.type == \"message\":\n-                for content in output.content:\n-                    if content.type == \"output_text\":\n-                        texts.append(content.text)\n-\n-        return \"\".join(texts)\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/responses/response_create_params.py",
            "diff": "diff --git a/src/openai/types/responses/response_create_params.py b/src/openai/types/responses/response_create_params.py\nindex 4a78d7c..53af325 100644\n--- a/src/openai/types/responses/response_create_params.py\n+++ b/src/openai/types/responses/response_create_params.py\n@@ -14,12 +14,15 @@ from .tool_choice_mcp_param import ToolChoiceMcpParam\n from ..shared_params.metadata import Metadata\n from .tool_choice_types_param import ToolChoiceTypesParam\n from ..shared_params.reasoning import Reasoning\n+from .tool_choice_custom_param import ToolChoiceCustomParam\n+from .tool_choice_allowed_param import ToolChoiceAllowedParam\n from .response_text_config_param import ResponseTextConfigParam\n from .tool_choice_function_param import ToolChoiceFunctionParam\n from ..shared_params.responses_model import ResponsesModel\n \n __all__ = [\n     \"ResponseCreateParamsBase\",\n+    \"StreamOptions\",\n     \"ToolChoice\",\n     \"ResponseCreateParamsNonStreaming\",\n     \"ResponseCreateParamsStreaming\",\n@@ -28,8 +31,8 @@ __all__ = [\n \n class ResponseCreateParamsBase(TypedDict, total=False):\n     background: Optional[bool]\n-    \"\"\"Whether to run the model response in the background.\n-\n+    \"\"\"\n+    Whether to run the model response in the background.\n     [Learn more](https://platform.openai.com/docs/guides/background).\n     \"\"\"\n \n@@ -169,6 +172,9 @@ class ResponseCreateParamsBase(TypedDict, total=False):\n     store: Optional[bool]\n     \"\"\"Whether to store the generated model response for later retrieval via API.\"\"\"\n \n+    stream_options: Optional[StreamOptions]\n+    \"\"\"Options for streaming responses. Only set this when you set `stream: true`.\"\"\"\n+\n     temperature: Optional[float]\n     \"\"\"What sampling temperature to use, between 0 and 2.\n \n@@ -207,8 +213,10 @@ class ResponseCreateParamsBase(TypedDict, total=False):\n       Learn more about\n       [built-in tools](https://platform.openai.com/docs/guides/tools).\n     - **Function calls (custom tools)**: Functions that are defined by you, enabling\n-      the model to call your own code. Learn more about\n+      the model to call your own code with strongly typed arguments and outputs.\n+      Learn more about\n       [function calling](https://platform.openai.com/docs/guides/function-calling).\n+      You can also use custom tools to call your own code.\n     \"\"\"\n \n     top_logprobs: Optional[int]\n@@ -245,8 +253,36 @@ class ResponseCreateParamsBase(TypedDict, total=False):\n     [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).\n     \"\"\"\n \n+    verbosity: Optional[Literal[\"low\", \"medium\", \"high\"]]\n+    \"\"\"Constrains the verbosity of the model's response.\n+\n+    Lower values will result in more concise responses, while higher values will\n+    result in more verbose responses. Currently supported values are `low`,\n+    `medium`, and `high`.\n+    \"\"\"\n+\n+\n+class StreamOptions(TypedDict, total=False):\n+    include_obfuscation: bool\n+    \"\"\"When true, stream obfuscation will be enabled.\n+\n+    Stream obfuscation adds random characters to an `obfuscation` field on streaming\n+    delta events to normalize payload sizes as a mitigation to certain side-channel\n+    attacks. These obfuscation fields are included by default, but add a small\n+    amount of overhead to the data stream. You can set `include_obfuscation` to\n+    false to optimize for bandwidth if you trust the network links between your\n+    application and the OpenAI API.\n+    \"\"\"\n+\n \n-ToolChoice: TypeAlias = Union[ToolChoiceOptions, ToolChoiceTypesParam, ToolChoiceFunctionParam, ToolChoiceMcpParam]\n+ToolChoice: TypeAlias = Union[\n+    ToolChoiceOptions,\n+    ToolChoiceAllowedParam,\n+    ToolChoiceTypesParam,\n+    ToolChoiceFunctionParam,\n+    ToolChoiceMcpParam,\n+    ToolChoiceCustomParam,\n+]\n \n \n class ResponseCreateParamsNonStreaming(ResponseCreateParamsBase, total=False):\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/responses/response_custom_tool_call.py",
            "diff": "diff --git a/src/openai/types/responses/response_custom_tool_call.py b/src/openai/types/responses/response_custom_tool_call.py\nnew file mode 100644\nindex 0000000..38c650e\n--- /dev/null\n+++ b/src/openai/types/responses/response_custom_tool_call.py\n@@ -0,0 +1,25 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ResponseCustomToolCall\"]\n+\n+\n+class ResponseCustomToolCall(BaseModel):\n+    call_id: str\n+    \"\"\"An identifier used to map this custom tool call to a tool call output.\"\"\"\n+\n+    input: str\n+    \"\"\"The input for the custom tool call generated by the model.\"\"\"\n+\n+    name: str\n+    \"\"\"The name of the custom tool being called.\"\"\"\n+\n+    type: Literal[\"custom_tool_call\"]\n+    \"\"\"The type of the custom tool call. Always `custom_tool_call`.\"\"\"\n+\n+    id: Optional[str] = None\n+    \"\"\"The unique ID of the custom tool call in the OpenAI platform.\"\"\"\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/responses/response_custom_tool_call_input_delta_event.py",
            "diff": "diff --git a/src/openai/types/responses/response_custom_tool_call_input_delta_event.py b/src/openai/types/responses/response_custom_tool_call_input_delta_event.py\nnew file mode 100644\nindex 0000000..6c33102\n--- /dev/null\n+++ b/src/openai/types/responses/response_custom_tool_call_input_delta_event.py\n@@ -0,0 +1,24 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ResponseCustomToolCallInputDeltaEvent\"]\n+\n+\n+class ResponseCustomToolCallInputDeltaEvent(BaseModel):\n+    delta: str\n+    \"\"\"The incremental input data (delta) for the custom tool call.\"\"\"\n+\n+    item_id: str\n+    \"\"\"Unique identifier for the API item associated with this event.\"\"\"\n+\n+    output_index: int\n+    \"\"\"The index of the output this delta applies to.\"\"\"\n+\n+    sequence_number: int\n+    \"\"\"The sequence number of this event.\"\"\"\n+\n+    type: Literal[\"response.custom_tool_call_input.delta\"]\n+    \"\"\"The event type identifier.\"\"\"\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/responses/response_custom_tool_call_input_done_event.py",
            "diff": "diff --git a/src/openai/types/responses/response_custom_tool_call_input_done_event.py b/src/openai/types/responses/response_custom_tool_call_input_done_event.py\nnew file mode 100644\nindex 0000000..35a2fee\n--- /dev/null\n+++ b/src/openai/types/responses/response_custom_tool_call_input_done_event.py\n@@ -0,0 +1,24 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ResponseCustomToolCallInputDoneEvent\"]\n+\n+\n+class ResponseCustomToolCallInputDoneEvent(BaseModel):\n+    input: str\n+    \"\"\"The complete input data for the custom tool call.\"\"\"\n+\n+    item_id: str\n+    \"\"\"Unique identifier for the API item associated with this event.\"\"\"\n+\n+    output_index: int\n+    \"\"\"The index of the output this event applies to.\"\"\"\n+\n+    sequence_number: int\n+    \"\"\"The sequence number of this event.\"\"\"\n+\n+    type: Literal[\"response.custom_tool_call_input.done\"]\n+    \"\"\"The event type identifier.\"\"\"\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/responses/response_custom_tool_call_output.py",
            "diff": "diff --git a/src/openai/types/responses/response_custom_tool_call_output.py b/src/openai/types/responses/response_custom_tool_call_output.py\nnew file mode 100644\nindex 0000000..a2b4cc3\n--- /dev/null\n+++ b/src/openai/types/responses/response_custom_tool_call_output.py\n@@ -0,0 +1,22 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Optional\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ResponseCustomToolCallOutput\"]\n+\n+\n+class ResponseCustomToolCallOutput(BaseModel):\n+    call_id: str\n+    \"\"\"The call ID, used to map this custom tool call output to a custom tool call.\"\"\"\n+\n+    output: str\n+    \"\"\"The output from the custom tool call generated by your code.\"\"\"\n+\n+    type: Literal[\"custom_tool_call_output\"]\n+    \"\"\"The type of the custom tool call output. Always `custom_tool_call_output`.\"\"\"\n+\n+    id: Optional[str] = None\n+    \"\"\"The unique ID of the custom tool call output in the OpenAI platform.\"\"\"\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/responses/response_custom_tool_call_output_param.py",
            "diff": "diff --git a/src/openai/types/responses/response_custom_tool_call_output_param.py b/src/openai/types/responses/response_custom_tool_call_output_param.py\nnew file mode 100644\nindex 0000000..d52c525\n--- /dev/null\n+++ b/src/openai/types/responses/response_custom_tool_call_output_param.py\n@@ -0,0 +1,21 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing_extensions import Literal, Required, TypedDict\n+\n+__all__ = [\"ResponseCustomToolCallOutputParam\"]\n+\n+\n+class ResponseCustomToolCallOutputParam(TypedDict, total=False):\n+    call_id: Required[str]\n+    \"\"\"The call ID, used to map this custom tool call output to a custom tool call.\"\"\"\n+\n+    output: Required[str]\n+    \"\"\"The output from the custom tool call generated by your code.\"\"\"\n+\n+    type: Required[Literal[\"custom_tool_call_output\"]]\n+    \"\"\"The type of the custom tool call output. Always `custom_tool_call_output`.\"\"\"\n+\n+    id: str\n+    \"\"\"The unique ID of the custom tool call output in the OpenAI platform.\"\"\"\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/responses/response_custom_tool_call_param.py",
            "diff": "diff --git a/src/openai/types/responses/response_custom_tool_call_param.py b/src/openai/types/responses/response_custom_tool_call_param.py\nnew file mode 100644\nindex 0000000..e15beac\n--- /dev/null\n+++ b/src/openai/types/responses/response_custom_tool_call_param.py\n@@ -0,0 +1,24 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing_extensions import Literal, Required, TypedDict\n+\n+__all__ = [\"ResponseCustomToolCallParam\"]\n+\n+\n+class ResponseCustomToolCallParam(TypedDict, total=False):\n+    call_id: Required[str]\n+    \"\"\"An identifier used to map this custom tool call to a tool call output.\"\"\"\n+\n+    input: Required[str]\n+    \"\"\"The input for the custom tool call generated by the model.\"\"\"\n+\n+    name: Required[str]\n+    \"\"\"The name of the custom tool being called.\"\"\"\n+\n+    type: Required[Literal[\"custom_tool_call\"]]\n+    \"\"\"The type of the custom tool call. Always `custom_tool_call`.\"\"\"\n+\n+    id: str\n+    \"\"\"The unique ID of the custom tool call in the OpenAI platform.\"\"\"\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/responses/response_input_item.py",
            "diff": "diff --git a/src/openai/types/responses/response_input_item.py b/src/openai/types/responses/response_input_item.py\nindex 5fbd7c2..d2b454f 100644\n--- a/src/openai/types/responses/response_input_item.py\n+++ b/src/openai/types/responses/response_input_item.py\n@@ -8,10 +8,12 @@ from ..._models import BaseModel\n from .easy_input_message import EasyInputMessage\n from .response_output_message import ResponseOutputMessage\n from .response_reasoning_item import ResponseReasoningItem\n+from .response_custom_tool_call import ResponseCustomToolCall\n from .response_computer_tool_call import ResponseComputerToolCall\n from .response_function_tool_call import ResponseFunctionToolCall\n from .response_function_web_search import ResponseFunctionWebSearch\n from .response_file_search_tool_call import ResponseFileSearchToolCall\n+from .response_custom_tool_call_output import ResponseCustomToolCallOutput\n from .response_code_interpreter_tool_call import ResponseCodeInterpreterToolCall\n from .response_input_message_content_list import ResponseInputMessageContentList\n from .response_computer_tool_call_output_screenshot import ResponseComputerToolCallOutputScreenshot\n@@ -299,6 +301,8 @@ ResponseInputItem: TypeAlias = Annotated[\n         McpApprovalRequest,\n         McpApprovalResponse,\n         McpCall,\n+        ResponseCustomToolCallOutput,\n+        ResponseCustomToolCall,\n         ItemReference,\n     ],\n     PropertyInfo(discriminator=\"type\"),\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/responses/response_input_item_param.py",
            "diff": "diff --git a/src/openai/types/responses/response_input_item_param.py b/src/openai/types/responses/response_input_item_param.py\nindex 70cd911..0d5dbda 100644\n--- a/src/openai/types/responses/response_input_item_param.py\n+++ b/src/openai/types/responses/response_input_item_param.py\n@@ -8,10 +8,12 @@ from typing_extensions import Literal, Required, TypeAlias, TypedDict\n from .easy_input_message_param import EasyInputMessageParam\n from .response_output_message_param import ResponseOutputMessageParam\n from .response_reasoning_item_param import ResponseReasoningItemParam\n+from .response_custom_tool_call_param import ResponseCustomToolCallParam\n from .response_computer_tool_call_param import ResponseComputerToolCallParam\n from .response_function_tool_call_param import ResponseFunctionToolCallParam\n from .response_function_web_search_param import ResponseFunctionWebSearchParam\n from .response_file_search_tool_call_param import ResponseFileSearchToolCallParam\n+from .response_custom_tool_call_output_param import ResponseCustomToolCallOutputParam\n from .response_code_interpreter_tool_call_param import ResponseCodeInterpreterToolCallParam\n from .response_input_message_content_list_param import ResponseInputMessageContentListParam\n from .response_computer_tool_call_output_screenshot_param import ResponseComputerToolCallOutputScreenshotParam\n@@ -298,5 +300,7 @@ ResponseInputItemParam: TypeAlias = Union[\n     McpApprovalRequest,\n     McpApprovalResponse,\n     McpCall,\n+    ResponseCustomToolCallOutputParam,\n+    ResponseCustomToolCallParam,\n     ItemReference,\n ]\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/responses/response_input_param.py",
            "diff": "diff --git a/src/openai/types/responses/response_input_param.py b/src/openai/types/responses/response_input_param.py\nindex 0249986..6ff36a4 100644\n--- a/src/openai/types/responses/response_input_param.py\n+++ b/src/openai/types/responses/response_input_param.py\n@@ -8,10 +8,12 @@ from typing_extensions import Literal, Required, TypeAlias, TypedDict\n from .easy_input_message_param import EasyInputMessageParam\n from .response_output_message_param import ResponseOutputMessageParam\n from .response_reasoning_item_param import ResponseReasoningItemParam\n+from .response_custom_tool_call_param import ResponseCustomToolCallParam\n from .response_computer_tool_call_param import ResponseComputerToolCallParam\n from .response_function_tool_call_param import ResponseFunctionToolCallParam\n from .response_function_web_search_param import ResponseFunctionWebSearchParam\n from .response_file_search_tool_call_param import ResponseFileSearchToolCallParam\n+from .response_custom_tool_call_output_param import ResponseCustomToolCallOutputParam\n from .response_code_interpreter_tool_call_param import ResponseCodeInterpreterToolCallParam\n from .response_input_message_content_list_param import ResponseInputMessageContentListParam\n from .response_computer_tool_call_output_screenshot_param import ResponseComputerToolCallOutputScreenshotParam\n@@ -299,6 +301,8 @@ ResponseInputItemParam: TypeAlias = Union[\n     McpApprovalRequest,\n     McpApprovalResponse,\n     McpCall,\n+    ResponseCustomToolCallOutputParam,\n+    ResponseCustomToolCallParam,\n     ItemReference,\n ]\n \n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/responses/response_output_item.py",
            "diff": "diff --git a/src/openai/types/responses/response_output_item.py b/src/openai/types/responses/response_output_item.py\nindex 62f8f6f..2d3ee7b 100644\n--- a/src/openai/types/responses/response_output_item.py\n+++ b/src/openai/types/responses/response_output_item.py\n@@ -7,6 +7,7 @@ from ..._utils import PropertyInfo\n from ..._models import BaseModel\n from .response_output_message import ResponseOutputMessage\n from .response_reasoning_item import ResponseReasoningItem\n+from .response_custom_tool_call import ResponseCustomToolCall\n from .response_computer_tool_call import ResponseComputerToolCall\n from .response_function_tool_call import ResponseFunctionToolCall\n from .response_function_web_search import ResponseFunctionWebSearch\n@@ -161,6 +162,7 @@ ResponseOutputItem: TypeAlias = Annotated[\n         McpCall,\n         McpListTools,\n         McpApprovalRequest,\n+        ResponseCustomToolCall,\n     ],\n     PropertyInfo(discriminator=\"type\"),\n ]\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/responses/response_retrieve_params.py",
            "diff": "diff --git a/src/openai/types/responses/response_retrieve_params.py b/src/openai/types/responses/response_retrieve_params.py\nindex a092bd7..4013db8 100644\n--- a/src/openai/types/responses/response_retrieve_params.py\n+++ b/src/openai/types/responses/response_retrieve_params.py\n@@ -17,6 +17,17 @@ class ResponseRetrieveParamsBase(TypedDict, total=False):\n     See the `include` parameter for Response creation above for more information.\n     \"\"\"\n \n+    include_obfuscation: bool\n+    \"\"\"When true, stream obfuscation will be enabled.\n+\n+    Stream obfuscation adds random characters to an `obfuscation` field on streaming\n+    delta events to normalize payload sizes as a mitigation to certain side-channel\n+    attacks. These obfuscation fields are included by default, but add a small\n+    amount of overhead to the data stream. You can set `include_obfuscation` to\n+    false to optimize for bandwidth if you trust the network links between your\n+    application and the OpenAI API.\n+    \"\"\"\n+\n     starting_after: int\n     \"\"\"The sequence number of the event after which to start streaming.\"\"\"\n \n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/responses/response_stream_event.py",
            "diff": "diff --git a/src/openai/types/responses/response_stream_event.py b/src/openai/types/responses/response_stream_event.py\nindex d62cf89..c0a317c 100644\n--- a/src/openai/types/responses/response_stream_event.py\n+++ b/src/openai/types/responses/response_stream_event.py\n@@ -40,9 +40,11 @@ from .response_file_search_call_completed_event import ResponseFileSearchCallCom\n from .response_file_search_call_searching_event import ResponseFileSearchCallSearchingEvent\n from .response_image_gen_call_in_progress_event import ResponseImageGenCallInProgressEvent\n from .response_mcp_list_tools_in_progress_event import ResponseMcpListToolsInProgressEvent\n+from .response_custom_tool_call_input_done_event import ResponseCustomToolCallInputDoneEvent\n from .response_reasoning_summary_part_done_event import ResponseReasoningSummaryPartDoneEvent\n from .response_reasoning_summary_text_done_event import ResponseReasoningSummaryTextDoneEvent\n from .response_web_search_call_in_progress_event import ResponseWebSearchCallInProgressEvent\n+from .response_custom_tool_call_input_delta_event import ResponseCustomToolCallInputDeltaEvent\n from .response_file_search_call_in_progress_event import ResponseFileSearchCallInProgressEvent\n from .response_function_call_arguments_done_event import ResponseFunctionCallArgumentsDoneEvent\n from .response_image_gen_call_partial_image_event import ResponseImageGenCallPartialImageEvent\n@@ -111,6 +113,8 @@ ResponseStreamEvent: TypeAlias = Annotated[\n         ResponseMcpListToolsInProgressEvent,\n         ResponseOutputTextAnnotationAddedEvent,\n         ResponseQueuedEvent,\n+        ResponseCustomToolCallInputDeltaEvent,\n+        ResponseCustomToolCallInputDoneEvent,\n     ],\n     PropertyInfo(discriminator=\"type\"),\n ]\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/responses/tool.py",
            "diff": "diff --git a/src/openai/types/responses/tool.py b/src/openai/types/responses/tool.py\nindex 4399871..455ba01 100644\n--- a/src/openai/types/responses/tool.py\n+++ b/src/openai/types/responses/tool.py\n@@ -5,6 +5,7 @@ from typing_extensions import Literal, Annotated, TypeAlias\n \n from ..._utils import PropertyInfo\n from ..._models import BaseModel\n+from .custom_tool import CustomTool\n from .computer_tool import ComputerTool\n from .function_tool import FunctionTool\n from .web_search_tool import WebSearchTool\n@@ -177,6 +178,16 @@ class LocalShell(BaseModel):\n \n \n Tool: TypeAlias = Annotated[\n-    Union[FunctionTool, FileSearchTool, WebSearchTool, ComputerTool, Mcp, CodeInterpreter, ImageGeneration, LocalShell],\n+    Union[\n+        FunctionTool,\n+        FileSearchTool,\n+        WebSearchTool,\n+        ComputerTool,\n+        Mcp,\n+        CodeInterpreter,\n+        ImageGeneration,\n+        LocalShell,\n+        CustomTool,\n+    ],\n     PropertyInfo(discriminator=\"type\"),\n ]\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/responses/tool_choice_allowed.py",
            "diff": "diff --git a/src/openai/types/responses/tool_choice_allowed.py b/src/openai/types/responses/tool_choice_allowed.py\nnew file mode 100644\nindex 0000000..d7921dc\n--- /dev/null\n+++ b/src/openai/types/responses/tool_choice_allowed.py\n@@ -0,0 +1,36 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Dict, List\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ToolChoiceAllowed\"]\n+\n+\n+class ToolChoiceAllowed(BaseModel):\n+    mode: Literal[\"auto\", \"required\"]\n+    \"\"\"Constrains the tools available to the model to a pre-defined set.\n+\n+    `auto` allows the model to pick from among the allowed tools and generate a\n+    message.\n+\n+    `required` requires the model to call one or more of the allowed tools.\n+    \"\"\"\n+\n+    tools: List[Dict[str, object]]\n+    \"\"\"A list of tool definitions that the model should be allowed to call.\n+\n+    For the Responses API, the list of tool definitions might look like:\n+\n+    ```json\n+    [\n+      { \"type\": \"function\", \"name\": \"get_weather\" },\n+      { \"type\": \"mcp\", \"server_label\": \"deepwiki\" },\n+      { \"type\": \"image_generation\" }\n+    ]\n+    ```\n+    \"\"\"\n+\n+    type: Literal[\"allowed_tools\"]\n+    \"\"\"Allowed tool configuration type. Always `allowed_tools`.\"\"\"\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/responses/tool_choice_allowed_param.py",
            "diff": "diff --git a/src/openai/types/responses/tool_choice_allowed_param.py b/src/openai/types/responses/tool_choice_allowed_param.py\nnew file mode 100644\nindex 0000000..0712cab\n--- /dev/null\n+++ b/src/openai/types/responses/tool_choice_allowed_param.py\n@@ -0,0 +1,36 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing import Dict, Iterable\n+from typing_extensions import Literal, Required, TypedDict\n+\n+__all__ = [\"ToolChoiceAllowedParam\"]\n+\n+\n+class ToolChoiceAllowedParam(TypedDict, total=False):\n+    mode: Required[Literal[\"auto\", \"required\"]]\n+    \"\"\"Constrains the tools available to the model to a pre-defined set.\n+\n+    `auto` allows the model to pick from among the allowed tools and generate a\n+    message.\n+\n+    `required` requires the model to call one or more of the allowed tools.\n+    \"\"\"\n+\n+    tools: Required[Iterable[Dict[str, object]]]\n+    \"\"\"A list of tool definitions that the model should be allowed to call.\n+\n+    For the Responses API, the list of tool definitions might look like:\n+\n+    ```json\n+    [\n+      { \"type\": \"function\", \"name\": \"get_weather\" },\n+      { \"type\": \"mcp\", \"server_label\": \"deepwiki\" },\n+      { \"type\": \"image_generation\" }\n+    ]\n+    ```\n+    \"\"\"\n+\n+    type: Required[Literal[\"allowed_tools\"]]\n+    \"\"\"Allowed tool configuration type. Always `allowed_tools`.\"\"\"\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/responses/tool_choice_custom.py",
            "diff": "diff --git a/src/openai/types/responses/tool_choice_custom.py b/src/openai/types/responses/tool_choice_custom.py\nnew file mode 100644\nindex 0000000..d600e53\n--- /dev/null\n+++ b/src/openai/types/responses/tool_choice_custom.py\n@@ -0,0 +1,15 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ToolChoiceCustom\"]\n+\n+\n+class ToolChoiceCustom(BaseModel):\n+    name: str\n+    \"\"\"The name of the custom tool to call.\"\"\"\n+\n+    type: Literal[\"custom\"]\n+    \"\"\"For custom tool calling, the type is always `custom`.\"\"\"\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/responses/tool_choice_custom_param.py",
            "diff": "diff --git a/src/openai/types/responses/tool_choice_custom_param.py b/src/openai/types/responses/tool_choice_custom_param.py\nnew file mode 100644\nindex 0000000..55bc53b\n--- /dev/null\n+++ b/src/openai/types/responses/tool_choice_custom_param.py\n@@ -0,0 +1,15 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing_extensions import Literal, Required, TypedDict\n+\n+__all__ = [\"ToolChoiceCustomParam\"]\n+\n+\n+class ToolChoiceCustomParam(TypedDict, total=False):\n+    name: Required[str]\n+    \"\"\"The name of the custom tool to call.\"\"\"\n+\n+    type: Required[Literal[\"custom\"]]\n+    \"\"\"For custom tool calling, the type is always `custom`.\"\"\"\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/responses/tool_param.py",
            "diff": "diff --git a/src/openai/types/responses/tool_param.py b/src/openai/types/responses/tool_param.py\nindex a977f06..ef9ec2a 100644\n--- a/src/openai/types/responses/tool_param.py\n+++ b/src/openai/types/responses/tool_param.py\n@@ -5,6 +5,7 @@ from __future__ import annotations\n from typing import Dict, List, Union, Optional\n from typing_extensions import Literal, Required, TypeAlias, TypedDict\n \n+from .custom_tool_param import CustomToolParam\n from .computer_tool_param import ComputerToolParam\n from .function_tool_param import FunctionToolParam\n from .web_search_tool_param import WebSearchToolParam\n@@ -186,6 +187,7 @@ ToolParam: TypeAlias = Union[\n     CodeInterpreter,\n     ImageGeneration,\n     LocalShell,\n+    CustomToolParam,\n ]\n \n \n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/shared/__init__.py",
            "diff": "diff --git a/src/openai/types/shared/__init__.py b/src/openai/types/shared/__init__.py\nindex 6ad0ed5..2930b9a 100644\n--- a/src/openai/types/shared/__init__.py\n+++ b/src/openai/types/shared/__init__.py\n@@ -12,5 +12,8 @@ from .comparison_filter import ComparisonFilter as ComparisonFilter\n from .function_definition import FunctionDefinition as FunctionDefinition\n from .function_parameters import FunctionParameters as FunctionParameters\n from .response_format_text import ResponseFormatText as ResponseFormatText\n+from .custom_tool_input_format import CustomToolInputFormat as CustomToolInputFormat\n from .response_format_json_object import ResponseFormatJSONObject as ResponseFormatJSONObject\n from .response_format_json_schema import ResponseFormatJSONSchema as ResponseFormatJSONSchema\n+from .response_format_text_python import ResponseFormatTextPython as ResponseFormatTextPython\n+from .response_format_text_grammar import ResponseFormatTextGrammar as ResponseFormatTextGrammar\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/shared/chat_model.py",
            "diff": "diff --git a/src/openai/types/shared/chat_model.py b/src/openai/types/shared/chat_model.py\nindex 309368a..727c60c 100644\n--- a/src/openai/types/shared/chat_model.py\n+++ b/src/openai/types/shared/chat_model.py\n@@ -5,6 +5,13 @@ from typing_extensions import Literal, TypeAlias\n __all__ = [\"ChatModel\"]\n \n ChatModel: TypeAlias = Literal[\n+    \"gpt-5\",\n+    \"gpt-5-mini\",\n+    \"gpt-5-nano\",\n+    \"gpt-5-2025-08-07\",\n+    \"gpt-5-mini-2025-08-07\",\n+    \"gpt-5-nano-2025-08-07\",\n+    \"gpt-5-chat-latest\",\n     \"gpt-4.1\",\n     \"gpt-4.1-mini\",\n     \"gpt-4.1-nano\",\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/shared/custom_tool_input_format.py",
            "diff": "diff --git a/src/openai/types/shared/custom_tool_input_format.py b/src/openai/types/shared/custom_tool_input_format.py\nnew file mode 100644\nindex 0000000..53c8323\n--- /dev/null\n+++ b/src/openai/types/shared/custom_tool_input_format.py\n@@ -0,0 +1,28 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing import Union\n+from typing_extensions import Literal, Annotated, TypeAlias\n+\n+from ..._utils import PropertyInfo\n+from ..._models import BaseModel\n+\n+__all__ = [\"CustomToolInputFormat\", \"Text\", \"Grammar\"]\n+\n+\n+class Text(BaseModel):\n+    type: Literal[\"text\"]\n+    \"\"\"Unconstrained text format. Always `text`.\"\"\"\n+\n+\n+class Grammar(BaseModel):\n+    definition: str\n+    \"\"\"The grammar definition.\"\"\"\n+\n+    syntax: Literal[\"lark\", \"regex\"]\n+    \"\"\"The syntax of the grammar definition. One of `lark` or `regex`.\"\"\"\n+\n+    type: Literal[\"grammar\"]\n+    \"\"\"Grammar format. Always `grammar`.\"\"\"\n+\n+\n+CustomToolInputFormat: TypeAlias = Annotated[Union[Text, Grammar], PropertyInfo(discriminator=\"type\")]\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/shared/reasoning.py",
            "diff": "diff --git a/src/openai/types/shared/reasoning.py b/src/openai/types/shared/reasoning.py\nindex 107aab2..24ce301 100644\n--- a/src/openai/types/shared/reasoning.py\n+++ b/src/openai/types/shared/reasoning.py\n@@ -11,12 +11,12 @@ __all__ = [\"Reasoning\"]\n \n class Reasoning(BaseModel):\n     effort: Optional[ReasoningEffort] = None\n-    \"\"\"**o-series models only**\n-\n+    \"\"\"\n     Constrains effort on reasoning for\n     [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n-    supported values are `low`, `medium`, and `high`. Reducing reasoning effort can\n-    result in faster responses and fewer tokens used on reasoning in a response.\n+    supported values are `minimal`, `low`, `medium`, and `high`. Reducing reasoning\n+    effort can result in faster responses and fewer tokens used on reasoning in a\n+    response.\n     \"\"\"\n \n     generate_summary: Optional[Literal[\"auto\", \"concise\", \"detailed\"]] = None\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/shared/reasoning_effort.py",
            "diff": "diff --git a/src/openai/types/shared/reasoning_effort.py b/src/openai/types/shared/reasoning_effort.py\nindex ace21b6..4b960cd 100644\n--- a/src/openai/types/shared/reasoning_effort.py\n+++ b/src/openai/types/shared/reasoning_effort.py\n@@ -5,4 +5,4 @@ from typing_extensions import Literal, TypeAlias\n \n __all__ = [\"ReasoningEffort\"]\n \n-ReasoningEffort: TypeAlias = Optional[Literal[\"low\", \"medium\", \"high\"]]\n+ReasoningEffort: TypeAlias = Optional[Literal[\"minimal\", \"low\", \"medium\", \"high\"]]\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/shared/response_format_text_grammar.py",
            "diff": "diff --git a/src/openai/types/shared/response_format_text_grammar.py b/src/openai/types/shared/response_format_text_grammar.py\nnew file mode 100644\nindex 0000000..b02f99c\n--- /dev/null\n+++ b/src/openai/types/shared/response_format_text_grammar.py\n@@ -0,0 +1,15 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ResponseFormatTextGrammar\"]\n+\n+\n+class ResponseFormatTextGrammar(BaseModel):\n+    grammar: str\n+    \"\"\"The custom grammar for the model to follow.\"\"\"\n+\n+    type: Literal[\"grammar\"]\n+    \"\"\"The type of response format being defined. Always `grammar`.\"\"\"\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/shared/response_format_text_python.py",
            "diff": "diff --git a/src/openai/types/shared/response_format_text_python.py b/src/openai/types/shared/response_format_text_python.py\nnew file mode 100644\nindex 0000000..4cd18d4\n--- /dev/null\n+++ b/src/openai/types/shared/response_format_text_python.py\n@@ -0,0 +1,12 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from typing_extensions import Literal\n+\n+from ..._models import BaseModel\n+\n+__all__ = [\"ResponseFormatTextPython\"]\n+\n+\n+class ResponseFormatTextPython(BaseModel):\n+    type: Literal[\"python\"]\n+    \"\"\"The type of response format being defined. Always `python`.\"\"\"\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/shared_params/__init__.py",
            "diff": "diff --git a/src/openai/types/shared_params/__init__.py b/src/openai/types/shared_params/__init__.py\nindex 8894710..b6c0912 100644\n--- a/src/openai/types/shared_params/__init__.py\n+++ b/src/openai/types/shared_params/__init__.py\n@@ -10,5 +10,6 @@ from .comparison_filter import ComparisonFilter as ComparisonFilter\n from .function_definition import FunctionDefinition as FunctionDefinition\n from .function_parameters import FunctionParameters as FunctionParameters\n from .response_format_text import ResponseFormatText as ResponseFormatText\n+from .custom_tool_input_format import CustomToolInputFormat as CustomToolInputFormat\n from .response_format_json_object import ResponseFormatJSONObject as ResponseFormatJSONObject\n from .response_format_json_schema import ResponseFormatJSONSchema as ResponseFormatJSONSchema\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/shared_params/chat_model.py",
            "diff": "diff --git a/src/openai/types/shared_params/chat_model.py b/src/openai/types/shared_params/chat_model.py\nindex 6cd8e7f..a1e5ab9 100644\n--- a/src/openai/types/shared_params/chat_model.py\n+++ b/src/openai/types/shared_params/chat_model.py\n@@ -7,6 +7,13 @@ from typing_extensions import Literal, TypeAlias\n __all__ = [\"ChatModel\"]\n \n ChatModel: TypeAlias = Literal[\n+    \"gpt-5\",\n+    \"gpt-5-mini\",\n+    \"gpt-5-nano\",\n+    \"gpt-5-2025-08-07\",\n+    \"gpt-5-mini-2025-08-07\",\n+    \"gpt-5-nano-2025-08-07\",\n+    \"gpt-5-chat-latest\",\n     \"gpt-4.1\",\n     \"gpt-4.1-mini\",\n     \"gpt-4.1-nano\",\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/shared_params/custom_tool_input_format.py",
            "diff": "diff --git a/src/openai/types/shared_params/custom_tool_input_format.py b/src/openai/types/shared_params/custom_tool_input_format.py\nnew file mode 100644\nindex 0000000..37df393\n--- /dev/null\n+++ b/src/openai/types/shared_params/custom_tool_input_format.py\n@@ -0,0 +1,27 @@\n+# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n+\n+from __future__ import annotations\n+\n+from typing import Union\n+from typing_extensions import Literal, Required, TypeAlias, TypedDict\n+\n+__all__ = [\"CustomToolInputFormat\", \"Text\", \"Grammar\"]\n+\n+\n+class Text(TypedDict, total=False):\n+    type: Required[Literal[\"text\"]]\n+    \"\"\"Unconstrained text format. Always `text`.\"\"\"\n+\n+\n+class Grammar(TypedDict, total=False):\n+    definition: Required[str]\n+    \"\"\"The grammar definition.\"\"\"\n+\n+    syntax: Required[Literal[\"lark\", \"regex\"]]\n+    \"\"\"The syntax of the grammar definition. One of `lark` or `regex`.\"\"\"\n+\n+    type: Required[Literal[\"grammar\"]]\n+    \"\"\"Grammar format. Always `grammar`.\"\"\"\n+\n+\n+CustomToolInputFormat: TypeAlias = Union[Text, Grammar]\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/shared_params/reasoning.py",
            "diff": "diff --git a/src/openai/types/shared_params/reasoning.py b/src/openai/types/shared_params/reasoning.py\nindex 73e1a00..7eab2c7 100644\n--- a/src/openai/types/shared_params/reasoning.py\n+++ b/src/openai/types/shared_params/reasoning.py\n@@ -12,12 +12,12 @@ __all__ = [\"Reasoning\"]\n \n class Reasoning(TypedDict, total=False):\n     effort: Optional[ReasoningEffort]\n-    \"\"\"**o-series models only**\n-\n+    \"\"\"\n     Constrains effort on reasoning for\n     [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n-    supported values are `low`, `medium`, and `high`. Reducing reasoning effort can\n-    result in faster responses and fewer tokens used on reasoning in a response.\n+    supported values are `minimal`, `low`, `medium`, and `high`. Reducing reasoning\n+    effort can result in faster responses and fewer tokens used on reasoning in a\n+    response.\n     \"\"\"\n \n     generate_summary: Optional[Literal[\"auto\", \"concise\", \"detailed\"]]\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "src/openai/types/shared_params/reasoning_effort.py",
            "diff": "diff --git a/src/openai/types/shared_params/reasoning_effort.py b/src/openai/types/shared_params/reasoning_effort.py\nindex 6052c5a..4c095a2 100644\n--- a/src/openai/types/shared_params/reasoning_effort.py\n+++ b/src/openai/types/shared_params/reasoning_effort.py\n@@ -7,4 +7,4 @@ from typing_extensions import Literal, TypeAlias\n \n __all__ = [\"ReasoningEffort\"]\n \n-ReasoningEffort: TypeAlias = Optional[Literal[\"low\", \"medium\", \"high\"]]\n+ReasoningEffort: TypeAlias = Optional[Literal[\"minimal\", \"low\", \"medium\", \"high\"]]\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "tests/api_resources/beta/test_assistants.py",
            "diff": "diff --git a/tests/api_resources/beta/test_assistants.py b/tests/api_resources/beta/test_assistants.py\nindex 8aeb654..875e024 100644\n--- a/tests/api_resources/beta/test_assistants.py\n+++ b/tests/api_resources/beta/test_assistants.py\n@@ -36,7 +36,7 @@ class TestAssistants:\n             instructions=\"instructions\",\n             metadata={\"foo\": \"string\"},\n             name=\"name\",\n-            reasoning_effort=\"low\",\n+            reasoning_effort=\"minimal\",\n             response_format=\"auto\",\n             temperature=1,\n             tool_resources={\n@@ -135,7 +135,7 @@ class TestAssistants:\n             metadata={\"foo\": \"string\"},\n             model=\"string\",\n             name=\"name\",\n-            reasoning_effort=\"low\",\n+            reasoning_effort=\"minimal\",\n             response_format=\"auto\",\n             temperature=1,\n             tool_resources={\n@@ -272,7 +272,7 @@ class TestAsyncAssistants:\n             instructions=\"instructions\",\n             metadata={\"foo\": \"string\"},\n             name=\"name\",\n-            reasoning_effort=\"low\",\n+            reasoning_effort=\"minimal\",\n             response_format=\"auto\",\n             temperature=1,\n             tool_resources={\n@@ -371,7 +371,7 @@ class TestAsyncAssistants:\n             metadata={\"foo\": \"string\"},\n             model=\"string\",\n             name=\"name\",\n-            reasoning_effort=\"low\",\n+            reasoning_effort=\"minimal\",\n             response_format=\"auto\",\n             temperature=1,\n             tool_resources={\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "tests/api_resources/beta/threads/test_runs.py",
            "diff": "diff --git a/tests/api_resources/beta/threads/test_runs.py b/tests/api_resources/beta/threads/test_runs.py\nindex 86a2966..440486b 100644\n--- a/tests/api_resources/beta/threads/test_runs.py\n+++ b/tests/api_resources/beta/threads/test_runs.py\n@@ -59,7 +59,7 @@ class TestRuns:\n                 metadata={\"foo\": \"string\"},\n                 model=\"string\",\n                 parallel_tool_calls=True,\n-                reasoning_effort=\"low\",\n+                reasoning_effort=\"minimal\",\n                 response_format=\"auto\",\n                 stream=False,\n                 temperature=1,\n@@ -150,7 +150,7 @@ class TestRuns:\n                 metadata={\"foo\": \"string\"},\n                 model=\"string\",\n                 parallel_tool_calls=True,\n-                reasoning_effort=\"low\",\n+                reasoning_effort=\"minimal\",\n                 response_format=\"auto\",\n                 temperature=1,\n                 tool_choice=\"none\",\n@@ -609,7 +609,7 @@ class TestAsyncRuns:\n                 metadata={\"foo\": \"string\"},\n                 model=\"string\",\n                 parallel_tool_calls=True,\n-                reasoning_effort=\"low\",\n+                reasoning_effort=\"minimal\",\n                 response_format=\"auto\",\n                 stream=False,\n                 temperature=1,\n@@ -700,7 +700,7 @@ class TestAsyncRuns:\n                 metadata={\"foo\": \"string\"},\n                 model=\"string\",\n                 parallel_tool_calls=True,\n-                reasoning_effort=\"low\",\n+                reasoning_effort=\"minimal\",\n                 response_format=\"auto\",\n                 temperature=1,\n                 tool_choice=\"none\",\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "tests/api_resources/chat/test_completions.py",
            "diff": "diff --git a/tests/api_resources/chat/test_completions.py b/tests/api_resources/chat/test_completions.py\nindex 2758d98..358ea18 100644\n--- a/tests/api_resources/chat/test_completions.py\n+++ b/tests/api_resources/chat/test_completions.py\n@@ -73,7 +73,7 @@ class TestCompletions:\n             },\n             presence_penalty=-2,\n             prompt_cache_key=\"prompt-cache-key-1234\",\n-            reasoning_effort=\"low\",\n+            reasoning_effort=\"minimal\",\n             response_format={\"type\": \"text\"},\n             safety_identifier=\"safety-identifier-1234\",\n             seed=-9007199254740991,\n@@ -81,7 +81,10 @@ class TestCompletions:\n             stop=\"\\n\",\n             store=True,\n             stream=False,\n-            stream_options={\"include_usage\": True},\n+            stream_options={\n+                \"include_obfuscation\": True,\n+                \"include_usage\": True,\n+            },\n             temperature=1,\n             tool_choice=\"none\",\n             tools=[\n@@ -98,6 +101,7 @@ class TestCompletions:\n             top_logprobs=0,\n             top_p=1,\n             user=\"user-1234\",\n+            verbosity=\"low\",\n             web_search_options={\n                 \"search_context_size\": \"low\",\n                 \"user_location\": {\n@@ -202,14 +206,17 @@ class TestCompletions:\n             },\n             presence_penalty=-2,\n             prompt_cache_key=\"prompt-cache-key-1234\",\n-            reasoning_effort=\"low\",\n+            reasoning_effort=\"minimal\",\n             response_format={\"type\": \"text\"},\n             safety_identifier=\"safety-identifier-1234\",\n             seed=-9007199254740991,\n             service_tier=\"auto\",\n             stop=\"\\n\",\n             store=True,\n-            stream_options={\"include_usage\": True},\n+            stream_options={\n+                \"include_obfuscation\": True,\n+                \"include_usage\": True,\n+            },\n             temperature=1,\n             tool_choice=\"none\",\n             tools=[\n@@ -226,6 +233,7 @@ class TestCompletions:\n             top_logprobs=0,\n             top_p=1,\n             user=\"user-1234\",\n+            verbosity=\"low\",\n             web_search_options={\n                 \"search_context_size\": \"low\",\n                 \"user_location\": {\n@@ -506,7 +514,7 @@ class TestAsyncCompletions:\n             },\n             presence_penalty=-2,\n             prompt_cache_key=\"prompt-cache-key-1234\",\n-            reasoning_effort=\"low\",\n+            reasoning_effort=\"minimal\",\n             response_format={\"type\": \"text\"},\n             safety_identifier=\"safety-identifier-1234\",\n             seed=-9007199254740991,\n@@ -514,7 +522,10 @@ class TestAsyncCompletions:\n             stop=\"\\n\",\n             store=True,\n             stream=False,\n-            stream_options={\"include_usage\": True},\n+            stream_options={\n+                \"include_obfuscation\": True,\n+                \"include_usage\": True,\n+            },\n             temperature=1,\n             tool_choice=\"none\",\n             tools=[\n@@ -531,6 +542,7 @@ class TestAsyncCompletions:\n             top_logprobs=0,\n             top_p=1,\n             user=\"user-1234\",\n+            verbosity=\"low\",\n             web_search_options={\n                 \"search_context_size\": \"low\",\n                 \"user_location\": {\n@@ -635,14 +647,17 @@ class TestAsyncCompletions:\n             },\n             presence_penalty=-2,\n             prompt_cache_key=\"prompt-cache-key-1234\",\n-            reasoning_effort=\"low\",\n+            reasoning_effort=\"minimal\",\n             response_format={\"type\": \"text\"},\n             safety_identifier=\"safety-identifier-1234\",\n             seed=-9007199254740991,\n             service_tier=\"auto\",\n             stop=\"\\n\",\n             store=True,\n-            stream_options={\"include_usage\": True},\n+            stream_options={\n+                \"include_obfuscation\": True,\n+                \"include_usage\": True,\n+            },\n             temperature=1,\n             tool_choice=\"none\",\n             tools=[\n@@ -659,6 +674,7 @@ class TestAsyncCompletions:\n             top_logprobs=0,\n             top_p=1,\n             user=\"user-1234\",\n+            verbosity=\"low\",\n             web_search_options={\n                 \"search_context_size\": \"low\",\n                 \"user_location\": {\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "tests/api_resources/test_completions.py",
            "diff": "diff --git a/tests/api_resources/test_completions.py b/tests/api_resources/test_completions.py\nindex 1c5271d..a8fb0e5 100644\n--- a/tests/api_resources/test_completions.py\n+++ b/tests/api_resources/test_completions.py\n@@ -41,7 +41,10 @@ class TestCompletions:\n             seed=0,\n             stop=\"\\n\",\n             stream=False,\n-            stream_options={\"include_usage\": True},\n+            stream_options={\n+                \"include_obfuscation\": True,\n+                \"include_usage\": True,\n+            },\n             suffix=\"test.\",\n             temperature=1,\n             top_p=1,\n@@ -100,7 +103,10 @@ class TestCompletions:\n             presence_penalty=-2,\n             seed=0,\n             stop=\"\\n\",\n-            stream_options={\"include_usage\": True},\n+            stream_options={\n+                \"include_obfuscation\": True,\n+                \"include_usage\": True,\n+            },\n             suffix=\"test.\",\n             temperature=1,\n             top_p=1,\n@@ -165,7 +171,10 @@ class TestAsyncCompletions:\n             seed=0,\n             stop=\"\\n\",\n             stream=False,\n-            stream_options={\"include_usage\": True},\n+            stream_options={\n+                \"include_obfuscation\": True,\n+                \"include_usage\": True,\n+            },\n             suffix=\"test.\",\n             temperature=1,\n             top_p=1,\n@@ -224,7 +233,10 @@ class TestAsyncCompletions:\n             presence_penalty=-2,\n             seed=0,\n             stop=\"\\n\",\n-            stream_options={\"include_usage\": True},\n+            stream_options={\n+                \"include_obfuscation\": True,\n+                \"include_usage\": True,\n+            },\n             suffix=\"test.\",\n             temperature=1,\n             top_p=1,\n"
        },
        {
            "commit": "ed370d805e4d5d1ec14a136f5b2516751277059f",
            "file_path": "tests/api_resources/test_responses.py",
            "diff": "diff --git a/tests/api_resources/test_responses.py b/tests/api_resources/test_responses.py\nindex 63e47d8..4f8c88f 100644\n--- a/tests/api_resources/test_responses.py\n+++ b/tests/api_resources/test_responses.py\n@@ -45,7 +45,7 @@ class TestResponses:\n             },\n             prompt_cache_key=\"prompt-cache-key-1234\",\n             reasoning={\n-                \"effort\": \"low\",\n+                \"effort\": \"minimal\",\n                 \"generate_summary\": \"auto\",\n                 \"summary\": \"auto\",\n             },\n@@ -53,6 +53,7 @@ class TestResponses:\n             service_tier=\"auto\",\n             store=True,\n             stream=False,\n+            stream_options={\"include_obfuscation\": True},\n             temperature=1,\n             text={\"format\": {\"type\": \"text\"}},\n             tool_choice=\"none\",\n@@ -69,6 +70,7 @@ class TestResponses:\n             top_p=1,\n             truncation=\"auto\",\n             user=\"user-1234\",\n+            verbosity=\"low\",\n         )\n         assert_matches_type(Response, response, path=[\"response\"])\n \n@@ -120,13 +122,14 @@ class TestResponses:\n             },\n             prompt_cache_key=\"prompt-cache-key-1234\",\n             reasoning={\n-                \"effort\": \"low\",\n+                \"effort\": \"minimal\",\n                 \"generate_summary\": \"auto\",\n                 \"summary\": \"auto\",\n             },\n             safety_identifier=\"safety-identifier-1234\",\n             service_tier=\"auto\",\n             store=True,\n+            stream_options={\"include_obfuscation\": True},\n             temperature=1,\n             text={\"format\": {\"type\": \"text\"}},\n             tool_choice=\"none\",\n@@ -143,6 +146,7 @@ class TestResponses:\n             top_p=1,\n             truncation=\"auto\",\n             user=\"user-1234\",\n+            verbosity=\"low\",\n         )\n         response_stream.response.close()\n \n@@ -181,6 +185,7 @@ class TestResponses:\n         response = client.responses.retrieve(\n             response_id=\"resp_677efb5139a88190b512bc3fef8e535d\",\n             include=[\"code_interpreter_call.outputs\"],\n+            include_obfuscation=True,\n             starting_after=0,\n             stream=False,\n         )\n@@ -231,6 +236,7 @@ class TestResponses:\n             response_id=\"resp_677efb5139a88190b512bc3fef8e535d\",\n             stream=True,\n             include=[\"code_interpreter_call.outputs\"],\n+            include_obfuscation=True,\n             starting_after=0,\n         )\n         response_stream.response.close()\n@@ -386,7 +392,7 @@ class TestAsyncResponses:\n             },\n             prompt_cache_key=\"prompt-cache-key-1234\",\n             reasoning={\n-                \"effort\": \"low\",\n+                \"effort\": \"minimal\",\n                 \"generate_summary\": \"auto\",\n                 \"summary\": \"auto\",\n             },\n@@ -394,6 +400,7 @@ class TestAsyncResponses:\n             service_tier=\"auto\",\n             store=True,\n             stream=False,\n+            stream_options={\"include_obfuscation\": True},\n             temperature=1,\n             text={\"format\": {\"type\": \"text\"}},\n             tool_choice=\"none\",\n@@ -410,6 +417,7 @@ class TestAsyncResponses:\n             top_p=1,\n             truncation=\"auto\",\n             user=\"user-1234\",\n+            verbosity=\"low\",\n         )\n         assert_matches_type(Response, response, path=[\"response\"])\n \n@@ -461,13 +469,14 @@ class TestAsyncResponses:\n             },\n             prompt_cache_key=\"prompt-cache-key-1234\",\n             reasoning={\n-                \"effort\": \"low\",\n+                \"effort\": \"minimal\",\n                 \"generate_summary\": \"auto\",\n                 \"summary\": \"auto\",\n             },\n             safety_identifier=\"safety-identifier-1234\",\n             service_tier=\"auto\",\n             store=True,\n+            stream_options={\"include_obfuscation\": True},\n             temperature=1,\n             text={\"format\": {\"type\": \"text\"}},\n             tool_choice=\"none\",\n@@ -484,6 +493,7 @@ class TestAsyncResponses:\n             top_p=1,\n             truncation=\"auto\",\n             user=\"user-1234\",\n+            verbosity=\"low\",\n         )\n         await response_stream.response.aclose()\n \n@@ -522,6 +532,7 @@ class TestAsyncResponses:\n         response = await async_client.responses.retrieve(\n             response_id=\"resp_677efb5139a88190b512bc3fef8e535d\",\n             include=[\"code_interpreter_call.outputs\"],\n+            include_obfuscation=True,\n             starting_after=0,\n             stream=False,\n         )\n@@ -572,6 +583,7 @@ class TestAsyncResponses:\n             response_id=\"resp_677efb5139a88190b512bc3fef8e535d\",\n             stream=True,\n             include=[\"code_interpreter_call.outputs\"],\n+            include_obfuscation=True,\n             starting_after=0,\n         )\n         await response_stream.response.aclose()\n"
        }
    ]
}