{
    "sha_fail": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
    "changed_files": [
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/agent_concepts/user_control_flows/__init__.py",
            "diff": "diff --git a/cookbook/agent_concepts/user_control_flows/__init__.py b/cookbook/agent_concepts/user_control_flows/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/agent_concepts/user_control_flows/confirmation_required.py",
            "diff": "diff --git a/cookbook/agent_concepts/user_control_flows/confirmation_required.py b/cookbook/agent_concepts/user_control_flows/confirmation_required.py\nnew file mode 100644\nindex 000000000..c23da64c2\n--- /dev/null\n+++ b/cookbook/agent_concepts/user_control_flows/confirmation_required.py\n@@ -0,0 +1,88 @@\n+\"\"\"\ud83e\udd1d Human-in-the-Loop: Adding User Confirmation to Tool Calls\n+\n+This example shows how to implement human-in-the-loop functionality in your Agno tools.\n+It shows how to:\n+- Handle user confirmation during tool execution\n+- Gracefully cancel operations based on user choice\n+\n+Some practical applications:\n+- Confirming sensitive operations before execution\n+- Reviewing API calls before they're made\n+- Validating data transformations\n+- Approving automated actions in critical systems\n+\n+Run `pip install openai httpx rich agno` to install dependencies.\n+\"\"\"\n+\n+import json\n+\n+import httpx\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+from agno.tools import tool\n+from agno.utils import pprint\n+from rich.console import Console\n+from rich.prompt import Prompt\n+\n+console = Console()\n+\n+\n+@tool(requires_confirmation=True)\n+def get_top_hackernews_stories(num_stories: int) -> str:\n+    \"\"\"Fetch top stories from Hacker News.\n+\n+    Args:\n+        num_stories (int): Number of stories to retrieve\n+\n+    Returns:\n+        str: JSON string containing story details\n+    \"\"\"\n+    # Fetch top story IDs\n+    response = httpx.get(\"https://hacker-news.firebaseio.com/v0/topstories.json\")\n+    story_ids = response.json()\n+\n+    # Yield story details\n+    all_stories = []\n+    for story_id in story_ids[:num_stories]:\n+        story_response = httpx.get(\n+            f\"https://hacker-news.firebaseio.com/v0/item/{story_id}.json\"\n+        )\n+        story = story_response.json()\n+        if \"text\" in story:\n+            story.pop(\"text\", None)\n+        all_stories.append(story)\n+    return json.dumps(all_stories)\n+\n+\n+agent = Agent(\n+    model=OpenAIChat(id=\"gpt-4o-mini\"),\n+    tools=[get_top_hackernews_stories],\n+    markdown=True,\n+)\n+\n+agent.run(\"Fetch the top 2 hackernews stories\")\n+if agent.is_paused:  # Or agent.run_response.is_paused\n+    for tool in agent.run_response.tools_requiring_confirmation:\n+        # Ask for confirmation\n+        console.print(\n+            f\"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] requires confirmation.\"\n+        )\n+        message = (\n+            Prompt.ask(\"Do you want to continue?\", choices=[\"y\", \"n\"], default=\"y\")\n+            .strip()\n+            .lower()\n+        )\n+\n+        if message == \"n\":\n+            break\n+        else:\n+            # We update the tools in place\n+            tool.confirmed = True\n+\n+    run_response = (\n+        agent.continue_run()\n+    )  # or agent.continue_run(run_response=agent.run_response)\n+    pprint.pprint_run_response(run_response)\n+\n+# Or for simple debug flow\n+# agent.print_response(\"Fetch the top 2 hackernews stories\")\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/agent_concepts/user_control_flows/confirmation_required_async.py",
            "diff": "diff --git a/cookbook/agent_concepts/user_control_flows/confirmation_required_async.py b/cookbook/agent_concepts/user_control_flows/confirmation_required_async.py\nnew file mode 100644\nindex 000000000..ef012c242\n--- /dev/null\n+++ b/cookbook/agent_concepts/user_control_flows/confirmation_required_async.py\n@@ -0,0 +1,88 @@\n+\"\"\"\ud83e\udd1d Human-in-the-Loop: Adding User Confirmation to Tool Calls\n+\n+This example shows how to implement human-in-the-loop functionality in your Agno tools.\n+It shows how to:\n+- Handle user confirmation during tool execution\n+- Gracefully cancel operations based on user choice\n+\n+Some practical applications:\n+- Confirming sensitive operations before execution\n+- Reviewing API calls before they're made\n+- Validating data transformations\n+- Approving automated actions in critical systems\n+\n+Run `pip install openai httpx rich agno` to install dependencies.\n+\"\"\"\n+\n+import asyncio\n+import json\n+\n+import httpx\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+from agno.tools import tool\n+from agno.utils import pprint\n+from rich.console import Console\n+from rich.prompt import Prompt\n+\n+console = Console()\n+\n+\n+@tool(requires_confirmation=True)\n+def get_top_hackernews_stories(num_stories: int) -> str:\n+    \"\"\"Fetch top stories from Hacker News.\n+\n+    Args:\n+        num_stories (int): Number of stories to retrieve\n+\n+    Returns:\n+        str: JSON string containing story details\n+    \"\"\"\n+    # Fetch top story IDs\n+    response = httpx.get(\"https://hacker-news.firebaseio.com/v0/topstories.json\")\n+    story_ids = response.json()\n+\n+    # Yield story details\n+    all_stories = []\n+    for story_id in story_ids[:num_stories]:\n+        story_response = httpx.get(\n+            f\"https://hacker-news.firebaseio.com/v0/item/{story_id}.json\"\n+        )\n+        story = story_response.json()\n+        if \"text\" in story:\n+            story.pop(\"text\", None)\n+        all_stories.append(story)\n+    return json.dumps(all_stories)\n+\n+\n+agent = Agent(\n+    model=OpenAIChat(id=\"gpt-4o-mini\"),\n+    tools=[get_top_hackernews_stories],\n+    markdown=True,\n+)\n+\n+run_response = asyncio.run(agent.arun(\"Fetch the top 2 hackernews stories\"))\n+if run_response.is_paused:\n+    for tool in run_response.tools_requiring_confirmation:\n+        # Ask for confirmation\n+        console.print(\n+            f\"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] requires confirmation.\"\n+        )\n+        message = (\n+            Prompt.ask(\"Do you want to continue?\", choices=[\"y\", \"n\"], default=\"y\")\n+            .strip()\n+            .lower()\n+        )\n+\n+        if message == \"n\":\n+            break\n+        else:\n+            # We update the tools in place\n+            tool.confirmed = True\n+\n+    run_response = asyncio.run(agent.acontinue_run(run_response=run_response))\n+    pprint.pprint_run_response(run_response)\n+\n+\n+# Or for simple debug flow\n+# asyncio.run(agent.aprint_response(\"Fetch the top 2 hackernews stories\"))\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/agent_concepts/user_control_flows/confirmation_required_multiple_tools.py",
            "diff": "diff --git a/cookbook/agent_concepts/user_control_flows/confirmation_required_multiple_tools.py b/cookbook/agent_concepts/user_control_flows/confirmation_required_multiple_tools.py\nnew file mode 100644\nindex 000000000..a5f999f86\n--- /dev/null\n+++ b/cookbook/agent_concepts/user_control_flows/confirmation_required_multiple_tools.py\n@@ -0,0 +1,99 @@\n+\"\"\"\ud83e\udd1d Human-in-the-Loop: Adding User Confirmation to Tool Calls\n+\n+This example shows how to implement human-in-the-loop functionality in your Agno tools.\n+\n+In this case we have multiple tools and only one of them requires confirmation.\n+\n+The agent should execute the tool that doesn't require confirmation and then pause for user confirmation.\n+\n+The user can then either approve or reject the tool call and the agent should continue from where it left off.\n+\"\"\"\n+\n+import json\n+\n+import httpx\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+from agno.tools import tool\n+from agno.utils import pprint\n+from rich.console import Console\n+from rich.prompt import Prompt\n+\n+console = Console()\n+\n+\n+def get_top_hackernews_stories(num_stories: int) -> str:\n+    \"\"\"Fetch top stories from Hacker News.\n+\n+    Args:\n+        num_stories (int): Number of stories to retrieve\n+\n+    Returns:\n+        str: JSON string containing story details\n+    \"\"\"\n+    # Fetch top story IDs\n+    response = httpx.get(\"https://hacker-news.firebaseio.com/v0/topstories.json\")\n+    story_ids = response.json()\n+\n+    # Yield story details\n+    all_stories = []\n+    for story_id in story_ids[:num_stories]:\n+        story_response = httpx.get(\n+            f\"https://hacker-news.firebaseio.com/v0/item/{story_id}.json\"\n+        )\n+        story = story_response.json()\n+        if \"text\" in story:\n+            story.pop(\"text\", None)\n+        all_stories.append(story)\n+    return json.dumps(all_stories)\n+\n+\n+@tool(requires_confirmation=True)\n+def send_email(to: str, subject: str, body: str) -> str:\n+    \"\"\"Send an email.\n+\n+    Args:\n+        to (str): Email address to send to\n+        subject (str): Subject of the email\n+        body (str): Body of the email\n+    \"\"\"\n+    return f\"Email sent to {to} with subject {subject} and body {body}\"\n+\n+\n+agent = Agent(\n+    model=OpenAIChat(id=\"gpt-4o-mini\"),\n+    tools=[get_top_hackernews_stories, send_email],\n+    markdown=True,\n+)\n+\n+run_response = agent.run(\n+    \"Fetch the top 2 hackernews stories and email them to john@doe.com.\"\n+)\n+if run_response.is_paused:  # Or agent.run_response.is_paused\n+    for tool in run_response.tools:\n+        if tool.requires_confirmation:\n+            # Ask for confirmation\n+            console.print(\n+                f\"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] requires confirmation.\"\n+            )\n+            message = (\n+                Prompt.ask(\"Do you want to continue?\", choices=[\"y\", \"n\"], default=\"y\")\n+                .strip()\n+                .lower()\n+            )\n+\n+            if message == \"n\":\n+                break\n+            else:\n+                # We update the tools in place\n+                tool.confirmed = True\n+        else:\n+            console.print(\n+                f\"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] was completed in [bold green]{tool.metrics.time:.2f}[/] seconds.\"\n+            )\n+\n+    run_response = agent.continue_run()\n+    pprint.pprint_run_response(run_response)\n+\n+# Or for simple debug flow\n+# agent.print_response(\"Fetch the top 2 hackernews stories\")\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/agent_concepts/user_control_flows/confirmation_required_stream.py",
            "diff": "diff --git a/cookbook/agent_concepts/user_control_flows/confirmation_required_stream.py b/cookbook/agent_concepts/user_control_flows/confirmation_required_stream.py\nnew file mode 100644\nindex 000000000..becd91c58\n--- /dev/null\n+++ b/cookbook/agent_concepts/user_control_flows/confirmation_required_stream.py\n@@ -0,0 +1,85 @@\n+\"\"\"\ud83e\udd1d Human-in-the-Loop: Adding User Confirmation to Tool Calls\n+\n+This example shows how to implement human-in-the-loop functionality in your Agno tools.\n+It shows how to:\n+- Handle user confirmation during tool execution\n+- Gracefully cancel operations based on user choice\n+\n+Some practical applications:\n+- Confirming sensitive operations before execution\n+- Reviewing API calls before they're made\n+- Validating data transformations\n+- Approving automated actions in critical systems\n+\n+Run `pip install openai httpx rich agno` to install dependencies.\n+\"\"\"\n+\n+import json\n+\n+import httpx\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+from agno.tools import tool\n+from agno.utils import pprint\n+from rich.console import Console\n+from rich.prompt import Prompt\n+\n+console = Console()\n+\n+\n+@tool(requires_confirmation=True)\n+def get_top_hackernews_stories(num_stories: int) -> str:\n+    \"\"\"Fetch top stories from Hacker News.\n+\n+    Args:\n+        num_stories (int): Number of stories to retrieve\n+\n+    Returns:\n+        str: JSON string containing story details\n+    \"\"\"\n+    # Fetch top story IDs\n+    response = httpx.get(\"https://hacker-news.firebaseio.com/v0/topstories.json\")\n+    story_ids = response.json()\n+\n+    # Yield story details\n+    all_stories = []\n+    for story_id in story_ids[:num_stories]:\n+        story_response = httpx.get(\n+            f\"https://hacker-news.firebaseio.com/v0/item/{story_id}.json\"\n+        )\n+        story = story_response.json()\n+        if \"text\" in story:\n+            story.pop(\"text\", None)\n+        all_stories.append(story)\n+    return json.dumps(all_stories)\n+\n+\n+agent = Agent(\n+    model=OpenAIChat(id=\"gpt-4o-mini\"),\n+    tools=[get_top_hackernews_stories],\n+    markdown=True,\n+)\n+\n+for run_response in agent.run(\"Fetch the top 2 hackernews stories\", stream=True):\n+    if run_response.is_paused:\n+        for tool in run_response.tools_requiring_confirmation:\n+            # Ask for confirmation\n+            console.print(\n+                f\"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] requires confirmation.\"\n+            )\n+            message = (\n+                Prompt.ask(\"Do you want to continue?\", choices=[\"y\", \"n\"], default=\"y\")\n+                .strip()\n+                .lower()\n+            )\n+\n+            if message == \"n\":\n+                break\n+            else:\n+                # We update the tools in place\n+                tool.confirmed = True\n+        run_response = agent.continue_run(run_response=run_response, stream=True)\n+        pprint.pprint_run_response(run_response)\n+\n+# Or for simple debug flow\n+# agent.print_response(\"Fetch the top 2 hackernews stories\", stream=True)\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/agent_concepts/user_control_flows/confirmation_required_stream_async.py",
            "diff": "diff --git a/cookbook/agent_concepts/user_control_flows/confirmation_required_stream_async.py b/cookbook/agent_concepts/user_control_flows/confirmation_required_stream_async.py\nnew file mode 100644\nindex 000000000..e268c5ecb\n--- /dev/null\n+++ b/cookbook/agent_concepts/user_control_flows/confirmation_required_stream_async.py\n@@ -0,0 +1,98 @@\n+\"\"\"\ud83e\udd1d Human-in-the-Loop: Adding User Confirmation to Tool Calls\n+\n+This example shows how to implement human-in-the-loop functionality in your Agno tools.\n+It shows how to:\n+- Handle user confirmation during tool execution\n+- Gracefully cancel operations based on user choice\n+\n+Some practical applications:\n+- Confirming sensitive operations before execution\n+- Reviewing API calls before they're made\n+- Validating data transformations\n+- Approving automated actions in critical systems\n+\n+Run `pip install openai httpx rich agno` to install dependencies.\n+\"\"\"\n+\n+import asyncio\n+import json\n+\n+import httpx\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+from agno.tools import tool\n+from rich.console import Console\n+from rich.prompt import Prompt\n+\n+console = Console()\n+\n+\n+@tool(requires_confirmation=True)\n+def get_top_hackernews_stories(num_stories: int) -> str:\n+    \"\"\"Fetch top stories from Hacker News.\n+\n+    Args:\n+        num_stories (int): Number of stories to retrieve\n+\n+    Returns:\n+        str: JSON string containing story details\n+    \"\"\"\n+    # Fetch top story IDs\n+    response = httpx.get(\"https://hacker-news.firebaseio.com/v0/topstories.json\")\n+    story_ids = response.json()\n+\n+    # Yield story details\n+    all_stories = []\n+    for story_id in story_ids[:num_stories]:\n+        story_response = httpx.get(\n+            f\"https://hacker-news.firebaseio.com/v0/item/{story_id}.json\"\n+        )\n+        story = story_response.json()\n+        if \"text\" in story:\n+            story.pop(\"text\", None)\n+        all_stories.append(story)\n+    return json.dumps(all_stories)\n+\n+\n+agent = Agent(\n+    model=OpenAIChat(id=\"gpt-4o-mini\"),\n+    tools=[get_top_hackernews_stories],\n+    markdown=True,\n+)\n+\n+\n+async def main():\n+    async for run_response in await agent.arun(\n+        \"Fetch the top 2 hackernews stories\", stream=True\n+    ):\n+        if run_response.is_paused:\n+            for tool in run_response.tools_requiring_confirmation:\n+                # Ask for confirmation\n+                console.print(\n+                    f\"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] requires confirmation.\"\n+                )\n+                message = (\n+                    Prompt.ask(\n+                        \"Do you want to continue?\", choices=[\"y\", \"n\"], default=\"y\"\n+                    )\n+                    .strip()\n+                    .lower()\n+                )\n+\n+                if message == \"n\":\n+                    break\n+                else:\n+                    # We update the tools in place\n+                    tool.confirmed = True\n+            run_response = await agent.acontinue_run(\n+                run_response=run_response, stream=True\n+            )\n+            async for resp in run_response:\n+                print(resp.content, end=\"\")\n+\n+    # Or for simple debug flow\n+    # await agent.aprint_response(\"Fetch the top 2 hackernews stories\", stream=True)\n+\n+\n+if __name__ == \"__main__\":\n+    asyncio.run(main())\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/agent_concepts/user_control_flows/confirmation_required_toolkit.py",
            "diff": "diff --git a/cookbook/agent_concepts/user_control_flows/confirmation_required_toolkit.py b/cookbook/agent_concepts/user_control_flows/confirmation_required_toolkit.py\nnew file mode 100644\nindex 000000000..79753fec9\n--- /dev/null\n+++ b/cookbook/agent_concepts/user_control_flows/confirmation_required_toolkit.py\n@@ -0,0 +1,53 @@\n+\"\"\"\ud83e\udd1d Human-in-the-Loop: Adding User Confirmation to Tool Calls\n+\n+This example shows how to implement human-in-the-loop functionality in your Agno tools.\n+It shows how to:\n+- Handle user confirmation during tool execution\n+- Gracefully cancel operations based on user choice\n+\n+Some practical applications:\n+- Confirming sensitive operations before execution\n+- Reviewing API calls before they're made\n+- Validating data transformations\n+- Approving automated actions in critical systems\n+\n+Run `pip install openai httpx rich agno` to install dependencies.\n+\"\"\"\n+\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+from agno.tools import tool\n+from agno.tools.yfinance import YFinanceTools\n+from agno.utils import pprint\n+from rich.console import Console\n+from rich.prompt import Prompt\n+\n+console = Console()\n+\n+agent = Agent(\n+    model=OpenAIChat(id=\"gpt-4o-mini\"),\n+    tools=[YFinanceTools(requires_confirmation_tools=[\"get_current_stock_price\"])],\n+    markdown=True,\n+)\n+\n+agent.run(\"What is the current stock price of Apple?\")\n+if agent.is_paused:  # Or agent.run_response.is_paused\n+    for tool in agent.run_response.tools_requiring_confirmation:\n+        # Ask for confirmation\n+        console.print(\n+            f\"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] requires confirmation.\"\n+        )\n+        message = (\n+            Prompt.ask(\"Do you want to continue?\", choices=[\"y\", \"n\"], default=\"y\")\n+            .strip()\n+            .lower()\n+        )\n+\n+        if message == \"n\":\n+            break\n+        else:\n+            # We update the tools in place\n+            tool.confirmed = True\n+\n+    run_response = agent.continue_run()\n+    pprint.pprint_run_response(run_response)\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/agent_concepts/user_control_flows/external_tool_execution.py",
            "diff": "diff --git a/cookbook/agent_concepts/user_control_flows/external_tool_execution.py b/cookbook/agent_concepts/user_control_flows/external_tool_execution.py\nnew file mode 100644\nindex 000000000..c6c2fb6d1\n--- /dev/null\n+++ b/cookbook/agent_concepts/user_control_flows/external_tool_execution.py\n@@ -0,0 +1,56 @@\n+\"\"\"\ud83e\udd1d Human-in-the-Loop: Execute a tool call outside of the agent\n+\n+This example shows how to implement human-in-the-loop functionality in your Agno tools.\n+It shows how to:\n+- Use external tool execution to execute a tool call outside of the agent\n+\n+Run `pip install openai agno` to install dependencies.\n+\"\"\"\n+\n+import subprocess\n+\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+from agno.tools import tool\n+from agno.utils import pprint\n+\n+\n+# We have to create a tool with the correct name, arguments and docstring for the agent to know what to call.\n+@tool(external_execution=True)\n+def execute_shell_command(command: str) -> str:\n+    \"\"\"Execute a shell command.\n+\n+    Args:\n+        command (str): The shell command to execute\n+\n+    Returns:\n+        str: The output of the shell command\n+    \"\"\"\n+    if command.startswith(\"ls\"):\n+        return subprocess.check_output(command, shell=True).decode(\"utf-8\")\n+    else:\n+        raise Exception(f\"Unsupported command: {command}\")\n+\n+\n+agent = Agent(\n+    model=OpenAIChat(id=\"gpt-4o-mini\"),\n+    tools=[execute_shell_command],\n+    markdown=True,\n+)\n+\n+run_response = agent.run(\"What files do I have in my current directory?\")\n+if run_response.is_paused:  # Or agent.run_response.is_paused\n+    for tool in run_response.tools_awaiting_external_execution:\n+        if tool.tool_name == execute_shell_command.name:\n+            print(f\"Executing {tool.tool_name} with args {tool.tool_args} externally\")\n+            # We execute the tool ourselves. You can also execute something completely external here.\n+            result = execute_shell_command.entrypoint(**tool.tool_args)\n+            # We have to set the result on the tool execution object so that the agent can continue\n+            tool.result = result\n+\n+    run_response = agent.continue_run()\n+    pprint.pprint_run_response(run_response)\n+\n+\n+# Or for simple debug flow\n+# agent.print_response(\"What files do I have in my current directory?\")\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/agent_concepts/user_control_flows/external_tool_execution_async.py",
            "diff": "diff --git a/cookbook/agent_concepts/user_control_flows/external_tool_execution_async.py b/cookbook/agent_concepts/user_control_flows/external_tool_execution_async.py\nnew file mode 100644\nindex 000000000..992d322c0\n--- /dev/null\n+++ b/cookbook/agent_concepts/user_control_flows/external_tool_execution_async.py\n@@ -0,0 +1,57 @@\n+\"\"\"\ud83e\udd1d Human-in-the-Loop: Execute a tool call outside of the agent\n+\n+This example shows how to implement human-in-the-loop functionality in your Agno tools.\n+It shows how to:\n+- Use external tool execution to execute a tool call outside of the agent\n+\n+Run `pip install openai agno` to install dependencies.\n+\"\"\"\n+\n+import asyncio\n+import subprocess\n+\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+from agno.tools import tool\n+from agno.utils import pprint\n+\n+\n+# We have to create a tool with the correct name, arguments and docstring for the agent to know what to call.\n+@tool(external_execution=True)\n+def execute_shell_command(command: str) -> str:\n+    \"\"\"Execute a shell command.\n+\n+    Args:\n+        command (str): The shell command to execute\n+\n+    Returns:\n+        str: The output of the shell command\n+    \"\"\"\n+    if command.startswith(\"ls\"):\n+        return subprocess.check_output(command, shell=True).decode(\"utf-8\")\n+    else:\n+        raise Exception(f\"Unsupported command: {command}\")\n+\n+\n+agent = Agent(\n+    model=OpenAIChat(id=\"gpt-4o-mini\"),\n+    tools=[execute_shell_command],\n+    markdown=True,\n+)\n+\n+run_response = asyncio.run(agent.arun(\"What files do I have in my current directory?\"))\n+if run_response.is_paused:  # Or agent.run_response.is_paused\n+    for tool in run_response.tools_awaiting_external_execution:\n+        if tool.tool_name == execute_shell_command.name:\n+            print(f\"Executing {tool.tool_name} with args {tool.tool_args} externally\")\n+            # We execute the tool ourselves. You can also execute something completely external here.\n+            result = execute_shell_command.entrypoint(**tool.tool_args)\n+            # We have to set the result on the tool execution object so that the agent can continue\n+            tool.result = result\n+\n+    run_response = asyncio.run(agent.acontinue_run(run_response=run_response))\n+    pprint.pprint_run_response(run_response)\n+\n+\n+# Or for simple debug flow\n+# agent.print_response(\"What files do I have in my current directory?\")\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/agent_concepts/user_control_flows/external_tool_execution_stream.py",
            "diff": "diff --git a/cookbook/agent_concepts/user_control_flows/external_tool_execution_stream.py b/cookbook/agent_concepts/user_control_flows/external_tool_execution_stream.py\nnew file mode 100644\nindex 000000000..fe0cb7a12\n--- /dev/null\n+++ b/cookbook/agent_concepts/user_control_flows/external_tool_execution_stream.py\n@@ -0,0 +1,60 @@\n+\"\"\"\ud83e\udd1d Human-in-the-Loop: Execute a tool call outside of the agent\n+\n+This example shows how to implement human-in-the-loop functionality in your Agno tools.\n+It shows how to:\n+- Use external tool execution to execute a tool call outside of the agent\n+\n+Run `pip install openai agno` to install dependencies.\n+\"\"\"\n+\n+import subprocess\n+\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+from agno.tools import tool\n+from agno.utils import pprint\n+\n+\n+# We have to create a tool with the correct name, arguments and docstring for the agent to know what to call.\n+@tool(external_execution=True)\n+def execute_shell_command(command: str) -> str:\n+    \"\"\"Execute a shell command.\n+\n+    Args:\n+        command (str): The shell command to execute\n+\n+    Returns:\n+        str: The output of the shell command\n+    \"\"\"\n+    if command.startswith(\"ls\"):\n+        return subprocess.check_output(command, shell=True).decode(\"utf-8\")\n+    else:\n+        raise Exception(f\"Unsupported command: {command}\")\n+\n+\n+agent = Agent(\n+    model=OpenAIChat(id=\"gpt-4o-mini\"),\n+    tools=[execute_shell_command],\n+    markdown=True,\n+)\n+\n+for run_response in agent.run(\n+    \"What files do I have in my current directory?\", stream=True\n+):\n+    if run_response.is_paused:\n+        for tool in run_response.tools_awaiting_external_execution:\n+            if tool.tool_name == execute_shell_command.name:\n+                print(\n+                    f\"Executing {tool.tool_name} with args {tool.tool_args} externally\"\n+                )\n+                # We execute the tool ourselves. You can also execute something completely external here.\n+                result = execute_shell_command.entrypoint(**tool.tool_args)\n+                # We have to set the result on the tool execution object so that the agent can continue\n+                tool.result = result\n+\n+        run_response = agent.continue_run(run_response=run_response, stream=True)\n+    pprint.pprint_run_response(run_response)\n+\n+\n+# Or for simple debug flow\n+# agent.print_response(\"What files do I have in my current directory?\", stream=True)\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/agent_concepts/user_control_flows/external_tool_execution_stream_async.py",
            "diff": "diff --git a/cookbook/agent_concepts/user_control_flows/external_tool_execution_stream_async.py b/cookbook/agent_concepts/user_control_flows/external_tool_execution_stream_async.py\nnew file mode 100644\nindex 000000000..4581b56a9\n--- /dev/null\n+++ b/cookbook/agent_concepts/user_control_flows/external_tool_execution_stream_async.py\n@@ -0,0 +1,70 @@\n+\"\"\"\ud83e\udd1d Human-in-the-Loop: Execute a tool call outside of the agent\n+\n+This example shows how to implement human-in-the-loop functionality in your Agno tools.\n+It shows how to:\n+- Use external tool execution to execute a tool call outside of the agent\n+\n+Run `pip install openai agno` to install dependencies.\n+\"\"\"\n+\n+import asyncio\n+import subprocess\n+\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+from agno.tools import tool\n+\n+\n+# We have to create a tool with the correct name, arguments and docstring for the agent to know what to call.\n+@tool(external_execution=True)\n+def execute_shell_command(command: str) -> str:\n+    \"\"\"Execute a shell command.\n+\n+    Args:\n+        command (str): The shell command to execute\n+\n+    Returns:\n+        str: The output of the shell command\n+    \"\"\"\n+    if command.startswith(\"ls\"):\n+        return subprocess.check_output(command, shell=True).decode(\"utf-8\")\n+    else:\n+        raise Exception(f\"Unsupported command: {command}\")\n+\n+\n+agent = Agent(\n+    model=OpenAIChat(id=\"gpt-4o-mini\"),\n+    tools=[execute_shell_command],\n+    markdown=True,\n+)\n+\n+\n+async def main():\n+    async for run_response in await agent.arun(\n+        \"What files do I have in my current directory?\", stream=True\n+    ):\n+        if run_response.is_paused:\n+            for tool in run_response.tools_awaiting_external_execution:\n+                if tool.tool_name == execute_shell_command.name:\n+                    print(\n+                        f\"Executing {tool.tool_name} with args {tool.tool_args} externally\"\n+                    )\n+                    # We execute the tool ourselves. You can also execute something completely external here.\n+                    result = execute_shell_command.entrypoint(**tool.tool_args)\n+                    # We have to set the result on the tool execution object so that the agent can continue\n+                    tool.result = result\n+            print(\"HERE\", run_response.tools)\n+            run_response = await agent.acontinue_run(\n+                run_response=run_response, stream=True\n+            )\n+            async for resp in run_response:\n+                print(resp.content, end=\"\")\n+        else:\n+            print(run_response.content, end=\"\")\n+\n+    # Or for simple debug flow\n+    # agent.print_response(\"What files do I have in my current directory?\", stream=True)\n+\n+\n+if __name__ == \"__main__\":\n+    asyncio.run(main())\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/agent_concepts/user_control_flows/external_tool_execution_toolkit.py",
            "diff": "diff --git a/cookbook/agent_concepts/user_control_flows/external_tool_execution_toolkit.py b/cookbook/agent_concepts/user_control_flows/external_tool_execution_toolkit.py\nnew file mode 100644\nindex 000000000..4f0774905\n--- /dev/null\n+++ b/cookbook/agent_concepts/user_control_flows/external_tool_execution_toolkit.py\n@@ -0,0 +1,60 @@\n+\"\"\"\ud83e\udd1d Human-in-the-Loop: Execute a tool call outside of the agent\n+\n+This example shows how to implement human-in-the-loop functionality in your Agno tools.\n+It shows how to:\n+- Use external tool execution to execute a tool call outside of the agent\n+\n+Run `pip install openai agno` to install dependencies.\n+\"\"\"\n+\n+import subprocess\n+\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+from agno.tools import tool\n+from agno.tools.toolkit import Toolkit\n+from agno.utils import pprint\n+\n+\n+class ShellTools(Toolkit):\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(\n+            tools=[self.list_dir],\n+            external_execution_required_tools=[\"list_dir\"],\n+            *args,\n+            **kwargs,\n+        )\n+\n+    def list_dir(self, directory: str):\n+        \"\"\"\n+        Lists the contents of a directory.\n+\n+        Args:\n+            directory: The directory to list.\n+\n+        Returns:\n+            A string containing the contents of the directory.\n+        \"\"\"\n+        return subprocess.check_output(f\"ls {directory}\", shell=True).decode(\"utf-8\")\n+\n+\n+tools = ShellTools()\n+\n+agent = Agent(\n+    model=OpenAIChat(id=\"gpt-4o-mini\"),\n+    tools=[tools],\n+    markdown=True,\n+)\n+\n+run_response = agent.run(\"What files do I have in my current directory?\")\n+if run_response.is_paused:  # Or agent.run_response.is_paused\n+    for tool in run_response.tools_awaiting_external_execution:\n+        if tool.tool_name == \"list_dir\":\n+            print(f\"Executing {tool.tool_name} with args {tool.tool_args} externally\")\n+            # We execute the tool ourselves. You can also execute something completely external here.\n+            result = tools.list_dir(**tool.tool_args)\n+            # We have to set the result on the tool execution object so that the agent can continue\n+            tool.result = result\n+\n+    run_response = agent.continue_run()\n+    pprint.pprint_run_response(run_response)\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/apps/playground/__init__.py",
            "diff": "diff --git a/cookbook/apps/playground/__init__.py b/cookbook/apps/playground/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/apps/playground/agno_assist.py",
            "diff": "diff --git a/cookbook/apps/playground/agno_assist.py b/cookbook/apps/playground/agno_assist.py\nnew file mode 100644\nindex 000000000..7b3e8a2ad\n--- /dev/null\n+++ b/cookbook/apps/playground/agno_assist.py\n@@ -0,0 +1,189 @@\n+\"\"\"\ud83e\udd16 Agno Assist - Your AI Assistant for Agno Framework!\n+\n+This example shows how to create an AI support assistant that combines iterative knowledge searching\n+with Agno's documentation to provide comprehensive, well-researched answers about the Agno framework.\n+\n+You can either use the \"AgnoAssist\" agent or the \"AgnoAssistVoice\" agent.\n+\n+Key Features:\n+- Iterative knowledge base searching\n+- Deep reasoning and comprehensive answers\n+- Source attribution and citations\n+- Interactive session management\n+\n+Example prompts for `AgnoAssist`:\n+- \"What is Agno and what are its key features? Generate some audio content to explain the key features.\"\n+- \"How do I create my first agent with Agno? Show me some example code.\"\n+- \"What's the difference between Level 0 and Level 1 agents?\"\n+- \"How can I add memory to my Agno agent?\"\n+- \"How do I implement RAG with Agno? Generate a diagram of the process.\"\n+\n+Example prompts for `AgnoAssistVoice`:\n+- \"What is Agno and what are its key features?\"\n+- \"How do I create my first agent with Agno?\"\n+- \"What's the difference between Level 0 and Level 1 agents?\"\n+- \"What models does Agno support?\"\n+\"\"\"\n+\n+from pathlib import Path\n+from textwrap import dedent\n+\n+from agno.agent import Agent\n+from agno.embedder.openai import OpenAIEmbedder\n+from agno.knowledge.url import UrlKnowledge\n+from agno.models.openai import OpenAIChat\n+from agno.playground import Playground, serve_playground_app\n+from agno.storage.sqlite import SqliteStorage\n+from agno.tools.dalle import DalleTools\n+from agno.tools.eleven_labs import ElevenLabsTools\n+from agno.tools.python import PythonTools\n+from agno.vectordb.lancedb import LanceDb, SearchType\n+\n+# Setup paths\n+cwd = Path(__file__).parent\n+tmp_dir = cwd.joinpath(\"tmp\")\n+tmp_dir.mkdir(parents=True, exist_ok=True)\n+\n+_description = dedent(\"\"\"\\\n+    You are AgnoAssist, an advanced AI Agent specialized in the Agno framework.\n+    Your goal is to help developers understand and effectively use Agno by providing\n+    explanations, working code examples, and optional audio explanations for complex concepts.\"\"\")\n+\n+_description_voice = dedent(\"\"\"\\\n+    You are AgnoAssistVoice, an advanced AI Agent specialized in the Agno framework.\n+    Your goal is to help developers understand and effectively use Agno by providing\n+    explanations and examples in audio format.\"\"\")\n+\n+_instructions = dedent(\"\"\"\\\n+    Your mission is to provide comprehensive support for Agno developers. Follow these steps to ensure the best possible response:\n+\n+    1. **Analyze the request**\n+        - Analyze the request to determine if it requires a knowledge search, creating an Agent, or both.\n+        - If you need to search the knowledge base, identify 1-3 key search terms related to Agno concepts.\n+        - If you need to create an Agent, search the knowledge base for relevant concepts and use the example code as a guide.\n+        - When the user asks for an Agent, they mean an Agno Agent.\n+        - All concepts are related to Agno, so you can search the knowledge base for relevant information\n+\n+    After Analysis, always start the iterative search process. No need to wait for approval from the user.\n+\n+    2. **Iterative Search Process**:\n+        - Use the `search_knowledge_base` tool to search for related concepts, code examples and implementation details\n+        - Continue searching until you have found all the information you need or you have exhausted all the search terms\n+\n+    After the iterative search process, determine if you need to create an Agent.\n+    If you do, ask the user if they want you to create the Agent and run it.\n+\n+    3. **Code Creation and Execution**\n+        - Create complete, working code examples that users can run. For example:\n+        ```python\n+        from agno.agent import Agent\n+        from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+        agent = Agent(tools=[DuckDuckGoTools()])\n+\n+        # Perform a web search and capture the response\n+        response = agent.run(\"What's happening in France?\")\n+        ```\n+        - You must remember to use agent.run() and NOT agent.print_response()\n+        - This way you can capture the response and return it to the user\n+        - Use the `save_to_file_and_run` tool to save it to a file and run.\n+        - Make sure to return the `response` variable that tells you the result\n+        - Remember to:\n+            * Build the complete agent implementation and test with `response = agent.run()`\n+            * Include all necessary imports and setup\n+            * Add comprehensive comments explaining the implementation\n+            * Test the agent with example queries\n+            * Ensure all dependencies are listed\n+            * Include error handling and best practices\n+            * Add type hints and documentation\n+\n+    4. **Explain important concepts using audio**\n+        - When explaining complex concepts or important features, ask the user if they'd like to hear an audio explanation\n+        - Use the ElevenLabs text_to_speech tool to create clear, professional audio content\n+        - The voice is pre-selected, so you don't need to specify the voice.\n+        - Keep audio explanations concise (60-90 seconds)\n+        - Make your explanation really engaging with:\n+            * Brief concept overview and avoid jargon\n+            * Talk about the concept in a way that is easy to understand\n+            * Use practical examples and real-world scenarios\n+            * Include common pitfalls to avoid\n+\n+    5. **Explain concepts with images**\n+        - You have access to the extremely powerful DALL-E 3 model.\n+        - Use the `create_image` tool to create extremely vivid images of your explanation.\n+\n+    Key topics to cover:\n+    - Agent levels and capabilities\n+    - Knowledge base and memory management\n+    - Tool integration\n+    - Model support and configuration\n+    - Best practices and common patterns\"\"\")\n+\n+# Initialize knowledge base\n+agent_knowledge = UrlKnowledge(\n+    urls=[\"https://docs.agno.com/llms-full.txt\"],\n+    vector_db=LanceDb(\n+        uri=\"tmp/lancedb\",\n+        table_name=\"agno_assist_knowledge\",\n+        search_type=SearchType.hybrid,\n+        embedder=OpenAIEmbedder(id=\"text-embedding-3-small\"),\n+    ),\n+)\n+\n+# Create the agent\n+agno_support = Agent(\n+    name=\"Agno_Assist\",\n+    agent_id=\"agno_assist\",\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    description=_description,\n+    instructions=_instructions,\n+    knowledge=agent_knowledge,\n+    tools=[\n+        PythonTools(base_dir=tmp_dir.joinpath(\"agents\"), read_files=True),\n+        ElevenLabsTools(\n+            voice_id=\"cgSgspJ2msm6clMCkdW9\",\n+            model_id=\"eleven_multilingual_v2\",\n+            target_directory=str(tmp_dir.joinpath(\"audio\").resolve()),\n+        ),\n+        DalleTools(model=\"dall-e-3\", size=\"1792x1024\", quality=\"hd\", style=\"vivid\"),\n+    ],\n+    storage=SqliteStorage(\n+        table_name=\"agno_assist_sessions\",\n+        db_file=\"tmp/agents.db\",\n+        auto_upgrade_schema=True,\n+    ),\n+    add_history_to_messages=True,\n+    add_datetime_to_instructions=True,\n+    markdown=True,\n+)\n+\n+agno_support_voice = Agent(\n+    name=\"Agno_Assist_Voice\",\n+    agent_id=\"agno-assist-voice\",\n+    model=OpenAIChat(\n+        id=\"gpt-4o-audio-preview\",\n+        modalities=[\"text\", \"audio\"],\n+        audio={\"voice\": \"alloy\", \"format\": \"pcm16\"},  # Wav not supported for streaming\n+    ),\n+    description=_description_voice,\n+    instructions=_instructions,\n+    knowledge=agent_knowledge,\n+    tools=[PythonTools(base_dir=tmp_dir.joinpath(\"agents\"), read_files=True)],\n+    storage=SqliteStorage(\n+        table_name=\"agno_assist_sessions\",\n+        db_file=\"tmp/agents.db\",\n+        auto_upgrade_schema=True,\n+    ),\n+    add_history_to_messages=True,\n+    add_datetime_to_instructions=True,\n+    markdown=True,\n+)\n+\n+# Create and configure the playground app\n+app = Playground(agents=[agno_support, agno_support_voice]).get_app()\n+\n+if __name__ == \"__main__\":\n+    load_kb = False\n+    if load_kb:\n+        agent_knowledge.load(recreate=True)\n+    serve_playground_app(\"agno_assist:app\", reload=True)\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/apps/playground/audio_conversation_agent.py",
            "diff": "diff --git a/cookbook/apps/playground/audio_conversation_agent.py b/cookbook/apps/playground/audio_conversation_agent.py\nnew file mode 100644\nindex 000000000..9acd94c27\n--- /dev/null\n+++ b/cookbook/apps/playground/audio_conversation_agent.py\n@@ -0,0 +1,25 @@\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+from agno.playground import Playground, serve_playground_app\n+from agno.storage.sqlite import SqliteStorage\n+\n+audio_and_text_agent = Agent(\n+    agent_id=\"audio-text-agent\",\n+    name=\"Audio and Text Chat Agent\",\n+    model=OpenAIChat(\n+        id=\"gpt-4o-audio-preview\",\n+        modalities=[\"text\", \"audio\"],\n+        audio={\"voice\": \"alloy\", \"format\": \"pcm16\"},  # Wav not supported for streaming\n+    ),\n+    debug_mode=True,\n+    add_history_to_messages=True,\n+    add_datetime_to_instructions=True,\n+    storage=SqliteStorage(\n+        table_name=\"audio_agent\", db_file=\"tmp/audio_agent.db\", auto_upgrade_schema=True\n+    ),\n+)\n+\n+app = Playground(agents=[audio_and_text_agent]).get_app()\n+\n+if __name__ == \"__main__\":\n+    serve_playground_app(\"audio_conversation_agent:app\", reload=True)\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/apps/playground/azure_openai_agents.py",
            "diff": "diff --git a/cookbook/apps/playground/azure_openai_agents.py b/cookbook/apps/playground/azure_openai_agents.py\nnew file mode 100644\nindex 000000000..f6e3f8174\n--- /dev/null\n+++ b/cookbook/apps/playground/azure_openai_agents.py\n@@ -0,0 +1,157 @@\n+\"\"\"Run `pip install openai exa_py duckduckgo-search yfinance pypdf sqlalchemy 'fastapi[standard]' agno youtube-transcript-api` to install dependencies.\"\"\"\n+\n+from datetime import datetime\n+from textwrap import dedent\n+\n+from agno.agent import Agent\n+from agno.models.azure.openai_chat import AzureOpenAI\n+from agno.playground import Playground, serve_playground_app\n+from agno.storage.sqlite import SqliteStorage\n+from agno.tools.dalle import DalleTools\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.tools.exa import ExaTools\n+from agno.tools.yfinance import YFinanceTools\n+from agno.tools.youtube import YouTubeTools\n+\n+agent_storage_file: str = \"tmp/azure_openai_agents.db\"\n+\n+web_agent = Agent(\n+    name=\"Web Agent\",\n+    role=\"Search the web for information\",\n+    agent_id=\"web-agent\",\n+    model=AzureOpenAI(id=\"gpt-4o\"),\n+    tools=[DuckDuckGoTools()],\n+    instructions=[\n+        \"Break down the users request into 2-3 different searches.\",\n+        \"Always include sources\",\n+    ],\n+    storage=SqliteStorage(\n+        table_name=\"web_agent\", db_file=agent_storage_file, auto_upgrade_schema=True\n+    ),\n+    add_history_to_messages=True,\n+    num_history_responses=5,\n+    add_datetime_to_instructions=True,\n+    markdown=True,\n+)\n+\n+finance_agent = Agent(\n+    name=\"Finance Agent\",\n+    role=\"Get financial data\",\n+    agent_id=\"finance-agent\",\n+    model=AzureOpenAI(id=\"gpt-4o\"),\n+    tools=[\n+        YFinanceTools(\n+            stock_price=True,\n+            analyst_recommendations=True,\n+            company_info=True,\n+            company_news=True,\n+        )\n+    ],\n+    instructions=[\"Always use tables to display data\"],\n+    storage=SqliteStorage(\n+        table_name=\"finance_agent\", db_file=agent_storage_file, auto_upgrade_schema=True\n+    ),\n+    add_history_to_messages=True,\n+    num_history_responses=5,\n+    add_datetime_to_instructions=True,\n+    markdown=True,\n+)\n+\n+image_agent = Agent(\n+    name=\"Image Agent\",\n+    agent_id=\"image_agent\",\n+    model=AzureOpenAI(id=\"gpt-4o\"),\n+    tools=[DalleTools(model=\"dall-e-3\", size=\"1792x1024\", quality=\"hd\", style=\"vivid\")],\n+    description=\"You are an AI agent that can generate images using DALL-E.\",\n+    instructions=[\n+        \"When the user asks you to create an image, use the `create_image` tool to create the image.\",\n+        \"Don't provide the URL of the image in the response. Only describe what image was generated.\",\n+    ],\n+    markdown=True,\n+    debug_mode=True,\n+    add_history_to_messages=True,\n+    add_datetime_to_instructions=True,\n+    storage=SqliteStorage(\n+        table_name=\"image_agent\", db_file=agent_storage_file, auto_upgrade_schema=True\n+    ),\n+)\n+\n+research_agent = Agent(\n+    name=\"Research Agent\",\n+    role=\"Write research reports for the New York Times\",\n+    agent_id=\"research-agent\",\n+    model=AzureOpenAI(id=\"gpt-4o\"),\n+    tools=[\n+        ExaTools(\n+            start_published_date=datetime.now().strftime(\"%Y-%m-%d\"), type=\"keyword\"\n+        )\n+    ],\n+    description=(\n+        \"You are a Research Agent that has the special skill of writing New York Times worthy articles. \"\n+        \"If you can directly respond to the user, do so. If the user asks for a report or provides a topic, follow the instructions below.\"\n+    ),\n+    instructions=[\n+        \"For the provided topic, run 3 different searches.\",\n+        \"Read the results carefully and prepare a NYT worthy article.\",\n+        \"Focus on facts and make sure to provide references.\",\n+    ],\n+    expected_output=dedent(\"\"\"\\\n+    Your articles should be engaging, informative, well-structured and in markdown format. They should follow the following structure:\n+\n+    ## Engaging Article Title\n+\n+    ### Overview\n+    {give a brief introduction of the article and why the user should read this report}\n+    {make this section engaging and create a hook for the reader}\n+\n+    ### Section 1\n+    {break the article into sections}\n+    {provide details/facts/processes in this section}\n+\n+    ... more sections as necessary...\n+\n+    ### Takeaways\n+    {provide key takeaways from the article}\n+\n+    ### References\n+    - [Reference 1](link)\n+    - [Reference 2](link)\n+    \"\"\"),\n+    storage=SqliteStorage(\n+        table_name=\"research_agent\",\n+        db_file=agent_storage_file,\n+        auto_upgrade_schema=True,\n+    ),\n+    add_history_to_messages=True,\n+    add_datetime_to_instructions=True,\n+    markdown=True,\n+)\n+\n+youtube_agent = Agent(\n+    name=\"YouTube Agent\",\n+    agent_id=\"youtube-agent\",\n+    model=AzureOpenAI(id=\"gpt-4o\"),\n+    tools=[YouTubeTools()],\n+    description=\"You are a YouTube agent that has the special skill of understanding YouTube videos and answering questions about them.\",\n+    instructions=[\n+        \"Using a video URL, get the video data using the `get_youtube_video_data` tool and captions using the `get_youtube_video_data` tool.\",\n+        \"Using the data and captions, answer the user's question in an engaging and thoughtful manner. Focus on the most important details.\",\n+        \"If you cannot find the answer in the video, say so and ask the user to provide more details.\",\n+        \"Keep your answers concise and engaging.\",\n+    ],\n+    add_history_to_messages=True,\n+    num_history_responses=5,\n+    show_tool_calls=True,\n+    add_datetime_to_instructions=True,\n+    storage=SqliteStorage(\n+        table_name=\"youtube_agent\", db_file=agent_storage_file, auto_upgrade_schema=True\n+    ),\n+    markdown=True,\n+)\n+\n+app = Playground(\n+    agents=[web_agent, finance_agent, youtube_agent, research_agent, image_agent]\n+).get_app()\n+\n+if __name__ == \"__main__\":\n+    serve_playground_app(\"azure_openai_agents:app\", reload=True)\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/apps/playground/basic.py",
            "diff": "diff --git a/cookbook/apps/playground/basic.py b/cookbook/apps/playground/basic.py\nnew file mode 100644\nindex 000000000..e5e05967a\n--- /dev/null\n+++ b/cookbook/apps/playground/basic.py\n@@ -0,0 +1,46 @@\n+from agno.agent import Agent\n+from agno.memory.agent import AgentMemory\n+from agno.memory.db.postgres import PgMemoryDb\n+from agno.models.openai import OpenAIChat\n+from agno.playground import Playground, serve_playground_app\n+from agno.storage.agent.sqlite import SqliteAgentStorage\n+from agno.storage.postgres import PostgresStorage\n+\n+agent_storage_file: str = \"tmp/agents.db\"\n+\n+db_url = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n+\n+basic_agent = Agent(\n+    name=\"Basic Agent\",\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    memory=AgentMemory(\n+        db=PgMemoryDb(\n+            table_name=\"agent_memory\",\n+            db_url=db_url,\n+        ),\n+        # Create and store personalized memories for this user\n+        create_user_memories=True,\n+        # Update memories for the user after each run\n+        update_user_memories_after_run=True,\n+        # Create and store session summaries\n+        create_session_summary=True,\n+        # Update session summaries after each run\n+        update_session_summary_after_run=True,\n+    ),\n+    storage=PostgresStorage(\n+        table_name=\"agent_sessions\", db_url=db_url, auto_upgrade_schema=True\n+    ),\n+    add_history_to_messages=True,\n+    num_history_responses=3,\n+    add_datetime_to_instructions=True,\n+    markdown=True,\n+)\n+\n+app = Playground(\n+    agents=[\n+        basic_agent,\n+    ]\n+).get_app()\n+\n+if __name__ == \"__main__\":\n+    serve_playground_app(\"basic:app\", reload=True)\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/apps/playground/blog_to_podcast.py",
            "diff": "diff --git a/cookbook/apps/playground/blog_to_podcast.py b/cookbook/apps/playground/blog_to_podcast.py\nnew file mode 100644\nindex 000000000..d44ff77d1\n--- /dev/null\n+++ b/cookbook/apps/playground/blog_to_podcast.py\n@@ -0,0 +1,52 @@\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+from agno.playground import Playground, serve_playground_app\n+from agno.storage.sqlite import SqliteStorage\n+from agno.tools.eleven_labs import ElevenLabsTools\n+from agno.tools.firecrawl import FirecrawlTools\n+\n+agent_storage_file: str = \"tmp/agents.db\"\n+image_agent_storage_file: str = \"tmp/image_agent.db\"\n+\n+\n+blog_to_podcast_agent = Agent(\n+    name=\"Blog to Podcast Agent\",\n+    agent_id=\"blog_to_podcast_agent\",\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    tools=[\n+        ElevenLabsTools(\n+            voice_id=\"JBFqnCBsd6RMkjVDRZzb\",\n+            model_id=\"eleven_multilingual_v2\",\n+            target_directory=\"audio_generations\",\n+        ),\n+        FirecrawlTools(),\n+    ],\n+    description=\"You are an AI agent that can generate audio using the ElevenLabs API.\",\n+    instructions=[\n+        \"When the user provides a blog URL:\",\n+        \"1. Use FirecrawlTools to scrape the blog content\",\n+        \"2. Create a concise summary of the blog content that is NO MORE than 2000 characters long\",\n+        \"3. The summary should capture the main points while being engaging and conversational\",\n+        \"4. Use the ElevenLabsTools to convert the summary to audio\",\n+        \"You don't need to find the appropriate voice first, I already specified the voice to user\",\n+        \"Don't return file name or file url in your response or markdown just tell the audio was created successfully\",\n+        \"Ensure the summary is within the 2000 character limit to avoid ElevenLabs API limits\",\n+    ],\n+    markdown=True,\n+    debug_mode=True,\n+    add_history_to_messages=True,\n+    storage=SqliteStorage(\n+        table_name=\"blog_to_podcast_agent\",\n+        db_file=image_agent_storage_file,\n+        auto_upgrade_schema=True,\n+    ),\n+)\n+\n+app = Playground(\n+    agents=[\n+        blog_to_podcast_agent,\n+    ]\n+).get_app(use_async=False)\n+\n+if __name__ == \"__main__\":\n+    serve_playground_app(\"blog_to_podcast:app\", reload=True)\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/apps/playground/coding_agent.py",
            "diff": "diff --git a/cookbook/apps/playground/coding_agent.py b/cookbook/apps/playground/coding_agent.py\nnew file mode 100644\nindex 000000000..0d69100bc\n--- /dev/null\n+++ b/cookbook/apps/playground/coding_agent.py\n@@ -0,0 +1,32 @@\n+\"\"\"Run `pip install ollama sqlalchemy 'fastapi[standard]'` to install dependencies.\"\"\"\n+\n+from agno.agent import Agent\n+from agno.models.ollama import Ollama\n+from agno.playground import Playground, serve_playground_app\n+from agno.storage.sqlite import SqliteStorage\n+\n+local_agent_storage_file: str = \"tmp/local_agents.db\"\n+common_instructions = [\n+    \"If the user asks about you or your skills, tell them your name and role.\",\n+]\n+\n+coding_agent = Agent(\n+    name=\"Coding Agent\",\n+    agent_id=\"coding_agent\",\n+    model=Ollama(id=\"hhao/qwen2.5-coder-tools:32b\"),\n+    reasoning=True,\n+    markdown=True,\n+    add_history_to_messages=True,\n+    description=\"You are a coding agent\",\n+    add_datetime_to_instructions=True,\n+    storage=SqliteStorage(\n+        table_name=\"coding_agent\",\n+        db_file=local_agent_storage_file,\n+        auto_upgrade_schema=True,\n+    ),\n+)\n+\n+app = Playground(agents=[coding_agent]).get_app()\n+\n+if __name__ == \"__main__\":\n+    serve_playground_app(\"coding_agent:app\", reload=True)\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/apps/playground/demo.py",
            "diff": "diff --git a/cookbook/apps/playground/demo.py b/cookbook/apps/playground/demo.py\nnew file mode 100644\nindex 000000000..e3eee218b\n--- /dev/null\n+++ b/cookbook/apps/playground/demo.py\n@@ -0,0 +1,232 @@\n+\"\"\"Run `pip install openai exa_py duckduckgo-search yfinance pypdf sqlalchemy 'fastapi[standard]' youtube-transcript-api python-docx agno` to install dependencies.\"\"\"\n+\n+from datetime import datetime\n+from textwrap import dedent\n+from typing import List\n+\n+from agno.agent import Agent\n+from agno.memory.v2 import Memory\n+from agno.memory.v2.db.sqlite import SqliteMemoryDb\n+from agno.models.openai import OpenAIChat\n+from agno.playground import Playground, serve_playground_app\n+from agno.storage.sqlite import SqliteStorage\n+from agno.tools.dalle import DalleTools\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.tools.exa import ExaTools\n+from agno.tools.yfinance import YFinanceTools\n+from agno.tools.youtube import YouTubeTools\n+from pydantic import BaseModel, Field\n+\n+agent_storage_file: str = \"tmp/agents.db\"\n+memory_storage_file: str = \"tmp/memory.db\"\n+image_agent_storage_file: str = \"tmp/image_agent.db\"\n+\n+db_url = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n+\n+memory_db = SqliteMemoryDb(table_name=\"memory\", db_file=memory_storage_file)\n+\n+# No need to set the model, it gets set by the agent to the agent's model\n+memory = Memory(db=memory_db)\n+\n+simple_agent = Agent(\n+    name=\"Simple Agent\",\n+    role=\"Answer basic questions\",\n+    agent_id=\"simple-agent\",\n+    model=OpenAIChat(id=\"gpt-4o-mini\"),\n+    storage=SqliteStorage(\n+        table_name=\"simple_agent\", db_file=agent_storage_file, auto_upgrade_schema=True\n+    ),\n+    memory=memory,\n+    enable_user_memories=True,\n+    add_history_to_messages=True,\n+    num_history_responses=5,\n+    add_datetime_to_instructions=True,\n+    markdown=True,\n+)\n+\n+web_agent = Agent(\n+    name=\"Web Agent\",\n+    role=\"Search the web for information\",\n+    agent_id=\"web-agent\",\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    tools=[DuckDuckGoTools()],\n+    instructions=[\n+        \"Break down the users request into 2-3 different searches.\",\n+        \"Always include sources\",\n+    ],\n+    storage=SqliteStorage(\n+        table_name=\"web_agent\", db_file=agent_storage_file, auto_upgrade_schema=True\n+    ),\n+    memory=memory,\n+    enable_user_memories=True,\n+    add_history_to_messages=True,\n+    num_history_responses=5,\n+    add_datetime_to_instructions=True,\n+    markdown=True,\n+)\n+\n+finance_agent = Agent(\n+    name=\"Finance Agent\",\n+    role=\"Get financial data\",\n+    agent_id=\"finance-agent\",\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    tools=[\n+        YFinanceTools(\n+            stock_price=True,\n+            analyst_recommendations=True,\n+            company_info=True,\n+            company_news=True,\n+        )\n+    ],\n+    instructions=[\"Always use tables to display data\"],\n+    storage=SqliteStorage(\n+        table_name=\"finance_agent\", db_file=agent_storage_file, auto_upgrade_schema=True\n+    ),\n+    memory=memory,\n+    enable_user_memories=True,\n+    add_history_to_messages=True,\n+    num_history_responses=5,\n+    add_datetime_to_instructions=True,\n+    markdown=True,\n+)\n+\n+image_agent = Agent(\n+    name=\"Image Agent\",\n+    agent_id=\"image_agent\",\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    tools=[DalleTools(model=\"dall-e-3\", size=\"1792x1024\", quality=\"hd\", style=\"vivid\")],\n+    description=\"You are an AI agent that can generate images using DALL-E.\",\n+    instructions=[\n+        \"When the user asks you to create an image, use the `create_image` tool to create the image.\",\n+        \"Don't provide the URL of the image in the response. Only describe what image was generated.\",\n+    ],\n+    memory=memory,\n+    markdown=True,\n+    enable_user_memories=True,\n+    add_history_to_messages=True,\n+    add_datetime_to_instructions=True,\n+    storage=SqliteStorage(\n+        table_name=\"image_agent\",\n+        db_file=image_agent_storage_file,\n+        auto_upgrade_schema=True,\n+    ),\n+)\n+\n+research_agent = Agent(\n+    name=\"Research Agent\",\n+    role=\"Write research reports for the New York Times\",\n+    agent_id=\"research-agent\",\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    tools=[\n+        ExaTools(\n+            start_published_date=datetime.now().strftime(\"%Y-%m-%d\"), type=\"keyword\"\n+        )\n+    ],\n+    description=(\n+        \"You are a Research Agent that has the special skill of writing New York Times worthy articles. \"\n+        \"If you can directly respond to the user, do so. If the user asks for a report or provides a topic, follow the instructions below.\"\n+    ),\n+    instructions=[\n+        \"For the provided topic, run 3 different searches.\",\n+        \"Read the results carefully and prepare a NYT worthy article.\",\n+        \"Focus on facts and make sure to provide references.\",\n+    ],\n+    expected_output=dedent(\"\"\"\\\n+    Your articles should be engaging, informative, well-structured and in markdown format. They should follow the following structure:\n+\n+    ## Engaging Article Title\n+\n+    ### Overview\n+    {give a brief introduction of the article and why the user should read this report}\n+    {make this section engaging and create a hook for the reader}\n+\n+    ### Section 1\n+    {break the article into sections}\n+    {provide details/facts/processes in this section}\n+\n+    ... more sections as necessary...\n+\n+    ### Takeaways\n+    {provide key takeaways from the article}\n+\n+    ### References\n+    - [Reference 1](link)\n+    - [Reference 2](link)\n+    \"\"\"),\n+    memory=memory,\n+    enable_user_memories=True,\n+    storage=SqliteStorage(\n+        table_name=\"research_agent\",\n+        db_file=agent_storage_file,\n+        auto_upgrade_schema=True,\n+    ),\n+    add_history_to_messages=True,\n+    add_datetime_to_instructions=True,\n+    markdown=True,\n+)\n+\n+youtube_agent = Agent(\n+    name=\"YouTube Agent\",\n+    agent_id=\"youtube-agent\",\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    tools=[YouTubeTools()],\n+    description=\"You are a YouTube agent that has the special skill of understanding YouTube videos and answering questions about them.\",\n+    instructions=[\n+        \"Using a video URL, get the video data using the `get_youtube_video_data` tool and captions using the `get_youtube_video_data` tool.\",\n+        \"Using the data and captions, answer the user's question in an engaging and thoughtful manner. Focus on the most important details.\",\n+        \"If you cannot find the answer in the video, say so and ask the user to provide more details.\",\n+        \"Keep your answers concise and engaging.\",\n+    ],\n+    memory=memory,\n+    enable_user_memories=True,\n+    add_history_to_messages=True,\n+    num_history_responses=5,\n+    show_tool_calls=True,\n+    add_datetime_to_instructions=True,\n+    storage=SqliteStorage(\n+        table_name=\"youtube_agent\", db_file=agent_storage_file, auto_upgrade_schema=True\n+    ),\n+    markdown=True,\n+)\n+\n+\n+class MovieScript(BaseModel):\n+    setting: str = Field(\n+        ..., description=\"Provide a nice setting for a blockbuster movie.\"\n+    )\n+    ending: str = Field(\n+        ...,\n+        description=\"Ending of the movie. If not available, provide a happy ending.\",\n+    )\n+    genre: str = Field(\n+        ...,\n+        description=\"Genre of the movie. If not available, select action, thriller or romantic comedy.\",\n+    )\n+    name: str = Field(..., description=\"Give a name to this movie\")\n+    characters: List[str] = Field(..., description=\"Name of characters for this movie.\")\n+    storyline: str = Field(\n+        ..., description=\"3 sentence storyline for the movie. Make it exciting!\"\n+    )\n+\n+\n+# Notice: agents with response model won't stream answers\n+movie_writer = Agent(\n+    name=\"Movie Writer Agent\",\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    response_model=MovieScript,\n+)\n+\n+app = Playground(\n+    agents=[\n+        simple_agent,\n+        web_agent,\n+        finance_agent,\n+        youtube_agent,\n+        research_agent,\n+        image_agent,\n+        movie_writer,\n+    ],\n+).get_app()\n+\n+if __name__ == \"__main__\":\n+    serve_playground_app(\"demo:app\", reload=True)\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/apps/playground/gemini_agents.py",
            "diff": "diff --git a/cookbook/apps/playground/gemini_agents.py b/cookbook/apps/playground/gemini_agents.py\nnew file mode 100644\nindex 000000000..c001eb9d9\n--- /dev/null\n+++ b/cookbook/apps/playground/gemini_agents.py\n@@ -0,0 +1,17 @@\n+from agno.agent import Agent\n+from agno.models.google import Gemini\n+from agno.playground import Playground, serve_playground_app\n+from agno.tools.yfinance import YFinanceTools\n+\n+finance_agent = Agent(\n+    name=\"Finance Agent\",\n+    agent_id=\"finance-agent\",\n+    model=Gemini(id=\"gemini-2.0-flash-exp\"),\n+    tools=[YFinanceTools(stock_price=True)],\n+    debug_mode=True,\n+)\n+\n+app = Playground(agents=[finance_agent]).get_app(use_async=False)\n+\n+if __name__ == \"__main__\":\n+    serve_playground_app(\"gemini_agents:app\", reload=True)\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/apps/playground/grok_agents.py",
            "diff": "diff --git a/cookbook/apps/playground/grok_agents.py b/cookbook/apps/playground/grok_agents.py\nnew file mode 100644\nindex 000000000..ea3f64da1\n--- /dev/null\n+++ b/cookbook/apps/playground/grok_agents.py\n@@ -0,0 +1,96 @@\n+\"\"\"Usage:\n+1. Install libraries: `pip install openai duckduckgo-search yfinance pypdf sqlalchemy 'fastapi[standard]' youtube-transcript-api agno`\n+2. Run the script: `python cookbook/playground/grok_agents.py`\n+\"\"\"\n+\n+from agno.agent import Agent\n+from agno.models.xai import xAI\n+from agno.playground import Playground, serve_playground_app\n+from agno.storage.sqlite import SqliteStorage\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.tools.yfinance import YFinanceTools\n+from agno.tools.youtube import YouTubeTools\n+\n+xai_agent_storage: str = \"tmp/xai_agents.db\"\n+common_instructions = [\n+    \"If the user about you or your skills, tell them your name and role.\",\n+]\n+\n+web_agent = Agent(\n+    name=\"Web Agent\",\n+    role=\"Search the web for information\",\n+    agent_id=\"web-agent\",\n+    model=xAI(id=\"grok-beta\"),\n+    tools=[DuckDuckGoTools()],\n+    instructions=[\n+        \"Use the `duckduckgo_search` or `duckduckgo_news` tools to search the web for information.\",\n+        \"Always include sources you used to generate the answer.\",\n+    ]\n+    + common_instructions,\n+    storage=SqliteStorage(\n+        table_name=\"web_agent\", db_file=xai_agent_storage, auto_upgrade_schema=True\n+    ),\n+    show_tool_calls=True,\n+    add_history_to_messages=True,\n+    num_history_responses=2,\n+    add_name_to_instructions=True,\n+    add_datetime_to_instructions=True,\n+    markdown=True,\n+)\n+\n+finance_agent = Agent(\n+    name=\"Finance Agent\",\n+    role=\"Get financial data\",\n+    agent_id=\"finance-agent\",\n+    model=xAI(id=\"grok-beta\"),\n+    tools=[\n+        YFinanceTools(\n+            stock_price=True,\n+            analyst_recommendations=True,\n+            company_info=True,\n+            company_news=True,\n+        )\n+    ],\n+    description=\"You are an investment analyst that researches stocks and helps users make informed decisions.\",\n+    instructions=[\"Always use tables to display data\"] + common_instructions,\n+    storage=SqliteStorage(\n+        table_name=\"finance_agent\", db_file=xai_agent_storage, auto_upgrade_schema=True\n+    ),\n+    show_tool_calls=True,\n+    add_history_to_messages=True,\n+    num_history_responses=5,\n+    add_name_to_instructions=True,\n+    add_datetime_to_instructions=True,\n+    markdown=True,\n+)\n+\n+\n+youtube_agent = Agent(\n+    name=\"YouTube Agent\",\n+    role=\"Understand YouTube videos and answer questions\",\n+    agent_id=\"youtube-agent\",\n+    model=xAI(id=\"grok-beta\"),\n+    tools=[YouTubeTools()],\n+    description=\"You are a YouTube agent that has the special skill of understanding YouTube videos and answering questions about them.\",\n+    instructions=[\n+        \"Using a video URL, get the video data using the `get_youtube_video_data` tool and captions using the `get_youtube_video_data` tool.\",\n+        \"Using the data and captions, answer the user's question in an engaging and thoughtful manner. Focus on the most important details.\",\n+        \"If you cannot find the answer in the video, say so and ask the user to provide more details.\",\n+        \"Keep your answers concise and engaging.\",\n+    ]\n+    + common_instructions,\n+    storage=SqliteStorage(\n+        table_name=\"youtube_agent\", db_file=xai_agent_storage, auto_upgrade_schema=True\n+    ),\n+    show_tool_calls=True,\n+    add_history_to_messages=True,\n+    num_history_responses=5,\n+    add_name_to_instructions=True,\n+    add_datetime_to_instructions=True,\n+    markdown=True,\n+)\n+\n+app = Playground(agents=[finance_agent, youtube_agent, web_agent]).get_app()\n+\n+if __name__ == \"__main__\":\n+    serve_playground_app(\"grok_agents:app\", reload=True)\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/apps/playground/groq_agents.py",
            "diff": "diff --git a/cookbook/apps/playground/groq_agents.py b/cookbook/apps/playground/groq_agents.py\nnew file mode 100644\nindex 000000000..17eb6d8e8\n--- /dev/null\n+++ b/cookbook/apps/playground/groq_agents.py\n@@ -0,0 +1,99 @@\n+\"\"\"Usage:\n+1. Install libraries: `pip install groq duckduckgo-search yfinance pypdf sqlalchemy 'fastapi[standard]' youtube-transcript-api agno`\n+2. Run the script: `python cookbook/playground/groq_agents.py`\n+\"\"\"\n+\n+from agno.agent import Agent\n+from agno.models.groq import Groq\n+from agno.playground import Playground, serve_playground_app\n+from agno.storage.sqlite import SqliteStorage\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.tools.yfinance import YFinanceTools\n+from agno.tools.youtube import YouTubeTools\n+\n+xai_agent_storage: str = \"tmp/groq_agents.db\"\n+common_instructions = [\n+    \"If the user about you or your skills, tell them your name and role.\",\n+]\n+\n+web_agent = Agent(\n+    name=\"Web Agent\",\n+    role=\"Search the web for information\",\n+    agent_id=\"web-agent\",\n+    model=Groq(id=\"llama-3.3-70b-versatile\"),\n+    tools=[DuckDuckGoTools()],\n+    instructions=[\n+        \"Use the `duckduckgo_search` or `duckduckgo_news` tools to search the web for information.\",\n+        \"Always include sources you used to generate the answer.\",\n+    ]\n+    + common_instructions,\n+    storage=SqliteStorage(\n+        table_name=\"web_agent\", db_file=xai_agent_storage, auto_upgrade_schema=True\n+    ),\n+    show_tool_calls=True,\n+    add_history_to_messages=True,\n+    num_history_responses=2,\n+    add_name_to_instructions=True,\n+    add_datetime_to_instructions=True,\n+    markdown=True,\n+)\n+\n+finance_agent = Agent(\n+    name=\"Finance Agent\",\n+    role=\"Get financial data\",\n+    agent_id=\"finance-agent\",\n+    model=Groq(id=\"llama-3.3-70b-versatile\"),\n+    tools=[\n+        YFinanceTools(\n+            stock_price=True,\n+            analyst_recommendations=True,\n+            company_info=True,\n+            company_news=True,\n+        )\n+    ],\n+    description=\"You are an investment analyst that researches stocks and helps users make informed decisions.\",\n+    instructions=[\"Always use tables to display data\"] + common_instructions,\n+    storage=SqliteStorage(\n+        table_name=\"finance_agent\", db_file=xai_agent_storage, auto_upgrade_schema=True\n+    ),\n+    show_tool_calls=True,\n+    add_history_to_messages=True,\n+    num_history_responses=5,\n+    add_name_to_instructions=True,\n+    add_datetime_to_instructions=True,\n+    markdown=True,\n+)\n+\n+\n+youtube_agent = Agent(\n+    name=\"YouTube Agent\",\n+    role=\"Understand YouTube videos and answer questions\",\n+    agent_id=\"youtube-agent\",\n+    model=Groq(id=\"llama-3.3-70b-versatile\"),\n+    tools=[YouTubeTools()],\n+    description=\"You are a YouTube agent that has the special skill of understanding YouTube videos and answering questions about them.\",\n+    instructions=[\n+        \"Using a video URL, get the video data using the `get_youtube_video_data` tool and captions using the `get_youtube_video_data` tool.\",\n+        \"Using the data and captions, answer the user's question in an engaging and thoughtful manner. Focus on the most important details.\",\n+        \"If you cannot find the answer in the video, say so and ask the user to provide more details.\",\n+        \"Keep your answers concise and engaging.\",\n+        \"If the user just provides a URL, summarize the video and answer questions about it.\",\n+    ]\n+    + common_instructions,\n+    storage=SqliteStorage(\n+        table_name=\"youtube_agent\", db_file=xai_agent_storage, auto_upgrade_schema=True\n+    ),\n+    show_tool_calls=True,\n+    add_history_to_messages=True,\n+    num_history_responses=5,\n+    add_name_to_instructions=True,\n+    add_datetime_to_instructions=True,\n+    markdown=True,\n+)\n+\n+app = Playground(agents=[finance_agent, youtube_agent, web_agent]).get_app(\n+    use_async=False\n+)\n+\n+if __name__ == \"__main__\":\n+    serve_playground_app(\"groq_agents:app\", reload=True)\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/apps/playground/mcp_demo.py",
            "diff": "diff --git a/cookbook/apps/playground/mcp_demo.py b/cookbook/apps/playground/mcp_demo.py\nnew file mode 100644\nindex 000000000..d5a8a09ee\n--- /dev/null\n+++ b/cookbook/apps/playground/mcp_demo.py\n@@ -0,0 +1,86 @@\n+import asyncio\n+from os import getenv\n+from textwrap import dedent\n+\n+import nest_asyncio\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+from agno.playground import Playground, serve_playground_app\n+from agno.storage.agent.sqlite import SqliteAgentStorage\n+from agno.tools.mcp import MCPTools\n+from mcp import StdioServerParameters\n+\n+# Allow nested event loops\n+nest_asyncio.apply()\n+\n+agent_storage_file: str = \"tmp/agents.db\"\n+\n+\n+async def run_server() -> None:\n+    \"\"\"Run the GitHub agent server.\"\"\"\n+    github_token = getenv(\"GITHUB_TOKEN\") or getenv(\"GITHUB_ACCESS_TOKEN\")\n+    if not github_token:\n+        raise ValueError(\"GITHUB_TOKEN environment variable is required\")\n+\n+    # Initialize the MCP server\n+    server_params = StdioServerParameters(\n+        command=\"npx\",\n+        args=[\"-y\", \"@modelcontextprotocol/server-github\"],\n+    )\n+\n+    # Create a client session to connect to the MCP server\n+    async with MCPTools(server_params=server_params) as mcp_tools:\n+        agent = Agent(\n+            name=\"MCP GitHub Agent\",\n+            tools=[mcp_tools],\n+            instructions=dedent(\"\"\"\\\n+                You are a GitHub assistant. Help users explore repositories and their activity.\n+\n+                - Use headings to organize your responses\n+                - Be concise and focus on relevant information\\\n+            \"\"\"),\n+            model=OpenAIChat(id=\"gpt-4o\"),\n+            storage=SqliteAgentStorage(\n+                table_name=\"basic_agent\",\n+                db_file=agent_storage_file,\n+                auto_upgrade_schema=True,\n+            ),\n+            add_history_to_messages=True,\n+            num_history_responses=3,\n+            add_datetime_to_instructions=True,\n+            markdown=True,\n+        )\n+\n+        playground = Playground(agents=[agent])\n+        app = playground.get_app()\n+\n+        # Serve the app while keeping the MCPTools context manager alive\n+        serve_playground_app(app)\n+\n+\n+if __name__ == \"__main__\":\n+    asyncio.run(run_server())\n+\n+# Example prompts to explore:\n+\"\"\"\n+Issue queries:\n+1. \"Find issues needing attention\"\n+2. \"Show me issues by label\"\n+3. \"What issues are being actively discussed?\"\n+4. \"Find related issues\"\n+5. \"Analyze issue resolution patterns\"\n+\n+Pull request queries:\n+1. \"What PRs need review?\"\n+2. \"Show me recent merged PRs\"\n+3. \"Find PRs with conflicts\"\n+4. \"What features are being developed?\"\n+5. \"Analyze PR review patterns\"\n+\n+Repository queries:\n+1. \"Show repository health metrics\"\n+2. \"What are the contribution guidelines?\"\n+3. \"Find documentation gaps\"\n+4. \"Analyze code quality trends\"\n+5. \"Show repository activity patterns\"\n+\"\"\"\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/apps/playground/multimodal_agents.py",
            "diff": "diff --git a/cookbook/apps/playground/multimodal_agents.py b/cookbook/apps/playground/multimodal_agents.py\nnew file mode 100644\nindex 000000000..f47366c27\n--- /dev/null\n+++ b/cookbook/apps/playground/multimodal_agents.py\n@@ -0,0 +1,192 @@\n+\"\"\"\n+1. Install dependencies: `pip install openai sqlalchemy 'fastapi[standard]' agno requests`\n+2. Authenticate with agno: `agno setup`\n+3. Run the agent: `python cookbook/playground/multimodal_agent.py`\n+\n+Docs on Agent UI: https://docs.agno.com/agent-ui\n+\"\"\"\n+\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+from agno.models.response import FileType\n+from agno.playground import Playground, serve_playground_app\n+from agno.storage.sqlite import SqliteStorage\n+from agno.tools.dalle import DalleTools\n+from agno.tools.eleven_labs import ElevenLabsTools\n+from agno.tools.fal import FalTools\n+from agno.tools.giphy import GiphyTools\n+from agno.tools.models_labs import ModelsLabTools\n+\n+image_agent_storage_file: str = \"tmp/image_agent.db\"\n+\n+image_agent = Agent(\n+    name=\"DALL-E Image Agent\",\n+    agent_id=\"image_agent\",\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    tools=[DalleTools(model=\"dall-e-3\", size=\"1792x1024\", quality=\"hd\", style=\"vivid\")],\n+    description=\"You are an AI agent that can generate images using DALL-E.\",\n+    instructions=[\n+        \"When the user asks you to create an image, use the `create_image` tool to create the image.\",\n+        \"Don't provide the URL of the image in the response. Only describe what image was generated.\",\n+    ],\n+    markdown=True,\n+    add_history_to_messages=True,\n+    add_datetime_to_instructions=True,\n+    storage=SqliteStorage(\n+        table_name=\"image_agent\",\n+        db_file=image_agent_storage_file,\n+        auto_upgrade_schema=True,\n+    ),\n+)\n+\n+ml_gif_agent = Agent(\n+    name=\"ModelsLab GIF Agent\",\n+    agent_id=\"ml_gif_agent\",\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    tools=[ModelsLabTools(wait_for_completion=True, file_type=FileType.GIF)],\n+    description=\"You are an AI agent that can generate gifs using the ModelsLabs API.\",\n+    instructions=[\n+        \"When the user asks you to create an image, use the `generate_media` tool to create the image.\",\n+        \"Don't provide the URL of the image in the response. Only describe what image was generated.\",\n+    ],\n+    markdown=True,\n+    add_history_to_messages=True,\n+    add_datetime_to_instructions=True,\n+    storage=SqliteStorage(\n+        table_name=\"ml_gif_agent\",\n+        db_file=image_agent_storage_file,\n+        auto_upgrade_schema=True,\n+    ),\n+)\n+\n+ml_music_agent = Agent(\n+    name=\"ModelsLab Music Agent\",\n+    agent_id=\"ml_music_agent\",\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    tools=[ModelsLabTools(wait_for_completion=True, file_type=FileType.MP3)],\n+    description=\"You are an AI agent that can generate music using the ModelsLabs API.\",\n+    instructions=[\n+        \"When generating music, use the `generate_media` tool with detailed prompts that specify:\",\n+        \"- The genre and style of music (e.g., classical, jazz, electronic)\",\n+        \"- The instruments and sounds to include\",\n+        \"- The tempo, mood and emotional qualities\",\n+        \"- The structure (intro, verses, chorus, bridge, etc.)\",\n+        \"Create rich, descriptive prompts that capture the desired musical elements.\",\n+        \"Focus on generating high-quality, complete instrumental pieces.\",\n+        \"Keep responses simple and only confirm when music is generated successfully.\",\n+        \"Do not include any file names, URLs or technical details in responses.\",\n+    ],\n+    markdown=True,\n+    add_history_to_messages=True,\n+    add_datetime_to_instructions=True,\n+    storage=SqliteStorage(\n+        table_name=\"ml_music_agent\",\n+        db_file=image_agent_storage_file,\n+        auto_upgrade_schema=True,\n+    ),\n+)\n+\n+ml_video_agent = Agent(\n+    name=\"ModelsLab Video Agent\",\n+    agent_id=\"ml_video_agent\",\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    tools=[ModelsLabTools(wait_for_completion=True, file_type=FileType.MP4)],\n+    description=\"You are an AI agent that can generate videos using the ModelsLabs API.\",\n+    instructions=[\n+        \"When the user asks you to create a video, use the `generate_media` tool to create the video.\",\n+        \"Don't provide the URL of the video in the response. Only describe what video was generated.\",\n+    ],\n+    markdown=True,\n+    add_history_to_messages=True,\n+    add_datetime_to_instructions=True,\n+    storage=SqliteStorage(\n+        table_name=\"ml_video_agent\",\n+        db_file=image_agent_storage_file,\n+        auto_upgrade_schema=True,\n+    ),\n+)\n+\n+fal_agent = Agent(\n+    name=\"Fal Video Agent\",\n+    agent_id=\"fal_agent\",\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    tools=[FalTools(\"fal-ai/hunyuan-video\")],\n+    description=\"You are an AI agent that can generate videos using the Fal API.\",\n+    instructions=[\n+        \"When the user asks you to create a video, use the `generate_media` tool to create the video.\",\n+        \"Don't provide the URL of the video in the response. Only describe what video was generated.\",\n+    ],\n+    markdown=True,\n+    add_history_to_messages=True,\n+    add_datetime_to_instructions=True,\n+    storage=SqliteStorage(\n+        table_name=\"fal_agent\",\n+        db_file=image_agent_storage_file,\n+        auto_upgrade_schema=True,\n+    ),\n+)\n+\n+gif_agent = Agent(\n+    name=\"Gif Generator Agent\",\n+    agent_id=\"gif_agent\",\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    tools=[GiphyTools()],\n+    description=\"You are an AI agent that can generate gifs using Giphy.\",\n+    instructions=[\n+        \"When the user asks you to create a gif, come up with the appropriate Giphy query and use the `search_gifs` tool to find the appropriate gif.\",\n+        \"Don't return the URL, only describe what you created.\",\n+    ],\n+    markdown=True,\n+    add_history_to_messages=True,\n+    add_datetime_to_instructions=True,\n+    storage=SqliteStorage(\n+        table_name=\"gif_agent\",\n+        db_file=image_agent_storage_file,\n+        auto_upgrade_schema=True,\n+    ),\n+)\n+\n+audio_agent = Agent(\n+    name=\"Audio Generator Agent\",\n+    agent_id=\"audio_agent\",\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    tools=[\n+        ElevenLabsTools(\n+            voice_id=\"JBFqnCBsd6RMkjVDRZzb\",\n+            model_id=\"eleven_multilingual_v2\",\n+            target_directory=\"audio_generations\",\n+        )\n+    ],\n+    description=\"You are an AI agent that can generate audio using the ElevenLabs API.\",\n+    instructions=[\n+        \"When the user asks you to generate audio, use the `text_to_speech` tool to generate the audio.\",\n+        \"You'll generate the appropriate prompt to send to the tool to generate audio.\",\n+        \"You don't need to find the appropriate voice first, I already specified the voice to user.\"\n+        \"Don't return file name or file url in your response or markdown just tell the audio was created successfully.\",\n+        \"The audio should be long and detailed.\",\n+    ],\n+    markdown=True,\n+    add_history_to_messages=True,\n+    add_datetime_to_instructions=True,\n+    storage=SqliteStorage(\n+        table_name=\"audio_agent\",\n+        db_file=image_agent_storage_file,\n+        auto_upgrade_schema=True,\n+    ),\n+)\n+\n+\n+app = Playground(\n+    agents=[\n+        image_agent,\n+        ml_gif_agent,\n+        ml_music_agent,\n+        ml_video_agent,\n+        fal_agent,\n+        gif_agent,\n+        audio_agent,\n+    ]\n+).get_app(use_async=False)\n+\n+if __name__ == \"__main__\":\n+    serve_playground_app(\"multimodal_agents:app\", reload=True)\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/apps/playground/ollama_agents.py",
            "diff": "diff --git a/cookbook/apps/playground/ollama_agents.py b/cookbook/apps/playground/ollama_agents.py\nnew file mode 100644\nindex 000000000..96c9263e8\n--- /dev/null\n+++ b/cookbook/apps/playground/ollama_agents.py\n@@ -0,0 +1,94 @@\n+\"\"\"Run `pip install ollama duckduckgo-search yfinance pypdf sqlalchemy 'fastapi[standard]' youtube-transcript-api agno` to install dependencies.\"\"\"\n+\n+from agno.agent import Agent\n+from agno.models.ollama import Ollama\n+from agno.playground import Playground, serve_playground_app\n+from agno.storage.sqlite import SqliteStorage\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.tools.yfinance import YFinanceTools\n+from agno.tools.youtube import YouTubeTools\n+\n+local_agent_storage_file: str = \"tmp/local_agents.db\"\n+common_instructions = [\n+    \"If the user about you or your skills, tell them your name and role.\",\n+]\n+\n+web_agent = Agent(\n+    name=\"Web Agent\",\n+    role=\"Search the web for information\",\n+    agent_id=\"web-agent\",\n+    model=Ollama(id=\"llama3.1:8b\"),\n+    tools=[DuckDuckGoTools()],\n+    instructions=[\"Always include sources.\"] + common_instructions,\n+    storage=SqliteStorage(\n+        table_name=\"web_agent\",\n+        db_file=local_agent_storage_file,\n+        auto_upgrade_schema=True,\n+    ),\n+    show_tool_calls=True,\n+    add_history_to_messages=True,\n+    num_history_responses=2,\n+    add_name_to_instructions=True,\n+    add_datetime_to_instructions=True,\n+    markdown=True,\n+)\n+\n+finance_agent = Agent(\n+    name=\"Finance Agent\",\n+    role=\"Get financial data\",\n+    agent_id=\"finance-agent\",\n+    model=Ollama(id=\"llama3.1:8b\"),\n+    tools=[\n+        YFinanceTools(\n+            stock_price=True,\n+            analyst_recommendations=True,\n+            company_info=True,\n+            company_news=True,\n+        )\n+    ],\n+    description=\"You are an investment analyst that researches stocks and helps users make informed decisions.\",\n+    instructions=[\"Always use tables to display data\"] + common_instructions,\n+    storage=SqliteStorage(\n+        table_name=\"finance_agent\",\n+        db_file=local_agent_storage_file,\n+        auto_upgrade_schema=True,\n+    ),\n+    add_history_to_messages=True,\n+    num_history_responses=5,\n+    add_name_to_instructions=True,\n+    add_datetime_to_instructions=True,\n+    markdown=True,\n+)\n+\n+\n+youtube_agent = Agent(\n+    name=\"YouTube Agent\",\n+    role=\"Understand YouTube videos and answer questions\",\n+    agent_id=\"youtube-agent\",\n+    model=Ollama(id=\"llama3.1:8b\"),\n+    tools=[YouTubeTools()],\n+    description=\"You are a YouTube agent that has the special skill of understanding YouTube videos and answering questions about them.\",\n+    instructions=[\n+        \"Using a video URL, get the video data using the `get_youtube_video_data` tool and captions using the `get_youtube_video_data` tool.\",\n+        \"Using the data and captions, answer the user's question in an engaging and thoughtful manner. Focus on the most important details.\",\n+        \"If you cannot find the answer in the video, say so and ask the user to provide more details.\",\n+        \"Keep your answers concise and engaging.\",\n+    ]\n+    + common_instructions,\n+    add_history_to_messages=True,\n+    num_history_responses=5,\n+    show_tool_calls=True,\n+    add_name_to_instructions=True,\n+    add_datetime_to_instructions=True,\n+    storage=SqliteStorage(\n+        table_name=\"youtube_agent\",\n+        db_file=local_agent_storage_file,\n+        auto_upgrade_schema=True,\n+    ),\n+    markdown=True,\n+)\n+\n+app = Playground(agents=[web_agent, finance_agent, youtube_agent]).get_app()\n+\n+if __name__ == \"__main__\":\n+    serve_playground_app(\"ollama_agents:app\", reload=True)\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/apps/playground/reasoning_demo.py",
            "diff": "diff --git a/cookbook/apps/playground/reasoning_demo.py b/cookbook/apps/playground/reasoning_demo.py\nnew file mode 100644\nindex 000000000..6fdbdb207\n--- /dev/null\n+++ b/cookbook/apps/playground/reasoning_demo.py\n@@ -0,0 +1,248 @@\n+\"\"\"Run `pip install openai exa_py duckduckgo-search yfinance pypdf sqlalchemy 'fastapi[standard]' youtube-transcript-api python-docx agno` to install dependencies.\"\"\"\n+\n+import asyncio\n+from textwrap import dedent\n+\n+from agno.agent import Agent\n+from agno.knowledge.url import UrlKnowledge\n+from agno.models.openai import OpenAIChat\n+from agno.playground import Playground, serve_playground_app\n+from agno.storage.sqlite import SqliteStorage\n+from agno.team import Team\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.tools.knowledge import KnowledgeTools\n+from agno.tools.reasoning import ReasoningTools\n+from agno.tools.thinking import ThinkingTools\n+from agno.tools.yfinance import YFinanceTools\n+from agno.vectordb.lancedb import LanceDb, SearchType\n+\n+agent_storage_file: str = \"tmp/agents.db\"\n+image_agent_storage_file: str = \"tmp/image_agent.db\"\n+\n+db_url = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n+\n+\n+finance_agent = Agent(\n+    name=\"Finance Agent\",\n+    role=\"Get financial data\",\n+    agent_id=\"finance-agent\",\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    tools=[\n+        YFinanceTools(\n+            stock_price=True,\n+            analyst_recommendations=True,\n+            company_info=True,\n+            company_news=True,\n+        )\n+    ],\n+    instructions=[\"Always use tables to display data\"],\n+    storage=SqliteStorage(\n+        table_name=\"finance_agent\", db_file=agent_storage_file, auto_upgrade_schema=True\n+    ),\n+    add_history_to_messages=True,\n+    num_history_responses=5,\n+    add_datetime_to_instructions=True,\n+    markdown=True,\n+)\n+\n+cot_agent = Agent(\n+    name=\"Chain-of-Thought Agent\",\n+    role=\"Answer basic questions\",\n+    agent_id=\"cot-agent\",\n+    model=OpenAIChat(id=\"gpt-4o-mini\"),\n+    storage=SqliteStorage(\n+        table_name=\"cot_agent\", db_file=agent_storage_file, auto_upgrade_schema=True\n+    ),\n+    add_history_to_messages=True,\n+    num_history_responses=3,\n+    add_datetime_to_instructions=True,\n+    markdown=True,\n+    reasoning=True,\n+)\n+\n+reasoning_model_agent = Agent(\n+    name=\"Reasoning Model Agent\",\n+    role=\"Reasoning about Math\",\n+    agent_id=\"reasoning-model-agent\",\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    reasoning_model=OpenAIChat(id=\"o3-mini\"),\n+    instructions=[\"You are a reasoning agent that can reason about math.\"],\n+    show_tool_calls=True,\n+    markdown=True,\n+    debug_mode=True,\n+    storage=SqliteStorage(\n+        table_name=\"reasoning_model_agent\",\n+        db_file=agent_storage_file,\n+        auto_upgrade_schema=True,\n+    ),\n+)\n+\n+reasoning_tool_agent = Agent(\n+    name=\"Reasoning Tool Agent\",\n+    role=\"Answer basic questions\",\n+    agent_id=\"reasoning-tool-agent\",\n+    model=OpenAIChat(id=\"gpt-4o-mini\"),\n+    storage=SqliteStorage(\n+        table_name=\"reasoning_tool_agent\",\n+        db_file=agent_storage_file,\n+        auto_upgrade_schema=True,\n+    ),\n+    add_history_to_messages=True,\n+    num_history_responses=3,\n+    add_datetime_to_instructions=True,\n+    markdown=True,\n+    tools=[ReasoningTools()],\n+)\n+\n+\n+web_agent = Agent(\n+    name=\"Web Search Agent\",\n+    role=\"Handle web search requests\",\n+    model=OpenAIChat(id=\"gpt-4o-mini\"),\n+    agent_id=\"web_agent\",\n+    tools=[DuckDuckGoTools()],\n+    instructions=\"Always include sources\",\n+    add_datetime_to_instructions=True,\n+    storage=SqliteStorage(\n+        table_name=\"web_agent\",\n+        db_file=agent_storage_file,\n+        auto_upgrade_schema=True,\n+    ),\n+)\n+\n+thinking_tool_agent = Agent(\n+    name=\"Thinking Tool Agent\",\n+    agent_id=\"thinking_tool_agent\",\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    tools=[ThinkingTools(add_instructions=True), YFinanceTools(enable_all=True)],\n+    instructions=dedent(\"\"\"\\\n+        You are a seasoned Wall Street analyst with deep expertise in market analysis! \ud83d\udcca\n+\n+        Follow these steps for comprehensive financial analysis:\n+        1. Market Overview\n+           - Latest stock price\n+           - 52-week high and low\n+        2. Financial Deep Dive\n+           - Key metrics (P/E, Market Cap, EPS)\n+        3. Professional Insights\n+           - Analyst recommendations breakdown\n+           - Recent rating changes\n+\n+        4. Market Context\n+           - Industry trends and positioning\n+           - Competitive analysis\n+           - Market sentiment indicators\n+\n+        Your reporting style:\n+        - Begin with an executive summary\n+        - Use tables for data presentation\n+        - Include clear section headers\n+        - Add emoji indicators for trends (\ud83d\udcc8 \ud83d\udcc9)\n+        - Highlight key insights with bullet points\n+        - Compare metrics to industry averages\n+        - Include technical term explanations\n+        - End with a forward-looking analysis\n+\n+        Risk Disclosure:\n+        - Always highlight potential risk factors\n+        - Note market uncertainties\n+        - Mention relevant regulatory concerns\\\n+    \"\"\"),\n+    add_datetime_to_instructions=True,\n+    show_tool_calls=True,\n+    markdown=True,\n+    stream_intermediate_steps=True,\n+    storage=SqliteStorage(\n+        table_name=\"thinking_tool_agent\",\n+        db_file=agent_storage_file,\n+        auto_upgrade_schema=True,\n+    ),\n+)\n+\n+\n+agno_docs = UrlKnowledge(\n+    urls=[\"https://www.paulgraham.com/read.html\"],\n+    # Use LanceDB as the vector database and store embeddings in the `agno_docs` table\n+    vector_db=LanceDb(\n+        uri=\"tmp/lancedb\",\n+        table_name=\"agno_docs\",\n+        search_type=SearchType.hybrid,\n+    ),\n+)\n+\n+knowledge_tools = KnowledgeTools(\n+    knowledge=agno_docs,\n+    think=True,\n+    search=True,\n+    analyze=True,\n+    add_few_shot=True,\n+)\n+knowledge_agent = Agent(\n+    agent_id=\"knowledge_agent\",\n+    name=\"Knowledge Agent\",\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    tools=[knowledge_tools],\n+    show_tool_calls=True,\n+    markdown=True,\n+    storage=SqliteStorage(\n+        table_name=\"knowledge_agent\",\n+        db_file=agent_storage_file,\n+        auto_upgrade_schema=True,\n+    ),\n+)\n+\n+reasoning_finance_team = Team(\n+    name=\"Reasoning Finance Team\",\n+    mode=\"coordinate\",\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    members=[\n+        web_agent,\n+        finance_agent,\n+    ],\n+    # reasoning=True,\n+    tools=[ReasoningTools(add_instructions=True)],\n+    # uncomment it to use knowledge tools\n+    # tools=[knowledge_tools],\n+    team_id=\"reasoning_finance_team\",\n+    debug_mode=True,\n+    instructions=[\n+        \"Only output the final answer, no other text.\",\n+        \"Use tables to display data\",\n+    ],\n+    markdown=True,\n+    show_members_responses=True,\n+    enable_agentic_context=True,\n+    add_datetime_to_instructions=True,\n+    success_criteria=\"The team has successfully completed the task.\",\n+    storage=SqliteStorage(\n+        table_name=\"reasoning_finance_team\",\n+        db_file=agent_storage_file,\n+        auto_upgrade_schema=True,\n+    ),\n+)\n+\n+\n+app = Playground(\n+    agents=[\n+        cot_agent,\n+        reasoning_tool_agent,\n+        reasoning_model_agent,\n+        knowledge_agent,\n+        thinking_tool_agent,\n+    ],\n+    teams=[reasoning_finance_team],\n+).get_app()\n+\n+if __name__ == \"__main__\":\n+    asyncio.run(agno_docs.aload(recreate=True))\n+    serve_playground_app(\"reasoning_demo:app\", reload=True)\n+\n+\n+# reasoning_tool_agent\n+# Solve this logic puzzle: A man has to take a fox, a chicken, and a sack of grain across a river.\n+# The boat is only big enough for the man and one item. If left unattended together,\n+# the fox will eat the chicken, and the chicken will eat the grain. How can the man get everything across safely?\n+\n+\n+# knowledge_agent prompt\n+# What does Paul Graham explain here with respect to need to read?\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/apps/playground/teams_demo.py",
            "diff": "diff --git a/cookbook/apps/playground/teams_demo.py b/cookbook/apps/playground/teams_demo.py\nnew file mode 100644\nindex 000000000..157333e18\n--- /dev/null\n+++ b/cookbook/apps/playground/teams_demo.py\n@@ -0,0 +1,227 @@\n+from textwrap import dedent\n+\n+from agno.agent import Agent\n+from agno.memory.v2 import Memory\n+from agno.memory.v2.db.postgres import PostgresMemoryDb\n+from agno.models.anthropic import Claude\n+from agno.models.google.gemini import Gemini\n+from agno.models.openai import OpenAIChat\n+from agno.playground import Playground, serve_playground_app\n+from agno.storage.postgres import PostgresStorage\n+from agno.team.team import Team\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.tools.exa import ExaTools\n+from agno.tools.yfinance import YFinanceTools\n+\n+db_url = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n+\n+memory_db = PostgresMemoryDb(table_name=\"memory\", db_url=db_url)\n+\n+# No need to set the model, it gets set by the agent to the agent's model\n+memory = Memory(db=memory_db)\n+\n+\n+file_agent = Agent(\n+    name=\"File Upload Agent\",\n+    agent_id=\"file-upload-agent\",\n+    role=\"Answer questions about the uploaded files\",\n+    model=Claude(id=\"claude-3-7-sonnet-latest\"),\n+    storage=PostgresStorage(\n+        table_name=\"agent_sessions\", db_url=db_url, auto_upgrade_schema=True\n+    ),\n+    memory=memory,\n+    enable_user_memories=True,\n+    instructions=[\n+        \"You are an AI agent that can analyze files.\",\n+        \"You are given a file and you need to answer questions about the file.\",\n+    ],\n+    show_tool_calls=True,\n+    markdown=True,\n+)\n+\n+video_agent = Agent(\n+    name=\"Video Understanding Agent\",\n+    model=Gemini(id=\"gemini-2.0-flash\"),\n+    agent_id=\"video-understanding-agent\",\n+    role=\"Answer questions about video files\",\n+    storage=PostgresStorage(\n+        table_name=\"agent_sessions\", db_url=db_url, auto_upgrade_schema=True\n+    ),\n+    memory=memory,\n+    enable_user_memories=True,\n+    add_history_to_messages=True,\n+    add_datetime_to_instructions=True,\n+    show_tool_calls=True,\n+    markdown=True,\n+)\n+\n+audio_agent = Agent(\n+    name=\"Audio Understanding Agent\",\n+    agent_id=\"audio-understanding-agent\",\n+    role=\"Answer questions about audio files\",\n+    model=OpenAIChat(id=\"gpt-4o-audio-preview\"),\n+    storage=PostgresStorage(\n+        table_name=\"agent_sessions\", db_url=db_url, auto_upgrade_schema=True\n+    ),\n+    memory=memory,\n+    enable_user_memories=True,\n+    add_history_to_messages=True,\n+    add_datetime_to_instructions=True,\n+    show_tool_calls=True,\n+    markdown=True,\n+)\n+\n+web_agent = Agent(\n+    name=\"Web Agent\",\n+    role=\"Search the web for information\",\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    tools=[DuckDuckGoTools()],\n+    agent_id=\"web_agent\",\n+    instructions=[\n+        \"You are an experienced web researcher and news analyst! \ud83d\udd0d\",\n+    ],\n+    memory=memory,\n+    enable_user_memories=True,\n+    show_tool_calls=True,\n+    markdown=True,\n+    storage=PostgresStorage(\n+        table_name=\"web_agent\", db_url=db_url, auto_upgrade_schema=True\n+    ),\n+)\n+\n+finance_agent = Agent(\n+    name=\"Finance Agent\",\n+    role=\"Get financial data\",\n+    agent_id=\"finance_agent\",\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    tools=[\n+        YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True)\n+    ],\n+    instructions=[\n+        \"You are a skilled financial analyst with expertise in market data! \ud83d\udcca\",\n+        \"Follow these steps when analyzing financial data:\",\n+        \"Start with the latest stock price, trading volume, and daily range\",\n+        \"Present detailed analyst recommendations and consensus target prices\",\n+        \"Include key metrics: P/E ratio, market cap, 52-week range\",\n+        \"Analyze trading patterns and volume trends\",\n+    ],\n+    memory=memory,\n+    enable_user_memories=True,\n+    show_tool_calls=True,\n+    markdown=True,\n+)\n+\n+simple_agent = Agent(\n+    name=\"Simple Agent\",\n+    role=\"Simple agent\",\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    instructions=[\"You are a simple agent\"],\n+    memory=memory,\n+    enable_user_memories=True,\n+)\n+\n+research_agent = Agent(\n+    name=\"Research Agent\",\n+    role=\"Research agent\",\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    instructions=[\"You are a research agent\"],\n+    tools=[DuckDuckGoTools(), ExaTools()],\n+    agent_id=\"research_agent\",\n+    memory=memory,\n+    enable_user_memories=True,\n+)\n+\n+research_team = Team(\n+    name=\"Research Team\",\n+    description=\"A team of agents that research the web\",\n+    members=[research_agent, simple_agent],\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    mode=\"coordinate\",\n+    team_id=\"research_team\",\n+    success_criteria=dedent(\"\"\"\\\n+        A comprehensive research report with clear sections and data-driven insights.\n+    \"\"\"),\n+    instructions=[\n+        \"You are the lead researcher of a research team! \ud83d\udd0d\",\n+    ],\n+    memory=memory,\n+    enable_user_memories=True,\n+    add_datetime_to_instructions=True,\n+    show_tool_calls=True,\n+    markdown=True,\n+    enable_agentic_context=True,\n+    storage=PostgresStorage(\n+        table_name=\"research_team\",\n+        db_url=db_url,\n+        mode=\"team\",\n+        auto_upgrade_schema=True,\n+    ),\n+)\n+\n+multimodal_team = Team(\n+    name=\"Multimodal Team\",\n+    description=\"A team of agents that can handle multiple modalities\",\n+    members=[file_agent, audio_agent, video_agent],\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    mode=\"route\",\n+    team_id=\"multimodal_team\",\n+    success_criteria=dedent(\"\"\"\\\n+        A comprehensive report with clear sections and data-driven insights.\n+    \"\"\"),\n+    instructions=[\n+        \"You are the lead editor of a prestigious financial news desk! \ud83d\udcf0\",\n+    ],\n+    memory=memory,\n+    enable_user_memories=True,\n+    storage=PostgresStorage(\n+        table_name=\"multimodal_team\",\n+        db_url=db_url,\n+        mode=\"team\",\n+        auto_upgrade_schema=True,\n+    ),\n+)\n+financial_news_team = Team(\n+    name=\"Financial News Team\",\n+    description=\"A team of agents that search the web for financial news and analyze it.\",\n+    members=[\n+        web_agent,\n+        finance_agent,\n+        research_agent,\n+        file_agent,\n+        audio_agent,\n+        video_agent,\n+    ],\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    mode=\"route\",\n+    team_id=\"financial_news_team\",\n+    instructions=[\n+        \"You are the lead editor of a prestigious financial news desk! \ud83d\udcf0\",\n+        \"If you are given a file send it to the file agent.\",\n+        \"If you are given an audio file send it to the audio agent.\",\n+        \"If you are given a video file send it to the video agent.\",\n+        \"Use USD as currency.\",\n+        \"If the user is just being conversational, you should respond directly WITHOUT forwarding a task to a member.\",\n+    ],\n+    add_datetime_to_instructions=True,\n+    show_tool_calls=True,\n+    markdown=True,\n+    enable_agentic_context=True,\n+    show_members_responses=True,\n+    storage=PostgresStorage(\n+        table_name=\"financial_news_team\",\n+        db_url=db_url,\n+        mode=\"team\",\n+        auto_upgrade_schema=True,\n+    ),\n+    memory=memory,\n+    enable_user_memories=True,\n+    expected_output=\"A good financial news report.\",\n+)\n+\n+app = Playground(\n+    teams=[research_team, financial_news_team, multimodal_team],\n+    agents=[web_agent, finance_agent, research_agent, simple_agent],\n+).get_app()\n+\n+if __name__ == \"__main__\":\n+    serve_playground_app(\"teams_demo:app\", reload=True)\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/apps/playground/upload_files.py",
            "diff": "diff --git a/cookbook/apps/playground/upload_files.py b/cookbook/apps/playground/upload_files.py\nnew file mode 100644\nindex 000000000..25e4d7aa0\n--- /dev/null\n+++ b/cookbook/apps/playground/upload_files.py\n@@ -0,0 +1,82 @@\n+from agno.agent import Agent\n+from agno.knowledge.combined import CombinedKnowledgeBase\n+from agno.knowledge.csv import CSVKnowledgeBase\n+from agno.knowledge.docx import DocxKnowledgeBase\n+from agno.knowledge.json import JSONKnowledgeBase\n+from agno.knowledge.pdf import PDFKnowledgeBase\n+from agno.knowledge.text import TextKnowledgeBase\n+from agno.models.google.gemini import Gemini\n+from agno.models.openai import OpenAIChat\n+from agno.playground import Playground, serve_playground_app\n+from agno.storage.postgres import PostgresStorage\n+from agno.vectordb.pgvector import PgVector\n+\n+db_url = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n+\n+knowledge_base = CombinedKnowledgeBase(\n+    sources=[\n+        PDFKnowledgeBase(\n+            vector_db=PgVector(table_name=\"recipes_pdf\", db_url=db_url), path=\"\"\n+        ),\n+        CSVKnowledgeBase(\n+            vector_db=PgVector(table_name=\"recipes_csv\", db_url=db_url), path=\"\"\n+        ),\n+        DocxKnowledgeBase(\n+            vector_db=PgVector(table_name=\"recipes_docx\", db_url=db_url), path=\"\"\n+        ),\n+        JSONKnowledgeBase(\n+            vector_db=PgVector(table_name=\"recipes_json\", db_url=db_url), path=\"\"\n+        ),\n+        TextKnowledgeBase(\n+            vector_db=PgVector(table_name=\"recipes_text\", db_url=db_url), path=\"\"\n+        ),\n+    ],\n+    vector_db=PgVector(table_name=\"recipes_combined\", db_url=db_url),\n+)\n+\n+file_agent = Agent(\n+    name=\"File Upload Agent\",\n+    agent_id=\"file-upload-agent\",\n+    role=\"Answer questions about the uploaded files\",\n+    model=OpenAIChat(id=\"gpt-4o-mini\"),\n+    storage=PostgresStorage(\n+        table_name=\"agent_sessions\", db_url=db_url, auto_upgrade_schema=True\n+    ),\n+    knowledge=knowledge_base,\n+    show_tool_calls=True,\n+    markdown=True,\n+)\n+\n+\n+audio_agent = Agent(\n+    name=\"Audio Understanding Agent\",\n+    agent_id=\"audio-understanding-agent\",\n+    role=\"Answer questions about audio files\",\n+    model=OpenAIChat(id=\"gpt-4o-audio-preview\"),\n+    storage=PostgresStorage(\n+        table_name=\"agent_sessions\", db_url=db_url, auto_upgrade_schema=True\n+    ),\n+    add_history_to_messages=True,\n+    add_datetime_to_instructions=True,\n+    show_tool_calls=True,\n+    markdown=True,\n+)\n+\n+video_agent = Agent(\n+    name=\"Video Understanding Agent\",\n+    model=Gemini(id=\"gemini-2.0-flash\"),\n+    agent_id=\"video-understanding-agent\",\n+    role=\"Answer questions about video files\",\n+    storage=PostgresStorage(\n+        table_name=\"agent_sessions\", db_url=db_url, auto_upgrade_schema=True\n+    ),\n+    add_history_to_messages=True,\n+    add_datetime_to_instructions=True,\n+    show_tool_calls=True,\n+    markdown=True,\n+)\n+\n+app = Playground(agents=[file_agent, audio_agent, video_agent]).get_app()\n+\n+if __name__ == \"__main__\":\n+    serve_playground_app(\"upload_files:app\", reload=True)\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/__init__.py b/cookbook/examples/streamlit_applications/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/agentic_rag/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/agentic_rag/__init__.py b/cookbook/examples/streamlit_applications/agentic_rag/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/agentic_rag/agentic_rag.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/agentic_rag/agentic_rag.py b/cookbook/examples/streamlit_applications/agentic_rag/agentic_rag.py\nnew file mode 100644\nindex 000000000..bb3638615\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/agentic_rag/agentic_rag.py\n@@ -0,0 +1,132 @@\n+\"\"\"\ud83e\udd16 Agentic RAG Agent - Your AI Knowledge Assistant!\n+\n+This advanced example shows how to build a sophisticated RAG (Retrieval Augmented Generation) system that\n+leverages vector search and LLMs to provide deep insights from any knowledge base.\n+\n+The agent can:\n+- Process and understand documents from multiple sources (PDFs, websites, text files)\n+- Build a searchable knowledge base using vector embeddings\n+- Maintain conversation context and memory across sessions\n+- Provide relevant citations and sources for its responses\n+- Generate summaries and extract key insights\n+- Answer follow-up questions and clarifications\n+\n+Example queries to try:\n+- \"What are the key points from this document?\"\n+- \"Can you summarize the main arguments and supporting evidence?\"\n+- \"What are the important statistics and findings?\"\n+- \"How does this relate to [topic X]?\"\n+- \"What are the limitations or gaps in this analysis?\"\n+- \"Can you explain [concept X] in more detail?\"\n+- \"What other sources support or contradict these claims?\"\n+\n+The agent uses:\n+- Vector similarity search for relevant document retrieval\n+- Conversation memory for contextual responses\n+- Citation tracking for source attribution\n+- Dynamic knowledge base updates\n+\n+View the README for instructions on how to run the application.\n+\"\"\"\n+\n+from typing import Optional\n+\n+from agno.agent import Agent\n+from agno.embedder.openai import OpenAIEmbedder\n+from agno.knowledge import AgentKnowledge\n+from agno.models.anthropic import Claude\n+from agno.models.google import Gemini\n+from agno.models.groq import Groq\n+from agno.models.openai import OpenAIChat\n+from agno.storage.agent.postgres import PostgresAgentStorage\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.vectordb.pgvector import PgVector\n+\n+db_url = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n+\n+\n+def get_agentic_rag_agent(\n+    model_id: str = \"openai:gpt-4o\",\n+    user_id: Optional[str] = None,\n+    session_id: Optional[str] = None,\n+    debug_mode: bool = True,\n+) -> Agent:\n+    \"\"\"Get an Agentic RAG Agent with Memory.\"\"\"\n+    # Parse model provider and name\n+    provider, model_name = model_id.split(\":\")\n+\n+    # Select appropriate model class based on provider\n+    if provider == \"openai\":\n+        model = OpenAIChat(id=model_name)\n+    elif provider == \"google\":\n+        model = Gemini(id=model_name)\n+    elif provider == \"anthropic\":\n+        model = Claude(id=model_name)\n+    elif provider == \"groq\":\n+        model = Groq(id=model_name)\n+    else:\n+        raise ValueError(f\"Unsupported model provider: {provider}\")\n+    # Define persistent memory for chat history\n+\n+    # Define the knowledge base\n+    knowledge_base = AgentKnowledge(\n+        vector_db=PgVector(\n+            db_url=db_url,\n+            table_name=\"agentic_rag_documents\",\n+            schema=\"ai\",\n+            # Use OpenAI embeddings\n+            embedder=OpenAIEmbedder(id=\"text-embedding-3-small\"),\n+        ),\n+        num_documents=3,  # Retrieve 3 most relevant documents\n+    )\n+\n+    # Create the Agent\n+    return Agent(\n+        name=\"agentic_rag_agent\",\n+        session_id=session_id,  # Track session ID for persistent conversations\n+        user_id=user_id,\n+        model=model,\n+        storage=PostgresAgentStorage(\n+            table_name=\"agentic_rag_agent_sessions\", db_url=db_url\n+        ),  # Persist session data\n+        knowledge=knowledge_base,  # Add knowledge base\n+        description=\"You are a helpful Agent called 'Agentic RAG' and your goal is to assist the user in the best way possible.\",\n+        instructions=[\n+            \"1. Knowledge Base Search:\",\n+            \"   - ALWAYS start by searching the knowledge base using search_knowledge_base tool\",\n+            \"   - Analyze ALL returned documents thoroughly before responding\",\n+            \"   - If multiple documents are returned, synthesize the information coherently\",\n+            \"2. External Search:\",\n+            \"   - If knowledge base search yields insufficient results, use duckduckgo_search\",\n+            \"   - Focus on reputable sources and recent information\",\n+            \"   - Cross-reference information from multiple sources when possible\",\n+            \"3. Context Management:\",\n+            \"   - Use get_chat_history tool to maintain conversation continuity\",\n+            \"   - Reference previous interactions when relevant\",\n+            \"   - Keep track of user preferences and prior clarifications\",\n+            \"4. Response Quality:\",\n+            \"   - Provide specific citations and sources for claims\",\n+            \"   - Structure responses with clear sections and bullet points when appropriate\",\n+            \"   - Include relevant quotes from source materials\",\n+            \"   - Avoid hedging phrases like 'based on my knowledge' or 'depending on the information'\",\n+            \"5. User Interaction:\",\n+            \"   - Ask for clarification if the query is ambiguous\",\n+            \"   - Break down complex questions into manageable parts\",\n+            \"   - Proactively suggest related topics or follow-up questions\",\n+            \"6. Error Handling:\",\n+            \"   - If no relevant information is found, clearly state this\",\n+            \"   - Suggest alternative approaches or questions\",\n+            \"   - Be transparent about limitations in available information\",\n+        ],\n+        search_knowledge=True,  # This setting gives the model a tool to search the knowledge base for information\n+        read_chat_history=True,  # This setting gives the model a tool to get chat history\n+        tools=[DuckDuckGoTools()],\n+        markdown=True,  # This setting tellss the model to format messages in markdown\n+        # add_chat_history_to_messages=True,\n+        show_tool_calls=True,\n+        add_history_to_messages=True,  # Adds chat history to messages\n+        add_datetime_to_instructions=True,\n+        debug_mode=debug_mode,\n+        read_tool_call_history=True,\n+        num_history_responses=3,\n+    )\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/agentic_rag/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/agentic_rag/app.py b/cookbook/examples/streamlit_applications/agentic_rag/app.py\nnew file mode 100644\nindex 000000000..35aa20eaf\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/agentic_rag/app.py\n@@ -0,0 +1,331 @@\n+import os\n+import tempfile\n+from typing import List\n+\n+import nest_asyncio\n+import requests\n+import streamlit as st\n+from agentic_rag import get_agentic_rag_agent\n+from agno.agent import Agent\n+from agno.document import Document\n+from agno.document.reader.csv_reader import CSVReader\n+from agno.document.reader.pdf_reader import PDFReader\n+from agno.document.reader.text_reader import TextReader\n+from agno.document.reader.website_reader import WebsiteReader\n+from agno.utils.log import logger\n+from utils import (\n+    CUSTOM_CSS,\n+    about_widget,\n+    add_message,\n+    display_tool_calls,\n+    export_chat_history,\n+    rename_session_widget,\n+    session_selector_widget,\n+)\n+\n+nest_asyncio.apply()\n+st.set_page_config(\n+    page_title=\"Agentic RAG\",\n+    page_icon=\"\ud83d\udc8e\",\n+    layout=\"wide\",\n+    initial_sidebar_state=\"expanded\",\n+)\n+\n+# Add custom CSS\n+\n+st.markdown(CUSTOM_CSS, unsafe_allow_html=True)\n+\n+\n+def restart_agent():\n+    \"\"\"Reset the agent and clear chat history\"\"\"\n+    logger.debug(\"---*--- Restarting agent ---*---\")\n+    st.session_state[\"agentic_rag_agent\"] = None\n+    st.session_state[\"agentic_rag_agent_session_id\"] = None\n+    st.session_state[\"messages\"] = []\n+    st.rerun()\n+\n+\n+def get_reader(file_type: str):\n+    \"\"\"Return appropriate reader based on file type.\"\"\"\n+    readers = {\n+        \"pdf\": PDFReader(),\n+        \"csv\": CSVReader(),\n+        \"txt\": TextReader(),\n+    }\n+    return readers.get(file_type.lower(), None)\n+\n+\n+def main():\n+    ####################################################################\n+    # App header\n+    ####################################################################\n+    st.markdown(\"<h1 class='main-title'>Agentic RAG </h1>\", unsafe_allow_html=True)\n+    st.markdown(\n+        \"<p class='subtitle'>Your intelligent research assistant powered by Agno</p>\",\n+        unsafe_allow_html=True,\n+    )\n+\n+    ####################################################################\n+    # Model selector\n+    ####################################################################\n+    model_options = {\n+        \"o3-mini\": \"openai:o3-mini\",\n+        \"gpt-4o\": \"openai:gpt-4o\",\n+        \"gemini-2.0-flash-exp\": \"google:gemini-2.0-flash-exp\",\n+        \"claude-3-5-sonnet\": \"anthropic:claude-3-5-sonnet-20241022\",\n+        \"llama-3.3-70b\": \"groq:llama-3.3-70b-versatile\",\n+    }\n+    selected_model = st.sidebar.selectbox(\n+        \"Select a model\",\n+        options=list(model_options.keys()),\n+        index=0,\n+        key=\"model_selector\",\n+    )\n+    model_id = model_options[selected_model]\n+\n+    ####################################################################\n+    # Initialize Agent\n+    ####################################################################\n+    agentic_rag_agent: Agent\n+    if (\n+        \"agentic_rag_agent\" not in st.session_state\n+        or st.session_state[\"agentic_rag_agent\"] is None\n+        or st.session_state.get(\"current_model\") != model_id\n+    ):\n+        logger.info(\"---*--- Creating new Agentic RAG  ---*---\")\n+        agentic_rag_agent = get_agentic_rag_agent(model_id=model_id)\n+        st.session_state[\"agentic_rag_agent\"] = agentic_rag_agent\n+        st.session_state[\"current_model\"] = model_id\n+    else:\n+        agentic_rag_agent = st.session_state[\"agentic_rag_agent\"]\n+\n+    ####################################################################\n+    # Load Agent Session from the database\n+    ####################################################################\n+    # Check if session ID is already in session state\n+    session_id_exists = (\n+        \"agentic_rag_agent_session_id\" in st.session_state\n+        and st.session_state[\"agentic_rag_agent_session_id\"]\n+    )\n+\n+    if not session_id_exists:\n+        try:\n+            st.session_state[\"agentic_rag_agent_session_id\"] = (\n+                agentic_rag_agent.load_session()\n+            )\n+        except Exception as e:\n+            logger.error(f\"Session load error: {str(e)}\")\n+            st.warning(\"Could not create Agent session, is the database running?\")\n+            # Continue anyway instead of returning, to avoid breaking session switching\n+    elif (\n+        st.session_state[\"agentic_rag_agent_session_id\"]\n+        and hasattr(agentic_rag_agent, \"memory\")\n+        and agentic_rag_agent.memory is not None\n+        and not agentic_rag_agent.memory.runs\n+    ):\n+        # If we have a session ID but no runs, try to load the session explicitly\n+        try:\n+            agentic_rag_agent.load_session(\n+                st.session_state[\"agentic_rag_agent_session_id\"]\n+            )\n+        except Exception as e:\n+            logger.error(f\"Failed to load existing session: {str(e)}\")\n+            # Continue anyway\n+\n+    ####################################################################\n+    # Load runs from memory\n+    ####################################################################\n+    agent_runs = []\n+    if hasattr(agentic_rag_agent, \"memory\") and agentic_rag_agent.memory is not None:\n+        agent_runs = agentic_rag_agent.memory.runs\n+\n+    # Initialize messages if it doesn't exist yet\n+    if \"messages\" not in st.session_state:\n+        st.session_state[\"messages\"] = []\n+\n+    # Only populate messages from agent runs if we haven't already\n+    if len(st.session_state[\"messages\"]) == 0 and len(agent_runs) > 0:\n+        logger.debug(\"Loading run history\")\n+        for _run in agent_runs:\n+            # Check if _run is an object with message attribute\n+            if hasattr(_run, \"message\") and _run.message is not None:\n+                add_message(_run.message.role, _run.message.content)\n+            # Check if _run is an object with response attribute\n+            if hasattr(_run, \"response\") and _run.response is not None:\n+                add_message(\"assistant\", _run.response.content, _run.response.tools)\n+    elif len(agent_runs) == 0 and len(st.session_state[\"messages\"]) == 0:\n+        logger.debug(\"No run history found\")\n+\n+    if prompt := st.chat_input(\"\ud83d\udc4b Ask me anything!\"):\n+        add_message(\"user\", prompt)\n+\n+    ####################################################################\n+    # Track loaded URLs and files in session state\n+    ####################################################################\n+    if \"loaded_urls\" not in st.session_state:\n+        st.session_state.loaded_urls = set()\n+    if \"loaded_files\" not in st.session_state:\n+        st.session_state.loaded_files = set()\n+    if \"knowledge_base_initialized\" not in st.session_state:\n+        st.session_state.knowledge_base_initialized = False\n+\n+    st.sidebar.markdown(\"#### \ud83d\udcda Document Management\")\n+    input_url = st.sidebar.text_input(\"Add URL to Knowledge Base\")\n+    if (\n+        input_url and not prompt and not st.session_state.knowledge_base_initialized\n+    ):  # Only load if KB not initialized\n+        if input_url not in st.session_state.loaded_urls:\n+            alert = st.sidebar.info(\"Processing URLs...\", icon=\"\u2139\ufe0f\")\n+            if input_url.lower().endswith(\".pdf\"):\n+                try:\n+                    # Download PDF to temporary file\n+                    response = requests.get(input_url, stream=True, verify=False)\n+                    response.raise_for_status()\n+\n+                    with tempfile.NamedTemporaryFile(\n+                        suffix=\".pdf\", delete=False\n+                    ) as tmp_file:\n+                        for chunk in response.iter_content(chunk_size=8192):\n+                            tmp_file.write(chunk)\n+                        tmp_path = tmp_file.name\n+\n+                    reader = PDFReader()\n+                    docs: List[Document] = reader.read(tmp_path)\n+\n+                    # Clean up temporary file\n+                    os.unlink(tmp_path)\n+                except Exception as e:\n+                    st.sidebar.error(f\"Error processing PDF: {str(e)}\")\n+                    docs = []\n+            else:\n+                scraper = WebsiteReader(max_links=2, max_depth=1)\n+                docs: List[Document] = scraper.read(input_url)\n+\n+            if docs:\n+                agentic_rag_agent.knowledge.load_documents(docs, upsert=True)\n+                st.session_state.loaded_urls.add(input_url)\n+                st.sidebar.success(\"URL added to knowledge base\")\n+            else:\n+                st.sidebar.error(\"Could not process the provided URL\")\n+            alert.empty()\n+        else:\n+            st.sidebar.info(\"URL already loaded in knowledge base\")\n+\n+    uploaded_file = st.sidebar.file_uploader(\n+        \"Add a Document (.pdf, .csv, or .txt)\", key=\"file_upload\"\n+    )\n+    if (\n+        uploaded_file and not prompt and not st.session_state.knowledge_base_initialized\n+    ):  # Only load if KB not initialized\n+        file_identifier = f\"{uploaded_file.name}_{uploaded_file.size}\"\n+        if file_identifier not in st.session_state.loaded_files:\n+            alert = st.sidebar.info(\"Processing document...\", icon=\"\u2139\ufe0f\")\n+            file_type = uploaded_file.name.split(\".\")[-1].lower()\n+            reader = get_reader(file_type)\n+            if reader:\n+                docs = reader.read(uploaded_file)\n+                agentic_rag_agent.knowledge.load_documents(docs, upsert=True)\n+                st.session_state.loaded_files.add(file_identifier)\n+                st.sidebar.success(f\"{uploaded_file.name} added to knowledge base\")\n+                st.session_state.knowledge_base_initialized = True\n+            alert.empty()\n+        else:\n+            st.sidebar.info(f\"{uploaded_file.name} already loaded in knowledge base\")\n+\n+    if st.sidebar.button(\"Clear Knowledge Base\"):\n+        agentic_rag_agent.knowledge.vector_db.delete()\n+        st.session_state.loaded_urls.clear()\n+        st.session_state.loaded_files.clear()\n+        st.session_state.knowledge_base_initialized = False  # Reset initialization flag\n+        st.sidebar.success(\"Knowledge base cleared\")\n+    ###############################################################\n+    # Sample Question\n+    ###############################################################\n+    st.sidebar.markdown(\"#### \u2753 Sample Questions\")\n+    if st.sidebar.button(\"\ud83d\udcdd Summarize\"):\n+        add_message(\n+            \"user\",\n+            \"Can you summarize what is currently in the knowledge base (use `search_knowledge_base` tool)?\",\n+        )\n+\n+    ###############################################################\n+    # Utility buttons\n+    ###############################################################\n+    st.sidebar.markdown(\"#### \ud83d\udee0\ufe0f Utilities\")\n+    col1, col2 = st.sidebar.columns([1, 1])  # Equal width columns\n+    with col1:\n+        if st.sidebar.button(\n+            \"\ud83d\udd04 New Chat\", use_container_width=True\n+        ):  # Added use_container_width\n+            restart_agent()\n+    with col2:\n+        if st.sidebar.download_button(\n+            \"\ud83d\udcbe Export Chat\",\n+            export_chat_history(),\n+            file_name=\"rag_chat_history.md\",\n+            mime=\"text/markdown\",\n+            use_container_width=True,  # Added use_container_width\n+        ):\n+            st.sidebar.success(\"Chat history exported!\")\n+\n+    ####################################################################\n+    # Display chat history\n+    ####################################################################\n+    for message in st.session_state[\"messages\"]:\n+        if message[\"role\"] in [\"user\", \"assistant\"]:\n+            _content = message[\"content\"]\n+            if _content is not None:\n+                with st.chat_message(message[\"role\"]):\n+                    # Display tool calls if they exist in the message\n+                    if \"tool_calls\" in message and message[\"tool_calls\"]:\n+                        display_tool_calls(st.empty(), message[\"tool_calls\"])\n+                    st.markdown(_content)\n+\n+    ####################################################################\n+    # Generate response for user message\n+    ####################################################################\n+    last_message = (\n+        st.session_state[\"messages\"][-1] if st.session_state[\"messages\"] else None\n+    )\n+    if last_message and last_message.get(\"role\") == \"user\":\n+        question = last_message[\"content\"]\n+        with st.chat_message(\"assistant\"):\n+            # Create container for tool calls\n+            tool_calls_container = st.empty()\n+            resp_container = st.empty()\n+            with st.spinner(\"\ud83e\udd14 Thinking...\"):\n+                response = \"\"\n+                try:\n+                    # Run the agent and stream the response\n+                    run_response = agentic_rag_agent.run(question, stream=True)\n+                    for _resp_chunk in run_response:\n+                        # Display tool calls if available\n+                        if _resp_chunk.tools and len(_resp_chunk.tools) > 0:\n+                            display_tool_calls(tool_calls_container, _resp_chunk.tools)\n+\n+                        # Display response\n+                        if _resp_chunk.content is not None:\n+                            response += _resp_chunk.content\n+                            resp_container.markdown(response)\n+\n+                    add_message(\n+                        \"assistant\", response, agentic_rag_agent.run_response.tools\n+                    )\n+                except Exception as e:\n+                    error_message = f\"Sorry, I encountered an error: {str(e)}\"\n+                    add_message(\"assistant\", error_message)\n+                    st.error(error_message)\n+\n+    ####################################################################\n+    # Session selector\n+    ####################################################################\n+    session_selector_widget(agentic_rag_agent, model_id)\n+    rename_session_widget(agentic_rag_agent)\n+\n+    ####################################################################\n+    # About section\n+    ####################################################################\n+    about_widget()\n+\n+\n+main()\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/agentic_rag/generate_requirements.sh",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/agentic_rag/generate_requirements.sh b/cookbook/examples/streamlit_applications/agentic_rag/generate_requirements.sh\nnew file mode 100755\nindex 000000000..78f8c7cec\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/agentic_rag/generate_requirements.sh\n@@ -0,0 +1,12 @@\n+#!/bin/bash\n+\n+############################################################################\n+# Generate requirements.txt from requirements.in\n+############################################################################\n+\n+echo \"Generating requirements.txt\"\n+\n+CURR_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n+\n+UV_CUSTOM_COMPILE_COMMAND=\"./generate_requirements.sh\" \\\n+  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/agentic_rag/requirements.in",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/agentic_rag/requirements.in b/cookbook/examples/streamlit_applications/agentic_rag/requirements.in\nnew file mode 100644\nindex 000000000..b5ec5dd7c\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/agentic_rag/requirements.in\n@@ -0,0 +1,9 @@\n+agno\n+anthropic\n+duckduckgo_search\n+google-genai\n+groq\n+nest_asyncio\n+openai\n+sqlalchemy\n+streamlit\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/agentic_rag/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/agentic_rag/requirements.txt b/cookbook/examples/streamlit_applications/agentic_rag/requirements.txt\nnew file mode 100644\nindex 000000000..ee8818881\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/agentic_rag/requirements.txt\n@@ -0,0 +1,230 @@\n+# This file was autogenerated by uv via the following command:\n+#    ./generate_requirements.sh\n+agno==1.4.5\n+    # via -r cookbook/examples/apps/agentic_rag/requirements.in\n+altair==5.5.0\n+    # via streamlit\n+annotated-types==0.7.0\n+    # via pydantic\n+anthropic==0.51.0\n+    # via -r cookbook/examples/apps/agentic_rag/requirements.in\n+anyio==4.9.0\n+    # via\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   httpx\n+    #   openai\n+attrs==25.3.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+blinker==1.9.0\n+    # via streamlit\n+cachetools==5.5.2\n+    # via\n+    #   google-auth\n+    #   streamlit\n+certifi==2025.4.26\n+    # via\n+    #   httpcore\n+    #   httpx\n+    #   requests\n+charset-normalizer==3.4.2\n+    # via requests\n+click==8.1.8\n+    # via\n+    #   duckduckgo-search\n+    #   streamlit\n+    #   typer\n+distro==1.9.0\n+    # via\n+    #   anthropic\n+    #   groq\n+    #   openai\n+docstring-parser==0.16\n+    # via agno\n+duckduckgo-search==8.0.1\n+    # via -r cookbook/examples/apps/agentic_rag/requirements.in\n+exceptiongroup==1.2.2\n+    # via anyio\n+gitdb==4.0.12\n+    # via gitpython\n+gitpython==3.1.44\n+    # via\n+    #   agno\n+    #   streamlit\n+google-auth==2.40.1\n+    # via google-genai\n+google-genai==1.14.0\n+    # via -r cookbook/examples/apps/agentic_rag/requirements.in\n+groq==0.24.0\n+    # via -r cookbook/examples/apps/agentic_rag/requirements.in\n+h11==0.16.0\n+    # via httpcore\n+httpcore==1.0.9\n+    # via httpx\n+httpx==0.28.1\n+    # via\n+    #   agno\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   openai\n+idna==3.10\n+    # via\n+    #   anyio\n+    #   httpx\n+    #   requests\n+jinja2==3.1.6\n+    # via\n+    #   altair\n+    #   pydeck\n+jiter==0.9.0\n+    # via\n+    #   anthropic\n+    #   openai\n+jsonschema==4.23.0\n+    # via altair\n+jsonschema-specifications==2025.4.1\n+    # via jsonschema\n+lxml==5.4.0\n+    # via duckduckgo-search\n+markdown-it-py==3.0.0\n+    # via rich\n+markupsafe==3.0.2\n+    # via jinja2\n+mdurl==0.1.2\n+    # via markdown-it-py\n+narwhals==1.38.1\n+    # via altair\n+nest-asyncio==1.6.0\n+    # via -r cookbook/examples/apps/agentic_rag/requirements.in\n+numpy==2.2.5\n+    # via\n+    #   pandas\n+    #   pydeck\n+    #   streamlit\n+openai==1.77.0\n+    # via -r cookbook/examples/apps/agentic_rag/requirements.in\n+packaging==24.2\n+    # via\n+    #   altair\n+    #   streamlit\n+pandas==2.2.3\n+    # via streamlit\n+pillow==11.2.1\n+    # via streamlit\n+primp==0.15.0\n+    # via duckduckgo-search\n+protobuf==6.30.2\n+    # via streamlit\n+pyarrow==20.0.0\n+    # via streamlit\n+pyasn1==0.6.1\n+    # via\n+    #   pyasn1-modules\n+    #   rsa\n+pyasn1-modules==0.4.2\n+    # via google-auth\n+pydantic==2.11.4\n+    # via\n+    #   agno\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   openai\n+    #   pydantic-settings\n+pydantic-core==2.33.2\n+    # via pydantic\n+pydantic-settings==2.9.1\n+    # via agno\n+pydeck==0.9.1\n+    # via streamlit\n+pygments==2.19.1\n+    # via rich\n+python-dateutil==2.9.0.post0\n+    # via pandas\n+python-dotenv==1.1.0\n+    # via\n+    #   agno\n+    #   pydantic-settings\n+python-multipart==0.0.20\n+    # via agno\n+pytz==2025.2\n+    # via pandas\n+pyyaml==6.0.2\n+    # via agno\n+referencing==0.36.2\n+    # via\n+    #   jsonschema\n+    #   jsonschema-specifications\n+requests==2.32.3\n+    # via\n+    #   google-genai\n+    #   streamlit\n+rich==14.0.0\n+    # via\n+    #   agno\n+    #   typer\n+rpds-py==0.24.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+rsa==4.9.1\n+    # via google-auth\n+shellingham==1.5.4\n+    # via typer\n+six==1.17.0\n+    # via python-dateutil\n+smmap==5.0.2\n+    # via gitdb\n+sniffio==1.3.1\n+    # via\n+    #   anthropic\n+    #   anyio\n+    #   groq\n+    #   openai\n+sqlalchemy==2.0.40\n+    # via -r cookbook/examples/apps/agentic_rag/requirements.in\n+streamlit==1.45.0\n+    # via -r cookbook/examples/apps/agentic_rag/requirements.in\n+tenacity==9.1.2\n+    # via streamlit\n+toml==0.10.2\n+    # via streamlit\n+tomli==2.2.1\n+    # via agno\n+tornado==6.4.2\n+    # via streamlit\n+tqdm==4.67.1\n+    # via openai\n+typer==0.15.3\n+    # via agno\n+typing-extensions==4.13.2\n+    # via\n+    #   agno\n+    #   altair\n+    #   anthropic\n+    #   anyio\n+    #   google-genai\n+    #   groq\n+    #   openai\n+    #   pydantic\n+    #   pydantic-core\n+    #   referencing\n+    #   rich\n+    #   sqlalchemy\n+    #   streamlit\n+    #   typer\n+    #   typing-inspection\n+typing-inspection==0.4.0\n+    # via\n+    #   pydantic\n+    #   pydantic-settings\n+tzdata==2025.2\n+    # via pandas\n+urllib3==2.4.0\n+    # via requests\n+websockets==15.0.1\n+    # via google-genai\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/agentic_rag/utils.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/agentic_rag/utils.py b/cookbook/examples/streamlit_applications/agentic_rag/utils.py\nnew file mode 100644\nindex 000000000..b75d431f7\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/agentic_rag/utils.py\n@@ -0,0 +1,334 @@\n+from typing import Any, Dict, List, Optional\n+\n+import streamlit as st\n+from agentic_rag import get_agentic_rag_agent\n+from agno.agent import Agent\n+from agno.utils.log import logger\n+\n+\n+def add_message(\n+    role: str, content: str, tool_calls: Optional[List[Dict[str, Any]]] = None\n+) -> None:\n+    \"\"\"Safely add a message to the session state\"\"\"\n+    if \"messages\" not in st.session_state or not isinstance(\n+        st.session_state[\"messages\"], list\n+    ):\n+        st.session_state[\"messages\"] = []\n+    st.session_state[\"messages\"].append(\n+        {\"role\": role, \"content\": content, \"tool_calls\": tool_calls}\n+    )\n+\n+\n+def export_chat_history():\n+    \"\"\"Export chat history as markdown\"\"\"\n+    if \"messages\" in st.session_state:\n+        chat_text = \"# Auto RAG Agent - Chat History\\n\\n\"\n+        for msg in st.session_state[\"messages\"]:\n+            role = \"\ud83e\udd16 Assistant\" if msg[\"role\"] == \"agent\" else \"\ud83d\udc64 User\"\n+            chat_text += f\"### {role}\\n{msg['content']}\\n\\n\"\n+            if msg.get(\"tool_calls\"):\n+                chat_text += \"#### Tools Used:\\n\"\n+                for tool in msg[\"tool_calls\"]:\n+                    if isinstance(tool, dict):\n+                        tool_name = tool.get(\"name\", \"Unknown Tool\")\n+                    else:\n+                        tool_name = getattr(tool, \"name\", \"Unknown Tool\")\n+                    chat_text += f\"- {tool_name}\\n\"\n+        return chat_text\n+    return \"\"\n+\n+\n+def display_tool_calls(tool_calls_container, tools):\n+    \"\"\"Display tool calls in a streamlit container with expandable sections.\n+\n+    Args:\n+        tool_calls_container: Streamlit container to display the tool calls\n+        tools: List of tool call dictionaries containing name, args, content, and metrics\n+    \"\"\"\n+    if not tools:\n+        return\n+\n+    with tool_calls_container.container():\n+        for tool_call in tools:\n+            # Handle different tool call formats\n+            _tool_name = (\n+                tool_call.get(\"tool_name\") or tool_call.get(\"name\") or \"Unknown Tool\"\n+            )\n+            _tool_args = tool_call.get(\"tool_args\") or tool_call.get(\"arguments\") or {}\n+            _content = tool_call.get(\"content\") or tool_call.get(\"result\", \"\")\n+            _metrics = tool_call.get(\"metrics\", {})\n+\n+            # Handle function objects\n+            if hasattr(tool_call, \"function\") and tool_call.function:\n+                if hasattr(tool_call.function, \"name\"):\n+                    _tool_name = tool_call.function.name\n+                if hasattr(tool_call.function, \"arguments\"):\n+                    _tool_args = tool_call.function.arguments\n+\n+            # Safely create the title with a default if tool name is None\n+            title = f\"\ud83d\udee0\ufe0f {_tool_name.replace('_', ' ').title() if _tool_name else 'Tool Call'}\"\n+\n+            with st.expander(title, expanded=False):\n+                if isinstance(_tool_args, dict) and \"query\" in _tool_args:\n+                    st.code(_tool_args[\"query\"], language=\"sql\")\n+                # Handle string arguments\n+                elif isinstance(_tool_args, str) and _tool_args:\n+                    try:\n+                        # Try to parse as JSON\n+                        import json\n+\n+                        args_dict = json.loads(_tool_args)\n+                        st.markdown(\"**Arguments:**\")\n+                        st.json(args_dict)\n+                    except:\n+                        # If not valid JSON, display as string\n+                        st.markdown(\"**Arguments:**\")\n+                        st.markdown(f\"```\\n{_tool_args}\\n```\")\n+                # Handle dict arguments\n+                elif _tool_args and _tool_args != {\"query\": None}:\n+                    st.markdown(\"**Arguments:**\")\n+                    st.json(_tool_args)\n+\n+                if _content:\n+                    st.markdown(\"**Results:**\")\n+                    if isinstance(_content, (dict, list)):\n+                        st.json(_content)\n+                    else:\n+                        try:\n+                            st.json(_content)\n+                        except Exception:\n+                            st.markdown(_content)\n+\n+                if _metrics:\n+                    st.markdown(\"**Metrics:**\")\n+                    st.json(_metrics)\n+\n+\n+def rename_session_widget(agent: Agent) -> None:\n+    \"\"\"Rename the current session of the agent and save to storage\"\"\"\n+\n+    container = st.sidebar.container()\n+\n+    # Initialize session_edit_mode if needed\n+    if \"session_edit_mode\" not in st.session_state:\n+        st.session_state.session_edit_mode = False\n+\n+    if st.sidebar.button(\"\u270e Rename Session\"):\n+        st.session_state.session_edit_mode = True\n+        st.rerun()\n+\n+    if st.session_state.session_edit_mode:\n+        new_session_name = st.sidebar.text_input(\n+            \"Enter new name:\",\n+            value=agent.session_name,\n+            key=\"session_name_input\",\n+        )\n+        if st.sidebar.button(\"Save\", type=\"primary\"):\n+            if new_session_name:\n+                agent.rename_session(new_session_name)\n+                st.session_state.session_edit_mode = False\n+                st.rerun()\n+\n+\n+def session_selector_widget(agent: Agent, model_id: str) -> None:\n+    \"\"\"Display a session selector in the sidebar\"\"\"\n+\n+    if agent.storage:\n+        agent_sessions = agent.storage.get_all_sessions()\n+        # print(f\"Agent sessions: {agent_sessions}\")\n+\n+        session_options = []\n+        for session in agent_sessions:\n+            session_id = session.session_id\n+            session_name = (\n+                session.session_data.get(\"session_name\", None)\n+                if session.session_data\n+                else None\n+            )\n+            display_name = session_name if session_name else session_id\n+            session_options.append({\"id\": session_id, \"display\": display_name})\n+\n+        if session_options:\n+            selected_session = st.sidebar.selectbox(\n+                \"Session\",\n+                options=[s[\"display\"] for s in session_options],\n+                key=\"session_selector\",\n+            )\n+            # Find the selected session ID\n+            selected_session_id = next(\n+                s[\"id\"] for s in session_options if s[\"display\"] == selected_session\n+            )\n+\n+            if (\n+                st.session_state.get(\"agentic_rag_agent_session_id\")\n+                != selected_session_id\n+            ):\n+                logger.info(\n+                    f\"---*--- Loading {model_id} run: {selected_session_id} ---*---\"\n+                )\n+\n+                try:\n+                    new_agent = get_agentic_rag_agent(\n+                        model_id=model_id,\n+                        session_id=selected_session_id,\n+                    )\n+\n+                    st.session_state[\"agentic_rag_agent\"] = new_agent\n+                    st.session_state[\"agentic_rag_agent_session_id\"] = (\n+                        selected_session_id\n+                    )\n+\n+                    st.session_state[\"messages\"] = []\n+\n+                    selected_session_obj = next(\n+                        (\n+                            s\n+                            for s in agent_sessions\n+                            if s.session_id == selected_session_id\n+                        ),\n+                        None,\n+                    )\n+\n+                    if (\n+                        selected_session_obj\n+                        and selected_session_obj.memory\n+                        and \"runs\" in selected_session_obj.memory\n+                    ):\n+                        seen_messages = set()\n+\n+                        for run in selected_session_obj.memory[\"runs\"]:\n+                            if \"messages\" in run:\n+                                for msg in run[\"messages\"]:\n+                                    msg_role = msg.get(\"role\")\n+                                    msg_content = msg.get(\"content\")\n+\n+                                    if not msg_content or msg_role == \"system\":\n+                                        continue\n+\n+                                    msg_id = f\"{msg_role}:{msg_content}\"\n+\n+                                    if msg_id in seen_messages:\n+                                        continue\n+\n+                                    seen_messages.add(msg_id)\n+\n+                                    if msg_role == \"assistant\":\n+                                        tool_calls = None\n+                                        if \"tool_calls\" in msg:\n+                                            tool_calls = msg[\"tool_calls\"]\n+                                        elif \"metrics\" in msg and msg.get(\"metrics\"):\n+                                            tools = run.get(\"tools\")\n+                                            if tools:\n+                                                tool_calls = tools\n+\n+                                        add_message(msg_role, msg_content, tool_calls)\n+                                    else:\n+                                        add_message(msg_role, msg_content)\n+\n+                            elif (\n+                                \"message\" in run\n+                                and isinstance(run[\"message\"], dict)\n+                                and \"content\" in run[\"message\"]\n+                            ):\n+                                user_msg = run[\"message\"][\"content\"]\n+                                msg_id = f\"user:{user_msg}\"\n+\n+                                if msg_id not in seen_messages:\n+                                    seen_messages.add(msg_id)\n+                                    add_message(\"user\", user_msg)\n+\n+                                if \"content\" in run and run[\"content\"]:\n+                                    asst_msg = run[\"content\"]\n+                                    msg_id = f\"assistant:{asst_msg}\"\n+\n+                                    if msg_id not in seen_messages:\n+                                        seen_messages.add(msg_id)\n+                                        add_message(\n+                                            \"assistant\", asst_msg, run.get(\"tools\")\n+                                        )\n+\n+                    st.rerun()\n+                except Exception as e:\n+                    logger.error(f\"Error switching sessions: {str(e)}\")\n+                    st.sidebar.error(f\"Error loading session: {str(e)}\")\n+        else:\n+            st.sidebar.info(\"No saved sessions available.\")\n+\n+\n+def about_widget() -> None:\n+    \"\"\"Display an about section in the sidebar\"\"\"\n+    st.sidebar.markdown(\"---\")\n+    st.sidebar.markdown(\"### \u2139\ufe0f About\")\n+    st.sidebar.markdown(\"\"\"\n+    This Agentic RAG Assistant helps you analyze documents and web content using natural language queries.\n+\n+    Built with:\n+    - \ud83d\ude80 Agno\n+    - \ud83d\udcab Streamlit\n+    \"\"\")\n+\n+\n+CUSTOM_CSS = \"\"\"\n+    <style>\n+    /* Main Styles */\n+   .main-title {\n+        text-align: center;\n+        background: linear-gradient(45deg, #FF4B2B, #FF416C);\n+        -webkit-background-clip: text;\n+        -webkit-text-fill-color: transparent;\n+        font-size: 3em;\n+        font-weight: bold;\n+        padding: 1em 0;\n+    }\n+    .subtitle {\n+        text-align: center;\n+        color: #666;\n+        margin-bottom: 2em;\n+    }\n+    .stButton button {\n+        width: 100%;\n+        border-radius: 20px;\n+        margin: 0.2em 0;\n+        transition: all 0.3s ease;\n+    }\n+    .stButton button:hover {\n+        transform: translateY(-2px);\n+        box-shadow: 0 5px 15px rgba(0,0,0,0.1);\n+    }\n+    .chat-container {\n+        border-radius: 15px;\n+        padding: 1em;\n+        margin: 1em 0;\n+        background-color: #f5f5f5;\n+    }\n+    .tool-result {\n+        background-color: #f8f9fa;\n+        border-radius: 10px;\n+        padding: 1em;\n+        margin: 1em 0;\n+        border-left: 4px solid #3B82F6;\n+    }\n+    .status-message {\n+        padding: 1em;\n+        border-radius: 10px;\n+        margin: 1em 0;\n+    }\n+    .success-message {\n+        background-color: #d4edda;\n+        color: #155724;\n+    }\n+    .error-message {\n+        background-color: #f8d7da;\n+        color: #721c24;\n+    }\n+    /* Dark mode adjustments */\n+    @media (prefers-color-scheme: dark) {\n+        .chat-container {\n+            background-color: #2b2b2b;\n+        }\n+        .tool-result {\n+            background-color: #1e1e1e;\n+        }\n+    }\n+    </style>\n+\"\"\"\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/answer_engine/.gitignore",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/answer_engine/.gitignore b/cookbook/examples/streamlit_applications/answer_engine/.gitignore\nnew file mode 100644\nindex 000000000..1d3a629a5\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/answer_engine/.gitignore\n@@ -0,0 +1,2 @@\n+output\n+agents.db\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/answer_engine/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/answer_engine/__init__.py b/cookbook/examples/streamlit_applications/answer_engine/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/answer_engine/agents.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/answer_engine/agents.py b/cookbook/examples/streamlit_applications/answer_engine/agents.py\nnew file mode 100644\nindex 000000000..0de3c95a5\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/answer_engine/agents.py\n@@ -0,0 +1,154 @@\n+\"\"\"\n+Sage: An Answer Engine\n+---------------------------------\n+This example shows how to build Sage, a Perplexity-like Answer Engine that intelligently\n+determines whether to perform a web search or conduct a deep analysis using ExaTools based on the user's query.\n+It also prompts the user to save the generated answer to a file using FileTools.\n+\n+Usage Examples:\n+---------------\n+1. Quick real-time search:\n+   sage = get_sage()\n+   answer = sage.run(\"What are the latest trends in renewable energy?\")\n+\n+2. In-depth analysis:\n+   sage = get_sage()\n+   answer = sage.run(\"Perform a detailed analysis of the impact of climate change on agriculture.\")\n+\n+3. Combined query with saving option:\n+   sage = get_sage()\n+   answer = sage.run(\"What's new in AI regulations in the EU and could you save the summary for me?\")\n+\n+Sage integrates:\n+  - DuckDuckGoTools for real-time web searches.\n+  - ExaTools for structured, in-depth analysis.\n+  - FileTools for saving the output upon user confirmation.\n+\n+Sage intelligently selects the optimal tool based on query complexity to provide insightful, comprehensive answers.\n+\"\"\"\n+\n+import uuid\n+from datetime import datetime\n+from pathlib import Path\n+from typing import Optional\n+\n+# Importing the Agent and model classes\n+from agno.agent import Agent\n+from agno.models.anthropic import Claude\n+from agno.models.google import Gemini\n+from agno.models.groq import Groq\n+from agno.models.openai import OpenAIChat\n+\n+# Importing storage and tool classes\n+from agno.storage.agent.sqlite import SqliteAgentStorage\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.tools.exa import ExaTools\n+from agno.tools.file import FileTools\n+\n+# Import the Agent template\n+from prompts import AGENT_DESCRIPTION, AGENT_INSTRUCTIONS, EXPECTED_OUTPUT_TEMPLATE\n+\n+# ************* Setup Paths *************\n+# Define the current working directory and output directory for saving files\n+cwd = Path(__file__).parent\n+output_dir = cwd.joinpath(\"output\")\n+# Create output directory if it doesn't exist\n+output_dir.mkdir(parents=True, exist_ok=True)\n+# Create tmp directory if it doesn't exist\n+tmp_dir = cwd.joinpath(\"tmp\")\n+tmp_dir.mkdir(parents=True, exist_ok=True)\n+# *************************************\n+\n+# ************* Agent Storage *************\n+# Configure SQLite storage for agent sessions\n+agent_storage = SqliteAgentStorage(\n+    table_name=\"answer_engine_sessions\",  # Table to store agent sessions\n+    db_file=str(tmp_dir.joinpath(\"agents.db\")),  # SQLite database file\n+    auto_upgrade_schema=True,\n+)\n+# *************************************\n+\n+\n+def get_sage(\n+    user_id: Optional[str] = None,\n+    model_id: str = \"openai:gpt-4o\",\n+    session_id: Optional[str] = None,\n+    num_history_responses: int = 5,\n+    debug_mode: bool = True,\n+) -> Agent:\n+    \"\"\"\n+    Returns an instance of Sage, the Answer Engine Agent with integrated tools for web search,\n+    deep contextual analysis, and file management.\n+\n+    Sage will:\n+      - Decide whether a query requires a real-time web search (using DuckDuckGoTools)\n+        or an in-depth analysis (using ExaTools).\n+      - Generate a comprehensive answer that includes:\n+          \u2022 A direct, succinct answer.\n+          \u2022 Detailed explanations and supporting evidence.\n+          \u2022 Examples and clarification of misconceptions.\n+      - Prompt the user:\n+            \"Would you like to save this answer to a file? (yes/no)\"\n+        If confirmed, it will use FileTools to save the answer in markdown format in the output directory.\n+\n+    Args:\n+        user_id: Optional identifier for the user.\n+        model_id: Model identifier in the format 'provider:model_name' (e.g., \"openai:gpt-4o\").\n+        session_id: Optional session identifier for tracking conversation history.\n+        num_history_responses: Number of previous responses to include for context.\n+        debug_mode: Enable logging and debug features.\n+\n+    Returns:\n+        An instance of the configured Agent.\n+    \"\"\"\n+\n+    # Parse model provider and name\n+    provider, model_name = model_id.split(\":\")\n+\n+    # Select appropriate model class based on provider\n+    if provider == \"openai\":\n+        model = OpenAIChat(id=model_name)\n+    elif provider == \"google\":\n+        model = Gemini(id=model_name)\n+    elif provider == \"anthropic\":\n+        model = Claude(id=model_name)\n+    elif provider == \"groq\":\n+        model = Groq(id=model_name)\n+    else:\n+        raise ValueError(f\"Unsupported model provider: {provider}\")\n+\n+    # Tools for Sage\n+    tools = [\n+        ExaTools(\n+            start_published_date=datetime.now().strftime(\"%Y-%m-%d\"),\n+            type=\"keyword\",\n+            num_results=10,\n+        ),\n+        DuckDuckGoTools(\n+            timeout=20,\n+            fixed_max_results=5,\n+        ),\n+        FileTools(base_dir=output_dir),\n+    ]\n+\n+    return Agent(\n+        name=\"Sage\",\n+        model=model,\n+        user_id=user_id,\n+        session_id=session_id or str(uuid.uuid4()),\n+        storage=agent_storage,\n+        tools=tools,\n+        # Allow Sage to read both chat history and tool call history for better context.\n+        read_chat_history=True,\n+        read_tool_call_history=True,\n+        # Append previous conversation responses into the new messages for context.\n+        add_history_to_messages=True,\n+        num_history_responses=num_history_responses,\n+        add_datetime_to_instructions=True,\n+        add_name_to_instructions=True,\n+        description=AGENT_DESCRIPTION,\n+        instructions=AGENT_INSTRUCTIONS,\n+        expected_output=EXPECTED_OUTPUT_TEMPLATE,\n+        debug_mode=debug_mode,\n+        markdown=True,\n+    )\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/answer_engine/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/answer_engine/app.py b/cookbook/examples/streamlit_applications/answer_engine/app.py\nnew file mode 100644\nindex 000000000..0c03fddb0\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/answer_engine/app.py\n@@ -0,0 +1,207 @@\n+import nest_asyncio\n+import streamlit as st\n+from agents import get_sage\n+from agno.agent import Agent\n+from agno.utils.log import logger\n+from utils import (\n+    CUSTOM_CSS,\n+    about_widget,\n+    add_message,\n+    display_tool_calls,\n+    rename_session_widget,\n+    session_selector_widget,\n+    sidebar_widget,\n+)\n+\n+nest_asyncio.apply()\n+\n+# Page configuration\n+st.set_page_config(\n+    page_title=\"Sage: The Answer Engine\",\n+    page_icon=\":crystal_ball:\",\n+    layout=\"wide\",\n+    initial_sidebar_state=\"expanded\",\n+)\n+\n+# Load custom CSS with dark mode support\n+st.markdown(CUSTOM_CSS, unsafe_allow_html=True)\n+\n+\n+def main() -> None:\n+    ####################################################################\n+    # App header\n+    ####################################################################\n+    st.markdown(\"<h1 class='main-title'>Sage</h1>\", unsafe_allow_html=True)\n+    st.markdown(\n+        \"<p class='subtitle'>Your intelligent answer engine powered by Agno</p>\",\n+        unsafe_allow_html=True,\n+    )\n+\n+    ####################################################################\n+    # Model selector\n+    ####################################################################\n+    model_options = {\n+        \"llama-3.3-70b\": \"groq:llama-3.3-70b-versatile\",\n+        \"gpt-4o\": \"openai:gpt-4o\",\n+        \"o3-mini\": \"openai:o3-mini\",\n+        \"gemini-2.0-flash-exp\": \"google:gemini-2.0-flash-exp\",\n+        \"claude-3-5-sonnet\": \"anthropic:claude-3-5-sonnet-20241022\",\n+    }\n+    selected_model = st.sidebar.selectbox(\n+        \"Choose a model\",\n+        options=list(model_options.keys()),\n+        index=0,\n+        key=\"model_selector\",\n+    )\n+    model_id = model_options[selected_model]\n+\n+    ####################################################################\n+    # Initialize Agent\n+    ####################################################################\n+    sage: Agent\n+    if (\n+        \"sage\" not in st.session_state\n+        or st.session_state[\"sage\"] is None\n+        or st.session_state.get(\"current_model\") != model_id\n+    ):\n+        logger.info(\"---*--- Creating new Sage agent ---*---\")\n+        sage = get_sage(model_id=model_id)\n+        st.session_state[\"sage\"] = sage\n+        st.session_state[\"current_model\"] = model_id\n+        # Initialize messages array if needed\n+        if \"messages\" not in st.session_state:\n+            st.session_state[\"messages\"] = []\n+    else:\n+        sage = st.session_state[\"sage\"]\n+\n+    ####################################################################\n+    # Load Agent Session from the database\n+    ####################################################################\n+    # Initialize session state\n+    if \"sage_session_id\" not in st.session_state:\n+        st.session_state[\"sage_session_id\"] = None\n+\n+    # Attempt to load or create a session\n+    if not st.session_state[\"sage_session_id\"]:\n+        try:\n+            logger.info(\"---*--- Loading Sage session ---*---\")\n+            st.session_state[\"sage_session_id\"] = sage.load_session()\n+            logger.info(\n+                f\"---*--- Sage session: {st.session_state['sage_session_id']} ---*---\"\n+            )\n+        except Exception as e:\n+            logger.error(f\"Session load error: {str(e)}\")\n+            st.warning(\"Database connection unavailable. Running in memory-only mode.\")\n+            # Generate a temporary session ID to allow the app to function without storage\n+            if not st.session_state[\"sage_session_id\"]:\n+                import uuid\n+\n+                st.session_state[\"sage_session_id\"] = f\"temp-{str(uuid.uuid4())}\"\n+                logger.info(\n+                    f\"---*--- Created temporary session: {st.session_state['sage_session_id']} ---*---\"\n+                )\n+\n+    ####################################################################\n+    # Load runs from memory\n+    ####################################################################\n+    # Initialize the messages array if not already done\n+    if \"messages\" not in st.session_state:\n+        st.session_state[\"messages\"] = []\n+\n+    # Only try to load runs from memory if we have a valid session and no messages yet\n+    if (\n+        len(st.session_state[\"messages\"]) == 0\n+        and hasattr(sage, \"memory\")\n+        and sage.memory is not None\n+    ):\n+        agent_runs = []\n+        # Check if memory is a dict or an object with runs attribute\n+        if isinstance(sage.memory, dict) and \"runs\" in sage.memory:\n+            agent_runs = sage.memory[\"runs\"]\n+        elif hasattr(sage.memory, \"runs\"):\n+            agent_runs = sage.memory.runs\n+\n+        # Load messages from agent runs\n+        if len(agent_runs) > 0:\n+            logger.debug(\"Loading run history\")\n+            for _run in agent_runs:\n+                # Check if _run is an object with message attribute\n+                if hasattr(_run, \"message\") and _run.message is not None:\n+                    add_message(_run.message.role, _run.message.content)\n+                # Check if _run is an object with response attribute\n+                if hasattr(_run, \"response\") and _run.response is not None:\n+                    add_message(\"assistant\", _run.response.content, _run.response.tools)\n+        else:\n+            logger.debug(\"No run history found\")\n+\n+    ####################################################################\n+    # Sidebar\n+    ####################################################################\n+    sidebar_widget()\n+\n+    ####################################################################\n+    # Get user input\n+    ####################################################################\n+    if prompt := st.chat_input(\"\u2728 What would you like to know, bestie?\"):\n+        add_message(\"user\", prompt)\n+\n+    ####################################################################\n+    # Display chat history\n+    ####################################################################\n+    for message in st.session_state[\"messages\"]:\n+        if message[\"role\"] in [\"user\", \"assistant\"]:\n+            _content = message[\"content\"]\n+            if _content is not None:\n+                with st.chat_message(message[\"role\"]):\n+                    # Display tool calls if they exist in the message\n+                    if \"tool_calls\" in message and message[\"tool_calls\"]:\n+                        display_tool_calls(st.empty(), message[\"tool_calls\"])\n+                    st.markdown(_content)\n+\n+    ####################################################################\n+    # Generate response for user message\n+    ####################################################################\n+    last_message = (\n+        st.session_state[\"messages\"][-1] if st.session_state[\"messages\"] else None\n+    )\n+    if last_message and last_message.get(\"role\") == \"user\":\n+        question = last_message[\"content\"]\n+        with st.chat_message(\"assistant\"):\n+            # Create container for tool calls\n+            tool_calls_container = st.empty()\n+            resp_container = st.empty()\n+            with st.spinner(\":crystal_ball: Sage is working its magic...\"):\n+                response = \"\"\n+                try:\n+                    # Run the agent and stream the response\n+                    run_response = sage.run(question, stream=True)\n+                    for _resp_chunk in run_response:\n+                        # Display tool calls if available\n+                        if _resp_chunk.tools and len(_resp_chunk.tools) > 0:\n+                            display_tool_calls(tool_calls_container, _resp_chunk.tools)\n+\n+                        # Display response\n+                        if _resp_chunk.content is not None:\n+                            response += _resp_chunk.content\n+                            resp_container.markdown(response)\n+\n+                    add_message(\"assistant\", response, sage.run_response.tools)\n+                except Exception as e:\n+                    error_message = f\"Sorry, I encountered an error: {str(e)}\"\n+                    add_message(\"assistant\", error_message)\n+                    st.error(error_message)\n+\n+    ####################################################################\n+    # Session selector\n+    ####################################################################\n+    session_selector_widget(sage, model_id)\n+    rename_session_widget(sage)\n+\n+    ####################################################################\n+    # About section\n+    ####################################################################\n+    about_widget()\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/answer_engine/generate_requirements.sh",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/answer_engine/generate_requirements.sh b/cookbook/examples/streamlit_applications/answer_engine/generate_requirements.sh\nnew file mode 100755\nindex 000000000..78f8c7cec\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/answer_engine/generate_requirements.sh\n@@ -0,0 +1,12 @@\n+#!/bin/bash\n+\n+############################################################################\n+# Generate requirements.txt from requirements.in\n+############################################################################\n+\n+echo \"Generating requirements.txt\"\n+\n+CURR_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n+\n+UV_CUSTOM_COMPILE_COMMAND=\"./generate_requirements.sh\" \\\n+  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/answer_engine/prompts.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/answer_engine/prompts.py b/cookbook/examples/streamlit_applications/answer_engine/prompts.py\nnew file mode 100644\nindex 000000000..6b3e96040\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/answer_engine/prompts.py\n@@ -0,0 +1,96 @@\n+\"\"\"Templates for the Answer Engine.\"\"\"\n+\n+from textwrap import dedent\n+\n+AGENT_DESCRIPTION = dedent(\"\"\"\\\n+    You are Sage, a cutting-edge Answer Engine built to deliver precise, context-rich, and engaging responses.\n+    You have the following tools at your disposal:\n+      - DuckDuckGoTools for real-time web searches to fetch up-to-date information.\n+      - ExaTools for structured, in-depth analysis.\n+      - FileTools for saving the output upon user confirmation.\n+\n+    Your response should always be clear, concise, and detailed. Blend direct answers with extended analysis,\n+    supporting evidence, illustrative examples, and clarifications on common misconceptions. Engage the user\n+    with follow-up questions, such as asking if they'd like to save the answer.\n+\n+    <critical>\n+    - Before you answer, you must search both DuckDuckGo and ExaTools to generate your answer. If you don't, you will be penalized.\n+    - You must provide sources, whenever you provide a data point or a statistic.\n+    - When the user asks a follow-up question, you can use the previous answer as context.\n+    - If you don't have the relevant information, you must search both DuckDuckGo and ExaTools to generate your answer.\n+    </critical>\\\n+\"\"\")\n+\n+AGENT_INSTRUCTIONS = dedent(\"\"\"\\\n+    Here's how you should answer the user's question:\n+\n+    1. Gather Relevant Information\n+      - First, carefully analyze the query to identify the intent of the user.\n+      - Break down the query into core components, then construct 1-3 precise search terms that help cover all possible aspects of the query.\n+      - Then, search using BOTH `duckduckgo_search` and `search_exa` with the search terms. Remember to search both tools.\n+      - Combine the insights from both tools to craft a comprehensive and balanced answer.\n+      - If you need to get the contents from a specific URL, use the `get_contents` tool with the URL as the argument.\n+      - CRITICAL: BEFORE YOU ANSWER, YOU MUST SEARCH BOTH DuckDuckGo and Exa to generate your answer, otherwise you will be penalized.\n+\n+    2. Construct Your Response\n+      - **Start** with a succinct, clear and direct answer that immediately addresses the user's query.\n+      - **Then expand** the answer by including:\n+          \u2022 A clear explanation with context and definitions.\n+          \u2022 Supporting evidence such as statistics, real-world examples, and data points.\n+          \u2022 Clarifications that address common misconceptions.\n+      - Expand the answer only if the query requires more detail. Simple questions like: \"What is the weather in Tokyo?\" or \"What is the capital of France?\" don't need an in-depth analysis.\n+      - Ensure the response is structured so that it provides quick answers as well as in-depth analysis for further exploration.\n+\n+    3. Enhance Engagement\n+      - After generating your answer, ask the user if they would like to save this answer to a file? (yes/no)\"\n+      - If the user wants to save the response, use FileTools to save the response in markdown format in the output directory.\n+\n+    4. Final Quality Check & Presentation \u2728\n+      - Review your response to ensure clarity, depth, and engagement.\n+      - Strive to be both informative for quick queries and thorough for detailed exploration.\n+\n+    5. In case of any uncertainties, clarify limitations and encourage follow-up queries.\\\n+\"\"\")\n+\n+EXPECTED_OUTPUT_TEMPLATE = dedent(\"\"\"\\\n+    {# If this is the first message, include the question title #}\n+    {% if this is the first message %}\n+    ## {An engaging title for this report. Keep it short.}\n+    {% endif %}\n+\n+    **{A clear and direct response that answers the question.}**\n+\n+    {# If the query requires more detail, include the sections below #}\n+    {% if detailed_response %}\n+\n+    ### {Secion title}\n+    {Add detailed analysis & explanation in this section}\n+    {A comprehensive breakdown covering key insights, context, and definitions.}\n+\n+    ### {Section title}\n+    {Add evidence & support in this section}\n+    {Add relevant data points and statistics in this section}\n+    {Add links or names of reputable sources supporting the answer in this section}\n+\n+    ### {Section title}\n+    {Add real-world examples or case studies that help illustrate the key points in this section}\n+\n+    ### {Section title}\n+    {Add clarifications addressing any common misunderstandings related to the topic in this section}\n+\n+    ### {Section title}\n+    {Add further details, implications, or suggestions for ongoing exploration in this section}\n+    {% endif %}\n+\n+    {Add any more sections you think are relevant, covering all the aspects of the query}\n+\n+    ### Sources\n+    - [1] {Source 1 url}\n+    - [2] {Source 2 url}\n+    - [3] {Source 3 url}\n+    - {any more sources you think are relevant}\n+\n+    Generated by Sage on: {current_time}\n+\n+    Stay curious and keep exploring \u2728\\\n+    \"\"\")\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/answer_engine/requirements.in",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/answer_engine/requirements.in b/cookbook/examples/streamlit_applications/answer_engine/requirements.in\nnew file mode 100644\nindex 000000000..32a90e8e8\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/answer_engine/requirements.in\n@@ -0,0 +1,11 @@\n+agno\n+anthropic\n+duckduckgo_search\n+exa_py\n+google-generativeai\n+google-search-results\n+groq\n+nest_asyncio\n+openai\n+sqlalchemy\n+streamlit\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/answer_engine/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/answer_engine/requirements.txt b/cookbook/examples/streamlit_applications/answer_engine/requirements.txt\nnew file mode 100644\nindex 000000000..bad8ba226\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/answer_engine/requirements.txt\n@@ -0,0 +1,274 @@\n+# This file was autogenerated by uv via the following command:\n+#    ./generate_requirements.sh\n+agno==1.1.11\n+    # via -r cookbook/examples/apps/answer_engine/requirements.in\n+altair==5.5.0\n+    # via streamlit\n+annotated-types==0.7.0\n+    # via pydantic\n+anthropic==0.49.0\n+    # via -r cookbook/examples/apps/answer_engine/requirements.in\n+anyio==4.8.0\n+    # via\n+    #   anthropic\n+    #   groq\n+    #   httpx\n+    #   openai\n+attrs==25.3.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+blinker==1.9.0\n+    # via streamlit\n+cachetools==5.5.2\n+    # via\n+    #   google-auth\n+    #   streamlit\n+certifi==2025.1.31\n+    # via\n+    #   httpcore\n+    #   httpx\n+    #   requests\n+charset-normalizer==3.4.1\n+    # via requests\n+click==8.1.8\n+    # via\n+    #   duckduckgo-search\n+    #   streamlit\n+    #   typer\n+distro==1.9.0\n+    # via\n+    #   anthropic\n+    #   groq\n+    #   openai\n+docstring-parser==0.16\n+    # via agno\n+duckduckgo-search==7.5.2\n+    # via -r cookbook/examples/apps/answer_engine/requirements.in\n+exa-py==1.9.0\n+    # via -r cookbook/examples/apps/answer_engine/requirements.in\n+gitdb==4.0.12\n+    # via gitpython\n+gitpython==3.1.44\n+    # via\n+    #   agno\n+    #   streamlit\n+google-ai-generativelanguage==0.6.15\n+    # via google-generativeai\n+google-api-core==2.24.2\n+    # via\n+    #   google-ai-generativelanguage\n+    #   google-api-python-client\n+    #   google-generativeai\n+google-api-python-client==2.164.0\n+    # via google-generativeai\n+google-auth==2.38.0\n+    # via\n+    #   google-ai-generativelanguage\n+    #   google-api-core\n+    #   google-api-python-client\n+    #   google-auth-httplib2\n+    #   google-generativeai\n+google-auth-httplib2==0.2.0\n+    # via google-api-python-client\n+google-generativeai==0.8.4\n+    # via -r cookbook/examples/apps/answer_engine/requirements.in\n+google-search-results==2.4.2\n+    # via -r cookbook/examples/apps/answer_engine/requirements.in\n+googleapis-common-protos==1.69.1\n+    # via\n+    #   google-api-core\n+    #   grpcio-status\n+groq==0.19.0\n+    # via -r cookbook/examples/apps/answer_engine/requirements.in\n+grpcio==1.71.0\n+    # via\n+    #   google-api-core\n+    #   grpcio-status\n+grpcio-status==1.71.0\n+    # via google-api-core\n+h11==0.14.0\n+    # via httpcore\n+httpcore==1.0.7\n+    # via httpx\n+httplib2==0.22.0\n+    # via\n+    #   google-api-python-client\n+    #   google-auth-httplib2\n+httpx==0.28.1\n+    # via\n+    #   agno\n+    #   anthropic\n+    #   groq\n+    #   openai\n+idna==3.10\n+    # via\n+    #   anyio\n+    #   httpx\n+    #   requests\n+jinja2==3.1.6\n+    # via\n+    #   altair\n+    #   pydeck\n+jiter==0.9.0\n+    # via\n+    #   anthropic\n+    #   openai\n+jsonschema==4.23.0\n+    # via altair\n+jsonschema-specifications==2024.10.1\n+    # via jsonschema\n+lxml==5.3.1\n+    # via duckduckgo-search\n+markdown-it-py==3.0.0\n+    # via rich\n+markupsafe==3.0.2\n+    # via jinja2\n+mdurl==0.1.2\n+    # via markdown-it-py\n+narwhals==1.30.0\n+    # via altair\n+nest-asyncio==1.6.0\n+    # via -r cookbook/examples/apps/answer_engine/requirements.in\n+numpy==2.2.3\n+    # via\n+    #   pandas\n+    #   pydeck\n+    #   streamlit\n+openai==1.66.3\n+    # via\n+    #   -r cookbook/examples/apps/answer_engine/requirements.in\n+    #   exa-py\n+packaging==24.2\n+    # via\n+    #   altair\n+    #   streamlit\n+pandas==2.2.3\n+    # via streamlit\n+pillow==11.1.0\n+    # via streamlit\n+primp==0.14.0\n+    # via duckduckgo-search\n+proto-plus==1.26.1\n+    # via\n+    #   google-ai-generativelanguage\n+    #   google-api-core\n+protobuf==5.29.3\n+    # via\n+    #   google-ai-generativelanguage\n+    #   google-api-core\n+    #   google-generativeai\n+    #   googleapis-common-protos\n+    #   grpcio-status\n+    #   proto-plus\n+    #   streamlit\n+pyarrow==19.0.1\n+    # via streamlit\n+pyasn1==0.6.1\n+    # via\n+    #   pyasn1-modules\n+    #   rsa\n+pyasn1-modules==0.4.1\n+    # via google-auth\n+pydantic==2.10.6\n+    # via\n+    #   agno\n+    #   anthropic\n+    #   google-generativeai\n+    #   groq\n+    #   openai\n+    #   pydantic-settings\n+pydantic-core==2.27.2\n+    # via pydantic\n+pydantic-settings==2.8.1\n+    # via agno\n+pydeck==0.9.1\n+    # via streamlit\n+pygments==2.19.1\n+    # via rich\n+pyparsing==3.2.1\n+    # via httplib2\n+python-dateutil==2.9.0.post0\n+    # via pandas\n+python-dotenv==1.0.1\n+    # via\n+    #   agno\n+    #   pydantic-settings\n+python-multipart==0.0.20\n+    # via agno\n+pytz==2025.1\n+    # via pandas\n+pyyaml==6.0.2\n+    # via agno\n+referencing==0.36.2\n+    # via\n+    #   jsonschema\n+    #   jsonschema-specifications\n+requests==2.32.3\n+    # via\n+    #   exa-py\n+    #   google-api-core\n+    #   google-search-results\n+    #   streamlit\n+rich==13.9.4\n+    # via\n+    #   agno\n+    #   typer\n+rpds-py==0.23.1\n+    # via\n+    #   jsonschema\n+    #   referencing\n+rsa==4.9\n+    # via google-auth\n+shellingham==1.5.4\n+    # via typer\n+six==1.17.0\n+    # via python-dateutil\n+smmap==5.0.2\n+    # via gitdb\n+sniffio==1.3.1\n+    # via\n+    #   anthropic\n+    #   anyio\n+    #   groq\n+    #   openai\n+sqlalchemy==2.0.39\n+    # via -r cookbook/examples/apps/answer_engine/requirements.in\n+streamlit==1.43.2\n+    # via -r cookbook/examples/apps/answer_engine/requirements.in\n+tenacity==9.0.0\n+    # via streamlit\n+toml==0.10.2\n+    # via streamlit\n+tomli==2.2.1\n+    # via agno\n+tornado==6.4.2\n+    # via streamlit\n+tqdm==4.67.1\n+    # via\n+    #   google-generativeai\n+    #   openai\n+typer==0.15.2\n+    # via agno\n+typing-extensions==4.12.2\n+    # via\n+    #   agno\n+    #   altair\n+    #   anthropic\n+    #   anyio\n+    #   exa-py\n+    #   google-generativeai\n+    #   groq\n+    #   openai\n+    #   pydantic\n+    #   pydantic-core\n+    #   referencing\n+    #   sqlalchemy\n+    #   streamlit\n+    #   typer\n+tzdata==2025.1\n+    # via pandas\n+uritemplate==4.1.1\n+    # via google-api-python-client\n+urllib3==2.3.0\n+    # via requests\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/answer_engine/test.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/answer_engine/test.py b/cookbook/examples/streamlit_applications/answer_engine/test.py\nnew file mode 100644\nindex 000000000..b11d0c47f\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/answer_engine/test.py\n@@ -0,0 +1,7 @@\n+from agents import get_sage\n+\n+sage = get_sage()\n+\n+if __name__ == \"__main__\":\n+    sage.show_tool_calls = True\n+    sage.print_response(\"Tell me about the tarrifs the US is imposing\", stream=True)\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/answer_engine/utils.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/answer_engine/utils.py b/cookbook/examples/streamlit_applications/answer_engine/utils.py\nnew file mode 100644\nindex 000000000..d9430d34b\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/answer_engine/utils.py\n@@ -0,0 +1,361 @@\n+from typing import Any, Dict, List, Optional\n+\n+import streamlit as st\n+from agents import get_sage\n+from agno.agent.agent import Agent\n+from agno.utils.log import logger\n+\n+\n+def add_message(\n+    role: str, content: str, tool_calls: Optional[List[Dict[str, Any]]] = None\n+) -> None:\n+    \"\"\"Safely add a message to the session state.\"\"\"\n+    if \"messages\" not in st.session_state or not isinstance(\n+        st.session_state[\"messages\"], list\n+    ):\n+        st.session_state[\"messages\"] = []\n+    st.session_state[\"messages\"].append(\n+        {\"role\": role, \"content\": content, \"tool_calls\": tool_calls}\n+    )\n+\n+\n+def restart_agent():\n+    \"\"\"Reset the agent and clear chat history.\"\"\"\n+    logger.debug(\"---*--- Restarting agent ---*---\")\n+    st.session_state[\"sage\"] = None\n+    st.session_state[\"sage_session_id\"] = None\n+    st.session_state[\"messages\"] = []\n+    st.rerun()\n+\n+\n+def export_chat_history():\n+    \"\"\"Export chat history as markdown.\"\"\"\n+    if \"messages\" in st.session_state:\n+        chat_text = \"# Sage - Chat History\\n\\n\"\n+        for msg in st.session_state[\"messages\"]:\n+            role_label = \"\ud83e\udd16 Assistant\" if msg[\"role\"] == \"assistant\" else \"\ud83d\udc64 User\"\n+            chat_text += f\"### {role_label}\\n{msg['content']}\\n\\n\"\n+        return chat_text\n+    return \"\"\n+\n+\n+def display_tool_calls(tool_calls_container, tools):\n+    \"\"\"Display tool calls in a streamlit container with expandable sections.\n+\n+    Args:\n+        tool_calls_container: Streamlit container to display the tool calls\n+        tools: List of tool call dictionaries containing name, args, content, and metrics\n+    \"\"\"\n+    try:\n+        with tool_calls_container.container():\n+            for tool_call in tools:\n+                tool_name = tool_call.get(\"tool_name\", \"Unknown Tool\")\n+                tool_args = tool_call.get(\"tool_args\", {})\n+                content = tool_call.get(\"content\")\n+                metrics = tool_call.get(\"metrics\", {})\n+\n+                # Add timing information\n+                execution_time_str = \"N/A\"\n+                try:\n+                    if metrics:\n+                        execution_time = (\n+                            metrics[\"time\"]\n+                            if isinstance(metrics, dict)\n+                            else metrics.time\n+                        )\n+                        if execution_time is not None:\n+                            execution_time_str = f\"{execution_time:.2f}s\"\n+                except Exception as e:\n+                    logger.error(f\"Error displaying tool calls: {str(e)}\")\n+                    pass\n+\n+                with st.expander(\n+                    f\"\ud83d\udee0\ufe0f {tool_name.replace('_', ' ').title()} ({execution_time_str})\",\n+                    expanded=False,\n+                ):\n+                    # Show query with syntax highlighting\n+                    if isinstance(tool_args, dict) and \"query\" in tool_args:\n+                        st.code(tool_args[\"query\"], language=\"sql\")\n+\n+                    # Display arguments in a more readable format\n+                    if tool_args and tool_args != {\"query\": None}:\n+                        st.markdown(\"**Arguments:**\")\n+                        st.json(tool_args)\n+\n+                    if content:\n+                        st.markdown(\"**Results:**\")\n+                        try:\n+                            st.json(content)\n+                        except Exception as e:\n+                            st.markdown(content)\n+\n+    except Exception as e:\n+        logger.error(f\"Error displaying tool calls: {str(e)}\")\n+        tool_calls_container.error(\"Failed to display tool results\")\n+\n+\n+def sidebar_widget() -> None:\n+    \"\"\"Display a sidebar with sample user queries for Sage.\"\"\"\n+    with st.sidebar:\n+        st.markdown(\"#### \ud83d\udcdc Try me!\")\n+        if st.button(\"\ud83d\udca1 US Tariffs\"):\n+            add_message(\n+                \"user\",\n+                \"Tell me about the tariffs the US is imposing in 2025\",\n+            )\n+        if st.button(\"\ud83e\udd14 Reasoning Models\"):\n+            add_message(\n+                \"user\",\n+                \"Which is a better reasoning model: o3-mini or DeepSeek R1?\",\n+            )\n+        if st.button(\"\ud83e\udd16 Tell me about Agno\"):\n+            add_message(\n+                \"user\",\n+                \"Tell me about Agno: https://github.com/agno-agi/agno and https://docs.agno.com\",\n+            )\n+        if st.button(\"\u2696\ufe0f Impact of AI Regulations\"):\n+            add_message(\n+                \"user\",\n+                \"Evaluate how emerging AI regulations could influence innovation, privacy, and ethical AI deployment in the near future.\",\n+            )\n+\n+        st.markdown(\"---\")\n+        st.markdown(\"#### \ud83d\udee0\ufe0f Utilities\")\n+        col1, col2 = st.columns(2)\n+        with col1:\n+            if st.button(\"\ud83d\udd04 New Chat\"):\n+                restart_agent()\n+        with col2:\n+            fn = \"sage_chat_history.md\"\n+            if \"sage_session_id\" in st.session_state:\n+                fn = f\"sage_{st.session_state.sage_session_id}.md\"\n+            if st.download_button(\n+                \"\ud83d\udcbe Export Chat\",\n+                export_chat_history(),\n+                file_name=fn,\n+                mime=\"text/markdown\",\n+            ):\n+                st.sidebar.success(\"Chat history exported!\")\n+\n+\n+def session_selector_widget(agent: Agent, model_id: str) -> None:\n+    \"\"\"Display a session selector in the sidebar.\"\"\"\n+    if agent.storage:\n+        agent_sessions = agent.storage.get_all_sessions()\n+\n+        session_options = []\n+        for session in agent_sessions:\n+            session_id = session.session_id\n+            session_name = (\n+                session.session_data.get(\"session_name\", None)\n+                if session.session_data\n+                else None\n+            )\n+            display_name = session_name if session_name else session_id\n+            session_options.append({\"id\": session_id, \"display\": display_name})\n+\n+        selected_session = st.sidebar.selectbox(\n+            \"Session\",\n+            options=[s[\"display\"] for s in session_options],\n+            key=\"session_selector\",\n+        )\n+\n+        selected_session_id = next(\n+            s[\"id\"] for s in session_options if s[\"display\"] == selected_session\n+        )\n+\n+        if st.session_state.get(\"sage_session_id\") != selected_session_id:\n+            logger.info(\n+                f\"---*--- Loading {model_id} run: {selected_session_id} ---*---\"\n+            )\n+\n+            try:\n+                new_agent = get_sage(\n+                    model_id=model_id,\n+                    session_id=selected_session_id,\n+                )\n+\n+                st.session_state[\"sage\"] = new_agent\n+                st.session_state[\"sage_session_id\"] = selected_session_id\n+\n+                st.session_state[\"messages\"] = []\n+\n+                selected_session_obj = next(\n+                    (s for s in agent_sessions if s.session_id == selected_session_id),\n+                    None,\n+                )\n+\n+                if (\n+                    selected_session_obj\n+                    and selected_session_obj.memory\n+                    and \"runs\" in selected_session_obj.memory\n+                ):\n+                    seen_messages = set()\n+\n+                    for run in selected_session_obj.memory[\"runs\"]:\n+                        if \"messages\" in run:\n+                            for msg in run[\"messages\"]:\n+                                msg_role = msg.get(\"role\")\n+                                msg_content = msg.get(\"content\")\n+\n+                                if not msg_content or msg_role == \"system\":\n+                                    continue\n+\n+                                msg_id = f\"{msg_role}:{msg_content}\"\n+\n+                                if msg_id in seen_messages:\n+                                    continue\n+\n+                                seen_messages.add(msg_id)\n+\n+                                if msg_role == \"assistant\":\n+                                    tool_calls = None\n+                                    if \"tool_calls\" in msg:\n+                                        tool_calls = msg[\"tool_calls\"]\n+                                    elif \"metrics\" in msg and msg.get(\"metrics\"):\n+                                        tools = run.get(\"tools\")\n+                                        if tools:\n+                                            tool_calls = tools\n+\n+                                    add_message(msg_role, msg_content, tool_calls)\n+                                else:\n+                                    # For user and other messages\n+                                    add_message(msg_role, msg_content)\n+\n+                        elif (\n+                            \"message\" in run\n+                            and isinstance(run[\"message\"], dict)\n+                            and \"content\" in run[\"message\"]\n+                        ):\n+                            user_msg = run[\"message\"][\"content\"]\n+                            msg_id = f\"user:{user_msg}\"\n+\n+                            if msg_id not in seen_messages:\n+                                seen_messages.add(msg_id)\n+                                add_message(\"user\", user_msg)\n+\n+                            if \"content\" in run and run[\"content\"]:\n+                                asst_msg = run[\"content\"]\n+                                msg_id = f\"assistant:{asst_msg}\"\n+\n+                                if msg_id not in seen_messages:\n+                                    seen_messages.add(msg_id)\n+                                    add_message(\"assistant\", asst_msg, run.get(\"tools\"))\n+\n+                st.rerun()\n+            except Exception as e:\n+                logger.error(f\"Error switching sessions: {str(e)}\")\n+                st.sidebar.error(f\"Error loading session: {str(e)}\")\n+\n+\n+def rename_session_widget(agent: Agent) -> None:\n+    \"\"\"Rename the current session of the agent and save to storage.\"\"\"\n+    container = st.sidebar.container()\n+    session_row = container.columns([3, 1], vertical_alignment=\"center\")\n+\n+    # Initialize session_edit_mode if needed.\n+    if \"session_edit_mode\" not in st.session_state:\n+        st.session_state.session_edit_mode = False\n+\n+    with session_row[0]:\n+        if st.session_state.session_edit_mode:\n+            new_session_name = st.text_input(\n+                \"Session Name\",\n+                value=agent.session_name,\n+                key=\"session_name_input\",\n+                label_visibility=\"collapsed\",\n+            )\n+        else:\n+            st.markdown(f\"Session Name: **{agent.session_name}**\")\n+\n+    with session_row[1]:\n+        if st.session_state.session_edit_mode:\n+            if st.button(\"\u2713\", key=\"save_session_name\", type=\"primary\"):\n+                if new_session_name:\n+                    agent.rename_session(new_session_name)\n+                    st.session_state.session_edit_mode = False\n+                    container.success(\"Renamed!\")\n+        else:\n+            if st.button(\"\u270e\", key=\"edit_session_name\"):\n+                st.session_state.session_edit_mode = True\n+\n+\n+def about_widget() -> None:\n+    \"\"\"Display an about section in the sidebar.\"\"\"\n+    st.sidebar.markdown(\"---\")\n+    st.sidebar.markdown(\"### \u2139\ufe0f About\")\n+    st.sidebar.markdown(\n+        \"\"\"\n+        Sage is a cutting-edge answer engine that delivers real-time insights and in-depth analysis on a wide range of topics.\n+\n+        Built with:\n+        - \ud83d\ude80 Agno\n+        - \ud83d\udcab Streamlit\n+        \"\"\"\n+    )\n+\n+\n+CUSTOM_CSS = \"\"\"\n+    <style>\n+    /* Main Styles */\n+    .main-title {\n+        text-align: center;\n+        background: linear-gradient(45deg, #FF4B2B, #FF416C);\n+        -webkit-background-clip: text;\n+        -webkit-text-fill-color: transparent;\n+        font-size: 3em;\n+        font-weight: bold;\n+        padding: 1em 0;\n+    }\n+    .subtitle {\n+        text-align: center;\n+        color: #666;\n+        margin-bottom: 2em;\n+    }\n+    .stButton button {\n+        width: 100%;\n+        border-radius: 20px;\n+        margin: 0.2em 0;\n+        transition: all 0.3s ease;\n+    }\n+    .stButton button:hover {\n+        transform: translateY(-2px);\n+        box-shadow: 0 5px 15px rgba(0,0,0,0.1);\n+    }\n+    .chat-container {\n+        border-radius: 15px;\n+        padding: 1em;\n+        margin: 1em 0;\n+        background-color: #f5f5f5;\n+    }\n+    .sql-result {\n+        background-color: #f8f9fa;\n+        border-radius: 10px;\n+        padding: 1em;\n+        margin: 1em 0;\n+        border-left: 4px solid #FF4B2B;\n+    }\n+    .status-message {\n+        padding: 1em;\n+        border-radius: 10px;\n+        margin: 1em 0;\n+    }\n+    .success-message {\n+        background-color: #d4edda;\n+        color: #155724;\n+    }\n+    .error-message {\n+        background-color: #f8d7da;\n+        color: #721c24;\n+    }\n+    /* Dark mode adjustments */\n+    @media (prefers-color-scheme: dark) {\n+        .chat-container {\n+            background-color: #2b2b2b;\n+        }\n+        .sql-result {\n+            background-color: #1e1e1e;\n+        }\n+    }\n+    </style>\n+\"\"\"\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/chess_team/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/chess_team/__init__.py b/cookbook/examples/streamlit_applications/chess_team/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/chess_team/agents.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/chess_team/agents.py b/cookbook/examples/streamlit_applications/chess_team/agents.py\nnew file mode 100644\nindex 000000000..fe067cd67\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/chess_team/agents.py\n@@ -0,0 +1,163 @@\n+\"\"\"\n+Chess Team Battle\n+---------------------------------\n+This example shows how to build a Chess game where AI agents play different roles in a team.\n+The game features specialized agents for strategy for white pieces, strategy for black pieces,\n+and a master agent overseeing the game. Move validation is handled by python-chess.\n+\n+Usage Examples:\n+---------------\n+1. Quick game with default settings:\n+   team = get_chess_team()\n+\n+2. Game with debug mode off:\n+   team = get_chess_team(debug_mode=False)\n+\n+The game integrates:\n+  - Multiple AI models (Claude, GPT-4, etc.)\n+  - Specialized agent roles (strategist, master)\n+  - Turn-based gameplay coordination\n+  - Move validation using python-chess\n+\"\"\"\n+\n+import sys\n+from pathlib import Path\n+\n+from agno.agent import Agent\n+from agno.models.anthropic import Claude\n+from agno.models.google import Gemini\n+from agno.models.groq import Groq\n+from agno.models.openai import OpenAIChat\n+from agno.team.team import Team\n+from agno.utils.log import logger\n+\n+project_root = str(Path(__file__).parent.parent.parent.parent)\n+if project_root not in sys.path:\n+    sys.path.append(project_root)\n+\n+\n+def get_model_for_provider(provider: str, model_name: str):\n+    \"\"\"\n+    Creates and returns the appropriate model instance based on the provider.\n+\n+    Args:\n+        provider: The model provider (e.g., 'openai', 'google', 'anthropic', 'groq')\n+        model_name: The specific model name/ID\n+\n+    Returns:\n+        An instance of the appropriate model class\n+\n+    Raises:\n+        ValueError: If the provider is not supported\n+    \"\"\"\n+    if provider == \"openai\":\n+        return OpenAIChat(id=model_name)\n+    elif provider == \"google\":\n+        return Gemini(id=model_name)\n+    elif provider == \"anthropic\":\n+        if model_name == \"claude-3-5-sonnet\":\n+            return Claude(id=\"claude-3-5-sonnet-20241022\", max_tokens=8192)\n+        elif model_name == \"claude-3-7-sonnet\":\n+            return Claude(\n+                id=\"claude-3-7-sonnet-20250219\",\n+                max_tokens=8192,\n+            )\n+        elif model_name == \"claude-3-7-sonnet-thinking\":\n+            return Claude(\n+                id=\"claude-3-7-sonnet-20250219\",\n+                max_tokens=8192,\n+                thinking={\"type\": \"enabled\", \"budget_tokens\": 4096},\n+            )\n+        else:\n+            return Claude(id=model_name)\n+    elif provider == \"groq\":\n+        return Groq(id=model_name)\n+    else:\n+        raise ValueError(f\"Unsupported model provider: {provider}\")\n+\n+\n+def get_chess_team(\n+    white_model: str = \"openai:gpt-4o\",\n+    black_model: str = \"anthropic:claude-3-7-sonnet\",\n+    master_model: str = \"openai:gpt-4o\",\n+    debug_mode: bool = True,\n+) -> Team:\n+    \"\"\"\n+    Returns a chess team with specialized agents for white pieces, black pieces, and game master.\n+\n+    Args:\n+        white_model: Model for white piece strategy\n+        black_model: Model for black piece strategy\n+        master_model: Model for game state evaluation\n+        debug_mode: Enable logging and debug features\n+\n+    Returns:\n+        Team instance configured for chess gameplay\n+    \"\"\"\n+    try:\n+        white_provider, white_name = white_model.split(\":\")\n+        black_provider, black_name = black_model.split(\":\")\n+        master_provider, master_name = master_model.split(\":\")\n+\n+        white_piece_model = get_model_for_provider(white_provider, white_name)\n+        black_piece_model = get_model_for_provider(black_provider, black_name)\n+        master_model = get_model_for_provider(master_provider, master_name)\n+\n+        white_piece_agent = Agent(\n+            name=\"white_piece_agent\",\n+            role=\"White Piece Strategist\",\n+            description=\"\"\"You are a chess strategist for white pieces. Given a list of legal moves,\n+                    analyze them and choose the best one based on standard chess strategy.\n+                    Consider piece development, center control, and king safety.\n+                    Respond ONLY with your chosen move in UCI notation (e.g., 'e2e4').\"\"\",\n+            model=white_piece_model,\n+            debug_mode=debug_mode,\n+        )\n+\n+        black_piece_agent = Agent(\n+            name=\"black_piece_agent\",\n+            role=\"Black Piece Strategist\",\n+            description=\"\"\"You are a chess strategist for black pieces. Given a list of legal moves,\n+                    analyze them and choose the best one based on standard chess strategy.\n+                    Consider piece development, center control, and king safety.\n+                    Respond ONLY with your chosen move in UCI notation (e.g., 'e7e5').\"\"\",\n+            model=black_piece_model,\n+            debug_mode=debug_mode,\n+        )\n+\n+        return Team(\n+            name=\"Chess Team\",\n+            mode=\"route\",\n+            model=master_model,\n+            success_criteria=\"The game is completed with a win, loss, or draw\",\n+            members=[white_piece_agent, black_piece_agent],\n+            instructions=[\n+                \"You are the chess game coordinator and master analyst.\",\n+                \"Your role is to coordinate between two player agents and provide game analysis:\",\n+                \"1. white_piece_agent - Makes moves for white pieces\",\n+                \"2. black_piece_agent - Makes moves for black pieces\",\n+                \"\",\n+                \"When receiving a task:\",\n+                \"1. Check the 'current_player' in the context\",\n+                \"2. If current_player is white_piece_agent or black_piece_agent:\",\n+                \"   - Forward the move request to that agent\",\n+                \"   - Return their move response directly without modification\",\n+                \"3. If no current_player is specified:\",\n+                \"   - This indicates a request for position analysis\",\n+                \"   - Analyze the position yourself and respond with a JSON object:\",\n+                \"   {\",\n+                \"       'game_over': true/false,\",\n+                \"       'result': 'white_win'/'black_win'/'draw'/null,\",\n+                \"   }\",\n+                \"\",\n+                \"Do not modify player agent responses.\",\n+                \"For analysis requests, provide detailed evaluation of the position.\",\n+            ],\n+            debug_mode=debug_mode,\n+            markdown=True,\n+            show_members_responses=True,\n+        )\n+\n+    except Exception as e:\n+        logger.error(f\"Error initializing chess team: {str(e)}\")\n+        raise\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/chess_team/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/chess_team/app.py b/cookbook/examples/streamlit_applications/chess_team/app.py\nnew file mode 100644\nindex 000000000..77caa17bc\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/chess_team/app.py\n@@ -0,0 +1,454 @@\n+import logging\n+from typing import Dict, List\n+\n+import chess\n+import nest_asyncio\n+import streamlit as st\n+from agents import get_chess_team\n+from agno.utils.log import logger\n+from utils import (\n+    CUSTOM_CSS,\n+    WHITE,\n+    ChessBoard,\n+    display_board,\n+    display_move_history,\n+    parse_move,\n+    show_agent_status,\n+)\n+\n+# Configure logging\n+logging.basicConfig(\n+    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n+)\n+\n+nest_asyncio.apply()\n+\n+# Page configuration\n+st.set_page_config(\n+    page_title=\"Chess Team Battle\",\n+    page_icon=\"\u265f\ufe0f\",\n+    layout=\"wide\",\n+    initial_sidebar_state=\"expanded\",\n+)\n+\n+# Load custom CSS with dark mode support\n+st.markdown(CUSTOM_CSS, unsafe_allow_html=True)\n+\n+\n+def get_legal_moves_with_descriptions(board: ChessBoard) -> List[Dict]:\n+    \"\"\"\n+    Get all legal moves with descriptions for the current player.\n+\n+    Args:\n+        board: ChessBoard instance\n+\n+    Returns:\n+        List of dictionaries with move information\n+    \"\"\"\n+    legal_moves = []\n+\n+    # Get python-chess board\n+    chess_board = board.board\n+\n+    # Get all legal moves\n+    for move in chess_board.legal_moves:\n+        # Get source and destination squares\n+        from_square = chess.square_name(move.from_square)\n+        to_square = chess.square_name(move.to_square)\n+\n+        # Get piece type\n+        piece = chess_board.piece_at(move.from_square)\n+        piece_type = piece.symbol().upper() if piece else \"?\"\n+\n+        # Check if it's a capture\n+        is_capture = chess_board.is_capture(move)\n+\n+        # Check if it's a promotion\n+        promotion = None\n+        if move.promotion:\n+            promotion = chess.piece_name(move.promotion)\n+\n+        # Check if it's a castling move\n+        is_kingside_castle = chess_board.is_kingside_castling(move)\n+        is_queenside_castle = chess_board.is_queenside_castling(move)\n+\n+        # Create move description\n+        if is_kingside_castle:\n+            description = \"Kingside castle (O-O)\"\n+        elif is_queenside_castle:\n+            description = \"Queenside castle (O-O-O)\"\n+        elif promotion:\n+            description = f\"Pawn {from_square} to {to_square}, promote to {promotion}\"\n+        elif is_capture:\n+            captured_piece = chess_board.piece_at(move.to_square)\n+            captured_type = captured_piece.symbol().upper() if captured_piece else \"?\"\n+            description = f\"{piece_type} from {from_square} captures {captured_type} at {to_square}\"\n+        else:\n+            description = f\"{piece_type} from {from_square} to {to_square}\"\n+\n+        # Add move to list\n+        legal_moves.append(\n+            {\n+                \"uci\": move.uci(),\n+                \"san\": chess_board.san(move),\n+                \"description\": description,\n+                \"is_capture\": is_capture,\n+                \"is_castle\": is_kingside_castle or is_queenside_castle,\n+                \"promotion\": promotion,\n+            }\n+        )\n+\n+    return legal_moves\n+\n+\n+def main():\n+    ####################################################################\n+    # App header\n+    ####################################################################\n+    st.markdown(\n+        \"<h1 class='main-title'>\u265f\ufe0f Chess Team Battle</h1>\",\n+        unsafe_allow_html=True,\n+    )\n+    ####################################################################\n+    # Initialize session state\n+    ####################################################################\n+    if \"game_started\" not in st.session_state:\n+        st.session_state.game_started = False\n+    if \"game_paused\" not in st.session_state:\n+        st.session_state.game_paused = False\n+    if \"move_history\" not in st.session_state:\n+        st.session_state.move_history = []\n+\n+    ####################################################################\n+    # Sidebar controls\n+    ####################################################################\n+    with st.sidebar:\n+        st.markdown(\"### Game Settings\")\n+\n+        # Model selection\n+        model_options = {\n+            \"gpt-4o\": \"openai:gpt-4o\",\n+            \"o3-mini\": \"openai:o3-mini\",\n+            \"claude-3.5\": \"anthropic:claude-3-5-sonnet\",\n+            \"claude-3.7\": \"anthropic:claude-3-7-sonnet\",\n+            \"claude-3.7-thinking\": \"anthropic:claude-3-7-sonnet-thinking\",\n+            \"gemini-flash\": \"google:gemini-2.0-flash\",\n+            \"gemini-pro\": \"google:gemini-2.0-pro-exp-02-05\",\n+        }\n+        ################################################################\n+        # Model selection\n+        ################################################################\n+        st.markdown(\"#### White Player\")\n+        selected_white = st.selectbox(\n+            \"Select White Player\",\n+            list(model_options.keys()),\n+            index=list(model_options.keys()).index(\"gpt-4o\"),\n+            key=\"model_white\",\n+        )\n+\n+        st.markdown(\"#### Black Player\")\n+        selected_black = st.selectbox(\n+            \"Select Black Player\",\n+            list(model_options.keys()),\n+            index=list(model_options.keys()).index(\"claude-3.7\"),\n+            key=\"model_black\",\n+        )\n+\n+        st.markdown(\"#### Game Master\")\n+        selected_master = st.selectbox(\n+            \"Select Game Master\",\n+            list(model_options.keys()),\n+            index=list(model_options.keys()).index(\"gpt-4o\"),\n+            key=\"model_master\",\n+        )\n+\n+        ################################################################\n+        # Game controls\n+        ################################################################\n+        col1, col2 = st.columns(2)\n+        with col1:\n+            if not st.session_state.game_started:\n+                if st.button(\"\u25b6\ufe0f Start Game\"):\n+                    st.session_state.team = get_chess_team(\n+                        white_model=model_options[selected_white],\n+                        black_model=model_options[selected_black],\n+                        master_model=model_options[selected_master],\n+                        debug_mode=True,\n+                    )\n+                    st.session_state.game_board = ChessBoard()\n+                    st.session_state.game_started = True\n+                    st.session_state.game_paused = False\n+                    st.session_state.move_history = []\n+                    st.rerun()\n+            else:\n+                game_over, _ = st.session_state.game_board.get_game_state()\n+                if not game_over:\n+                    if st.button(\n+                        \"\u23f8\ufe0f Pause\" if not st.session_state.game_paused else \"\u25b6\ufe0f Resume\"\n+                    ):\n+                        st.session_state.game_paused = not st.session_state.game_paused\n+                        st.rerun()\n+        with col2:\n+            if st.session_state.game_started:\n+                if st.button(\"\ud83d\udd04 New Game\"):\n+                    st.session_state.team = get_chess_team(\n+                        white_model=model_options[selected_white],\n+                        black_model=model_options[selected_black],\n+                        master_model=model_options[selected_master],\n+                        debug_mode=True,\n+                    )\n+                    st.session_state.game_board = ChessBoard()\n+                    st.session_state.game_paused = False\n+                    st.session_state.move_history = []\n+                    st.rerun()\n+\n+    ####################################################################\n+    # Header showing current models\n+    ####################################################################\n+    if st.session_state.game_started:\n+        st.markdown(\n+            f\"<h3 style='color:#87CEEB; text-align:center;'>{selected_white} vs {selected_black}</h3>\",\n+            unsafe_allow_html=True,\n+        )\n+\n+    ####################################################################\n+    # Main game area\n+    ####################################################################\n+    if st.session_state.game_started:\n+        game_over, state_info = st.session_state.game_board.get_game_state()\n+\n+        display_board(st.session_state.game_board)\n+\n+        # Show game status (winner/draw/current player)\n+        if game_over:\n+            result = state_info.get(\"result\", \"\")\n+            reason = state_info.get(\"reason\", \"\")\n+\n+            if \"white_win\" in result:\n+                st.success(f\"\ud83c\udfc6 Game Over! White ({selected_white}) wins by {reason}!\")\n+            elif \"black_win\" in result:\n+                st.success(f\"\ud83c\udfc6 Game Over! Black ({selected_black}) wins by {reason}!\")\n+            else:\n+                st.info(f\"\ud83e\udd1d Game Over! It's a draw by {reason}!\")\n+        else:\n+            # Show current player status\n+            current_color = st.session_state.game_board.current_color\n+            current_model_name = (\n+                selected_white if current_color == WHITE else selected_black\n+            )\n+\n+            show_agent_status(\n+                f\"{current_color.capitalize()} Player ({current_model_name})\",\n+                \"It's your turn\",\n+                is_white=(current_color == WHITE),\n+            )\n+\n+        display_move_history(st.session_state.move_history)\n+\n+        if not st.session_state.game_paused and not game_over:\n+            # Thinking indicator\n+            st.markdown(\n+                f\"\"\"<div class=\"thinking-container\">\n+                    <div class=\"agent-thinking\">\n+                        <div style=\"margin-right: 10px; display: inline-block;\">\ud83d\udd04</div>\n+                        {current_color.capitalize()} Player ({current_model_name}) is thinking...\n+                    </div>\n+                </div>\"\"\",\n+                unsafe_allow_html=True,\n+            )\n+\n+            # Get legal moves using python-chess directly\n+            legal_moves = get_legal_moves_with_descriptions(st.session_state.game_board)\n+\n+            # Format legal moves for the agent\n+            legal_moves_descriptions = \"\\n\".join(\n+                [\n+                    f\"- {move['san']} ({move['uci']}): {move['description']}\"\n+                    for move in legal_moves\n+                ]\n+            )\n+\n+            # Get board state\n+            board_state = st.session_state.game_board.get_board_state()\n+            fen = st.session_state.game_board.get_fen()\n+\n+            # Get move from current player agent through team\n+            current_agent_name = (\n+                \"white_piece_agent\" if current_color == WHITE else \"black_piece_agent\"\n+            )\n+\n+            # Create the task message for the team\n+            task_message = f\"\"\"\\\n+Current board state (FEN): {fen}\n+Board visualization:\n+{board_state}\n+\n+Legal moves available:\n+{legal_moves_descriptions}\n+\n+Choose your next move from the legal moves above.\n+Respond with ONLY your chosen move in UCI notation (e.g., 'e2e4').\n+Do not include any other text in your response.\"\"\"\n+\n+            response = st.session_state.team.run(\n+                task_message,\n+                stream=False,\n+                context={\n+                    \"current_player\": current_agent_name,\n+                    \"board_state\": board_state,\n+                    \"legal_moves\": legal_moves,\n+                },\n+            )\n+\n+            try:\n+                # Parse the move from the response\n+                move_str = parse_move(response.content if response else \"\")\n+\n+                # Verify the move is in the list of legal moves\n+                legal_move_ucis = [move[\"uci\"] for move in legal_moves]\n+\n+                if move_str not in legal_move_ucis:\n+                    # Try to find a matching move\n+                    for move in legal_moves:\n+                        if move[\"san\"].lower() == move_str.lower():\n+                            move_str = move[\"uci\"]\n+                            break\n+\n+                # Make the move\n+                success, message = st.session_state.game_board.make_move(move_str)\n+\n+                if success:\n+                    # Find the move description\n+                    move_description = next(\n+                        (\n+                            move[\"description\"]\n+                            for move in legal_moves\n+                            if move[\"uci\"] == move_str\n+                        ),\n+                        \"\",\n+                    )\n+\n+                    move_number = len(st.session_state.move_history) + 1\n+                    st.session_state.move_history.append(\n+                        {\n+                            \"number\": move_number,\n+                            \"player\": f\"{current_color.capitalize()} ({current_model_name})\",\n+                            \"move\": move_str,\n+                            \"description\": move_description,\n+                        }\n+                    )\n+\n+                    logger.info(\n+                        f\"Move {move_number}: {current_color.capitalize()} ({current_model_name}) played {move_str} - {move_description}\"\n+                    )\n+                    logger.info(\n+                        f\"Board state:\\n{st.session_state.game_board.get_board_state()}\"\n+                    )\n+\n+                    # Check game state after move\n+                    game_over, state_info = st.session_state.game_board.get_game_state()\n+\n+                    # If game is not over, get analysis from coordinator after black's move\n+                    if not game_over and move_number % 2 == 0:  # After black's move\n+                        analysis_message = f\"\"\"\\\n+Current board state (FEN): {fen}\n+Board visualization:\n+{board_state}\n+\n+Last move: {move_str} ({move_description})\n+\n+Analyze the current position and provide your evaluation.\n+Respond with a JSON object containing:\n+{{\n+    \"game_over\": false,\n+    \"result\": null,\n+    \"reason\": null,\n+    \"commentary\": \"Your analysis of the position\",\n+    \"advantage\": \"white\"/\"black\"/\"equal\"\n+}}\"\"\"\n+\n+                        st.session_state.team.run(\n+                            message=analysis_message,\n+                            stream=False,\n+                            context={\n+                                \"board_state\": board_state,\n+                                \"last_move\": move_str,\n+                                \"last_move_description\": move_description,\n+                            },\n+                        )\n+\n+                    if game_over:\n+                        result = state_info.get(\"result\", \"\")\n+                        reason = state_info.get(\"reason\", \"\")\n+\n+                        if \"white_win\" in result:\n+                            logger.info(f\"Game Over - White wins by {reason}\")\n+                            st.success(\n+                                f\"\ud83c\udfc6 Game Over! White ({selected_white}) wins by {reason}!\"\n+                            )\n+                        elif \"black_win\" in result:\n+                            logger.info(f\"Game Over - Black wins by {reason}\")\n+                            st.success(\n+                                f\"\ud83c\udfc6 Game Over! Black ({selected_black}) wins by {reason}!\"\n+                            )\n+                        else:\n+                            logger.info(f\"Game Over - Draw by {reason}\")\n+                            st.info(f\"\ud83e\udd1d Game Over! It's a draw by {reason}!\")\n+\n+                        st.session_state.game_paused = True\n+\n+                    st.rerun()\n+                else:\n+                    logger.error(f\"Invalid move attempt: {message}\")\n+                    st.session_state.team.run(\n+                        message=f\"\"\"\\\n+Invalid move: {message}\n+\n+Current board state (FEN): {fen}\n+Board visualization:\n+{board_state}\n+\n+Legal moves available:\n+{legal_moves_descriptions}\n+\n+Please choose a valid move from the list above.\n+Respond with ONLY your chosen move in UCI notation (e.g., 'e2e4').\n+Do not include any other text in your response.\"\"\",\n+                        stream=False,\n+                        context={\n+                            \"current_player\": current_agent_name,\n+                            \"board_state\": board_state,\n+                            \"legal_moves\": legal_moves,\n+                            \"last_error\": message,\n+                        },\n+                    )\n+                    st.rerun()\n+\n+            except Exception as e:\n+                logger.error(f\"Error processing move: {str(e)}\")\n+                st.rerun()\n+    else:\n+        st.info(\"\ud83d\udc48 Press 'Start Game' to begin!\")\n+\n+    ####################################################################\n+    # About section\n+    ####################################################################\n+    st.sidebar.markdown(f\"\"\"\n+    ### \u265f\ufe0f Chess Team Battle\n+    Watch AI agents play chess with specialized roles!\n+\n+    **Current Teams:**\n+    * \u2654 White: `{selected_white}`\n+    * \u265a Black: `{selected_black}`\n+    * \ud83e\udde0 Game Master: `{selected_master}`\n+\n+    **How it Works:**\n+    1. Python-chess validates all legal moves\n+    2. The White/Black Player agents choose the best move\n+    3. The Game Master analyzes the position\n+    4. The process repeats until the game ends\n+    \"\"\")\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/chess_team/generate_requirements.sh",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/chess_team/generate_requirements.sh b/cookbook/examples/streamlit_applications/chess_team/generate_requirements.sh\nnew file mode 100755\nindex 000000000..78f8c7cec\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/chess_team/generate_requirements.sh\n@@ -0,0 +1,12 @@\n+#!/bin/bash\n+\n+############################################################################\n+# Generate requirements.txt from requirements.in\n+############################################################################\n+\n+echo \"Generating requirements.txt\"\n+\n+CURR_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n+\n+UV_CUSTOM_COMPILE_COMMAND=\"./generate_requirements.sh\" \\\n+  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/chess_team/requirements.in",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/chess_team/requirements.in b/cookbook/examples/streamlit_applications/chess_team/requirements.in\nnew file mode 100644\nindex 000000000..9d7918f95\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/chess_team/requirements.in\n@@ -0,0 +1,14 @@\n+agno\n+anthropic\n+groq\n+google-genai\n+nest-asyncio\n+openai\n+pathlib\n+Pillow\n+rich\n+streamlit\n+pydantic\n+typing-extensions\n+python-dotenv\n+python-chess\n\\ No newline at end of file\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/chess_team/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/chess_team/requirements.txt b/cookbook/examples/streamlit_applications/chess_team/requirements.txt\nnew file mode 100644\nindex 000000000..3f12a0273\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/chess_team/requirements.txt\n@@ -0,0 +1,224 @@\n+# This file was autogenerated by uv via the following command:\n+#    ./generate_requirements.sh\n+agno==1.1.17\n+    # via -r cookbook/examples/apps/chess_team/requirements.in\n+altair==5.5.0\n+    # via streamlit\n+annotated-types==0.7.0\n+    # via pydantic\n+anthropic==0.49.0\n+    # via -r cookbook/examples/apps/chess_team/requirements.in\n+anyio==4.9.0\n+    # via\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   httpx\n+    #   openai\n+attrs==25.3.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+blinker==1.9.0\n+    # via streamlit\n+cachetools==5.5.2\n+    # via\n+    #   google-auth\n+    #   streamlit\n+certifi==2025.1.31\n+    # via\n+    #   httpcore\n+    #   httpx\n+    #   requests\n+charset-normalizer==3.4.1\n+    # via requests\n+chess==1.11.2\n+    # via python-chess\n+click==8.1.8\n+    # via\n+    #   streamlit\n+    #   typer\n+distro==1.9.0\n+    # via\n+    #   anthropic\n+    #   groq\n+    #   openai\n+docstring-parser==0.16\n+    # via agno\n+gitdb==4.0.12\n+    # via gitpython\n+gitpython==3.1.44\n+    # via\n+    #   agno\n+    #   streamlit\n+google-auth==2.38.0\n+    # via google-genai\n+google-genai==1.7.0\n+    # via -r cookbook/examples/apps/chess_team/requirements.in\n+groq==0.20.0\n+    # via -r cookbook/examples/apps/chess_team/requirements.in\n+h11==0.14.0\n+    # via httpcore\n+httpcore==1.0.7\n+    # via httpx\n+httpx==0.28.1\n+    # via\n+    #   agno\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   openai\n+idna==3.10\n+    # via\n+    #   anyio\n+    #   httpx\n+    #   requests\n+jinja2==3.1.6\n+    # via\n+    #   altair\n+    #   pydeck\n+jiter==0.9.0\n+    # via\n+    #   anthropic\n+    #   openai\n+jsonschema==4.23.0\n+    # via altair\n+jsonschema-specifications==2024.10.1\n+    # via jsonschema\n+markdown-it-py==3.0.0\n+    # via rich\n+markupsafe==3.0.2\n+    # via jinja2\n+mdurl==0.1.2\n+    # via markdown-it-py\n+narwhals==1.32.0\n+    # via altair\n+nest-asyncio==1.6.0\n+    # via -r cookbook/examples/apps/chess_team/requirements.in\n+numpy==2.2.4\n+    # via\n+    #   pandas\n+    #   pydeck\n+    #   streamlit\n+openai==1.68.2\n+    # via -r cookbook/examples/apps/chess_team/requirements.in\n+packaging==24.2\n+    # via\n+    #   altair\n+    #   streamlit\n+pandas==2.2.3\n+    # via streamlit\n+pathlib==1.0.1\n+    # via -r cookbook/examples/apps/chess_team/requirements.in\n+pillow==11.1.0\n+    # via\n+    #   -r cookbook/examples/apps/chess_team/requirements.in\n+    #   streamlit\n+protobuf==5.29.4\n+    # via streamlit\n+pyarrow==19.0.1\n+    # via streamlit\n+pyasn1==0.6.1\n+    # via\n+    #   pyasn1-modules\n+    #   rsa\n+pyasn1-modules==0.4.1\n+    # via google-auth\n+pydantic==2.10.6\n+    # via\n+    #   -r cookbook/examples/apps/chess_team/requirements.in\n+    #   agno\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   openai\n+    #   pydantic-settings\n+pydantic-core==2.27.2\n+    # via pydantic\n+pydantic-settings==2.8.1\n+    # via agno\n+pydeck==0.9.1\n+    # via streamlit\n+pygments==2.19.1\n+    # via rich\n+python-chess==1.999\n+    # via -r cookbook/examples/apps/chess_team/requirements.in\n+python-dateutil==2.9.0.post0\n+    # via pandas\n+python-dotenv==1.0.1\n+    # via\n+    #   -r cookbook/examples/apps/chess_team/requirements.in\n+    #   agno\n+    #   pydantic-settings\n+python-multipart==0.0.20\n+    # via agno\n+pytz==2025.1\n+    # via pandas\n+pyyaml==6.0.2\n+    # via agno\n+referencing==0.36.2\n+    # via\n+    #   jsonschema\n+    #   jsonschema-specifications\n+requests==2.32.3\n+    # via\n+    #   google-genai\n+    #   streamlit\n+rich==13.9.4\n+    # via\n+    #   -r cookbook/examples/apps/chess_team/requirements.in\n+    #   agno\n+    #   typer\n+rpds-py==0.23.1\n+    # via\n+    #   jsonschema\n+    #   referencing\n+rsa==4.9\n+    # via google-auth\n+shellingham==1.5.4\n+    # via typer\n+six==1.17.0\n+    # via python-dateutil\n+smmap==5.0.2\n+    # via gitdb\n+sniffio==1.3.1\n+    # via\n+    #   anthropic\n+    #   anyio\n+    #   groq\n+    #   openai\n+streamlit==1.43.2\n+    # via -r cookbook/examples/apps/chess_team/requirements.in\n+tenacity==9.0.0\n+    # via streamlit\n+toml==0.10.2\n+    # via streamlit\n+tomli==2.2.1\n+    # via agno\n+tornado==6.4.2\n+    # via streamlit\n+tqdm==4.67.1\n+    # via openai\n+typer==0.15.2\n+    # via agno\n+typing-extensions==4.12.2\n+    # via\n+    #   -r cookbook/examples/apps/chess_team/requirements.in\n+    #   agno\n+    #   altair\n+    #   anthropic\n+    #   anyio\n+    #   google-genai\n+    #   groq\n+    #   openai\n+    #   pydantic\n+    #   pydantic-core\n+    #   referencing\n+    #   streamlit\n+    #   typer\n+tzdata==2025.2\n+    # via pandas\n+urllib3==2.3.0\n+    # via requests\n+websockets==15.0.1\n+    # via google-genai\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/chess_team/utils.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/chess_team/utils.py b/cookbook/examples/streamlit_applications/chess_team/utils.py\nnew file mode 100644\nindex 000000000..4d53019a7\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/chess_team/utils.py\n@@ -0,0 +1,797 @@\n+from dataclasses import dataclass\n+from typing import Dict, List, Tuple\n+\n+import chess\n+import streamlit as st\n+from agno.agent import Agent\n+\n+WHITE = \"white\"\n+BLACK = \"black\"\n+\n+# Custom CSS for the chess board\n+CUSTOM_CSS = \"\"\"\n+<style>\n+    /* Dark mode styling */\n+    .main-title {\n+        color: #87CEEB;\n+        text-align: center;\n+        margin-bottom: 20px;\n+    }\n+    \n+    .chess-board {\n+        width: 100%;\n+        max-width: 500px;\n+        margin: 0 auto;\n+        border: 2px solid #555;\n+        border-radius: 5px;\n+        overflow: hidden;\n+    }\n+    \n+    .chess-square {\n+        width: 12.5%;\n+        aspect-ratio: 1;\n+        display: inline-block;\n+        text-align: center;\n+        font-size: 24px;\n+        line-height: 60px;\n+        vertical-align: middle;\n+    }\n+    \n+    .white-square {\n+        background-color: #f0d9b5;\n+        color: #000;\n+    }\n+    \n+    .black-square {\n+        background-color: #b58863;\n+        color: #000;\n+    }\n+    \n+    .piece {\n+        font-size: 32px;\n+        line-height: 60px;\n+    }\n+    \n+    .move-history {\n+        margin-top: 20px;\n+        border: 1px solid #555;\n+        border-radius: 5px;\n+        padding: 10px;\n+        max-height: 300px;\n+        overflow-y: auto;\n+    }\n+    \n+    .move-entry {\n+        padding: 5px;\n+        border-bottom: 1px solid #444;\n+    }\n+    \n+    .move-entry:last-child {\n+        border-bottom: none;\n+    }\n+    \n+    .agent-thinking {\n+        display: flex;\n+        align-items: center;\n+        background-color: rgba(100, 100, 100, 0.2);\n+        padding: 10px;\n+        border-radius: 5px;\n+        margin: 10px 0;\n+        animation: pulse 1.5s infinite;\n+    }\n+    \n+    @keyframes pulse {\n+        0% { opacity: 0.6; }\n+        50% { opacity: 1; }\n+        100% { opacity: 0.6; }\n+    }\n+    \n+    .agent-status {\n+        display: flex;\n+        align-items: center;\n+        background-color: rgba(100, 100, 100, 0.2);\n+        padding: 10px;\n+        border-radius: 5px;\n+        margin: 10px 0;\n+    }\n+    \n+    .agent-status.white {\n+        border-left: 4px solid #f0d9b5;\n+    }\n+    \n+    .agent-status.black {\n+        border-left: 4px solid #b58863;\n+    }\n+    \n+    .thinking-container {\n+        margin: 10px 0;\n+    }\n+    \n+    /* Additional CSS for the simple board representation */\n+    .chess-board-wrapper {\n+        font-family: 'Courier New', monospace;\n+        background: #2b2b2b;\n+        padding: 20px;\n+        border-radius: 10px;\n+        display: inline-block;\n+        margin: 20px auto;\n+        text-align: center;\n+    }\n+    \n+    .board-container {\n+        display: flex;\n+        justify-content: center;\n+        width: 100%;\n+    }\n+    \n+    .chess-files {\n+        color: #888;\n+        text-align: center;\n+        padding: 5px 0;\n+        margin-left: 30px;\n+        display: flex;\n+        justify-content: space-around;\n+        width: calc(100% - 30px);\n+        margin-bottom: 5px;\n+    }\n+    \n+    .chess-file-label {\n+        width: 40px;\n+        text-align: center;\n+    }\n+    \n+    .chess-grid {\n+        border: 1px solid #666;\n+        display: inline-block;\n+    }\n+    \n+    .chess-row {\n+        display: flex;\n+        align-items: center;\n+    }\n+    \n+    .chess-rank {\n+        color: #888;\n+        width: 25px;\n+        text-align: center;\n+        padding-right: 5px;\n+    }\n+    \n+    .chess-cell {\n+        width: 40px;\n+        height: 40px;\n+        display: flex;\n+        align-items: center;\n+        justify-content: center;\n+        border: 1px solid #666;\n+        font-size: 24px;\n+    }\n+    \n+    .piece-white {\n+        color: #fff;\n+    }\n+    \n+    .piece-black {\n+        color: #aaa;\n+    }\n+    \n+    .piece-empty {\n+        color: transparent;\n+    }\n+    \n+    .chess-row:nth-child(odd) .chess-cell:nth-child(even),\n+    .chess-row:nth-child(even) .chess-cell:nth-child(odd) {\n+        background-color: #3c3c3c;\n+    }\n+    \n+    .chess-row:nth-child(even) .chess-cell:nth-child(even),\n+    .chess-row:nth-child(odd) .chess-cell:nth-child(odd) {\n+        background-color: #262626;\n+    }\n+    \n+    /* Additional CSS for move history grid */\n+    .move-history-grid {\n+        display: grid;\n+        grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));\n+        gap: 10px;\n+        padding: 10px;\n+        max-height: 600px;  /* Height of approximately 2 rows of boards */\n+        overflow-y: auto;\n+        scrollbar-width: thin;\n+        scrollbar-color: #666 #333;\n+    }\n+    \n+    /* Webkit scrollbar styling */\n+    .move-history-grid::-webkit-scrollbar {\n+        width: 8px;\n+    }\n+\n+    .move-history-grid::-webkit-scrollbar-track {\n+        background: #333;\n+        border-radius: 4px;\n+    }\n+\n+    .move-history-grid::-webkit-scrollbar-thumb {\n+        background: #666;\n+        border-radius: 4px;\n+    }\n+\n+    .move-history-grid::-webkit-scrollbar-thumb:hover {\n+        background: #888;\n+    }\n+    \n+    .move-history-item {\n+        background: rgba(40, 40, 40, 0.8);\n+        border-radius: 5px;\n+        padding: 10px;\n+        text-align: center;\n+        border: 1px solid #444;\n+    }\n+    \n+    .move-history-item .move-text {\n+        font-family: monospace;\n+        font-size: 1.1em;\n+        margin: 5px 0;\n+    }\n+\n+    .move-history-item .move-text.white-move {\n+        color: #4CAF50;\n+    }\n+\n+    .move-history-item .move-text.black-move {\n+        color: #ff4444;\n+    }\n+    \n+    .move-history-item .description {\n+        color: #888;\n+        font-size: 0.9em;\n+        margin-top: 5px;\n+    }\n+    \n+    /* Mini chess board for history */\n+    .mini-chess-board {\n+        display: grid;\n+        grid-template-columns: repeat(8, 1fr);\n+        width: 160px;\n+        margin: 10px auto;\n+        border: 1px solid #555;\n+        position: relative;\n+    }\n+    \n+    .mini-square {\n+        aspect-ratio: 1;\n+        display: flex;\n+        align-items: center;\n+        justify-content: center;\n+        font-size: 14px;\n+        position: relative;\n+    }\n+    \n+    .mini-white-square {\n+        background-color: #f0d9b5;\n+    }\n+    \n+    .mini-black-square {\n+        background-color: #b58863;\n+    }\n+    \n+    .mini-piece {\n+        font-size: 14px;\n+        line-height: 20px;\n+        z-index: 2;\n+    }\n+\n+    .mini-piece.white-piece {\n+        color: #ffffff;\n+        text-shadow: 0 0 2px #000;\n+    }\n+\n+    .mini-piece.black-piece {\n+        color: #000000;\n+        text-shadow: 0 0 2px #fff;\n+    }\n+\n+    .mini-square.move-from.white-move {\n+        background-color: rgba(76, 175, 80, 0.5) !important;\n+    }\n+\n+    .mini-square.move-to.white-move {\n+        background-color: rgba(76, 175, 80, 0.7) !important;\n+    }\n+\n+    .mini-square.move-from.black-move {\n+        background-color: rgba(255, 68, 68, 0.5) !important;\n+    }\n+\n+    .mini-square.move-to.black-move {\n+        background-color: rgba(255, 68, 68, 0.7) !important;\n+    }\n+</style>\n+\"\"\"\n+\n+\n+@dataclass\n+class SimpleChessBoard:\n+    \"\"\"Represents a simple chess board state without full rules validation\"\"\"\n+\n+    def __init__(self):\n+        # Use Unicode chess pieces for better visualization\n+        self.piece_map = {\n+            \"K\": \"\u2654\",\n+            \"Q\": \"\u2655\",\n+            \"R\": \"\u2656\",\n+            \"B\": \"\u2657\",\n+            \"N\": \"\u2658\",\n+            \"P\": \"\u2659\",  # White pieces\n+            \"k\": \"\u265a\",\n+            \"q\": \"\u265b\",\n+            \"r\": \"\u265c\",\n+            \"b\": \"\u265d\",\n+            \"n\": \"\u265e\",\n+            \"p\": \"\u265f\",  # Black pieces\n+            \".\": \" \",  # Empty square\n+        }\n+\n+        self.board = [\n+            [\"r\", \"n\", \"b\", \"q\", \"k\", \"b\", \"n\", \"r\"],  # Black pieces\n+            [\"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\"],  # Black pawns\n+            [\".\", \".\", \".\", \".\", \".\", \".\", \".\", \".\"],  # Empty row\n+            [\".\", \".\", \".\", \".\", \".\", \".\", \".\", \".\"],  # Empty row\n+            [\".\", \".\", \".\", \".\", \".\", \".\", \".\", \".\"],  # Empty row\n+            [\".\", \".\", \".\", \".\", \".\", \".\", \".\", \".\"],  # Empty row\n+            [\"P\", \"P\", \"P\", \"P\", \"P\", \"P\", \"P\", \"P\"],  # White pawns\n+            [\"R\", \"N\", \"B\", \"Q\", \"K\", \"B\", \"N\", \"R\"],  # White pieces\n+        ]\n+\n+    def get_board_state(self) -> str:\n+        \"\"\"Returns a formatted string representation of the current board state with HTML/CSS classes\"\"\"\n+        # First create the HTML structure with CSS classes\n+        html_output = [\n+            '<div class=\"chess-board-wrapper\">',\n+            '<div class=\"chess-files\">',\n+        ]\n+\n+        # Add individual file labels\n+        for file in \"abcdefgh\":\n+            html_output.append(f'<div class=\"chess-file-label\">{file}</div>')\n+\n+        html_output.extend(\n+            [\n+                \"</div>\",  # Close chess-files\n+                '<div class=\"chess-grid\">',\n+            ]\n+        )\n+\n+        for i, row in enumerate(self.board):\n+            html_output.append('<div class=\"chess-row\">')\n+            html_output.append(f'<div class=\"chess-rank\">{8 - i}</div>')\n+\n+            for piece in row:\n+                piece_char = self.piece_map[piece]\n+                piece_class = \"piece-white\" if piece.isupper() else \"piece-black\"\n+                if piece == \".\":\n+                    piece_class = \"piece-empty\"\n+                html_output.append(\n+                    f'<div class=\"chess-cell {piece_class}\">{piece_char}</div>'\n+                )\n+\n+            html_output.append(\"</div>\")\n+\n+        html_output.append(\"</div>\")\n+        html_output.append(\"</div>\")\n+\n+        return \"\\n\".join(html_output)\n+\n+    def update_position(\n+        self, from_pos: Tuple[int, int], to_pos: Tuple[int, int]\n+    ) -> None:\n+        \"\"\"Updates the board with a new move\"\"\"\n+        piece = self.board[from_pos[0]][from_pos[1]]\n+        self.board[from_pos[0]][from_pos[1]] = \".\"\n+        self.board[to_pos[0]][to_pos[1]] = piece\n+\n+    def is_valid_position(self, pos: Tuple[int, int]) -> bool:\n+        \"\"\"Checks if a position is within the board boundaries\"\"\"\n+        return 0 <= pos[0] < 8 and 0 <= pos[1] < 8\n+\n+    def is_valid_move(self, move: str) -> bool:\n+        \"\"\"Validates if a move string is in the correct format (e.g., 'e2e4')\"\"\"\n+        if len(move) != 4:\n+            return False\n+\n+        file_chars = \"abcdefgh\"\n+        rank_chars = \"12345678\"\n+\n+        from_file, from_rank = move[0], move[1]\n+        to_file, to_rank = move[2], move[3]\n+\n+        return all(\n+            [\n+                from_file in file_chars,\n+                from_rank in rank_chars,\n+                to_file in file_chars,\n+                to_rank in rank_chars,\n+            ]\n+        )\n+\n+    def algebraic_to_index(self, move: str) -> tuple[tuple[int, int], tuple[int, int]]:\n+        \"\"\"Converts algebraic notation (e.g., 'e2e4') to board indices\"\"\"\n+        file_map = {\"a\": 0, \"b\": 1, \"c\": 2, \"d\": 3, \"e\": 4, \"f\": 5, \"g\": 6, \"h\": 7}\n+\n+        from_file, from_rank = move[0], int(move[1])\n+        to_file, to_rank = move[2], int(move[3])\n+\n+        from_pos = (8 - from_rank, file_map[from_file])\n+        to_pos = (8 - to_rank, file_map[to_file])\n+\n+        return from_pos, to_pos\n+\n+    def get_piece_name(self, piece: str) -> str:\n+        \"\"\"Returns the full name of a piece from its symbol\"\"\"\n+        piece_names = {\n+            \"K\": \"King\",\n+            \"Q\": \"Queen\",\n+            \"R\": \"Rook\",\n+            \"B\": \"Bishop\",\n+            \"N\": \"Knight\",\n+            \"P\": \"Pawn\",\n+            \"k\": \"King\",\n+            \"q\": \"Queen\",\n+            \"r\": \"Rook\",\n+            \"b\": \"Bishop\",\n+            \"n\": \"Knight\",\n+            \"p\": \"Pawn\",\n+            \".\": \"Empty\",\n+        }\n+        return piece_names.get(piece, \"Unknown\")\n+\n+    def get_piece_at_position(self, pos: Tuple[int, int]) -> str:\n+        \"\"\"Returns the piece at the given position\"\"\"\n+        return self.board[pos[0]][pos[1]]\n+\n+\n+class ChessBoard:\n+    def __init__(self):\n+        self.board = chess.Board()\n+        self.current_color = WHITE\n+        self.move_history = []\n+\n+    def make_move(self, move_str: str) -> Tuple[bool, str]:\n+        \"\"\"\n+        Make a move on the board using python-chess for validation.\n+\n+        Args:\n+            move_str: Move in UCI notation (e.g., \"e2e4\")\n+\n+        Returns:\n+            Tuple[bool, str]: (Success status, Message with current board state or error)\n+        \"\"\"\n+        try:\n+            # Convert to python-chess move\n+            move = chess.Move.from_uci(move_str)\n+\n+            # Check if move is legal\n+            if move not in self.board.legal_moves:\n+                return False, f\"Invalid move: {move_str} is not a legal move.\"\n+\n+            self.board.push(move)\n+\n+            # Switch player\n+            self.current_color = BLACK if self.current_color == WHITE else WHITE\n+\n+            return True, f\"Move successful!\\n{self.get_board_state()}\"\n+        except ValueError:\n+            return False, f\"Invalid move format: {move_str}. Use format like 'e2e4'.\"\n+        except Exception as e:\n+            return False, f\"Error making move: {str(e)}\"\n+\n+    def get_board_state(self) -> str:\n+        \"\"\"\n+        Get a string representation of the current board state.\n+\n+        Returns:\n+            String representation of the board\n+        \"\"\"\n+        return str(self.board)\n+\n+    def get_fen(self) -> str:\n+        \"\"\"\n+        Get the FEN notation of the current board state.\n+\n+        Returns:\n+            FEN string\n+        \"\"\"\n+        return self.board.fen()\n+\n+    def get_legal_moves(self, color: str = None) -> List[str]:\n+        \"\"\"\n+        Get all legal moves for the current player or specified color.\n+\n+        Args:\n+            color: Optional color to get moves for (WHITE or BLACK)\n+\n+        Returns:\n+            List of legal moves in UCI notation\n+        \"\"\"\n+\n+        # If it's not the specified color's turn, return empty list\n+        if (color == WHITE and not self.board.turn) or (\n+            color == BLACK and self.board.turn\n+        ):\n+            return []\n+\n+        return [move.uci() for move in self.board.legal_moves]\n+\n+    def is_game_over(self) -> bool:\n+        \"\"\"\n+        Check if the game is over.\n+\n+        Returns:\n+            True if game is over, False otherwise\n+        \"\"\"\n+        return self.board.is_game_over()\n+\n+    def get_game_state(self) -> Tuple[bool, Dict]:\n+        \"\"\"\n+        Get the current game state.\n+\n+        Returns:\n+            Tuple[bool, Dict]: (is_game_over, state_info)\n+        \"\"\"\n+        is_game_over = self.board.is_game_over()\n+\n+        state_info = {\n+            \"current_player\": self.current_color,\n+            \"fen\": self.board.fen(),\n+            \"halfmove_clock\": self.board.halfmove_clock,\n+            \"fullmove_number\": self.board.fullmove_number,\n+        }\n+\n+        if is_game_over:\n+            if self.board.is_checkmate():\n+                winner = BLACK if self.board.turn else WHITE\n+                state_info[\"result\"] = f\"{winner}_win\"\n+                state_info[\"reason\"] = \"checkmate\"\n+            elif self.board.is_stalemate():\n+                state_info[\"result\"] = \"draw\"\n+                state_info[\"reason\"] = \"stalemate\"\n+            elif self.board.is_insufficient_material():\n+                state_info[\"result\"] = \"draw\"\n+                state_info[\"reason\"] = \"insufficient_material\"\n+            elif self.board.is_fifty_moves():\n+                state_info[\"result\"] = \"draw\"\n+                state_info[\"reason\"] = \"fifty_move_rule\"\n+            elif self.board.is_repetition():\n+                state_info[\"result\"] = \"draw\"\n+                state_info[\"reason\"] = \"repetition\"\n+            else:\n+                state_info[\"result\"] = \"draw\"\n+                state_info[\"reason\"] = \"unknown\"\n+        else:\n+            state_info[\"result\"] = None\n+            state_info[\"reason\"] = None\n+\n+        return is_game_over, state_info\n+\n+\n+def display_board(board: ChessBoard):\n+    \"\"\"\n+    Display the chess board in the Streamlit app.\n+\n+    Args:\n+        board: ChessBoard instance\n+    \"\"\"\n+    board_obj = board.board\n+\n+    # Create HTML for the chess board\n+    html = '<div class=\"chess-board\">'\n+\n+    # Unicode chess pieces\n+    pieces = {\n+        \"r\": \"\u265c\",\n+        \"n\": \"\u265e\",\n+        \"b\": \"\u265d\",\n+        \"q\": \"\u265b\",\n+        \"k\": \"\u265a\",\n+        \"p\": \"\u265f\",\n+        \"R\": \"\u2656\",\n+        \"N\": \"\u2658\",\n+        \"B\": \"\u2657\",\n+        \"Q\": \"\u2655\",\n+        \"K\": \"\u2654\",\n+        \"P\": \"\u2659\",\n+        \".\": \"\",\n+    }\n+\n+    # Convert board to a 2D array for easier rendering\n+    board_array = []\n+    for row in str(board_obj).split(\"\\n\"):\n+        board_array.append(row.split(\" \"))\n+\n+    for i in range(8):\n+        for j in range(8):\n+            square_color = \"white-square\" if (i + j) % 2 == 0 else \"black-square\"\n+            piece = board_array[i][j]\n+            piece_unicode = pieces.get(piece, \"\")\n+\n+            html += f'<div class=\"chess-square {square_color}\">'\n+            if piece_unicode:\n+                html += f'<span class=\"piece\">{piece_unicode}</span>'\n+            html += \"</div>\"\n+\n+    html += \"</div>\"\n+\n+    st.markdown(html, unsafe_allow_html=True)\n+\n+\n+def show_agent_status(agent_name: str, status: str, is_white: bool = True):\n+    \"\"\"\n+    Display the status of an agent.\n+\n+    Args:\n+        agent_name: Name of the agent\n+        status: Status message\n+        is_white: Whether the agent plays white pieces\n+    \"\"\"\n+    color_class = \"white\" if is_white else \"black\"\n+    st.markdown(\n+        f\"\"\"<div class=\"agent-status {color_class}\">\n+            <div style=\"margin-right: 10px;\">{\"\u2654\" if is_white else \"\u265a\"}</div>\n+            <div>\n+                <strong>{agent_name}</strong><br>\n+                {status}\n+            </div>\n+        </div>\"\"\",\n+        unsafe_allow_html=True,\n+    )\n+\n+\n+def display_move_history(move_history):\n+    \"\"\"\n+    Display the move history with miniature chess boards.\n+\n+    Args:\n+        move_history: List of move history entries\n+    \"\"\"\n+    if not move_history:\n+        return\n+\n+    st.markdown(\"<h3>Move History</h3>\", unsafe_allow_html=True)\n+\n+    html = '<div class=\"move-history-grid\">'\n+\n+    # Unicode chess pieces\n+    pieces = {\n+        \"r\": \"\u265c\",\n+        \"n\": \"\u265e\",\n+        \"b\": \"\u265d\",\n+        \"q\": \"\u265b\",\n+        \"k\": \"\u265a\",\n+        \"p\": \"\u265f\",\n+        \"R\": \"\u2656\",\n+        \"N\": \"\u2658\",\n+        \"B\": \"\u2657\",\n+        \"Q\": \"\u2655\",\n+        \"K\": \"\u2654\",\n+        \"P\": \"\u2659\",\n+        \".\": \"\",\n+    }\n+\n+    for move in move_history:\n+        board = chess.Board()\n+        # Play all moves up to this point\n+        for i in range(move[\"number\"]):\n+            if i < len(move_history):\n+                try:\n+                    board.push(chess.Move.from_uci(move_history[i][\"move\"]))\n+                except (chess.InvalidMoveError, ValueError) as e:\n+                    continue\n+\n+        # Get the current move's from and to squares\n+        current_move = move[\"move\"]\n+        from_square = current_move[:2]\n+        to_square = current_move[2:]\n+\n+        # Convert algebraic notation to board coordinates\n+        file_map = {\"a\": 0, \"b\": 1, \"c\": 2, \"d\": 3, \"e\": 4, \"f\": 5, \"g\": 6, \"h\": 7}\n+        from_file, from_rank = from_square[0], int(from_square[1])\n+        to_file, to_rank = to_square[0], int(to_square[1])\n+        from_coords = (8 - from_rank, file_map[from_file])\n+        to_coords = (8 - to_rank, file_map[to_file])\n+\n+        # Determine if it's a white or black move\n+        is_white_move = \"white\" in move[\"player\"].lower()\n+        move_color_class = \"white-move\" if is_white_move else \"black-move\"\n+\n+        # Board to 2D array\n+        board_array = []\n+        for row in str(board).split(\"\\n\"):\n+            board_array.append(row.split(\" \"))\n+\n+        # Create move history item with mini board\n+        html += '<div class=\"move-history-item\">'\n+        html += f\"<strong>Move {move['number']}</strong><br>\"\n+        html += f\"{move['player']}<br>\"\n+        html += f'<div class=\"move-text {move_color_class}\">{move[\"move\"]}</div>'\n+\n+        # Add mini chess board\n+        html += '<div class=\"mini-chess-board\">'\n+        for i in range(8):\n+            for j in range(8):\n+                square_color = (\n+                    \"mini-white-square\" if (i + j) % 2 == 0 else \"mini-black-square\"\n+                )\n+                piece = board_array[i][j]\n+                piece_unicode = pieces.get(piece, \"\")\n+\n+                # highlighting moves\n+                highlight_class = \"\"\n+                if (i, j) == from_coords:\n+                    highlight_class = f\" move-from {move_color_class}\"\n+                elif (i, j) == to_coords:\n+                    highlight_class = f\" move-to {move_color_class}\"\n+\n+                html += f'<div class=\"mini-square {square_color}{highlight_class}\">'\n+                if piece_unicode:\n+                    piece_color = \"white-piece\" if piece.isupper() else \"black-piece\"\n+                    html += (\n+                        f'<span class=\"mini-piece {piece_color}\">{piece_unicode}</span>'\n+                    )\n+                html += \"</div>\"\n+        html += \"</div>\"\n+\n+        if move.get(\"description\"):\n+            html += f'<div class=\"description\">{move[\"description\"]}</div>'\n+\n+        html += \"</div>\"\n+\n+    html += \"</div>\"\n+\n+    st.markdown(html, unsafe_allow_html=True)\n+\n+\n+def parse_move(move_text: str) -> str:\n+    \"\"\"\n+    Parse a move from agent response.\n+\n+    Args:\n+        move_text: Text containing the move\n+\n+    Returns:\n+        Extracted move in UCI format\n+    \"\"\"\n+    move_text = move_text.strip()\n+\n+    # If the move is already in UCI format (e.g., \"e2e4\"), return it\n+    if (\n+        len(move_text) == 4\n+        and move_text[0].isalpha()\n+        and move_text[1].isdigit()\n+        and move_text[2].isalpha()\n+        and move_text[3].isdigit()\n+    ):\n+        return move_text\n+\n+    # Try to extract the move from text\n+    import re\n+\n+    move_match = re.search(r\"([a-h][1-8][a-h][1-8])\", move_text)\n+    if move_match:\n+        return move_match.group(1)\n+\n+    return move_text\n+\n+\n+def is_claude_thinking_model(agent: Agent) -> bool:\n+    \"\"\"\n+    Args:\n+        agent: The agent to check\n+    Returns:\n+        bool: True if the agent uses a Claude model with thinking enabled\n+    \"\"\"\n+    return (\n+        hasattr(agent.model, \"id\")\n+        and isinstance(agent.model.id, str)\n+        and \"claude\" in agent.model.id.lower()\n+        and \"thinking\" in agent.model.id.lower()\n+    )\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/game_generator/.gitignore",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/game_generator/.gitignore b/cookbook/examples/streamlit_applications/game_generator/.gitignore\nnew file mode 100644\nindex 000000000..2d19fc766\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/game_generator/.gitignore\n@@ -0,0 +1 @@\n+*.html\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/game_generator/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/game_generator/__init__.py b/cookbook/examples/streamlit_applications/game_generator/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/game_generator/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/game_generator/app.py b/cookbook/examples/streamlit_applications/game_generator/app.py\nnew file mode 100644\nindex 000000000..208b1904c\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/game_generator/app.py\n@@ -0,0 +1,90 @@\n+from pathlib import Path\n+\n+import streamlit as st\n+from agno.utils.string import hash_string_sha256\n+from game_generator import GameGenerator, SqliteWorkflowStorage\n+\n+st.set_page_config(\n+    page_title=\"HTML5 Game Generator\",\n+    page_icon=\"\ud83c\udfae\",\n+    layout=\"wide\",\n+)\n+\n+\n+st.title(\"Game Generator\")\n+st.markdown(\"##### \ud83c\udfae built using [Agno](https://github.com/agno-agi/agno)\")\n+\n+\n+def main() -> None:\n+    game_description = st.sidebar.text_area(\n+        \"\ud83c\udfae Describe your game\",\n+        value=\"An asteroids game. Make sure the asteroids move randomly and are random sizes.\",\n+        height=100,\n+    )\n+\n+    generate_game = st.sidebar.button(\"Generate Game! \ud83d\ude80\")\n+\n+    st.sidebar.markdown(\"## Example Games\")\n+    example_games = [\n+        \"A simple snake game where the snake grows longer as it eats food\",\n+        \"A breakout clone with colorful blocks and power-ups\",\n+        \"A space invaders game with multiple enemy types\",\n+        \"A simple platformer with jumping mechanics\",\n+    ]\n+\n+    for game in example_games:\n+        if st.sidebar.button(game):\n+            st.session_state[\"game_description\"] = game\n+            generate_game = True\n+\n+    if generate_game:\n+        with st.spinner(\"Generating your game... This might take a minute...\"):\n+            try:\n+                hash_of_description = hash_string_sha256(game_description)\n+                game_generator = GameGenerator(\n+                    session_id=f\"game-gen-{hash_of_description}\",\n+                    storage=SqliteWorkflowStorage(\n+                        table_name=\"game_generator_workflows\",\n+                        db_file=\"tmp/workflows.db\",\n+                    ),\n+                )\n+\n+                result = list(game_generator.run(game_description=game_description))\n+\n+                games_dir = Path(__file__).parent.joinpath(\"games\")\n+                game_path = games_dir / \"game_output_file.html\"\n+\n+                if game_path.exists():\n+                    game_code = game_path.read_text()\n+\n+                    with st.status(\n+                        \"Game Generated Successfully!\", expanded=True\n+                    ) as status:\n+                        st.subheader(\"Play the Game\")\n+                        st.components.v1.html(game_code, height=700, scrolling=False)\n+\n+                        st.subheader(\"Game Instructions\")\n+                        st.write(result[-1].content)\n+\n+                        st.download_button(\n+                            label=\"Download Game HTML\",\n+                            data=game_code,\n+                            file_name=\"game.html\",\n+                            mime=\"text/html\",\n+                        )\n+\n+                        status.update(\n+                            label=\"Game ready to play!\",\n+                            state=\"complete\",\n+                            expanded=True,\n+                        )\n+\n+            except Exception as e:\n+                st.error(f\"Failed to generate game: {str(e)}\")\n+\n+    st.sidebar.markdown(\"---\")\n+    if st.sidebar.button(\"Restart\"):\n+        st.rerun()\n+\n+\n+main()\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/game_generator/game_generator.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/game_generator/game_generator.py b/cookbook/examples/streamlit_applications/game_generator/game_generator.py\nnew file mode 100644\nindex 000000000..d640d6995\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/game_generator/game_generator.py\n@@ -0,0 +1,147 @@\n+\"\"\"\n+1. Install dependencies using: `pip install openai agno`\n+2. Run the script using: `python cookbook/examples/streamlit/game_generator/game_generator.py`\n+\"\"\"\n+\n+import json\n+from pathlib import Path\n+from typing import Iterator\n+\n+from agno.agent import Agent, RunResponse\n+from agno.models.openai import OpenAIChat\n+from agno.run.response import RunEvent\n+from agno.storage.workflow.sqlite import SqliteWorkflowStorage\n+from agno.utils.log import logger\n+from agno.utils.pprint import pprint_run_response\n+from agno.utils.string import hash_string_sha256\n+from agno.utils.web import open_html_file\n+from agno.workflow import Workflow\n+from pydantic import BaseModel, Field\n+\n+games_dir = Path(__file__).parent.joinpath(\"games\")\n+games_dir.mkdir(parents=True, exist_ok=True)\n+game_output_path = games_dir / \"game_output_file.html\"\n+game_output_path.unlink(missing_ok=True)\n+\n+\n+class GameOutput(BaseModel):\n+    reasoning: str = Field(..., description=\"Explain your reasoning\")\n+    code: str = Field(..., description=\"The html5 code for the game\")\n+    instructions: str = Field(..., description=\"Instructions how to play the game\")\n+\n+\n+class QAOutput(BaseModel):\n+    reasoning: str = Field(..., description=\"Explain your reasoning\")\n+    correct: bool = Field(False, description=\"Does the game pass your criteria?\")\n+\n+\n+class GameGenerator(Workflow):\n+    # This description is only used in the workflow UI\n+    description: str = \"Generator for single-page HTML5 games\"\n+\n+    game_developer: Agent = Agent(\n+        name=\"Game Developer Agent\",\n+        description=\"You are a game developer that produces working HTML5 code.\",\n+        model=OpenAIChat(id=\"gpt-4o\"),\n+        instructions=[\n+            \"Create a game based on the user's prompt. \"\n+            \"The game should be HTML5, completely self-contained and must be runnable simply by opening on a browser\",\n+            \"Ensure the game has a alert that pops up if the user dies and then allows the user to restart or exit the game.\",\n+            \"add full screen mode to the game\",\n+            \"Ensure instructions for the game are displayed on the HTML page.\"\n+            \"Use user-friendly colours and make the game canvas large enough for the game to be playable on a larger screen.\",\n+        ],\n+        response_model=GameOutput,\n+    )\n+\n+    qa_agent: Agent = Agent(\n+        name=\"QA Agent\",\n+        model=OpenAIChat(id=\"gpt-4o\"),\n+        description=\"You are a game QA and you evaluate html5 code for correctness.\",\n+        instructions=[\n+            \"You will be given some HTML5 code.\"\n+            \"Your task is to read the code and evaluate it for correctness, but also that it matches the original task description.\",\n+        ],\n+        response_model=QAOutput,\n+    )\n+\n+    def run(self, game_description: str) -> Iterator[RunResponse]:\n+        logger.info(f\"Game description: {game_description}\")\n+\n+        game_output = self.game_developer.run(game_description)\n+\n+        if (\n+            game_output\n+            and game_output.content\n+            and isinstance(game_output.content, GameOutput)\n+        ):\n+            game_code = game_output.content.code\n+            logger.info(f\"Game code: {game_code}\")\n+        else:\n+            yield RunResponse(\n+                run_id=self.run_id,\n+                event=RunEvent.workflow_completed,\n+                content=\"Sorry, could not generate a game.\",\n+            )\n+            return\n+\n+        logger.info(\"QA'ing the game code\")\n+        qa_input = {\n+            \"game_description\": game_description,\n+            \"game_code\": game_code,\n+        }\n+        qa_output = self.qa_agent.run({\"role\": \"user\", \"content\": json.dumps(qa_input)})\n+\n+        if qa_output and qa_output.content and isinstance(qa_output.content, QAOutput):\n+            logger.info(qa_output.content)\n+            if not qa_output.content.correct:\n+                raise Exception(f\"QA failed for code: {game_code}\")\n+\n+            # Store the resulting code\n+            game_output_path.write_text(game_code)\n+\n+            yield RunResponse(\n+                run_id=self.run_id,\n+                event=RunEvent.workflow_completed,\n+                content=game_output.content.instructions,\n+            )\n+        else:\n+            yield RunResponse(\n+                run_id=self.run_id,\n+                event=RunEvent.workflow_completed,\n+                content=\"Sorry, could not QA the game.\",\n+            )\n+            return\n+\n+\n+# Run the workflow if the script is executed directly\n+if __name__ == \"__main__\":\n+    from rich.prompt import Prompt\n+\n+    game_description = Prompt.ask(\n+        \"[bold]Describe the game you want to make (keep it simple)[/bold]\\n\u2728\",\n+        # default=\"An asteroids game.\"\n+        default=\"An asteroids game. Make sure the asteroids move randomly and are random sizes. They should continually spawn more and become more difficult over time. Keep score. Make my spaceship's movement realistic.\",\n+    )\n+\n+    hash_of_description = hash_string_sha256(game_description)\n+\n+    # Initialize the investment analyst workflow\n+    game_generator = GameGenerator(\n+        session_id=f\"game-gen-{hash_of_description}\",\n+        storage=SqliteWorkflowStorage(\n+            table_name=\"game_generator_workflows\",\n+            db_file=\"tmp/workflows.db\",\n+        ),\n+    )\n+\n+    # Execute the workflow\n+    result: Iterator[RunResponse] = game_generator.run(\n+        game_description=game_description\n+    )\n+\n+    # Print the report\n+    pprint_run_response(result)\n+\n+    if game_output_path.exists():\n+        open_html_file(game_output_path)\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/game_generator/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/game_generator/requirements.txt b/cookbook/examples/streamlit_applications/game_generator/requirements.txt\nnew file mode 100644\nindex 000000000..171e1262f\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/game_generator/requirements.txt\n@@ -0,0 +1,4 @@\n+agno\n+openai\n+streamlit\n+\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/gemini-tutor/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/gemini-tutor/__init__.py b/cookbook/examples/streamlit_applications/gemini-tutor/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/gemini-tutor/agents.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/gemini-tutor/agents.py b/cookbook/examples/streamlit_applications/gemini-tutor/agents.py\nnew file mode 100644\nindex 000000000..5af2db03b\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/gemini-tutor/agents.py\n@@ -0,0 +1,145 @@\n+\"\"\"\n+Gemini Tutor: Advanced Educational AI Assistant powered by Gemini 2.5\n+\"\"\"\n+\n+import json\n+import uuid\n+from pathlib import Path\n+from typing import Any, Dict, Optional\n+\n+from agno.agent import Agent, RunResponse\n+from agno.models.google import Gemini\n+from agno.models.message import Message\n+from agno.tools.file import FileTools\n+from agno.tools.googlesearch import GoogleSearchTools\n+from agno.utils.log import logger\n+\n+# Import prompt templates\n+from prompts import (\n+    SEARCH_GROUNDING_INSTRUCTIONS,\n+    TUTOR_DESCRIPTION_TEMPLATE,\n+    TUTOR_INSTRUCTIONS_TEMPLATE,\n+)\n+\n+\n+class TutorAppAgent:\n+    \"\"\"\n+    Central agent that handles all tutoring functionality.\n+    Offloads search, content preparation, and learning experience generation.\n+    \"\"\"\n+\n+    def __init__(\n+        self, model_id=\"gemini-2.5-pro-exp-03-25\", education_level=\"High School\"\n+    ):\n+        \"\"\"\n+        Initialize the TutorAppAgent.\n+\n+        Args:\n+            model_id: Model identifier to use\n+            education_level: Target education level for content\n+        \"\"\"\n+        self.model_id = model_id\n+        self.education_level = education_level\n+        self.agent = self._create_agent()\n+        logger.info(\n+            f\"TutorAppAgent initialized with {model_id} model and {education_level} education level\"\n+        )\n+\n+    def _create_agent(self):\n+        \"\"\"Create and configure the agent with all necessary capabilities\"\"\"\n+\n+        gemini_model = Gemini(\n+            id=self.model_id,\n+            temperature=1,\n+            top_p=0.9,\n+            top_k=40,\n+        )\n+\n+        # Enable grounding if supported by the model\n+        if \"gemini-2.\" in self.model_id or \"gemini-1.5\" in self.model_id:\n+            try:\n+                setattr(gemini_model, \"grounding\", True)\n+                logger.info(\"Enabled model grounding (google_search_retrieval)\")\n+            except AttributeError:\n+                logger.warning(\n+                    f\"Model {self.model_id} does not support grounding attribute.\"\n+                )\n+            except Exception as e:\n+                logger.warning(f\"Could not enable model grounding: {e}\")\n+\n+        # Format description and instructions directly\n+        tutor_description = TUTOR_DESCRIPTION_TEMPLATE.format(\n+            education_level=self.education_level\n+        )\n+        tutor_instructions = TUTOR_INSTRUCTIONS_TEMPLATE.format(\n+            education_level=self.education_level,\n+        )\n+\n+        # Create agent with tutor capabilities, passing the created model\n+        return Agent(\n+            name=\"Gemini Tutor\",\n+            model=gemini_model,\n+            session_id=str(uuid.uuid4()),\n+            read_chat_history=True,\n+            read_tool_call_history=True,\n+            add_history_to_messages=True,\n+            num_history_responses=5,\n+            description=tutor_description,  # Pass formatted description\n+            instructions=tutor_instructions,  # Pass formatted instructions\n+            debug_mode=True,\n+            markdown=True,\n+        )\n+\n+    def create_learning_experience(self, search_topic, education_level=None):\n+        \"\"\"\n+        Create a complete learning experience from search topic to final content.\n+        This method offloads the entire process to the agent.\n+\n+        Args:\n+            search_topic: The topic to create a learning experience for\n+            education_level: Override the default education level for this specific call.\n+\n+        Returns:\n+            The learning experience response from the agent\n+        \"\"\"\n+        # Determine the education level for this specific request\n+        current_education_level = education_level or self.education_level\n+        if education_level and self.education_level != education_level:\n+            logger.info(\n+                f\"Using temporary education level for this request: {education_level}\"\n+            )\n+        else:\n+            # Use the agent's default education level if not overridden\n+            current_education_level = self.education_level\n+\n+        logger.info(\n+            f\"Creating learning experience for '{search_topic}' at {current_education_level} level\"\n+        )\n+\n+        # Construct a focused prompt for the agent, relying on its core instructions\n+        grounding_instructions = (\n+            SEARCH_GROUNDING_INSTRUCTIONS\n+            if \"gemini-2.\" in self.model_id or \"gemini-1.5\" in self.model_id\n+            else \"\"\n+        )\n+        # The agent's core instructions (set during init) already contain formatting rules.\n+        # This prompt focuses on the specific task.\n+        prompt = f\"\"\"\n+        Create a complete and engaging learning experience about '{search_topic}' specifically tailored for {current_education_level} students.\n+\n+        **Task:**\n+        Generate a comprehensive learning module covering the key aspects of '{search_topic}'.\n+\n+        **Follow your core instructions regarding:**\n+        *   Adapting content complexity and style for the {current_education_level} level.\n+        *   Structuring the response logically (introduction, key concepts, examples, etc.).\n+        *   Including interactive elements (thought experiments/questions) and assessments (2-3 simple questions with answers).\n+        *   Strictly adhering to the rules for embedding images and videos (using direct, stable URLs only or omitting embeds).\n+        *   Citing up to 5 key sources if external information was used.\n+\n+        {grounding_instructions}\n+        \"\"\"\n+\n+        # Create message\n+        user_message = Message(role=\"user\", content=prompt)\n+        return self.agent.run(prompt=prompt, messages=[user_message], stream=True)\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/gemini-tutor/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/gemini-tutor/app.py b/cookbook/examples/streamlit_applications/gemini-tutor/app.py\nnew file mode 100644\nindex 000000000..cac6a44a1\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/gemini-tutor/app.py\n@@ -0,0 +1,293 @@\n+\"\"\"\n+Gemini Tutor: Advanced Educational AI Assistant with Multimodal Learning\n+\"\"\"\n+\n+import os\n+\n+import nest_asyncio\n+import streamlit as st\n+from agents import TutorAppAgent\n+from agno.utils.log import logger\n+from utils import display_grounding_metadata, display_tool_calls\n+\n+# Initialize asyncio support\n+nest_asyncio.apply()\n+\n+# Page configuration\n+st.set_page_config(\n+    page_title=\"Gemini Multimodal Tutor\",\n+    page_icon=\"\ud83e\udde0\",\n+    layout=\"wide\",\n+    initial_sidebar_state=\"expanded\",\n+)\n+\n+# --- Constants ---\n+MODEL_OPTIONS = {\n+    \"Gemini 2.5 Pro Experimental (Recommended)\": \"gemini-2.5-pro-exp-03-25\",\n+    \"Gemini 2.0 Pro\": \"gemini-2.0-pro\",\n+    \"Gemini 2.0 Pro\": \"gemini-2.0-pro\",\n+    \"Gemini 1.5 Pro\": \"gemini-1.5-pro\",\n+}\n+\n+EDUCATION_LEVELS = [\n+    \"Elementary School\",\n+    \"High School\",\n+    \"College\",\n+    \"Graduate\",\n+    \"PhD\",\n+]\n+\n+\n+def initialize_session_state():\n+    \"\"\"Initialize Streamlit session state variables if they don't exist.\"\"\"\n+    if \"tutor_agent\" not in st.session_state:\n+        st.session_state.tutor_agent = None\n+    if \"model_id\" not in st.session_state:\n+        st.session_state.model_id = MODEL_OPTIONS[\n+            \"Gemini 2.5 Pro Experimental (Recommended)\"\n+        ]\n+    if \"education_level\" not in st.session_state:\n+        st.session_state.education_level = \"High School\"\n+    if \"messages\" not in st.session_state:\n+        st.session_state.messages = []\n+    if \"processing\" not in st.session_state:\n+        st.session_state.processing = False\n+    if \"agent_needs_reset\" not in st.session_state:\n+        st.session_state.agent_needs_reset = True  # Start needing initialization\n+\n+\n+def render_sidebar():\n+    \"\"\"Render the sidebar for configuration and reset.\"\"\"\n+    with st.sidebar:\n+        st.header(\"Configuration\")\n+\n+        # Store previous values to detect changes\n+        prev_model_id = st.session_state.model_id\n+        prev_education_level = st.session_state.education_level\n+\n+        selected_model_name = st.selectbox(\n+            \"Select Gemini Model\",\n+            options=list(MODEL_OPTIONS.keys()),\n+            index=list(MODEL_OPTIONS.values()).index(\n+                st.session_state.model_id\n+            ),  # Maintain selection\n+            key=\"selected_model_name\",\n+        )\n+        st.session_state.model_id = MODEL_OPTIONS[selected_model_name]\n+\n+        st.session_state.education_level = st.selectbox(\n+            \"Select Education Level\",\n+            options=EDUCATION_LEVELS,\n+            index=EDUCATION_LEVELS.index(\n+                st.session_state.education_level\n+            ),  # Maintain selection\n+            key=\"education_level_selector\",\n+        )\n+\n+        # Check if settings changed\n+        if (\n+            st.session_state.model_id != prev_model_id\n+            or st.session_state.education_level != prev_education_level\n+        ):\n+            st.session_state.agent_needs_reset = True\n+            st.info(\n+                \"Settings changed. Agent will be updated on next interaction or reset.\"\n+            )\n+\n+        if st.button(\"New chat\", key=\"apply_reset\"):\n+            st.session_state.agent_needs_reset = True\n+            st.session_state.messages = []  # Clear history on reset\n+            st.toast(\"Settings applied. Agent updated and chat reset.\")\n+\n+\n+def initialize_or_update_agent():\n+    \"\"\"Initialize or update the agent if settings have changed.\"\"\"\n+    if st.session_state.agent_needs_reset or st.session_state.tutor_agent is None:\n+        logger.info(\n+            f\"Initializing/Updating Tutor Agent: Model={st.session_state.model_id}, Level={st.session_state.education_level}\"\n+        )\n+        try:\n+            st.session_state.tutor_agent = TutorAppAgent(\n+                model_id=st.session_state.model_id,\n+                education_level=st.session_state.education_level,\n+            )\n+            st.session_state.agent_needs_reset = False\n+        except Exception as e:\n+            st.error(f\"Failed to initialize agent: {e}\")\n+            st.session_state.tutor_agent = None\n+            st.stop()\n+\n+\n+def render_chat_history():\n+    \"\"\"Display the chat messages stored in session state.\"\"\"\n+    st.markdown(\"### Learning Session\")\n+    for message in st.session_state.messages:\n+        # Skip empty messages if any occurred (e.g., during stream error)\n+        if (\n+            not message.get(\"content\")\n+            and not message.get(\"tools\")\n+            and not message.get(\"citations\")\n+        ):\n+            continue\n+        with st.chat_message(message[\"role\"]):\n+            # Display content if it exists\n+            if message.get(\"content\"):\n+                st.markdown(message[\"content\"])\n+\n+            # If assistant message, display tools and citations *within the same bubble*\n+            if message[\"role\"] == \"assistant\":\n+                if message.get(\"tools\"):\n+                    with st.expander(\"\ud83d\udee0\ufe0f Tool Calls\", expanded=False):\n+                        display_tool_calls(message[\"tools\"])\n+                if message.get(\"citations\"):\n+                    display_grounding_metadata(message[\"citations\"])\n+\n+\n+def handle_user_input():\n+    \"\"\"Render the chat input form and handle submission.\"\"\"\n+    with st.form(key=\"topic_form\"):\n+        search_topic = st.text_input(\n+            \"What would you like to learn about?\",\n+            key=\"search_topic_input\",\n+            placeholder=\"e.g., Quantum Physics, History of Rome, Python programming\",\n+        )\n+        submitted = st.form_submit_button(\"Start Learning\", type=\"primary\")\n+\n+        if submitted and search_topic and not st.session_state.processing:\n+            st.session_state.processing = True\n+            user_message = {\n+                \"role\": \"user\",\n+                \"content\": f\"Teach me about: {search_topic}\",\n+            }\n+            st.session_state.messages.append(user_message)\n+            st.rerun()  # Rerun to display user message immediately\n+\n+\n+def process_agent_response():\n+    \"\"\"Process the agent response if the last message was from the user.\"\"\"\n+    if (\n+        st.session_state.processing\n+        and st.session_state.messages\n+        and st.session_state.messages[-1][\"role\"] == \"user\"\n+    ):\n+        if st.session_state.tutor_agent is None:\n+            st.error(\"Agent is not initialized. Cannot process request.\")\n+            st.session_state.processing = False\n+            return\n+\n+        try:\n+            search_topic = st.session_state.messages[-1][\"content\"].replace(\n+                \"Teach me about: \", \"\"\n+            )\n+\n+            with st.spinner(\"\ud83e\udd14 Thinking...\"):\n+                response_stream = (\n+                    st.session_state.tutor_agent.create_learning_experience(\n+                        search_topic=search_topic,\n+                        education_level=st.session_state.education_level,\n+                    )\n+                )\n+\n+                st.session_state.current_tools = None\n+                st.session_state.current_citations = None\n+\n+                def stream_handler(stream_generator):\n+                    logger.info(\"Starting stream processing...\")\n+                    full_content = \"\"\n+                    for chunk in stream_generator:\n+                        content_delta = getattr(chunk, \"content\", None)\n+                        if content_delta:\n+                            full_content += content_delta\n+                            yield content_delta\n+                        tools = getattr(chunk, \"tools\", None)\n+                        if tools:\n+                            st.session_state.current_tools = tools\n+                        citations = getattr(chunk, \"citations\", None)\n+                        if citations:\n+                            st.session_state.current_citations = citations\n+                    logger.info(\"Finished stream processing.\")\n+                    st.session_state.full_content_from_stream = full_content\n+\n+                with st.chat_message(\"assistant\"):\n+                    st.write_stream(stream_handler(response_stream))\n+\n+                assistant_message = {\"role\": \"assistant\"}\n+                full_content = st.session_state.pop(\n+                    \"full_content_from_stream\", \"[No content received]\"\n+                )\n+                assistant_message[\"content\"] = (\n+                    full_content if full_content else \"[No content received]\"\n+                )\n+                if not full_content:\n+                    logger.warning(\"Stream finished with no content.\")\n+\n+                # --- Post-processing to remove duplicate grounding sources ---\n+                grounding_marker = \"\\n\ud83c\udf10 Sources\"\n+                if grounding_marker in full_content:\n+                    logger.info(\"Removing duplicate grounding sources section.\")\n+                    full_content = full_content.split(grounding_marker)[0].rstrip()\n+                # -------------------------------------------------------------\n+\n+                final_tools = st.session_state.pop(\"current_tools\", None)\n+                final_citations = st.session_state.pop(\"current_citations\", None)\n+                if final_tools:\n+                    assistant_message[\"tools\"] = final_tools\n+                if final_citations:\n+                    assistant_message[\"citations\"] = final_citations\n+\n+                st.session_state.final_assistant_message = assistant_message\n+\n+            if \"final_assistant_message\" in st.session_state:\n+                st.session_state.messages.append(\n+                    st.session_state.pop(\"final_assistant_message\")\n+                )\n+\n+        except Exception as e:\n+            logger.error(f\"Error during agent run: {e}\", exc_info=True)\n+            st.session_state.messages.append(\n+                {\"role\": \"assistant\", \"content\": f\"An error occurred: {e}\"}\n+            )\n+        finally:\n+            st.session_state.processing = False\n+            st.rerun()\n+\n+\n+# Custom CSS\n+CUSTOM_CSS = \"\"\"\n+<style>\n+    .main-title {\n+        font-size: 2.5rem;\n+        font-weight: 600;\n+        margin-bottom: 0.5rem;\n+        color: #5186EC;\n+    }\n+    .subtitle {\n+        font-size: 1.2rem;\n+        font-weight: 400;\n+        margin-bottom: 2rem;\n+        opacity: 0.8;\n+    }\n+    [data-testid=\"stChatMessageContent\"] img {\n+        max-width: 350px;\n+        max-height: 300px;\n+        display: block;\n+        margin-top: 10px;\n+        margin-bottom: 10px;\n+        border-radius: 5px;\n+    }\n+</style>\n+\"\"\"\n+\n+\n+st.title(\"\ud83d\udd0d Gemini Tutor \ud83d\udcda\")\n+st.markdown(\n+    '<p class=\"subtitle\">Your AI-powered guide for exploring any topic</p>',\n+    unsafe_allow_html=True,\n+)\n+initialize_session_state()\n+render_sidebar()\n+initialize_or_update_agent()\n+render_chat_history()\n+handle_user_input()\n+process_agent_response()\n+st.markdown(CUSTOM_CSS, unsafe_allow_html=True)\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/gemini-tutor/generate_requirements.sh",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/gemini-tutor/generate_requirements.sh b/cookbook/examples/streamlit_applications/gemini-tutor/generate_requirements.sh\nnew file mode 100755\nindex 000000000..78f8c7cec\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/gemini-tutor/generate_requirements.sh\n@@ -0,0 +1,12 @@\n+#!/bin/bash\n+\n+############################################################################\n+# Generate requirements.txt from requirements.in\n+############################################################################\n+\n+echo \"Generating requirements.txt\"\n+\n+CURR_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n+\n+UV_CUSTOM_COMPILE_COMMAND=\"./generate_requirements.sh\" \\\n+  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/gemini-tutor/prompts.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/gemini-tutor/prompts.py b/cookbook/examples/streamlit_applications/gemini-tutor/prompts.py\nnew file mode 100644\nindex 000000000..124558693\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/gemini-tutor/prompts.py\n@@ -0,0 +1,46 @@\n+\"\"\"\n+Prompt templates for the Gemini Tutor application.\n+\"\"\"\n+\n+# Instructions specific to using search grounding\n+SEARCH_GROUNDING_INSTRUCTIONS = \"\"\"\n+Use search to get accurate, up-to-date information and cite your sources **as specified in the formatting instructions**.\n+\"\"\"\n+\n+# Base template for the tutor description\n+# The {education_level} will be formatted in agents.py\n+TUTOR_DESCRIPTION_TEMPLATE = \"\"\"You are expertGemini Tutor, an educational AI assistant that provides personalized\n+learning for {education_level} students. You can analyze text and content\n+to create comprehensive learning experiences.\"\"\"\n+\n+# Base template for the tutor's core instructions\n+# The {education_level} will be formatted in agents.py\n+TUTOR_INSTRUCTIONS_TEMPLATE = \"\"\"\n+**Your Role: Expert Gemini Tutor**\n+You are an expert educational AI assistant designed to create personalized and engaging learning experiences for {education_level} students. Your goal is to foster understanding, not just present information. Adapt your tone, vocabulary, depth, and examples appropriately for the specified education level.\n+\n+**Core Task: Create Learning Experiences**\n+1.  **Understand & Research:** Analyze the user's query/topic. Use grounded search (if available) to gather accurate, up-to-date information. If the topic is ambiguous, either ask a clarifying question or make a reasonable assumption and state it clearly.\n+2.  **Structure the Content:** Organize the information logically with clear headings, introductions, explanations of key concepts, and summaries. Use Markdown for formatting (lists, emphasis, code blocks, tables).\n+3.  **Explain Clearly:** Provide explanations tailored to the {education_level} level. Use analogies, examples, and simple language where appropriate.\n+4.  **Engage & Assess:** Make the experience interactive. Include:\n+    *   **Interactive Elements:** At least one relevant thought experiment, practical analogy, or open-ended question to stimulate critical thinking.\n+    *   **Assessment:** 2-3 simple assessment questions (e.g., multiple-choice, true/false, or short fill-in-the-blank) with answers provided to check understanding.\n+5.  **Media Integration (Strict Rules):**\n+    *   Enhance explanations with relevant images or videos *only* if you can find stable, direct URLs.\n+    *   **Images:** Use `![Description](URL)`. **CRITICAL: The URL MUST be a direct link to the image file itself (ending in .png, .jpg, .jpeg, .gif). DO NOT use URLs pointing to webpages, intermediate services, or URLs with excessive query parameters.** Prioritize Wikimedia Commons direct file links if available.\n+    *   **Videos:** Use `[Video Title](URL)`. **CRITICAL: ONLY use standard, publicly accessible YouTube video URLs (e.g., https://www.youtube.com/watch?v=...).**\n+    *   **If you cannot find a URL meeting these strict criteria, DO NOT include the markdown embed.** Instead, describe the concept the media would illustrate or mention it textually (e.g., \"A helpful diagram showing X can be found online\").\n+6.  **Cite Sources:** Ensure factual accuracy. If you used search results or specific external documents to answer, cite **no more than 5** of the most relevant sources in a 'Sources' section at the end. Use the format: `* [Source Title](URL)`. **CRITICAL: Ensure this is the *only* list of sources provided. Do not include any automatically generated source lists (e.g., those labeled '\ud83c\udf10 Sources') that might come from the search tool.**\n+7.  **Formatting:** Follow the specific following formatting instructions provided in the user prompt for overall structure and citations.\n+\n+Format your response as Markdown with:\n+- Clear headings and subheadings\n+- Lists and emphasis for important concepts\n+- Tables and code blocks when relevant\n+- Only provide sources if you used them to answer the question. Limit to 5 sources.\n+- **Source Citations:** At the end of your response, include a 'Sources' section. List **no more than 5** of the most relevant sources you used. Format each source as a markdown link: `* [Source Title](URL)`.\n+- **Images:** Use `![Description](URL)`. **CRITICAL: The URL MUST be a direct link to the image file itself (ending in .png, .jpg, .jpeg, .gif). DO NOT use URLs pointing to webpages, intermediate services, or URLs with excessive query parameters.** Prioritize Wikimedia Commons direct file links if available.\n+- **Videos:** Use `[Video Title](URL)`. **CRITICAL: ONLY use standard, publicly accessible YouTube video URLs (e.g., https://www.youtube.com/watch?v=...).**\n+\n+\"\"\"\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/gemini-tutor/requirements.in",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/gemini-tutor/requirements.in b/cookbook/examples/streamlit_applications/gemini-tutor/requirements.in\nnew file mode 100644\nindex 000000000..73825ee18\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/gemini-tutor/requirements.in\n@@ -0,0 +1,24 @@\n+# Core dependencies\n+agno>=0.1.0\n+google-genai>=0.3.2\n+streamlit>=1.31.0\n+\n+# Utilities\n+nest-asyncio>=1.5.8\n+python-dotenv>=1.0.0\n+pydantic>=2.0.0\n+typing-extensions>=4.0.0\n+\n+# Media and data handling\n+Pillow>=10.0.0\n+matplotlib>=3.7.0\n+numpy>=1.24.0\n+pandas>=2.0.0\n+\n+# Search and web utilities\n+googlesearch-python>=1.2.3\n+requests>=2.31.0\n+beautifulsoup4>=4.12.0\n+\n+# Optional utilities for enhanced functionality\n+pycountry>=22.1.10  # For handling country data when discussing geography\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/gemini-tutor/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/gemini-tutor/requirements.txt b/cookbook/examples/streamlit_applications/gemini-tutor/requirements.txt\nnew file mode 100644\nindex 000000000..94b5a6d63\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/gemini-tutor/requirements.txt\n@@ -0,0 +1,221 @@\n+# This file was autogenerated by uv via the following command:\n+#    ./generate_requirements.sh\n+agno==1.3.2\n+    # via -r cookbook/examples/apps/gemini-tutor/requirements.in\n+altair==5.5.0\n+    # via streamlit\n+annotated-types==0.7.0\n+    # via pydantic\n+anyio==4.9.0\n+    # via\n+    #   google-genai\n+    #   httpx\n+attrs==25.3.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+beautifulsoup4==4.13.4\n+    # via\n+    #   -r cookbook/examples/apps/gemini-tutor/requirements.in\n+    #   googlesearch-python\n+blinker==1.9.0\n+    # via streamlit\n+cachetools==5.5.2\n+    # via\n+    #   google-auth\n+    #   streamlit\n+certifi==2025.1.31\n+    # via\n+    #   httpcore\n+    #   httpx\n+    #   requests\n+charset-normalizer==3.4.1\n+    # via requests\n+click==8.1.8\n+    # via\n+    #   streamlit\n+    #   typer\n+contourpy==1.3.2\n+    # via matplotlib\n+cycler==0.12.1\n+    # via matplotlib\n+docstring-parser==0.16\n+    # via agno\n+fonttools==4.57.0\n+    # via matplotlib\n+gitdb==4.0.12\n+    # via gitpython\n+gitpython==3.1.44\n+    # via\n+    #   agno\n+    #   streamlit\n+google-auth==2.39.0\n+    # via google-genai\n+google-genai==1.11.0\n+    # via -r cookbook/examples/apps/gemini-tutor/requirements.in\n+googlesearch-python==1.3.0\n+    # via -r cookbook/examples/apps/gemini-tutor/requirements.in\n+h11==0.14.0\n+    # via httpcore\n+httpcore==1.0.8\n+    # via httpx\n+httpx==0.28.1\n+    # via\n+    #   agno\n+    #   google-genai\n+idna==3.10\n+    # via\n+    #   anyio\n+    #   httpx\n+    #   requests\n+jinja2==3.1.6\n+    # via\n+    #   altair\n+    #   pydeck\n+jsonschema==4.23.0\n+    # via altair\n+jsonschema-specifications==2024.10.1\n+    # via jsonschema\n+kiwisolver==1.4.8\n+    # via matplotlib\n+markdown-it-py==3.0.0\n+    # via rich\n+markupsafe==3.0.2\n+    # via jinja2\n+matplotlib==3.10.1\n+    # via -r cookbook/examples/apps/gemini-tutor/requirements.in\n+mdurl==0.1.2\n+    # via markdown-it-py\n+narwhals==1.35.0\n+    # via altair\n+nest-asyncio==1.6.0\n+    # via -r cookbook/examples/apps/gemini-tutor/requirements.in\n+numpy==2.2.4\n+    # via\n+    #   -r cookbook/examples/apps/gemini-tutor/requirements.in\n+    #   contourpy\n+    #   matplotlib\n+    #   pandas\n+    #   pydeck\n+    #   streamlit\n+packaging==24.2\n+    # via\n+    #   altair\n+    #   matplotlib\n+    #   streamlit\n+pandas==2.2.3\n+    # via\n+    #   -r cookbook/examples/apps/gemini-tutor/requirements.in\n+    #   streamlit\n+pillow==11.2.1\n+    # via\n+    #   -r cookbook/examples/apps/gemini-tutor/requirements.in\n+    #   matplotlib\n+    #   streamlit\n+protobuf==5.29.4\n+    # via streamlit\n+pyarrow==19.0.1\n+    # via streamlit\n+pyasn1==0.6.1\n+    # via\n+    #   pyasn1-modules\n+    #   rsa\n+pyasn1-modules==0.4.2\n+    # via google-auth\n+pycountry==24.6.1\n+    # via -r cookbook/examples/apps/gemini-tutor/requirements.in\n+pydantic==2.11.3\n+    # via\n+    #   -r cookbook/examples/apps/gemini-tutor/requirements.in\n+    #   agno\n+    #   google-genai\n+    #   pydantic-settings\n+pydantic-core==2.33.1\n+    # via pydantic\n+pydantic-settings==2.8.1\n+    # via agno\n+pydeck==0.9.1\n+    # via streamlit\n+pygments==2.19.1\n+    # via rich\n+pyparsing==3.2.3\n+    # via matplotlib\n+python-dateutil==2.9.0.post0\n+    # via\n+    #   matplotlib\n+    #   pandas\n+python-dotenv==1.1.0\n+    # via\n+    #   -r cookbook/examples/apps/gemini-tutor/requirements.in\n+    #   agno\n+    #   pydantic-settings\n+python-multipart==0.0.20\n+    # via agno\n+pytz==2025.2\n+    # via pandas\n+pyyaml==6.0.2\n+    # via agno\n+referencing==0.36.2\n+    # via\n+    #   jsonschema\n+    #   jsonschema-specifications\n+requests==2.32.3\n+    # via\n+    #   -r cookbook/examples/apps/gemini-tutor/requirements.in\n+    #   google-genai\n+    #   googlesearch-python\n+    #   streamlit\n+rich==14.0.0\n+    # via\n+    #   agno\n+    #   typer\n+rpds-py==0.24.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+rsa==4.9.1\n+    # via google-auth\n+shellingham==1.5.4\n+    # via typer\n+six==1.17.0\n+    # via python-dateutil\n+smmap==5.0.2\n+    # via gitdb\n+sniffio==1.3.1\n+    # via anyio\n+soupsieve==2.6\n+    # via beautifulsoup4\n+streamlit==1.44.1\n+    # via -r cookbook/examples/apps/gemini-tutor/requirements.in\n+tenacity==9.1.2\n+    # via streamlit\n+toml==0.10.2\n+    # via streamlit\n+tomli==2.2.1\n+    # via agno\n+tornado==6.4.2\n+    # via streamlit\n+typer==0.15.2\n+    # via agno\n+typing-extensions==4.13.2\n+    # via\n+    #   -r cookbook/examples/apps/gemini-tutor/requirements.in\n+    #   agno\n+    #   altair\n+    #   anyio\n+    #   beautifulsoup4\n+    #   google-genai\n+    #   pydantic\n+    #   pydantic-core\n+    #   referencing\n+    #   streamlit\n+    #   typer\n+    #   typing-inspection\n+typing-inspection==0.4.0\n+    # via pydantic\n+tzdata==2025.2\n+    # via pandas\n+urllib3==2.4.0\n+    # via requests\n+websockets==15.0.1\n+    # via google-genai\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/gemini-tutor/utils.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/gemini-tutor/utils.py b/cookbook/examples/streamlit_applications/gemini-tutor/utils.py\nnew file mode 100644\nindex 000000000..51eaedbd5\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/gemini-tutor/utils.py\n@@ -0,0 +1,103 @@\n+\"\"\"\n+Utility functions for Gemini Tutor\n+\"\"\"\n+\n+import json\n+from typing import Any, Dict, List, Optional\n+\n+import streamlit as st\n+from agno.models.message import Citations\n+from agno.utils.log import logger\n+\n+\n+def add_message(\n+    role: str, content: str, tool_calls: Optional[List[Dict[str, Any]]] = None, **kwargs\n+) -> None:\n+    \"\"\"\n+    Safely add a message to the session state.\n+\n+    Args:\n+        role: The role of the message sender (user/assistant)\n+        content: The text content of the message\n+        tool_calls: Optional tool calls to include\n+        **kwargs: Additional message attributes (image, audio, video paths)\n+    \"\"\"\n+    if \"messages\" not in st.session_state or not isinstance(\n+        st.session_state[\"messages\"], list\n+    ):\n+        st.session_state[\"messages\"] = []\n+\n+    message = {\"role\": role, \"content\": content, \"tool_calls\": tool_calls}\n+\n+    # Add any additional attributes like image, audio, or video paths\n+    for key, value in kwargs.items():\n+        message[key] = value\n+\n+    st.session_state[\"messages\"].append(message)\n+\n+\n+def display_tool_calls(container: Any, tool_calls: List[Dict[str, Any]]) -> None:\n+    \"\"\"\n+    Display tool calls in a formatted way.\n+\n+    Args:\n+        container: Streamlit container to display the tool calls\n+        tool_calls: List of tool call dictionaries\n+    \"\"\"\n+    if not tool_calls:\n+        return\n+\n+    with container:\n+        st.markdown(\"**Tool Calls:**\")\n+\n+        for i, tool_call in enumerate(tool_calls):\n+            # Format the tool call name\n+            tool_name = tool_call.get(\"name\", \"Unknown Tool\")\n+\n+            # Format the args as pretty JSON\n+            args = tool_call.get(\"arguments\", {})\n+            formatted_args = json.dumps(args, indent=2)\n+\n+            expander_label = f\"\ud83d\udccb Tool Call {i + 1}: {tool_name}\"\n+            with st.expander(expander_label, expanded=False):\n+                st.code(formatted_args, language=\"json\")\n+\n+\n+def display_grounding_metadata(citations: Optional[Citations]) -> None:\n+    \"\"\"\n+    Display search grounding metadata (sources) if available.\n+\n+    Args:\n+        citations: Citations object from the agent response chunk or final message.\n+    \"\"\"\n+    # Check if citations object exists and has the 'urls' attribute and it's not empty\n+    if not citations or not hasattr(citations, \"urls\") or not citations.urls:\n+        return\n+\n+    try:\n+        st.markdown(\"---\")\n+        st.markdown(\"### \ud83c\udf10 Sources\")\n+\n+        # Display grounding sources from the pre-parsed list\n+        for citation_url in citations.urls:\n+            # Ensure url and title exist\n+            if (\n+                hasattr(citation_url, \"url\")\n+                and citation_url.url\n+                and hasattr(citation_url, \"title\")\n+                and citation_url.title\n+            ):\n+                st.markdown(f\"- [{citation_url.title}]({citation_url.url})\")\n+            elif (\n+                hasattr(citation_url, \"url\") and citation_url.url\n+            ):  # Fallback if title is missing\n+                st.markdown(f\"- [{citation_url.url}]({citation_url.url})\")\n+\n+        # Optionally, display raw metadata in an expander for debugging if needed\n+        # if hasattr(citations, 'raw') and citations.raw:\n+        #     with st.expander(\"Raw Grounding Metadata (Debug)\"):\n+        #         st.json(citations.raw)\n+\n+    except Exception as e:\n+        logger.error(f\"Error displaying grounding metadata: {e}\", exc_info=True)\n+        st.warning(\"Could not display sources.\")\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/geobuddy/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/geobuddy/__init__.py b/cookbook/examples/streamlit_applications/geobuddy/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/geobuddy/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/geobuddy/app.py b/cookbook/examples/streamlit_applications/geobuddy/app.py\nnew file mode 100644\nindex 000000000..4a86f4d0d\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/geobuddy/app.py\n@@ -0,0 +1,89 @@\n+import os\n+from pathlib import Path\n+\n+import streamlit as st\n+from geography_buddy import analyze_image\n+from PIL import Image\n+\n+# Streamlit App Configuration\n+st.set_page_config(\n+    page_title=\"Geography Location Buddy\",\n+    page_icon=\"\ud83c\udf0d\",\n+)\n+st.title(\"GeoBuddy \ud83c\udf0d\")\n+st.markdown(\"##### :orange_heart: built by [agno](https://github.com/agno-agi/agno)\")\n+st.markdown(\n+    \"\"\"\n+    **Upload your image** and let model guess the location based on visual cues such as landmarks, architecture, and more.\n+    \"\"\"\n+)\n+\n+\n+def main() -> None:\n+    # Sidebar Design\n+    with st.sidebar:\n+        st.markdown(\"<br><br>\", unsafe_allow_html=True)\n+        st.markdown(\"let me guess the location based on visible cues from your image!\")\n+\n+        # Upload Image\n+        uploaded_file = st.file_uploader(\n+            \"\ud83d\udcf7 Upload here..\", type=[\"jpg\", \"jpeg\", \"png\"]\n+        )\n+        st.markdown(\"---\")\n+\n+    # App Logic\n+    if uploaded_file:\n+        col1, col2 = st.columns([1, 2])\n+\n+        # Display Uploaded Image\n+        with col1:\n+            st.markdown(\"#### Uploaded Image\")\n+            image = Image.open(uploaded_file)\n+            resized_image = image.resize((400, 400))\n+            image_path = Path(\"temp_image.png\")\n+            with open(image_path, \"wb\") as f:\n+                f.write(uploaded_file.getbuffer())\n+            st.image(resized_image, caption=\"Your Image\", use_container_width=True)\n+\n+        # Analyze Button and Output\n+        with col2:\n+            st.markdown(\"#### Location Analysis\")\n+            analyze_button = st.button(\"\ud83d\udd0d Analyze Image\")\n+\n+            if analyze_button:\n+                with st.spinner(\"Analyzing the image... please wait.\"):\n+                    try:\n+                        result = analyze_image(image_path)\n+                        if result:\n+                            st.success(\"\ud83c\udf0d Here's my guess:\")\n+                            st.markdown(result)\n+                        else:\n+                            st.warning(\n+                                \"Sorry, I couldn't determine the location. Try another image.\"\n+                            )\n+                    except Exception as e:\n+                        st.error(f\"An error occurred: {e}\")\n+\n+                # Cleanup after analysis\n+                if image_path.exists():\n+                    os.remove(image_path)\n+            else:\n+                st.info(\"Click the **Analyze** button to get started!\")\n+    else:\n+        st.info(\"\ud83d\udcf7 Please upload an image to begin location analysis.\")\n+\n+    # Footer Section\n+    st.markdown(\"---\")\n+    st.markdown(\n+        \"\"\"\n+        **\ud83c\udf1f Features**:\n+        - Identify locations based on uploaded images.\n+        - Advanced reasoning based on landmarks, architecture, and cultural clues.\n+\n+        **\ud83d\udce2 Disclaimer**: GeoBuddy's guesses are based on visual cues and analysis and may not always be accurate.\n+        \"\"\"\n+    )\n+    st.markdown(\":orange_heart: Thank you for using GeoBuddy!\")\n+\n+\n+main()\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/geobuddy/geography_buddy.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/geobuddy/geography_buddy.py b/cookbook/examples/streamlit_applications/geobuddy/geography_buddy.py\nnew file mode 100644\nindex 000000000..0d3344fb5\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/geobuddy/geography_buddy.py\n@@ -0,0 +1,48 @@\n+import os\n+from pathlib import Path\n+from typing import Optional\n+\n+from agno.agent import Agent\n+from agno.media import Image\n+from agno.models.google import Gemini\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+from dotenv import load_dotenv\n+\n+# Load environment variables\n+load_dotenv()\n+GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n+\n+# Define the query for geography identification\n+geo_query = \"\"\"\n+You are a geography expert. Your task is to analyze the given image and provide a reasoned guess of the location based on visible clues such as:\n+- Landmarks\n+- Architecture\n+- Natural features (mountains, rivers, coastlines)\n+- Language or symbols (text, street signs, billboards, any names mentioned in the picture as clue)\n+- People\u2019s clothing or cultural aspects\n+- Environmental clues like weather, time of day\n+\n+Return in this format:\n+Location Name, City, Country and Reasoning\n+Structure the response in markdown.\n+\n+Instructions:\n+1. Examine the image thoroughly.\n+2. Provide a reasoned guess for the street name, city, state, and country.\n+3. Explain your reasoning in detail by pointing out the visual clues that led to your conclusion.\n+4. If uncertain, offer possible guesses with reasoning.\n+\"\"\"\n+\n+# Initialize the GeoBuddy agent\n+geo_agent = Agent(\n+    model=Gemini(id=\"gemini-2.0-flash-exp\"), tools=[DuckDuckGoTools()], markdown=True\n+)\n+\n+\n+# Function to analyze the image and return location information\n+def analyze_image(image_path: Path) -> Optional[str]:\n+    try:\n+        response = geo_agent.run(geo_query, images=[Image(filepath=image_path)])\n+        return response.content\n+    except Exception as e:\n+        raise RuntimeError(f\"An error occurred while analyzing the image: {e}\")\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/geobuddy/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/geobuddy/requirements.txt b/cookbook/examples/streamlit_applications/geobuddy/requirements.txt\nnew file mode 100644\nindex 000000000..3efa5fc40\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/geobuddy/requirements.txt\n@@ -0,0 +1,6 @@\n+agno\n+google-generativeai\n+openai\n+streamlit\n+pillow\n+duckduckgo-search\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/github_mcp_agent/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/github_mcp_agent/__init__.py b/cookbook/examples/streamlit_applications/github_mcp_agent/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/github_mcp_agent/agents.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/github_mcp_agent/agents.py b/cookbook/examples/streamlit_applications/github_mcp_agent/agents.py\nnew file mode 100644\nindex 000000000..f6c74c8e8\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/github_mcp_agent/agents.py\n@@ -0,0 +1,46 @@\n+import os\n+from textwrap import dedent\n+\n+from agno.agent import Agent\n+from agno.tools.mcp import MCPTools\n+from mcp import ClientSession, StdioServerParameters\n+from mcp.client.stdio import stdio_client\n+\n+\n+async def run_github_agent(message):\n+    if not os.getenv(\"GITHUB_TOKEN\"):\n+        return \"Error: GitHub token not provided\"\n+\n+    try:\n+        server_params = StdioServerParameters(\n+            command=\"npx\",\n+            args=[\"-y\", \"@modelcontextprotocol/server-github\"],\n+        )\n+\n+        # Create client session\n+        async with stdio_client(server_params) as (read, write):\n+            async with ClientSession(read, write) as session:\n+                # Initialize MCP toolkit\n+                mcp_tools = MCPTools(session=session)\n+                await mcp_tools.initialize()\n+\n+                # Create agent\n+                agent = Agent(\n+                    tools=[mcp_tools],\n+                    instructions=dedent(\"\"\"\\\n+                        You are a GitHub assistant. Help users explore repositories and their activity.\n+                        - Provide organized, concise insights about the repository\n+                        - Focus on facts and data from the GitHub API\n+                        - Use markdown formatting for better readability\n+                        - Present numerical data in tables when appropriate\n+                        - Include links to relevant GitHub pages when helpful\n+                    \"\"\"),\n+                    markdown=True,\n+                    show_tool_calls=True,\n+                )\n+\n+                # Run agent\n+                response = await agent.arun(message)\n+                return response.content\n+    except Exception as e:\n+        return f\"Error: {str(e)}\"\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/github_mcp_agent/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/github_mcp_agent/app.py b/cookbook/examples/streamlit_applications/github_mcp_agent/app.py\nnew file mode 100644\nindex 000000000..5cb153927\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/github_mcp_agent/app.py\n@@ -0,0 +1,117 @@\n+import asyncio\n+import os\n+\n+import streamlit as st\n+from agents import run_github_agent\n+\n+# Page config\n+st.set_page_config(page_title=\"\ud83d\udc19 GitHub MCP Agent\", page_icon=\"\ud83d\udc19\", layout=\"wide\")\n+\n+# Title and description\n+st.markdown(\"<h1 class='main-header'>\ud83d\udc19 GitHub MCP Agent</h1>\", unsafe_allow_html=True)\n+st.markdown(\n+    \"Explore GitHub repositories with natural language using the Model Context Protocol\"\n+)\n+\n+# Setup sidebar for API key\n+with st.sidebar:\n+    st.header(\"\ud83d\udd11 Authentication\")\n+    github_token = st.text_input(\n+        \"GitHub Token\",\n+        type=\"password\",\n+        help=\"Create a token with repo scope at github.com/settings/tokens\",\n+    )\n+\n+    if github_token:\n+        os.environ[\"GITHUB_TOKEN\"] = github_token\n+\n+    st.markdown(\"---\")\n+    st.markdown(\"### Example Queries\")\n+\n+    st.markdown(\"**Issues**\")\n+    st.markdown(\"- Show me issues by label\")\n+    st.markdown(\"- What issues are being actively discussed?\")\n+\n+    st.markdown(\"**Pull Requests**\")\n+    st.markdown(\"- What PRs need review?\")\n+    st.markdown(\"- Show me recent merged PRs\")\n+\n+    st.markdown(\"**Repository**\")\n+    st.markdown(\"- Show repository health metrics\")\n+    st.markdown(\"- Show repository activity patterns\")\n+\n+    st.markdown(\"---\")\n+    st.caption(\n+        \"Note: Always specify the repository in your query if not already selected in the main input.\"\n+    )\n+\n+# Query input\n+col1, col2 = st.columns([3, 1])\n+with col1:\n+    repo = st.text_input(\"Repository\", value=\"agno-agi/agno\", help=\"Format: owner/repo\")\n+with col2:\n+    query_type = st.selectbox(\n+        \"Query Type\", [\"Issues\", \"Pull Requests\", \"Repository Activity\", \"Custom\"]\n+    )\n+\n+# Create predefined queries based on type\n+if query_type == \"Issues\":\n+    query_template = f\"Find issues labeled as bugs in {repo}\"\n+elif query_type == \"Pull Requests\":\n+    query_template = f\"Show me recent merged PRs in {repo}\"\n+elif query_type == \"Repository Activity\":\n+    query_template = f\"Analyze code quality trends in {repo}\"\n+else:\n+    query_template = \"\"\n+\n+query = st.text_area(\n+    \"Your Query\",\n+    value=query_template,\n+    placeholder=\"What would you like to know about this repository?\",\n+)\n+\n+# Run button\n+if st.button(\"\ud83d\ude80 Run Query\", type=\"primary\", use_container_width=True):\n+    if not github_token:\n+        st.error(\"Please enter your GitHub token in the sidebar\")\n+    elif not query:\n+        st.error(\"Please enter a query\")\n+    else:\n+        with st.spinner(\"Analyzing GitHub repository...\"):\n+            # Ensure the repository is explicitly mentioned in the query\n+            if repo and repo not in query:\n+                full_query = f\"{query} in {repo}\"\n+            else:\n+                full_query = query\n+\n+            result = asyncio.run(run_github_agent(full_query))\n+\n+        # Display results in a nice container\n+        st.markdown(\"### Results\")\n+        st.markdown(result)\n+\n+# Display help text for first-time users\n+if \"result\" not in locals():\n+    st.markdown(\n+        \"\"\"<div class='info-box'>\n+        <h4>How to use this app:</h4>\n+        <ol>\n+            <li>Enter your GitHub token in the sidebar</li>\n+            <li>Specify a repository (e.g., agno-agi/agno)</li>\n+            <li>Select a query type or write your own</li>\n+            <li>Click 'Run Query' to see results</li>\n+        </ol>\n+        <p><strong>Important Notes:</strong></p>\n+        <ul>\n+            <li>The Model Context Protocol (MCP) provides real-time access to GitHub repositories</li>\n+            <li>Queries work best when they focus on specific aspects like issues, PRs, or repository info</li>\n+            <li>More specific queries yield better results</li>\n+            <li>This app requires Node.js to be installed (for the npx command)</li>\n+        </ul>\n+        </div>\"\"\",\n+        unsafe_allow_html=True,\n+    )\n+\n+# Footer\n+st.markdown(\"---\")\n+st.write(\"Built with Streamlit, Agno, and Model Context Protocol \u2764\ufe0f\")\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/github_mcp_agent/generate_requirements.sh",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/github_mcp_agent/generate_requirements.sh b/cookbook/examples/streamlit_applications/github_mcp_agent/generate_requirements.sh\nnew file mode 100755\nindex 000000000..78f8c7cec\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/github_mcp_agent/generate_requirements.sh\n@@ -0,0 +1,12 @@\n+#!/bin/bash\n+\n+############################################################################\n+# Generate requirements.txt from requirements.in\n+############################################################################\n+\n+echo \"Generating requirements.txt\"\n+\n+CURR_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n+\n+UV_CUSTOM_COMPILE_COMMAND=\"./generate_requirements.sh\" \\\n+  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/github_mcp_agent/requirements.in",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/github_mcp_agent/requirements.in b/cookbook/examples/streamlit_applications/github_mcp_agent/requirements.in\nnew file mode 100644\nindex 000000000..a3d463785\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/github_mcp_agent/requirements.in\n@@ -0,0 +1,5 @@\n+streamlit\n+agno\n+mcp\n+openai\n+asyncio\n\\ No newline at end of file\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/github_mcp_agent/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/github_mcp_agent/requirements.txt b/cookbook/examples/streamlit_applications/github_mcp_agent/requirements.txt\nnew file mode 100644\nindex 000000000..b76a2e230\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/github_mcp_agent/requirements.txt\n@@ -0,0 +1,193 @@\n+# This file was autogenerated by uv via the following command:\n+#    ./generate_requirements.sh\n+agno==1.1.9\n+    # via -r cookbook/examples/apps/github_mcp_agent/requirements.in\n+altair==5.5.0\n+    # via streamlit\n+annotated-types==0.7.0\n+    # via pydantic\n+anyio==4.8.0\n+    # via\n+    #   httpx\n+    #   mcp\n+    #   openai\n+    #   sse-starlette\n+    #   starlette\n+asyncio==3.4.3\n+    # via -r cookbook/examples/apps/github_mcp_agent/requirements.in\n+attrs==25.1.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+blinker==1.9.0\n+    # via streamlit\n+cachetools==5.5.2\n+    # via streamlit\n+certifi==2025.1.31\n+    # via\n+    #   httpcore\n+    #   httpx\n+    #   requests\n+charset-normalizer==3.4.1\n+    # via requests\n+click==8.1.8\n+    # via\n+    #   streamlit\n+    #   typer\n+    #   uvicorn\n+distro==1.9.0\n+    # via openai\n+docstring-parser==0.16\n+    # via agno\n+gitdb==4.0.12\n+    # via gitpython\n+gitpython==3.1.44\n+    # via\n+    #   agno\n+    #   streamlit\n+h11==0.14.0\n+    # via\n+    #   httpcore\n+    #   uvicorn\n+httpcore==1.0.7\n+    # via httpx\n+httpx==0.28.1\n+    # via\n+    #   agno\n+    #   mcp\n+    #   openai\n+httpx-sse==0.4.0\n+    # via mcp\n+idna==3.10\n+    # via\n+    #   anyio\n+    #   httpx\n+    #   requests\n+jinja2==3.1.6\n+    # via\n+    #   altair\n+    #   pydeck\n+jiter==0.8.2\n+    # via openai\n+jsonschema==4.23.0\n+    # via altair\n+jsonschema-specifications==2024.10.1\n+    # via jsonschema\n+markdown-it-py==3.0.0\n+    # via rich\n+markupsafe==3.0.2\n+    # via jinja2\n+mcp==1.3.0\n+    # via -r cookbook/examples/apps/github_mcp_agent/requirements.in\n+mdurl==0.1.2\n+    # via markdown-it-py\n+narwhals==1.29.1\n+    # via altair\n+numpy==2.2.3\n+    # via\n+    #   pandas\n+    #   pydeck\n+    #   streamlit\n+openai==1.65.4\n+    # via -r cookbook/examples/apps/github_mcp_agent/requirements.in\n+packaging==24.2\n+    # via\n+    #   altair\n+    #   streamlit\n+pandas==2.2.3\n+    # via streamlit\n+pillow==11.1.0\n+    # via streamlit\n+protobuf==5.29.3\n+    # via streamlit\n+pyarrow==19.0.1\n+    # via streamlit\n+pydantic==2.10.6\n+    # via\n+    #   agno\n+    #   mcp\n+    #   openai\n+    #   pydantic-settings\n+pydantic-core==2.27.2\n+    # via pydantic\n+pydantic-settings==2.8.1\n+    # via\n+    #   agno\n+    #   mcp\n+pydeck==0.9.1\n+    # via streamlit\n+pygments==2.19.1\n+    # via rich\n+python-dateutil==2.9.0.post0\n+    # via pandas\n+python-dotenv==1.0.1\n+    # via\n+    #   agno\n+    #   pydantic-settings\n+python-multipart==0.0.20\n+    # via agno\n+pytz==2025.1\n+    # via pandas\n+pyyaml==6.0.2\n+    # via agno\n+referencing==0.36.2\n+    # via\n+    #   jsonschema\n+    #   jsonschema-specifications\n+requests==2.32.3\n+    # via streamlit\n+rich==13.9.4\n+    # via\n+    #   agno\n+    #   typer\n+rpds-py==0.23.1\n+    # via\n+    #   jsonschema\n+    #   referencing\n+shellingham==1.5.4\n+    # via typer\n+six==1.17.0\n+    # via python-dateutil\n+smmap==5.0.2\n+    # via gitdb\n+sniffio==1.3.1\n+    # via\n+    #   anyio\n+    #   openai\n+sse-starlette==2.2.1\n+    # via mcp\n+starlette==0.46.0\n+    # via\n+    #   mcp\n+    #   sse-starlette\n+streamlit==1.43.0\n+    # via -r cookbook/examples/apps/github_mcp_agent/requirements.in\n+tenacity==9.0.0\n+    # via streamlit\n+toml==0.10.2\n+    # via streamlit\n+tomli==2.2.1\n+    # via agno\n+tornado==6.4.2\n+    # via streamlit\n+tqdm==4.67.1\n+    # via openai\n+typer==0.15.2\n+    # via agno\n+typing-extensions==4.12.2\n+    # via\n+    #   agno\n+    #   altair\n+    #   anyio\n+    #   openai\n+    #   pydantic\n+    #   pydantic-core\n+    #   referencing\n+    #   streamlit\n+    #   typer\n+tzdata==2025.1\n+    # via pandas\n+urllib3==2.3.0\n+    # via requests\n+uvicorn==0.34.0\n+    # via mcp\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/github_repo_analyzer/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/github_repo_analyzer/__init__.py b/cookbook/examples/streamlit_applications/github_repo_analyzer/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/github_repo_analyzer/agents.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/github_repo_analyzer/agents.py b/cookbook/examples/streamlit_applications/github_repo_analyzer/agents.py\nnew file mode 100644\nindex 000000000..6aed1332f\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/github_repo_analyzer/agents.py\n@@ -0,0 +1,77 @@\n+from textwrap import dedent\n+from typing import Optional\n+\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+from agno.tools.github import GithubTools\n+\n+\n+def get_github_agent(debug_mode: bool = True) -> Optional[Agent]:\n+    \"\"\"\n+    Args:\n+        repo_name: Optional repository name (\"owner/repo\"). If None, agent relies on user query.\n+        debug_mode: Whether to enable debug mode for tool calls.\n+    \"\"\"\n+\n+    return Agent(\n+        model=OpenAIChat(id=\"gpt-4.1\"),\n+        description=dedent(\"\"\"\n+            You are an expert Code Reviewing Agent specializing in analyzing GitHub repositories,\n+            with a strong focus on detailed code reviews for Pull Requests.\n+            Use your tools to answer questions accurately and provide insightful analysis.\n+        \"\"\"),\n+        instructions=dedent(f\"\"\"\\\n+        **Core Task:** Analyze GitHub repositories and answer user questions based on the available tools and conversation history.\n+\n+        **Repository Context Management:**\n+        1.  **Context Persistence:** Once a target repository (owner/repo) is identified (either initially or from a user query like 'analyze owner/repo'), **MAINTAIN THAT CONTEXT** for all subsequent questions in the current conversation unless the user clearly specifies a *different* repository.\n+        2.  **Determining Context:** If no repository is specified in the *current* user query, **CAREFULLY REVIEW THE CONVERSATION HISTORY** to find the most recently established target repository. Use that repository context.\n+        3.  **Accuracy:** When extracting a repository name (owner/repo) from the query or history, **BE EXTREMELY CAREFUL WITH SPELLING AND FORMATTING**. Double-check against the user's exact input.\n+        4.  **Ambiguity:** If no repository context has been established in the conversation history and the current query doesn't specify one, **YOU MUST ASK THE USER** to clarify which repository (using owner/repo format) they are interested in before using tools that require a repository name.\n+\n+        **How to Answer Questions:**\n+        *   **Identify Key Information:** Understand the user's goal and the target repository (using the context rules above).\n+        *   **Select Appropriate Tools:** Choose the best tool(s) for the task, ensuring you provide the correct `repo_name` argument (owner/repo format, checked for accuracy) if required by the tool.\n+            *   Project Overview: `get_repository`, `get_file_content` (for README.md).\n+            *   Libraries/Dependencies: `get_file_content` (for requirements.txt, pyproject.toml, etc.), `get_directory_content`, `search_code`.\n+            *   PRs/Issues: Use relevant PR/issue tools.\n+            *   List User Repos: `list_repositories` (no repo_name needed).\n+            *   Search Repos: `search_repositories` (no repo_name needed).\n+        *   **Execute Tools:** Run the selected tools.\n+        *   **Synthesize Answer:** Combine tool results into a clear, concise answer using markdown. If a tool fails (e.g., 404 error because the repo name was incorrect), state that you couldn't find the specified repository and suggest checking the name.\n+        *   **Cite Sources:** Mention specific files (e.g., \"According to README.md...\").\n+\n+        **Specific Analysis Areas (Most require a specific repository):**\n+        *   Issues: Listing, summarizing, searching.\n+        *   Pull Requests (PRs): Listing, summarizing, searching, getting details/changes.\n+        *   Code & Files: Searching code, getting file content, listing directory contents.\n+        *   Repository Stats & Activity: Stars, contributors, recent activity.\n+\n+        **Code Review Guidelines (Requires repository and PR):**\n+        *   Fetch Changes: Use `get_pull_request_changes` or `get_pull_request_with_details`.\n+        *   Analyze Patch: Evaluate based on functionality, best practices, style, clarity, efficiency.\n+        *   Present Review: Structure clearly, cite lines/code, be constructive.\n+        \"\"\"),\n+        tools=[\n+            GithubTools(\n+                get_repository=True,\n+                search_repositories=True,\n+                get_pull_request=True,\n+                get_pull_request_changes=True,\n+                list_branches=True,\n+                get_pull_request_count=True,\n+                get_pull_requests=True,\n+                get_pull_request_comments=True,\n+                get_pull_request_with_details=True,\n+                list_issues=True,\n+                get_issue=True,\n+                update_file=True,\n+                get_file_content=True,\n+                get_directory_content=True,\n+                search_code=True,\n+            ),\n+        ],\n+        markdown=True,\n+        debug_mode=debug_mode,\n+        add_history_to_messages=True,\n+    )\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/github_repo_analyzer/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/github_repo_analyzer/app.py b/cookbook/examples/streamlit_applications/github_repo_analyzer/app.py\nnew file mode 100644\nindex 000000000..a19472a35\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/github_repo_analyzer/app.py\n@@ -0,0 +1,153 @@\n+from os import getenv\n+\n+import nest_asyncio\n+import streamlit as st\n+from agents import get_github_agent\n+from agno.agent import Agent\n+from agno.utils.log import logger\n+from utils import (\n+    CUSTOM_CSS,\n+    about_widget,\n+    add_message,\n+    display_tool_calls,\n+    sidebar_widget,\n+)\n+\n+nest_asyncio.apply()\n+st.set_page_config(\n+    page_title=\"GitHub Repo Analyzer\",\n+    page_icon=\"\ud83d\udc68\u200d\ud83d\udcbb\",\n+    layout=\"wide\",\n+)\n+\n+# Load custom CSS with dark mode support\n+st.markdown(CUSTOM_CSS, unsafe_allow_html=True)\n+\n+\n+def main() -> None:\n+    #####################################################################\n+    # App header\n+    ####################################################################\n+    st.markdown(\n+        \"<h1 class='main-header'>\ud83d\udc68\u200d\ud83d\udcbb GitHub Repo Analyzer</h1>\", unsafe_allow_html=True\n+    )\n+    st.markdown(\"Analyze GitHub repositories\")\n+\n+    ####################################################################\n+    # Initialize Agent\n+    ####################################################################\n+    github_agent: Agent\n+    if (\n+        \"github_agent\" not in st.session_state\n+        or st.session_state[\"github_agent\"] is None\n+    ):\n+        logger.info(\"---*--- Creating new Github agent ---*---\")\n+        github_agent = get_github_agent()\n+        st.session_state[\"github_agent\"] = github_agent\n+        st.session_state[\"messages\"] = []\n+        st.session_state[\"github_token\"] = getenv(\"GITHUB_ACCESS_TOKEN\")\n+    else:\n+        github_agent = st.session_state[\"github_agent\"]\n+\n+    ####################################################################\n+    # Load Agent Session from the database\n+    ####################################################################\n+    try:\n+        st.session_state[\"github_agent_session_id\"] = github_agent.load_session()\n+    except Exception:\n+        st.warning(\"Could not create Agent session, is the database running?\")\n+        return\n+\n+    ####################################################################\n+    # Load runs from memory (v2 Memory) only on initial load\n+    ####################################################################\n+    if github_agent.memory is not None and not st.session_state.get(\"messages\"):\n+        session_id = st.session_state.get(\"github_agent_session_id\")\n+        # Fetch stored runs for this session\n+        agent_runs = github_agent.memory.get_runs(session_id)\n+        if agent_runs:\n+            logger.debug(\"Loading run history\")\n+            st.session_state[\"messages\"] = []\n+            for run_response in agent_runs:\n+                # Iterate through stored messages in the run\n+                for msg in run_response.messages or []:\n+                    if msg.role in [\"user\", \"assistant\"] and msg.content is not None:\n+                        # Include any tool calls attached to this message\n+                        add_message(\n+                            msg.role, msg.content, getattr(msg, \"tool_calls\", None)\n+                        )\n+        else:\n+            logger.debug(\"No run history found\")\n+            st.session_state[\"messages\"] = []\n+\n+    ####################################################################\n+    # Sidebar\n+    ####################################################################\n+    sidebar_widget()\n+\n+    ####################################################################\n+    # Get user input\n+    ####################################################################\n+    if prompt := st.chat_input(\"\ud83d\udc4b Ask me about GitHub repositories!\"):\n+        add_message(\"user\", prompt)\n+\n+    ####################################################################\n+    # Display chat history\n+    ####################################################################\n+    for message in st.session_state[\"messages\"]:\n+        if message[\"role\"] in [\"user\", \"assistant\"]:\n+            _content = message[\"content\"]\n+            if _content is not None:\n+                with st.chat_message(message[\"role\"]):\n+                    # Display tool calls if they exist in the message\n+                    if \"tool_calls\" in message and message[\"tool_calls\"]:\n+                        display_tool_calls(st.empty(), message[\"tool_calls\"])\n+                    st.markdown(_content)\n+\n+    ####################################################################\n+    # Generate response for user message\n+    ####################################################################\n+    last_message = (\n+        st.session_state[\"messages\"][-1] if st.session_state[\"messages\"] else None\n+    )\n+    if last_message and last_message.get(\"role\") == \"user\":\n+        question = last_message[\"content\"]\n+        with st.chat_message(\"assistant\"):\n+            # Create container for tool calls\n+            tool_calls_container = st.empty()\n+            resp_container = st.empty()\n+            with st.spinner(\"\ud83e\udd14 Thinking...\"):\n+                response = \"\"\n+                try:\n+                    # Run the agent and stream the response\n+                    run_response = github_agent.run(\n+                        question, stream=True, stream_intermediate_steps=True\n+                    )\n+                    for _resp_chunk in run_response:\n+                        # Display tool calls if available\n+                        if _resp_chunk.tools and len(_resp_chunk.tools) > 0:\n+                            display_tool_calls(tool_calls_container, _resp_chunk.tools)\n+\n+                        # Display response if available and event is RunResponse\n+                        if (\n+                            _resp_chunk.event == \"RunResponse\"\n+                            and _resp_chunk.content is not None\n+                        ):\n+                            response += _resp_chunk.content\n+                            resp_container.markdown(response)\n+\n+                    add_message(\"assistant\", response, github_agent.run_response.tools)\n+                except Exception as e:\n+                    logger.exception(e)\n+                    error_message = f\"Sorry, I encountered an error: {str(e)}\"\n+                    add_message(\"assistant\", error_message)\n+                    st.error(error_message)\n+\n+    ####################################################################\n+    # About section\n+    ####################################################################\n+    about_widget()\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/github_repo_analyzer/generate_requirements.sh",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/github_repo_analyzer/generate_requirements.sh b/cookbook/examples/streamlit_applications/github_repo_analyzer/generate_requirements.sh\nnew file mode 100755\nindex 000000000..78f8c7cec\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/github_repo_analyzer/generate_requirements.sh\n@@ -0,0 +1,12 @@\n+#!/bin/bash\n+\n+############################################################################\n+# Generate requirements.txt from requirements.in\n+############################################################################\n+\n+echo \"Generating requirements.txt\"\n+\n+CURR_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n+\n+UV_CUSTOM_COMPILE_COMMAND=\"./generate_requirements.sh\" \\\n+  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/github_repo_analyzer/requirements.in",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/github_repo_analyzer/requirements.in b/cookbook/examples/streamlit_applications/github_repo_analyzer/requirements.in\nnew file mode 100644\nindex 000000000..fa2c19684\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/github_repo_analyzer/requirements.in\n@@ -0,0 +1,8 @@\n+# Direct dependencies for the GitHub Repo Chat App\n+agno>=0.1.0\n+PyGithub>=2.1.1\n+python-dotenv>=1.0.0\n+matplotlib>=3.7.2\n+pandas>=2.0.3\n+streamlit>=1.24.0\n+openai>=1.67.0\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/github_repo_analyzer/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/github_repo_analyzer/requirements.txt b/cookbook/examples/streamlit_applications/github_repo_analyzer/requirements.txt\nnew file mode 100644\nindex 000000000..b52ee7f54\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/github_repo_analyzer/requirements.txt\n@@ -0,0 +1,217 @@\n+# This file was autogenerated by uv via the following command:\n+#    ./generate_requirements.sh\n+agno==1.4.2\n+    # via -r cookbook/examples/apps/github_repo_analyzer/requirements.in\n+altair==5.5.0\n+    # via streamlit\n+annotated-types==0.7.0\n+    # via pydantic\n+anyio==4.9.0\n+    # via\n+    #   httpx\n+    #   openai\n+attrs==25.3.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+blinker==1.9.0\n+    # via streamlit\n+cachetools==5.5.2\n+    # via streamlit\n+certifi==2025.1.31\n+    # via\n+    #   httpcore\n+    #   httpx\n+    #   requests\n+cffi==1.17.1\n+    # via\n+    #   cryptography\n+    #   pynacl\n+charset-normalizer==3.4.1\n+    # via requests\n+click==8.1.8\n+    # via\n+    #   streamlit\n+    #   typer\n+contourpy==1.3.2\n+    # via matplotlib\n+cryptography==44.0.2\n+    # via pyjwt\n+cycler==0.12.1\n+    # via matplotlib\n+deprecated==1.2.18\n+    # via pygithub\n+distro==1.9.0\n+    # via openai\n+docstring-parser==0.16\n+    # via agno\n+fonttools==4.57.0\n+    # via matplotlib\n+gitdb==4.0.12\n+    # via gitpython\n+gitpython==3.1.44\n+    # via\n+    #   agno\n+    #   streamlit\n+h11==0.14.0\n+    # via httpcore\n+httpcore==1.0.8\n+    # via httpx\n+httpx==0.28.1\n+    # via\n+    #   agno\n+    #   openai\n+idna==3.10\n+    # via\n+    #   anyio\n+    #   httpx\n+    #   requests\n+jinja2==3.1.6\n+    # via\n+    #   altair\n+    #   pydeck\n+jiter==0.9.0\n+    # via openai\n+jsonschema==4.23.0\n+    # via altair\n+jsonschema-specifications==2024.10.1\n+    # via jsonschema\n+kiwisolver==1.4.8\n+    # via matplotlib\n+markdown-it-py==3.0.0\n+    # via rich\n+markupsafe==3.0.2\n+    # via jinja2\n+matplotlib==3.10.1\n+    # via -r cookbook/examples/apps/github_repo_analyzer/requirements.in\n+mdurl==0.1.2\n+    # via markdown-it-py\n+narwhals==1.35.0\n+    # via altair\n+numpy==2.2.4\n+    # via\n+    #   contourpy\n+    #   matplotlib\n+    #   pandas\n+    #   pydeck\n+    #   streamlit\n+openai==1.75.0\n+    # via -r cookbook/examples/apps/github_repo_analyzer/requirements.in\n+packaging==24.2\n+    # via\n+    #   altair\n+    #   matplotlib\n+    #   streamlit\n+pandas==2.2.3\n+    # via\n+    #   -r cookbook/examples/apps/github_repo_analyzer/requirements.in\n+    #   streamlit\n+pillow==11.2.1\n+    # via\n+    #   matplotlib\n+    #   streamlit\n+protobuf==5.29.4\n+    # via streamlit\n+pyarrow==19.0.1\n+    # via streamlit\n+pycparser==2.22\n+    # via cffi\n+pydantic==2.11.3\n+    # via\n+    #   agno\n+    #   openai\n+    #   pydantic-settings\n+pydantic-core==2.33.1\n+    # via pydantic\n+pydantic-settings==2.8.1\n+    # via agno\n+pydeck==0.9.1\n+    # via streamlit\n+pygithub==2.6.1\n+    # via -r cookbook/examples/apps/github_repo_analyzer/requirements.in\n+pygments==2.19.1\n+    # via rich\n+pyjwt==2.10.1\n+    # via pygithub\n+pynacl==1.5.0\n+    # via pygithub\n+pyparsing==3.2.3\n+    # via matplotlib\n+python-dateutil==2.9.0.post0\n+    # via\n+    #   matplotlib\n+    #   pandas\n+python-dotenv==1.1.0\n+    # via\n+    #   -r cookbook/examples/apps/github_repo_analyzer/requirements.in\n+    #   agno\n+    #   pydantic-settings\n+python-multipart==0.0.20\n+    # via agno\n+pytz==2025.2\n+    # via pandas\n+pyyaml==6.0.2\n+    # via agno\n+referencing==0.36.2\n+    # via\n+    #   jsonschema\n+    #   jsonschema-specifications\n+requests==2.32.3\n+    # via\n+    #   pygithub\n+    #   streamlit\n+rich==14.0.0\n+    # via\n+    #   agno\n+    #   typer\n+rpds-py==0.24.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+shellingham==1.5.4\n+    # via typer\n+six==1.17.0\n+    # via python-dateutil\n+smmap==5.0.2\n+    # via gitdb\n+sniffio==1.3.1\n+    # via\n+    #   anyio\n+    #   openai\n+streamlit==1.44.1\n+    # via -r cookbook/examples/apps/github_repo_analyzer/requirements.in\n+tenacity==9.1.2\n+    # via streamlit\n+toml==0.10.2\n+    # via streamlit\n+tomli==2.2.1\n+    # via agno\n+tornado==6.4.2\n+    # via streamlit\n+tqdm==4.67.1\n+    # via openai\n+typer==0.15.2\n+    # via agno\n+typing-extensions==4.13.2\n+    # via\n+    #   agno\n+    #   altair\n+    #   anyio\n+    #   openai\n+    #   pydantic\n+    #   pydantic-core\n+    #   pygithub\n+    #   referencing\n+    #   streamlit\n+    #   typer\n+    #   typing-inspection\n+typing-inspection==0.4.0\n+    # via pydantic\n+tzdata==2025.2\n+    # via pandas\n+urllib3==2.4.0\n+    # via\n+    #   pygithub\n+    #   requests\n+wrapt==1.17.2\n+    # via deprecated\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/github_repo_analyzer/utils.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/github_repo_analyzer/utils.py b/cookbook/examples/streamlit_applications/github_repo_analyzer/utils.py\nnew file mode 100644\nindex 000000000..3af5868e3\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/github_repo_analyzer/utils.py\n@@ -0,0 +1,188 @@\n+import json\n+from typing import Any, Dict, List, Optional\n+\n+import streamlit as st\n+from agno.utils.log import log_debug, log_error, log_info\n+\n+\n+def add_message(\n+    role: str, content: str, tool_calls: Optional[List[Dict[str, Any]]] = None\n+) -> None:\n+    \"\"\"Safely add a message to the session state\"\"\"\n+    if \"messages\" not in st.session_state or not isinstance(\n+        st.session_state[\"messages\"], list\n+    ):\n+        st.session_state[\"messages\"] = []\n+    st.session_state[\"messages\"].append(\n+        {\"role\": role, \"content\": content, \"tool_calls\": tool_calls}\n+    )\n+\n+\n+def sidebar_widget() -> None:\n+    \"\"\"Renders the sidebar for configuration and example queries.\"\"\"\n+    with st.sidebar:\n+        # Configuration\n+        st.header(\"Configuration\")\n+\n+        st.markdown(\"**GitHub Token**\")\n+        token_input = st.text_input(\n+            \"Enter your GitHub Personal Access Token (required for most queries):\",\n+            type=\"password\",\n+            key=\"github_token_input\",\n+            value=st.session_state.get(\"github_token\", \"\"),\n+            help=\"Allows the agent to access GitHub API, including your private/org data.\",\n+        )\n+        st.markdown(\n+            \"[How to create a GitHub PAT?](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens#creating-a-personal-access-token-classic)\",\n+            unsafe_allow_html=True,\n+        )\n+\n+        # Update session state if token input changes\n+        current_token_in_state = st.session_state.get(\"github_token\")\n+        if token_input != current_token_in_state and (\n+            token_input or current_token_in_state is not None\n+        ):\n+            st.session_state.github_token = token_input if token_input else None\n+            log_info(\n+                f\"GitHub token updated via sidebar input {'(cleared)' if not token_input else ''}.\"\n+            )\n+            st.session_state.github_agent = None\n+            st.rerun()\n+\n+        st.markdown(\"---\")\n+\n+        st.markdown(\"#### \ud83c\udfc6 Sample Queries\")\n+        if st.button(\"\ud83d\udccb Summarize 'agno-agi/agno'\"):\n+            # Run this query in the current session\n+            add_message(\"user\", \"Summarize 'agno-agi/agno' repo\")\n+        if st.button(\"\ud83e\udd47 List my recent repositories\"):\n+            add_message(\"user\", \"List my recent repositories\")\n+        if st.button(\"\ud83c\udfc6 List latest issues in 'agno-agi/agno' \"):\n+            add_message(\"user\", \"List latest issues in 'agno-agi/agno'\")\n+        if st.button(\"\ud83e\udd47 List recent PRs in 'agno-agi/agno'\"):\n+            add_message(\"user\", \"List recent PRs in 'agno-agi/agno' repo\")\n+        # Chat controls\n+        st.header(\"Chat\")\n+        if st.button(\"\ud83c\udd95 New Chat\"):\n+            # Use restart logic to clear everything and rerun\n+            restart_agent()\n+\n+\n+def about_widget() -> None:\n+    \"\"\"Display an about section in the sidebar\"\"\"\n+    with st.sidebar:\n+        st.markdown(\"### About Agno \u2728\")\n+        st.markdown(\"\"\"\n+        Agno is a lightweight library for building Reasoning Agents.\n+\n+        [GitHub](https://github.com/agno-agi/agno) | [Docs](https://docs.agno.com)\n+        \"\"\")\n+\n+        st.markdown(\"### Need Help?\")\n+        st.markdown(\n+            \"If you have any questions, catch us on [discord](https://agno.link/discord) or post in the community [forum](https://agno.link/community).\"\n+        )\n+\n+\n+def is_json(myjson):\n+    \"\"\"Check if a string is valid JSON\"\"\"\n+    try:\n+        json.loads(myjson)\n+    except (ValueError, TypeError):\n+        return False\n+    return True\n+\n+\n+def display_tool_calls(tool_calls_container, tools):\n+    \"\"\"Display tool calls in a streamlit container with expandable sections.\n+\n+    Args:\n+        tool_calls_container: Streamlit container to display the tool calls\n+        tools: List of tool call dictionaries containing name, args, content, and metrics\n+    \"\"\"\n+    try:\n+        with tool_calls_container.container():\n+            for tool_call in tools:\n+                tool_name = tool_call.get(\"tool_name\", \"Unknown Tool\")\n+                tool_args = tool_call.get(\"tool_args\", {})\n+                content = tool_call.get(\"content\", None)\n+                metrics = tool_call.get(\"metrics\", None)\n+\n+                # Add timing information\n+                execution_time_str = \"N/A\"\n+                try:\n+                    if metrics is not None and hasattr(metrics, \"time\"):\n+                        execution_time = metrics.time\n+                        if execution_time is not None:\n+                            execution_time_str = f\"{execution_time:.4f}s\"\n+                except Exception as e:\n+                    log_error(f\"Error displaying tool calls: {str(e)}\")\n+                    pass\n+\n+                with st.expander(\n+                    f\"\ud83d\udee0\ufe0f {tool_name.replace('_', ' ').title()} ({execution_time_str})\",\n+                    expanded=False,\n+                ):\n+                    # Show query with syntax highlighting\n+                    if isinstance(tool_args, dict) and \"query\" in tool_args:\n+                        st.code(tool_args[\"query\"], language=\"sql\")\n+\n+                    # Display arguments in a more readable format\n+                    if tool_args and tool_args != {\"query\": None}:\n+                        st.markdown(\"**Arguments:**\")\n+                        st.json(tool_args)\n+\n+                    if content is not None:\n+                        try:\n+                            if is_json(content):\n+                                st.markdown(\"**Results:**\")\n+                                st.json(content)\n+                        except Exception as e:\n+                            log_debug(f\"Skipped tool call content: {e}\")\n+    except Exception as e:\n+        log_error(f\"Error displaying tool calls: {str(e)}\")\n+        tool_calls_container.error(\"Failed to display tool results\")\n+\n+\n+def restart_agent():\n+    \"\"\"Reset the agent and clear chat history\"\"\"\n+    log_debug(\"---*--- Restarting agent ---*---\")\n+    st.session_state[\"sql_agent\"] = None\n+    st.session_state[\"sql_agent_session_id\"] = None\n+    st.session_state[\"messages\"] = []\n+    st.session_state[\"github_agent\"] = None\n+    st.rerun()\n+\n+\n+# Keep only necessary CSS styles\n+CUSTOM_CSS = \"\"\"\n+<style>\n+    .main-header {\n+        font-size: 2.5rem;\n+        margin-bottom: 1rem;\n+        color: #0366d6;\n+        font-weight: 600;\n+    }\n+    .sub-header {\n+        font-size: 1.5rem;\n+        margin-top: 1rem;\n+        margin-bottom: 0.5rem;\n+        color: #2f363d;\n+        font-weight: 500;\n+    }\n+    .metric-card {\n+        background-color: #f6f8fa;\n+        border-radius: 8px;\n+        padding: 1.2rem;\n+        margin-bottom: 1rem;\n+        border-left: 5px solid #0366d6;\n+    }\n+    .pr-card {\n+        background-color: #f1f8ff;\n+        border-radius: 8px;\n+        padding: 1.2rem;\n+        margin-bottom: 1.2rem;\n+        border-left: 5px solid #6f42c1;\n+    }\n+</style>\n+\"\"\"\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/image_generation/agents.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/image_generation/agents.py b/cookbook/examples/streamlit_applications/image_generation/agents.py\nnew file mode 100644\nindex 000000000..beed5eee7\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/image_generation/agents.py\n@@ -0,0 +1,75 @@\n+# Recipe agent for image generation\n+from textwrap import dedent\n+from typing import Optional\n+\n+from agno.agent import Agent\n+from agno.document.reader.pdf_reader import PDFImageReader\n+from agno.embedder.cohere import CohereEmbedder\n+from agno.knowledge.pdf import PDFKnowledgeBase\n+from agno.knowledge.pdf_url import PDFUrlKnowledgeBase\n+from agno.models.groq import Groq\n+from agno.tools.openai import OpenAITools\n+from agno.vectordb.pgvector import PgVector\n+\n+# Database connection string for recipe knowledge base\n+# Adjust as needed for your environment\n+DB_URL = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n+\n+# Constants for recipe agent\n+DEFAULT_RECIPE_URL = \"https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf\"\n+DEFAULT_RECIPE_TABLE = \"recipe_documents\"\n+\n+\n+def get_recipe_agent(\n+    local_pdf_path: Optional[str] = None,\n+) -> Agent:\n+    \"\"\"\n+    Returns a RecipeImageAgent backed by a recipe PDF knowledge base.\n+    \"\"\"\n+    # Choose the appropriate knowledge base\n+    if local_pdf_path:\n+        knowledge_base = PDFKnowledgeBase(\n+            path=local_pdf_path,\n+            reader=PDFImageReader(),\n+            vector_db=PgVector(\n+                db_url=DB_URL,\n+                table_name=DEFAULT_RECIPE_TABLE,\n+                embedder=CohereEmbedder(id=\"embed-v4.0\"),\n+            ),\n+        )\n+    else:\n+        knowledge_base = PDFUrlKnowledgeBase(\n+            urls=[DEFAULT_RECIPE_URL],\n+            vector_db=PgVector(\n+                db_url=DB_URL,\n+                table_name=DEFAULT_RECIPE_TABLE,\n+                embedder=CohereEmbedder(id=\"embed-v4.0\"),\n+            ),\n+        )\n+\n+    model = Groq(id=\"meta-llama/llama-4-scout-17b-16e-instruct\")\n+\n+    # Instantiate and return the recipe agent\n+    return Agent(\n+        name=\"RecipeImageAgent\",\n+        model=model,\n+        knowledge=knowledge_base,\n+        tools=[OpenAITools(image_model=\"gpt-image-1\")],\n+        instructions=[\n+            dedent(\"\"\"\\\n+            You are a specialized recipe assistant.\n+            When asked for a recipe:\n+            1. Use the `search_knowledge_base` tool to find and load the most relevant recipe from the knowledge base.\n+            2. Extract and output exactly two formatted markdown sections:\n+               ## Ingredients\n+               - List each ingredient with a hyphen and space prefix.\n+               ## Directions\n+               1. Describe each cooking step succinctly, numbering steps starting at 1.\n+            3. After listing the Directions, invoke the `generate_image` tool exactly once, passing the entire recipe text and using a prompt like '<DishName>: a step-by-step visual guide showing all steps in one overhead image with bright natural lighting. In the prompt make sure to include the all the recipe ingredients and directions that were listed in the Ingredients and Directions sections.'.\n+            4. Maintain a consistent visual style across the image.\n+            5. After the image is generated, conclude with 'Recipe generation complete.'\n+        \"\"\"),\n+        ],\n+        markdown=True,\n+        debug_mode=True,\n+    )\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/image_generation/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/image_generation/app.py b/cookbook/examples/streamlit_applications/image_generation/app.py\nnew file mode 100644\nindex 000000000..50f01ccf3\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/image_generation/app.py\n@@ -0,0 +1,193 @@\n+import base64\n+import io\n+\n+import nest_asyncio\n+import streamlit as st\n+from agents import get_recipe_agent\n+from agno.utils.log import logger\n+from PIL import Image\n+from utils import (\n+    CUSTOM_CSS,\n+    about_widget,\n+    add_message,\n+    display_tool_calls,\n+    example_inputs,\n+)\n+\n+nest_asyncio.apply()\n+st.set_page_config(\n+    page_title=\"Recipe Image Generator\",\n+    page_icon=\"\ud83c\udf73\",\n+    layout=\"wide\",\n+    initial_sidebar_state=\"expanded\",\n+)\n+\n+# Load custom CSS with dark mode support\n+st.markdown(CUSTOM_CSS, unsafe_allow_html=True)\n+\n+\n+def main() -> None:\n+    ####################################################################\n+    # App header\n+    ####################################################################\n+    st.markdown(\n+        \"<h1 class='main-title'>Recipe Image Generator</h1>\",\n+        unsafe_allow_html=True,\n+    )\n+    st.markdown(\n+        \"<p class='subtitle'>Upload your recipe PDF or use the default. Ask for a recipe and receive step-by-step images!</p>\",\n+        unsafe_allow_html=True,\n+    )\n+\n+    ####################################################################\n+    # Model selector\n+    ####################################################################\n+    model_options = {\n+        \"llama-4-scout\": \"groq:meta-llama/llama-4-scout-17b-16e-instruct\",\n+    }\n+    selected_model = st.sidebar.selectbox(\n+        \"Select a model\",\n+        options=list(model_options.keys()),\n+        index=0,\n+        key=\"model_selector\",\n+    )\n+    model_id = model_options[selected_model]\n+\n+    example_inputs()\n+    ####################################################################\n+    # Recipe source selector & Agent initialization\n+    ####################################################################\n+    uploaded_file = st.sidebar.file_uploader(\"Upload recipe PDF\", type=[\"pdf\"])\n+    use_default = st.sidebar.checkbox(\n+        \"Use default sample recipe book\", value=(uploaded_file is None)\n+    )\n+    pdf_path = None\n+    if uploaded_file:\n+        import tempfile\n+\n+        tf = tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\")\n+        tf.write(uploaded_file.read())\n+        tf.flush()\n+        pdf_path = tf.name\n+    if use_default:\n+        pdf_path = None\n+\n+    if (\n+        \"recipe_agent\" not in st.session_state\n+        or st.session_state.get(\"pdf_path\") != pdf_path\n+        or st.session_state.get(\"current_model\") != model_id\n+    ):\n+        logger.info(\"---*--- Creating new Recipe agent ---*---\")\n+        recipe_agent = get_recipe_agent(\n+            local_pdf_path=pdf_path,\n+        )\n+        st.session_state[\"recipe_agent\"] = recipe_agent\n+        st.session_state[\"pdf_path\"] = pdf_path\n+        st.session_state[\"current_model\"] = model_id\n+    else:\n+        recipe_agent = st.session_state[\"recipe_agent\"]\n+\n+    # Track knowledge load state\n+    if \"knowledge_loaded\" not in st.session_state:\n+        st.session_state[\"knowledge_loaded\"] = False\n+\n+    # Manual load button\n+    if st.sidebar.button(\"Load recipes\"):\n+        st.sidebar.info(\"Loading default recipes...\")\n+        recipe_agent.knowledge.load(recreate=True)\n+        st.session_state[\"knowledge_loaded\"] = True\n+        st.sidebar.success(\"Recipes loaded!\")\n+\n+    # Initialize chat history\n+    if \"messages\" not in st.session_state:\n+        st.session_state[\"messages\"] = []\n+\n+    ####################################################################\n+    # Get user input\n+    ####################################################################\n+    if prompt := st.chat_input(\"\ud83d\udc4b Ask me for a recipe (e.g., 'Recipe for Pad Thai')\"):\n+        add_message(\"user\", prompt)\n+\n+    ####################################################################\n+    # Display chat history\n+    ####################################################################\n+    for message in st.session_state[\"messages\"]:\n+        if message[\"role\"] in [\"user\", \"assistant\"]:\n+            _content = message[\"content\"]\n+            if _content is not None:\n+                with st.chat_message(message[\"role\"]):\n+                    # Display tool calls if they exist in the message\n+                    if \"tool_calls\" in message and message[\"tool_calls\"]:\n+                        display_tool_calls(st.empty(), message[\"tool_calls\"])\n+                    st.markdown(_content)\n+\n+    ####################################################################\n+    # Generate response for user message\n+    ####################################################################\n+    last_message = (\n+        st.session_state[\"messages\"][-1] if st.session_state[\"messages\"] else None\n+    )\n+    if last_message and last_message.get(\"role\") == \"user\":\n+        question = last_message[\"content\"]\n+        # Auto-load knowledge if needed\n+        if not st.session_state.get(\"knowledge_loaded\", False):\n+            info = st.info(\"Loading default recipes...\")\n+            recipe_agent.knowledge.load(recreate=True)\n+            st.session_state[\"knowledge_loaded\"] = True\n+            info.empty()\n+        with st.chat_message(\"assistant\"):\n+            # Create container for tool calls\n+            tool_calls_container = st.empty()\n+            resp_container = st.empty()\n+            with st.spinner(\"\ud83e\udd14 Thinking...\"):\n+                response = \"\"\n+                try:\n+                    # Run the agent and stream the response\n+                    run_response = recipe_agent.run(\n+                        question, stream=True, stream_intermediate_steps=True\n+                    )\n+                    for _resp_chunk in run_response:\n+                        # Display tool calls if available\n+                        if _resp_chunk.tools and len(_resp_chunk.tools) > 0:\n+                            display_tool_calls(tool_calls_container, _resp_chunk.tools)\n+\n+                        # Display response if available and event is RunResponse\n+                        if (\n+                            _resp_chunk.event == \"RunResponse\"\n+                            and _resp_chunk.content is not None\n+                        ):\n+                            response += _resp_chunk.content\n+                            resp_container.markdown(response)\n+\n+                    # Display generated images\n+                    for img in recipe_agent.run_response.images or []:\n+                        # Inline base64 content\n+                        if getattr(img, \"content\", None):\n+                            try:\n+                                # img.content is base64-encoded bytes\n+                                decoded = base64.b64decode(img.content)\n+                                image = Image.open(io.BytesIO(decoded))\n+                                resp_container.image(image)\n+                            except Exception as e:\n+                                logger.error(f\"Failed to render inline image: {e}\")\n+                                # Fallback to URL if available\n+                                if getattr(img, \"url\", None):\n+                                    resp_container.image(img.url)\n+                        # URL fallback\n+                        elif getattr(img, \"url\", None):\n+                            resp_container.image(img.url)\n+                    add_message(\"assistant\", response, recipe_agent.run_response.tools)\n+                except Exception as e:\n+                    logger.exception(e)\n+                    error_message = f\"Sorry, I encountered an error: {str(e)}\"\n+                    add_message(\"assistant\", error_message)\n+                    st.error(error_message)\n+\n+    ####################################################################\n+    # About section\n+    ####################################################################\n+    about_widget()\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/image_generation/generate_requirements.sh",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/image_generation/generate_requirements.sh b/cookbook/examples/streamlit_applications/image_generation/generate_requirements.sh\nnew file mode 100755\nindex 000000000..78f8c7cec\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/image_generation/generate_requirements.sh\n@@ -0,0 +1,12 @@\n+#!/bin/bash\n+\n+############################################################################\n+# Generate requirements.txt from requirements.in\n+############################################################################\n+\n+echo \"Generating requirements.txt\"\n+\n+CURR_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n+\n+UV_CUSTOM_COMPILE_COMMAND=\"./generate_requirements.sh\" \\\n+  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/image_generation/requirements.in",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/image_generation/requirements.in b/cookbook/examples/streamlit_applications/image_generation/requirements.in\nnew file mode 100644\nindex 000000000..06a09f6b0\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/image_generation/requirements.in\n@@ -0,0 +1,12 @@\n+agno\n+cohere\n+groq\n+httpx\n+nest_asyncio\n+openai\n+pypdf\n+pgvector\n+psycopg[binary]\n+rapidocr-onnxruntime\n+sqlalchemy\n+streamlit\n\\ No newline at end of file\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/image_generation/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/image_generation/requirements.txt b/cookbook/examples/streamlit_applications/image_generation/requirements.txt\nnew file mode 100644\nindex 000000000..bade39e7a\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/image_generation/requirements.txt\n@@ -0,0 +1,266 @@\n+# This file was autogenerated by uv via the following command:\n+#    ./generate_requirements.sh\n+agno==1.4.5\n+    # via -r cookbook/examples/apps/image_generation/requirements.in\n+altair==5.5.0\n+    # via streamlit\n+annotated-types==0.7.0\n+    # via pydantic\n+anyio==4.9.0\n+    # via\n+    #   groq\n+    #   httpx\n+    #   openai\n+attrs==25.3.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+blinker==1.9.0\n+    # via streamlit\n+cachetools==5.5.2\n+    # via streamlit\n+certifi==2025.4.26\n+    # via\n+    #   httpcore\n+    #   httpx\n+    #   requests\n+charset-normalizer==3.4.2\n+    # via requests\n+click==8.1.8\n+    # via\n+    #   streamlit\n+    #   typer\n+cohere==5.15.0\n+    # via -r cookbook/examples/apps/image_generation/requirements.in\n+coloredlogs==15.0.1\n+    # via onnxruntime\n+distro==1.9.0\n+    # via\n+    #   groq\n+    #   openai\n+docstring-parser==0.16\n+    # via agno\n+fastavro==1.10.0\n+    # via cohere\n+filelock==3.18.0\n+    # via huggingface-hub\n+flatbuffers==25.2.10\n+    # via onnxruntime\n+fsspec==2025.3.2\n+    # via huggingface-hub\n+gitdb==4.0.12\n+    # via gitpython\n+gitpython==3.1.44\n+    # via\n+    #   agno\n+    #   streamlit\n+groq==0.24.0\n+    # via -r cookbook/examples/apps/image_generation/requirements.in\n+h11==0.16.0\n+    # via httpcore\n+hf-xet==1.1.0\n+    # via huggingface-hub\n+httpcore==1.0.9\n+    # via httpx\n+httpx==0.28.1\n+    # via\n+    #   -r cookbook/examples/apps/image_generation/requirements.in\n+    #   agno\n+    #   cohere\n+    #   groq\n+    #   openai\n+httpx-sse==0.4.0\n+    # via cohere\n+huggingface-hub==0.31.1\n+    # via tokenizers\n+humanfriendly==10.0\n+    # via coloredlogs\n+idna==3.10\n+    # via\n+    #   anyio\n+    #   httpx\n+    #   requests\n+jinja2==3.1.6\n+    # via\n+    #   altair\n+    #   pydeck\n+jiter==0.9.0\n+    # via openai\n+jsonschema==4.23.0\n+    # via altair\n+jsonschema-specifications==2025.4.1\n+    # via jsonschema\n+markdown-it-py==3.0.0\n+    # via rich\n+markupsafe==3.0.2\n+    # via jinja2\n+mdurl==0.1.2\n+    # via markdown-it-py\n+mpmath==1.3.0\n+    # via sympy\n+narwhals==1.38.2\n+    # via altair\n+nest-asyncio==1.6.0\n+    # via -r cookbook/examples/apps/image_generation/requirements.in\n+numpy==2.2.5\n+    # via\n+    #   onnxruntime\n+    #   opencv-python\n+    #   pandas\n+    #   pgvector\n+    #   pydeck\n+    #   rapidocr-onnxruntime\n+    #   shapely\n+    #   streamlit\n+onnxruntime==1.22.0\n+    # via rapidocr-onnxruntime\n+openai==1.78.0\n+    # via -r cookbook/examples/apps/image_generation/requirements.in\n+opencv-python==4.11.0.86\n+    # via rapidocr-onnxruntime\n+packaging==24.2\n+    # via\n+    #   altair\n+    #   huggingface-hub\n+    #   onnxruntime\n+    #   streamlit\n+pandas==2.2.3\n+    # via streamlit\n+pgvector==0.4.1\n+    # via -r cookbook/examples/apps/image_generation/requirements.in\n+pillow==11.2.1\n+    # via\n+    #   rapidocr-onnxruntime\n+    #   streamlit\n+protobuf==6.30.2\n+    # via\n+    #   onnxruntime\n+    #   streamlit\n+psycopg==3.2.7\n+    # via -r cookbook/examples/apps/image_generation/requirements.in\n+psycopg-binary==3.2.7\n+    # via psycopg\n+pyarrow==20.0.0\n+    # via streamlit\n+pyclipper==1.3.0.post6\n+    # via rapidocr-onnxruntime\n+pydantic==2.11.4\n+    # via\n+    #   agno\n+    #   cohere\n+    #   groq\n+    #   openai\n+    #   pydantic-settings\n+pydantic-core==2.33.2\n+    # via\n+    #   cohere\n+    #   pydantic\n+pydantic-settings==2.9.1\n+    # via agno\n+pydeck==0.9.1\n+    # via streamlit\n+pygments==2.19.1\n+    # via rich\n+pypdf==5.4.0\n+    # via -r cookbook/examples/apps/image_generation/requirements.in\n+python-dateutil==2.9.0.post0\n+    # via pandas\n+python-dotenv==1.1.0\n+    # via\n+    #   agno\n+    #   pydantic-settings\n+python-multipart==0.0.20\n+    # via agno\n+pytz==2025.2\n+    # via pandas\n+pyyaml==6.0.2\n+    # via\n+    #   agno\n+    #   huggingface-hub\n+    #   rapidocr-onnxruntime\n+rapidocr-onnxruntime==1.4.4\n+    # via -r cookbook/examples/apps/image_generation/requirements.in\n+referencing==0.36.2\n+    # via\n+    #   jsonschema\n+    #   jsonschema-specifications\n+requests==2.32.3\n+    # via\n+    #   cohere\n+    #   huggingface-hub\n+    #   streamlit\n+rich==14.0.0\n+    # via\n+    #   agno\n+    #   typer\n+rpds-py==0.24.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+shapely==2.1.0\n+    # via rapidocr-onnxruntime\n+shellingham==1.5.4\n+    # via typer\n+six==1.17.0\n+    # via\n+    #   python-dateutil\n+    #   rapidocr-onnxruntime\n+smmap==5.0.2\n+    # via gitdb\n+sniffio==1.3.1\n+    # via\n+    #   anyio\n+    #   groq\n+    #   openai\n+sqlalchemy==2.0.40\n+    # via -r cookbook/examples/apps/image_generation/requirements.in\n+streamlit==1.45.0\n+    # via -r cookbook/examples/apps/image_generation/requirements.in\n+sympy==1.14.0\n+    # via onnxruntime\n+tenacity==9.1.2\n+    # via streamlit\n+tokenizers==0.21.1\n+    # via cohere\n+toml==0.10.2\n+    # via streamlit\n+tomli==2.2.1\n+    # via agno\n+tornado==6.4.2\n+    # via streamlit\n+tqdm==4.67.1\n+    # via\n+    #   huggingface-hub\n+    #   openai\n+    #   rapidocr-onnxruntime\n+typer==0.15.3\n+    # via agno\n+types-requests==2.32.0.20250328\n+    # via cohere\n+typing-extensions==4.13.2\n+    # via\n+    #   agno\n+    #   altair\n+    #   anyio\n+    #   cohere\n+    #   groq\n+    #   huggingface-hub\n+    #   openai\n+    #   psycopg\n+    #   pydantic\n+    #   pydantic-core\n+    #   referencing\n+    #   sqlalchemy\n+    #   streamlit\n+    #   typer\n+    #   typing-inspection\n+typing-inspection==0.4.0\n+    # via\n+    #   pydantic\n+    #   pydantic-settings\n+tzdata==2025.2\n+    # via pandas\n+urllib3==2.4.0\n+    # via\n+    #   requests\n+    #   types-requests\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/image_generation/utils.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/image_generation/utils.py b/cookbook/examples/streamlit_applications/image_generation/utils.py\nnew file mode 100644\nindex 000000000..6ffe10407\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/image_generation/utils.py\n@@ -0,0 +1,132 @@\n+import json\n+from typing import Any, Dict, List, Optional\n+\n+import streamlit as st\n+from agno.utils.log import logger\n+\n+\n+def is_json(myjson: str) -> bool:\n+    \"\"\"Check if a string is valid JSON\"\"\"\n+    try:\n+        json.loads(myjson)\n+    except (ValueError, TypeError):\n+        return False\n+    return True\n+\n+\n+def add_message(\n+    role: str,\n+    content: str,\n+    tool_calls: Optional[List[Dict[str, Any]]] = None,\n+) -> None:\n+    \"\"\"Safely add a message to the session state\"\"\"\n+    if \"messages\" not in st.session_state or not isinstance(\n+        st.session_state[\"messages\"], list\n+    ):\n+        st.session_state[\"messages\"] = []\n+    st.session_state[\"messages\"].append(\n+        {\"role\": role, \"content\": content, \"tool_calls\": tool_calls}\n+    )\n+\n+\n+def display_tool_calls(tool_calls_container: Any, tools: List[Dict[str, Any]]) -> None:\n+    \"\"\"Display tool calls in a Streamlit container\"\"\"\n+    try:\n+        with tool_calls_container.container():\n+            for tool_call in tools:\n+                tool_name = tool_call.get(\"tool_name\", \"Unknown Tool\")\n+                tool_args = tool_call.get(\"tool_args\", {})\n+                content = tool_call.get(\"content\")\n+                metrics = tool_call.get(\"metrics\")\n+\n+                execution_time_str = \"N/A\"\n+                if metrics is not None and hasattr(metrics, \"time\"):\n+                    t = metrics.time\n+                    execution_time_str = f\"{t:.4f}s\" if t else execution_time_str\n+\n+                with st.expander(\n+                    f\"\ud83d\udee0\ufe0f {tool_name.replace('_', ' ').title()} ({execution_time_str})\",\n+                    expanded=False,\n+                ):\n+                    if isinstance(tool_args, dict) and \"query\" in tool_args:\n+                        st.code(tool_args[\"query\"], language=\"sql\")\n+                    if tool_args and tool_args != {\"query\": None}:\n+                        st.markdown(\"**Arguments:**\")\n+                        st.json(tool_args)\n+                    if content is not None and is_json(content):\n+                        st.markdown(\"**Results:**\")\n+                        st.json(content)\n+    except Exception as e:\n+        logger.error(f\"Error displaying tool calls: {e}\")\n+        tool_calls_container.error(\"Failed to display tool results\")\n+\n+\n+def about_widget() -> None:\n+    \"\"\"Display an about section in the sidebar\"\"\"\n+    with st.sidebar:\n+        st.markdown(\"### About Recipe Generator \u2728\")\n+        st.markdown(\n+            \"Recipe Image Generator powered by Agno. Upload or use default recipe PDF and get step-by-step visual cooking instructions.\"\n+        )\n+\n+\n+# Added example inputs for recipe generation\n+def example_inputs() -> None:\n+    \"\"\"Show example recipe inputs on the sidebar.\"\"\"\n+    with st.sidebar:\n+        st.markdown(\"#### :sparkles: Try an example recipe\")\n+        if st.button(\"Recipe for Pad Thai\"):\n+            add_message(\"user\", \"Recipe for Pad Thai\")\n+        if st.button(\"Recipe for Som Tum\"):\n+            add_message(\"user\", \"Recipe for Som Tum / Papaya Salad\")\n+        if st.button(\"Recipe for Massaman Curry\"):\n+            add_message(\"user\", \"Recipe for Massaman Curry / Massaman Gai\")\n+        if st.button(\"Recipe for Tom Kha Gai\"):\n+            add_message(\"user\", \"Recipe for Tom Kha Gai\")\n+\n+\n+CUSTOM_CSS = \"\"\"\n+<style>\n+.main-title {\n+    text-align: center;\n+    background: linear-gradient(45deg, #FF4B2B, #FF416C);\n+    -webkit-background-clip: text;\n+    -webkit-text-fill-color: transparent;\n+    font-size: 3em;\n+    font-weight: bold;\n+    padding: 1em 0;\n+}\n+.subtitle {\n+    text-align: center;\n+    color: #666;\n+    margin-bottom: 2em;\n+}\n+.stButton button {\n+    width: 100%;\n+    border-radius: 20px;\n+    margin: 0.2em 0;\n+    transition: all 0.3s ease;\n+}\n+.stButton button:hover {\n+    transform: translateY(-2px);\n+    box-shadow: 0 5px 15px rgba(0,0,0,0.1);\n+}\n+.chat-container {\n+    border-radius: 15px;\n+    padding: 1em;\n+    margin: 1em 0;\n+    background-color: #f5f5f5;\n+}\n+.success-message {\n+    background-color: #d4edda;\n+    color: #155724;\n+}\n+.error-message {\n+    background-color: #f8d7da;\n+    color: #721c24;\n+}\n+@media (prefers-color-scheme: dark) {\n+    .chat-container { background-color: #2b2b2b; }\n+}\n+</style>\n+\"\"\"\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/llama_tutor/.env.example",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/llama_tutor/.env.example b/cookbook/examples/streamlit_applications/llama_tutor/.env.example\nnew file mode 100644\nindex 000000000..543509a25\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/llama_tutor/.env.example\n@@ -0,0 +1,2 @@\n+GROQ_API_KEY=<your_groq_api_key>\n+EXA_API_KEY=<your_exa_api_key>\n\\ No newline at end of file\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/llama_tutor/.gitignore",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/llama_tutor/.gitignore b/cookbook/examples/streamlit_applications/llama_tutor/.gitignore\nnew file mode 100644\nindex 000000000..52a40145c\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/llama_tutor/.gitignore\n@@ -0,0 +1,5 @@\n+output\n+agents.db\n+.venv/\n+*.env\n+**/__pycache__/**\n\\ No newline at end of file\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/llama_tutor/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/llama_tutor/__init__.py b/cookbook/examples/streamlit_applications/llama_tutor/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/llama_tutor/agents.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/llama_tutor/agents.py b/cookbook/examples/streamlit_applications/llama_tutor/agents.py\nnew file mode 100644\nindex 000000000..e6c23c85a\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/llama_tutor/agents.py\n@@ -0,0 +1,190 @@\n+\"\"\"\n+Llama tutor integrates:\n+  - DuckDuckGoTools for real-time web searches.\n+  - ExaTools for structured, in-depth analysis.\n+  - FileTools for saving the output upon user confirmation.\n+\"\"\"\n+\n+import os\n+import uuid\n+from datetime import datetime\n+from pathlib import Path\n+from typing import Optional\n+\n+from dotenv import load_dotenv\n+\n+# Load environment variables from .env file\n+load_dotenv(override=True)\n+\n+# Importing the Agent and model classes\n+from agno.agent import Agent\n+from agno.models.groq import Groq\n+\n+# Importing storage and tool classes\n+from agno.storage.agent.sqlite import SqliteAgentStorage\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.tools.exa import ExaTools\n+from agno.tools.file import FileTools\n+\n+# Import the Agent template\n+from prompts import AGENT_DESCRIPTION, AGENT_INSTRUCTIONS, EXPECTED_OUTPUT_TEMPLATE\n+\n+# ************* Setup Paths *************\n+# Define the current working directory and output directory for saving files\n+cwd = Path(__file__).parent\n+output_dir = cwd.joinpath(\"output\")\n+# Create output directory if it doesn't exist\n+output_dir.mkdir(parents=True, exist_ok=True)\n+# Create tmp directory if it doesn't exist\n+tmp_dir = cwd.joinpath(\"tmp\")\n+tmp_dir.mkdir(parents=True, exist_ok=True)\n+# *************************************\n+\n+# ************* Agent Storage *************\n+# Configure SQLite storage for agent sessions\n+agent_storage = SqliteAgentStorage(\n+    table_name=\"answer_engine_sessions\",  # Table to store agent sessions\n+    db_file=str(tmp_dir.joinpath(\"agents.db\")),  # SQLite database file\n+)\n+# *************************************\n+\n+\n+def tutor_agent(\n+    user_id: Optional[str] = None,\n+    model_id: str = \"groq:llama-3.3-70b-versatile\",\n+    session_id: Optional[str] = None,\n+    num_history_responses: int = 5,\n+    debug_mode: bool = True,\n+    education_level: str = \"High School\",\n+) -> Agent:\n+    \"\"\"\n+    Returns an instance of Llama Tutor, an educational AI assistant with integrated tools for web search,\n+    deep contextual analysis, and file management.\n+\n+    Llama Tutor will:\n+      - Use DuckDuckGoTools for real-time web searches and ExaTools for in-depth analysis to gather information.\n+      - Generate comprehensive educational answers tailored to the specified education level that include:\n+          \u2022 Direct, succinct answers appropriate for the student's level.\n+          \u2022 Detailed explanations with supporting evidence.\n+          \u2022 Examples and clarification of common misconceptions.\n+          \u2022 Interactive elements like questions to check understanding.\n+      - Prompt the user:\n+            \"Would you like to save this answer to a file? (yes/no)\"\n+        If confirmed, it will use FileTools to save the answer in markdown format in the output directory.\n+\n+    Args:\n+        user_id: Optional identifier for the user.\n+        model_id: Model identifier in the format 'groq:model_name' (e.g., \"groq:llama-3.3-70b-versatile\").\n+                 Will always use Groq with a Llama model regardless of provider specified.\n+        session_id: Optional session identifier for tracking conversation history.\n+        num_history_responses: Number of previous responses to include for context.\n+        debug_mode: Enable logging and debug features.\n+        education_level: Education level for tailoring responses (e.g., \"Elementary School\", \"High School\", \"College\").\n+\n+    Returns:\n+        An instance of the configured Agent.\n+    \"\"\"\n+\n+    # Parse model provider and name\n+    provider, model_name = model_id.split(\":\")\n+\n+    # Always use Groq with Llama model\n+    groq_api_key = os.environ.get(\"GROQ_API_KEY\")\n+\n+    # Default to llama-3.3-70b-versatile if the model name doesn't contain \"llama\"\n+    if \"llama\" not in model_name.lower():\n+        model_name = \"llama-3.3-70b-versatile\"\n+\n+    model = Groq(id=model_name, api_key=groq_api_key)\n+\n+    # Get Exa API key from environment variable\n+    exa_api_key = os.environ.get(\"EXA_API_KEY\")\n+\n+    # Tools for Llama Tutor\n+    tools = [\n+        ExaTools(\n+            api_key=exa_api_key,\n+            start_published_date=datetime.now().strftime(\"%Y-%m-%d\"),\n+            type=\"keyword\",\n+            num_results=10,\n+        ),\n+        DuckDuckGoTools(\n+            timeout=20,\n+            fixed_max_results=5,\n+        ),\n+        FileTools(base_dir=output_dir),\n+    ]\n+\n+    # Modify the description to include the education level\n+    tutor_description = f\"\"\"You are Llama Tutor, an educational AI assistant designed to teach concepts at a {education_level} level.\n+    You have the following tools at your disposal:\n+      - DuckDuckGoTools for real-time web searches to fetch up-to-date information.\n+      - ExaTools for structured, in-depth analysis.\n+      - FileTools for saving the output upon user confirmation.\n+\n+    Your response should always be clear, concise, and detailed, tailored to a {education_level} student's understanding.\n+    Blend direct answers with extended analysis, supporting evidence, illustrative examples, and clarifications on common misconceptions.\n+    Engage the user with follow-up questions to check understanding and deepen learning.\n+\n+    <critical>\n+    - Before you answer, you must search both DuckDuckGo and ExaTools to generate your answer. If you don't, you will be penalized.\n+    - You must provide sources, whenever you provide a data point or a statistic.\n+    - When the user asks a follow-up question, you can use the previous answer as context.\n+    - If you don't have the relevant information, you must search both DuckDuckGo and ExaTools to generate your answer.\n+    - Always adapt your explanations to a {education_level} level of understanding.\n+    </critical>\"\"\"\n+\n+    # Modify the instructions to include the education level\n+    tutor_instructions = f\"\"\"Here's how you should answer the user's question:\n+\n+    1. Gather Relevant Information\n+      - First, carefully analyze the query to identify the intent of the user.\n+      - Break down the query into core components, then construct 1-3 precise search terms that help cover all possible aspects of the query.\n+      - Then, search using BOTH `duckduckgo_search` and `search_exa` with the search terms. Remember to search both tools.\n+      - Combine the insights from both tools to craft a comprehensive and balanced answer.\n+      - If you need to get the contents from a specific URL, use the `get_contents` tool with the URL as the argument.\n+      - CRITICAL: BEFORE YOU ANSWER, YOU MUST SEARCH BOTH DuckDuckGo and Exa to generate your answer, otherwise you will be penalized.\n+\n+    2. Construct Your Response\n+      - **Start** with a succinct, clear and direct answer that immediately addresses the user's query, tailored to a {education_level} level.\n+      - **Then expand** the answer by including:\n+          \u2022 A clear explanation with context and definitions appropriate for {education_level} students.\n+          \u2022 Supporting evidence such as statistics, real-world examples, and data points that are understandable at a {education_level} level.\n+          \u2022 Clarifications that address common misconceptions students at this level might have.\n+      - Structure your response with clear headings, bullet points, and organized paragraphs to make it easy to follow.\n+      - Include interactive elements like questions to check understanding or mini-quizzes when appropriate.\n+      - Use analogies and examples that would be familiar to students at a {education_level} level.\n+\n+    3. Enhance Engagement\n+      - After generating your answer, ask the user if they would like to save this answer to a file? (yes/no)\"\n+      - If the user wants to save the response, use FileTools to save the response in markdown format in the output directory.\n+      - Suggest follow-up topics or questions that might deepen their understanding.\n+\n+    4. Final Quality Check & Presentation \u2728\n+      - Review your response to ensure clarity, depth, and engagement.\n+      - Ensure the language and concepts are appropriate for a {education_level} level.\n+      - Make complex ideas accessible without oversimplifying to the point of inaccuracy.\n+\n+    5. In case of any uncertainties, clarify limitations and encourage follow-up queries.\"\"\"\n+\n+    return Agent(\n+        name=\"Llama Tutor\",\n+        model=model,\n+        user_id=user_id,\n+        session_id=session_id or str(uuid.uuid4()),\n+        storage=agent_storage,\n+        tools=tools,\n+        # Allow Llama Tutor to read both chat history and tool call history for better context.\n+        read_chat_history=True,\n+        read_tool_call_history=True,\n+        # Append previous conversation responses into the new messages for context.\n+        add_history_to_messages=True,\n+        num_history_responses=num_history_responses,\n+        add_datetime_to_instructions=True,\n+        add_name_to_instructions=True,\n+        description=tutor_description,\n+        instructions=tutor_instructions,\n+        expected_output=EXPECTED_OUTPUT_TEMPLATE,\n+        debug_mode=debug_mode,\n+        markdown=True,\n+    )\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/llama_tutor/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/llama_tutor/app.py b/cookbook/examples/streamlit_applications/llama_tutor/app.py\nnew file mode 100644\nindex 000000000..01b0a2f41\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/llama_tutor/app.py\n@@ -0,0 +1,185 @@\n+import nest_asyncio\n+import streamlit as st\n+from agents import tutor_agent\n+from agno.utils.log import logger\n+from utils import (\n+    CUSTOM_CSS,\n+    about_widget,\n+    add_message,\n+    display_tool_calls,\n+    rename_session_widget,\n+    session_selector_widget,\n+    sidebar_widget,\n+)\n+\n+nest_asyncio.apply()\n+\n+# Page configuration\n+st.set_page_config(\n+    page_title=\"Llama Tutor: Learn Anything\",\n+    page_icon=\":book:\",\n+    layout=\"wide\",\n+    initial_sidebar_state=\"expanded\",\n+)\n+\n+# Load custom CSS with dark mode support\n+st.markdown(CUSTOM_CSS, unsafe_allow_html=True)\n+\n+\n+def main() -> None:\n+    ####################################################################\n+    # App header\n+    ####################################################################\n+    st.markdown(\"<h1 class='main-title'>Llama Tutor</h1>\", unsafe_allow_html=True)\n+    st.markdown(\n+        \"<p class='subtitle'>Your intelligent answer engine powered by Agno</p>\",\n+        unsafe_allow_html=True,\n+    )\n+\n+    ####################################################################\n+    # Model configuration - always use Llama 3.3 70B\n+    ####################################################################\n+    model_id = \"groq:llama-3.3-70b-versatile\"\n+\n+    ####################################################################\n+    # Education level selector\n+    ####################################################################\n+    education_levels = [\n+        \"Elementary School\",\n+        \"Middle School\",\n+        \"High School\",\n+        \"College\",\n+        \"Undergrad\",\n+        \"Graduate\",\n+    ]\n+\n+    selected_education_level = st.sidebar.selectbox(\n+        \"Education Level\",\n+        options=education_levels,\n+        index=2,  # Default to High School\n+        key=\"education_level_selector\",\n+    )\n+\n+    # Store the education level in session state\n+    if \"education_level\" not in st.session_state:\n+        st.session_state[\"education_level\"] = selected_education_level\n+    elif st.session_state[\"education_level\"] != selected_education_level:\n+        st.session_state[\"education_level\"] = selected_education_level\n+        # Reset the agent if education level changes\n+        if \"llama_tutor\" in st.session_state:\n+            st.session_state[\"llama_tutor\"] = None\n+\n+    ####################################################################\n+    # Initialize Agent\n+    ####################################################################\n+    llama_tutor: Agent\n+    if (\n+        \"llama_tutor\" not in st.session_state\n+        or st.session_state[\"llama_tutor\"] is None\n+        or st.session_state.get(\"current_model\") != model_id\n+    ):\n+        logger.info(\"---*--- Creating new Llama Tutor agent ---*---\")\n+        llama_tutor = tutor_agent(\n+            model_id=model_id, education_level=st.session_state[\"education_level\"]\n+        )\n+        st.session_state[\"llama_tutor\"] = llama_tutor\n+        st.session_state[\"current_model\"] = model_id\n+    else:\n+        llama_tutor = st.session_state[\"llama_tutor\"]\n+\n+    ####################################################################\n+    # Load Agent Session from the database\n+    ####################################################################\n+    try:\n+        st.session_state[\"llama_tutor_session_id\"] = llama_tutor.load_session()\n+    except Exception:\n+        st.warning(\"Could not create Agent session, is the database running?\")\n+        return\n+\n+    ####################################################################\n+    # Load runs from memory\n+    ####################################################################\n+    agent_runs = llama_tutor.memory.runs\n+    if len(agent_runs) > 0:\n+        logger.debug(\"Loading run history\")\n+        st.session_state[\"messages\"] = []\n+        for _run in agent_runs:\n+            if _run.message is not None:\n+                add_message(_run.message.role, _run.message.content)\n+            if _run.response is not None:\n+                add_message(\"assistant\", _run.response.content, _run.response.tools)\n+    else:\n+        logger.debug(\"No run history found\")\n+        st.session_state[\"messages\"] = []\n+\n+    ####################################################################\n+    # Sidebar\n+    ####################################################################\n+    sidebar_widget()\n+\n+    ####################################################################\n+    # Get user input\n+    ####################################################################\n+    if prompt := st.chat_input(\"\u2728 What would you like to learn about?\"):\n+        add_message(\"user\", prompt)\n+\n+    ####################################################################\n+    # Display chat history\n+    ####################################################################\n+    for message in st.session_state[\"messages\"]:\n+        if message[\"role\"] in [\"user\", \"assistant\"]:\n+            _content = message[\"content\"]\n+            if _content is not None:\n+                with st.chat_message(message[\"role\"]):\n+                    # Display tool calls if they exist in the message\n+                    if \"tool_calls\" in message and message[\"tool_calls\"]:\n+                        display_tool_calls(st.empty(), message[\"tool_calls\"])\n+                    st.markdown(_content)\n+\n+    ####################################################################\n+    # Generate response for user message\n+    ####################################################################\n+    last_message = (\n+        st.session_state[\"messages\"][-1] if st.session_state[\"messages\"] else None\n+    )\n+    if last_message and last_message.get(\"role\") == \"user\":\n+        question = last_message[\"content\"]\n+        with st.chat_message(\"assistant\"):\n+            # Create container for tool calls\n+            tool_calls_container = st.empty()\n+            resp_container = st.empty()\n+            with st.spinner(\":book: Llama Tutor is preparing your lesson...\"):\n+                response = \"\"\n+                try:\n+                    # Run the agent and stream the response\n+                    run_response = llama_tutor.run(question, stream=True)\n+                    for _resp_chunk in run_response:\n+                        # Display tool calls if available\n+                        if _resp_chunk.tools and len(_resp_chunk.tools) > 0:\n+                            display_tool_calls(tool_calls_container, _resp_chunk.tools)\n+\n+                        # Display response\n+                        if _resp_chunk.content is not None:\n+                            response += _resp_chunk.content\n+                            resp_container.markdown(response)\n+\n+                    add_message(\"assistant\", response, llama_tutor.run_response.tools)\n+                except Exception as e:\n+                    error_message = f\"Sorry, I encountered an error: {str(e)}\"\n+                    add_message(\"assistant\", error_message)\n+                    st.error(error_message)\n+\n+    ####################################################################\n+    # Session selector\n+    ####################################################################\n+    session_selector_widget(llama_tutor, model_id)\n+    rename_session_widget(llama_tutor)\n+\n+    ####################################################################\n+    # About section\n+    ####################################################################\n+    about_widget()\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/llama_tutor/prompts.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/llama_tutor/prompts.py b/cookbook/examples/streamlit_applications/llama_tutor/prompts.py\nnew file mode 100644\nindex 000000000..6faeac318\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/llama_tutor/prompts.py\n@@ -0,0 +1,96 @@\n+\"\"\"Templates for the Answer Engine.\"\"\"\n+\n+from textwrap import dedent\n+\n+AGENT_DESCRIPTION = dedent(\"\"\"\\\n+    You are Llama Tutor, a cutting-edge Answer Engine built to deliver precise, context-rich, and engaging responses.\n+    You have the following tools at your disposal:\n+      - DuckDuckGoTools for real-time web searches to fetch up-to-date information.\n+      - ExaTools for structured, in-depth analysis.\n+      - FileTools for saving the output upon user confirmation.\n+\n+    Your response should always be clear, concise, and detailed. Blend direct answers with extended analysis,\n+    supporting evidence, illustrative examples, and clarifications on common misconceptions. Engage the user\n+    with follow-up questions, such as asking if they'd like to save the answer.\n+\n+    <critical>\n+    - Before you answer, you must search both DuckDuckGo and ExaTools to generate your answer. If you don't, you will be penalized.\n+    - You must provide sources, whenever you provide a data point or a statistic.\n+    - When the user asks a follow-up question, you can use the previous answer as context.\n+    - If you don't have the relevant information, you must search both DuckDuckGo and ExaTools to generate your answer.\n+    </critical>\\\n+\"\"\")\n+\n+AGENT_INSTRUCTIONS = dedent(\"\"\"\\\n+    Here's how you should answer the user's question:\n+\n+    1. Gather Relevant Information\n+      - First, carefully analyze the query to identify the intent of the user.\n+      - Break down the query into core components, then construct 1-3 precise search terms that help cover all possible aspects of the query.\n+      - Then, search using BOTH `duckduckgo_search` and `search_exa` with the search terms. Remember to search both tools.\n+      - Combine the insights from both tools to craft a comprehensive and balanced answer.\n+      - If you need to get the contents from a specific URL, use the `get_contents` tool with the URL as the argument.\n+      - CRITICAL: BEFORE YOU ANSWER, YOU MUST SEARCH BOTH DuckDuckGo and Exa to generate your answer, otherwise you will be penalized.\n+\n+    2. Construct Your Response\n+      - **Start** with a succinct, clear and direct answer that immediately addresses the user's query.\n+      - **Then expand** the answer by including:\n+          \u2022 A clear explanation with context and definitions.\n+          \u2022 Supporting evidence such as statistics, real-world examples, and data points.\n+          \u2022 Clarifications that address common misconceptions.\n+      - Expand the answer only if the query requires more detail. Simple questions like: \"What is the weather in Tokyo?\" or \"What is the capital of France?\" don't need an in-depth analysis.\n+      - Ensure the response is structured so that it provides quick answers as well as in-depth analysis for further exploration.\n+\n+    3. Enhance Engagement\n+      - After generating your answer, ask the user if they would like to save this answer to a file? (yes/no)\"\n+      - If the user wants to save the response, use FileTools to save the response in markdown format in the output directory.\n+\n+    4. Final Quality Check & Presentation \u2728\n+      - Review your response to ensure clarity, depth, and engagement.\n+      - Strive to be both informative for quick queries and thorough for detailed exploration.\n+\n+    5. In case of any uncertainties, clarify limitations and encourage follow-up queries.\\\n+\"\"\")\n+\n+EXPECTED_OUTPUT_TEMPLATE = dedent(\"\"\"\\\n+    {# If this is the first message, include the question title #}\n+    {% if this is the first message %}\n+    ## {An engaging title for this report. Keep it short.}\n+    {% endif %}\n+\n+    **{A clear and direct response that answers the question.}**\n+\n+    {# If the query requires more detail, include the sections below #}\n+    {% if detailed_response %}\n+\n+    ### {Secion title}\n+    {Add detailed analysis & explanation in this section}\n+    {A comprehensive breakdown covering key insights, context, and definitions.}\n+\n+    ### {Section title}\n+    {Add evidence & support in this section}\n+    {Add relevant data points and statistics in this section}\n+    {Add links or names of reputable sources supporting the answer in this section}\n+\n+    ### {Section title}\n+    {Add real-world examples or case studies that help illustrate the key points in this section}\n+\n+    ### {Section title}\n+    {Add clarifications addressing any common misunderstandings related to the topic in this section}\n+\n+    ### {Section title}\n+    {Add further details, implications, or suggestions for ongoing exploration in this section}\n+    {% endif %}\n+\n+    {Add any more sections you think are relevant, covering all the aspects of the query}\n+\n+    ### Sources\n+    - [1] {Source 1 url}\n+    - [2] {Source 2 url}\n+    - [3] {Source 3 url}\n+    - {any more sources you think are relevant}\n+\n+    Generated by Llama Tutor on: {current_time}\n+\n+    Stay curious and keep exploring \u2728\\\n+    \"\"\")\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/llama_tutor/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/llama_tutor/requirements.txt b/cookbook/examples/streamlit_applications/llama_tutor/requirements.txt\nnew file mode 100644\nindex 000000000..de2fb0df9\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/llama_tutor/requirements.txt\n@@ -0,0 +1,235 @@\n+agno==1.1.11\n+altair==5.5.0\n+    # via streamlit\n+annotated-types==0.7.0\n+    # via pydantic\n+anyio==4.8.0\n+    # via\n+    #   anthropic\n+    #   groq\n+    #   httpx\n+    #   openai\n+attrs==25.3.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+blinker==1.9.0\n+    # via streamlit\n+cachetools==5.5.2\n+    # via\n+    #   google-auth\n+    #   streamlit\n+certifi==2025.1.31\n+    # via\n+    #   httpcore\n+    #   httpx\n+    #   requests\n+charset-normalizer==3.4.1\n+    # via requests\n+click==8.1.8\n+    # via\n+    #   duckduckgo-search\n+    #   streamlit\n+    #   typer\n+distro==1.9.0\n+    # via\n+    #   anthropic\n+    #   groq\n+    #   openai\n+docstring-parser==0.16\n+    # via agno\n+duckduckgo-search==7.5.2\n+exa-py==1.9.0\n+gitdb==4.0.12\n+    # via gitpython\n+gitpython==3.1.44\n+    # via\n+    #   agno\n+    #   streamlit\n+groq==0.19.0\n+    # via -r cookbook/examples/apps/answer_engine/requirements.in\n+grpcio==1.71.0\n+    # via\n+    #   google-api-core\n+    #   grpcio-status\n+grpcio-status==1.71.0\n+    # via google-api-core\n+h11==0.14.0\n+    # via httpcore\n+httpcore==1.0.7\n+    # via httpx\n+httplib2==0.22.0\n+    # via\n+    #   google-api-python-client\n+    #   google-auth-httplib2\n+httpx==0.28.1\n+    # via\n+    #   agno\n+    #   anthropic\n+    #   groq\n+    #   openai\n+idna==3.10\n+    # via\n+    #   anyio\n+    #   httpx\n+    #   requests\n+jinja2==3.1.6\n+    # via\n+    #   altair\n+    #   pydeck\n+jiter==0.9.0\n+    # via\n+    #   anthropic\n+    #   openai\n+jsonschema==4.23.0\n+    # via altair\n+jsonschema-specifications==2024.10.1\n+    # via jsonschema\n+lxml==5.3.1\n+    # via duckduckgo-search\n+markdown-it-py==3.0.0\n+    # via rich\n+markupsafe==3.0.2\n+    # via jinja2\n+mdurl==0.1.2\n+    # via markdown-it-py\n+narwhals==1.30.0\n+    # via altair\n+nest-asyncio==1.6.0\n+    # via -r cookbook/examples/apps/answer_engine/requirements.in\n+numpy==2.2.3\n+    # via\n+    #   pandas\n+    #   pydeck\n+    #   streamlit\n+openai==1.66.3\n+    # via\n+    #   -r cookbook/examples/apps/answer_engine/requirements.in\n+    #   exa-py\n+packaging==24.2\n+    # via\n+    #   altair\n+    #   streamlit\n+pandas==2.2.3\n+    # via streamlit\n+pillow==11.1.0\n+    # via streamlit\n+primp==0.14.0\n+    # via duckduckgo-search\n+protobuf==5.29.3\n+    # via\n+    #   google-ai-generativelanguage\n+    #   google-api-core\n+    #   google-generativeai\n+    #   googleapis-common-protos\n+    #   grpcio-status\n+    #   proto-plus\n+    #   streamlit\n+pyarrow==19.0.1\n+    # via streamlit\n+pyasn1==0.6.1\n+    # via\n+    #   pyasn1-modules\n+    #   rsa\n+pyasn1-modules==0.4.1\n+    # via google-auth\n+pydantic==2.10.6\n+    # via\n+    #   agno\n+    #   anthropic\n+    #   google-generativeai\n+    #   groq\n+    #   openai\n+    #   pydantic-settings\n+pydantic-core==2.27.2\n+    # via pydantic\n+pydantic-settings==2.8.1\n+    # via agno\n+pydeck==0.9.1\n+    # via streamlit\n+pygments==2.19.1\n+    # via rich\n+pyparsing==3.2.1\n+    # via httplib2\n+python-dateutil==2.9.0.post0\n+    # via pandas\n+python-dotenv==1.0.1\n+    # via\n+    #   agno\n+    #   pydantic-settings\n+python-multipart==0.0.20\n+    # via agno\n+pytz==2025.1\n+    # via pandas\n+pyyaml==6.0.2\n+    # via agno\n+referencing==0.36.2\n+    # via\n+    #   jsonschema\n+    #   jsonschema-specifications\n+requests==2.32.3\n+    # via\n+    #   exa-py\n+    #   google-api-core\n+    #   google-search-results\n+    #   streamlit\n+rich==13.9.4\n+    # via\n+    #   agno\n+    #   typer\n+rpds-py==0.23.1\n+    # via\n+    #   jsonschema\n+    #   referencing\n+rsa==4.9\n+    # via google-auth\n+shellingham==1.5.4\n+    # via typer\n+six==1.17.0\n+    # via python-dateutil\n+smmap==5.0.2\n+    # via gitdb\n+sniffio==1.3.1\n+    # via\n+    #   anthropic\n+    #   anyio\n+    #   groq\n+    #   openai\n+sqlalchemy==2.0.39\n+streamlit==1.43.2\n+tenacity==9.0.0\n+    # via streamlit\n+toml==0.10.2\n+    # via streamlit\n+tomli==2.2.1\n+    # via agno\n+tornado==6.4.2\n+    # via streamlit\n+tqdm==4.67.1\n+    # via\n+    #   google-generativeai\n+    #   openai\n+typer==0.15.2\n+    # via agno\n+typing-extensions==4.12.2\n+    # via\n+    #   agno\n+    #   altair\n+    #   anthropic\n+    #   anyio\n+    #   exa-py\n+    #   google-generativeai\n+    #   groq\n+    #   openai\n+    #   pydantic\n+    #   pydantic-core\n+    #   referencing\n+    #   sqlalchemy\n+    #   streamlit\n+    #   typer\n+tzdata==2025.1\n+    # via pandas\n+uritemplate==4.1.1\n+    # via google-api-python-client\n+urllib3==2.3.0\n+    # via requests\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/llama_tutor/utils.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/llama_tutor/utils.py b/cookbook/examples/streamlit_applications/llama_tutor/utils.py\nnew file mode 100644\nindex 000000000..d08836008\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/llama_tutor/utils.py\n@@ -0,0 +1,297 @@\n+from typing import Any, Dict, List, Optional\n+\n+import streamlit as st\n+from agents import tutor_agent\n+from agno.agent.agent import Agent\n+from agno.utils.log import logger\n+\n+\n+def add_message(\n+    role: str, content: str, tool_calls: Optional[List[Dict[str, Any]]] = None\n+) -> None:\n+    \"\"\"Safely add a message to the session state.\"\"\"\n+    if \"messages\" not in st.session_state or not isinstance(\n+        st.session_state[\"messages\"], list\n+    ):\n+        st.session_state[\"messages\"] = []\n+    st.session_state[\"messages\"].append(\n+        {\"role\": role, \"content\": content, \"tool_calls\": tool_calls}\n+    )\n+\n+\n+def restart_agent():\n+    \"\"\"Reset the agent and clear chat history.\"\"\"\n+    logger.debug(\"---*--- Restarting agent ---*---\")\n+    st.session_state[\"llama_tutor\"] = None\n+    st.session_state[\"llama_tutor_session_id\"] = None\n+    st.session_state[\"messages\"] = []\n+    st.rerun()\n+\n+\n+def export_chat_history():\n+    \"\"\"Export chat history as markdown.\"\"\"\n+    if \"messages\" in st.session_state:\n+        chat_text = \"# Llama Tutor - Chat History\\n\\n\"\n+        for msg in st.session_state[\"messages\"]:\n+            role_label = \"\ud83e\udd16 Assistant\" if msg[\"role\"] == \"assistant\" else \"\ud83d\udc64 User\"\n+            chat_text += f\"### {role_label}\\n{msg['content']}\\n\\n\"\n+        return chat_text\n+    return \"\"\n+\n+\n+def display_tool_calls(tool_calls_container, tools):\n+    \"\"\"Display tool calls in a streamlit container with expandable sections.\n+\n+    Args:\n+        tool_calls_container: Streamlit container to display the tool calls\n+        tools: List of tool call dictionaries containing name, args, content, and metrics\n+    \"\"\"\n+    try:\n+        with tool_calls_container.container():\n+            for tool_call in tools:\n+                tool_name = tool_call.get(\"tool_name\", \"Unknown Tool\")\n+                tool_args = tool_call.get(\"tool_args\", {})\n+                content = tool_call.get(\"content\")\n+                metrics = tool_call.get(\"metrics\", {})\n+\n+                # Add timing information\n+                execution_time_str = \"N/A\"\n+                try:\n+                    if metrics:\n+                        execution_time = metrics.time\n+                        if execution_time is not None:\n+                            execution_time_str = f\"{execution_time:.2f}s\"\n+                except Exception as e:\n+                    logger.error(f\"Error displaying tool calls: {str(e)}\")\n+                    pass\n+\n+                with st.expander(\n+                    f\"\ud83d\udee0\ufe0f {tool_name.replace('_', ' ').title()} ({execution_time_str})\",\n+                    expanded=False,\n+                ):\n+                    # Show query with syntax highlighting\n+                    if isinstance(tool_args, dict) and \"query\" in tool_args:\n+                        st.code(tool_args[\"query\"], language=\"sql\")\n+\n+                    # Display arguments in a more readable format\n+                    if tool_args and tool_args != {\"query\": None}:\n+                        st.markdown(\"**Arguments:**\")\n+                        st.json(tool_args)\n+\n+                    if content:\n+                        st.markdown(\"**Results:**\")\n+                        try:\n+                            st.json(content)\n+                        except Exception as e:\n+                            st.markdown(content)\n+\n+    except Exception as e:\n+        logger.error(f\"Error displaying tool calls: {str(e)}\")\n+        tool_calls_container.error(\"Failed to display tool results\")\n+\n+\n+def sidebar_widget() -> None:\n+    \"\"\"Display a sidebar with sample user queries for Llama Tutor.\"\"\"\n+    with st.sidebar:\n+        st.markdown(\"#### \ud83d\udcdc Try me!\")\n+        if st.button(\"\ud83d\udca1 US Tariffs\"):\n+            add_message(\n+                \"user\",\n+                \"Tell me about the tariffs the US is imposing in 2025\",\n+            )\n+        if st.button(\"\ud83e\udd14 Reasoning Models\"):\n+            add_message(\n+                \"user\",\n+                \"Which is a better reasoning model: o3-mini or DeepSeek R1?\",\n+            )\n+        if st.button(\"\ud83e\udd16 Tell me about Agno\"):\n+            add_message(\n+                \"user\",\n+                \"Tell me about Agno: https://github.com/agno-agi/agno and https://docs.agno.com\",\n+            )\n+        if st.button(\"\u2696\ufe0f Impact of AI Regulations\"):\n+            add_message(\n+                \"user\",\n+                \"Evaluate how emerging AI regulations could influence innovation, privacy, and ethical AI deployment in the near future.\",\n+            )\n+\n+        st.markdown(\"---\")\n+        st.markdown(\"#### \ud83d\udee0\ufe0f Utilities\")\n+        col1, col2 = st.columns(2)\n+        with col1:\n+            if st.button(\"\ud83d\udd04 New Chat\"):\n+                restart_agent()\n+        with col2:\n+            fn = \"llama_tutor_chat_history.md\"\n+            if \"llama_tutor_session_id\" in st.session_state:\n+                fn = f\"llama_tutor_{st.session_state.llama_tutor_session_id}.md\"\n+            if st.download_button(\n+                \"\ud83d\udcbe Export Chat\",\n+                export_chat_history(),\n+                file_name=fn,\n+                mime=\"text/markdown\",\n+            ):\n+                st.sidebar.success(\"Chat history exported!\")\n+\n+\n+def session_selector_widget(agent: Agent, model_id: str) -> None:\n+    \"\"\"Display a session selector in the sidebar.\"\"\"\n+    if agent.storage:\n+        agent_sessions = agent.storage.get_all_sessions()\n+        # Get session names if available, otherwise use IDs.\n+        session_options = []\n+        for session in agent_sessions:\n+            session_id = session.session_id\n+            session_name = (\n+                session.session_data.get(\"session_name\", None)\n+                if session.session_data\n+                else None\n+            )\n+            display_name = session_name if session_name else session_id\n+            session_options.append({\"id\": session_id, \"display\": display_name})\n+\n+        # Display session selector.\n+        selected_session = st.sidebar.selectbox(\n+            \"Session\",\n+            options=[s[\"display\"] for s in session_options],\n+            key=\"session_selector\",\n+        )\n+        # Find the selected session ID.\n+        selected_session_id = next(\n+            s[\"id\"] for s in session_options if s[\"display\"] == selected_session\n+        )\n+\n+        if st.session_state.get(\"llama_tutor_session_id\") != selected_session_id:\n+            logger.info(\n+                f\"---*--- Loading {model_id} run: {selected_session_id} ---*---\"\n+            )\n+            st.session_state[\"llama_tutor\"] = tutor_agent(\n+                model_id=model_id,\n+                session_id=selected_session_id,\n+            )\n+            st.rerun()\n+\n+\n+def rename_session_widget(agent: Agent) -> None:\n+    \"\"\"Rename the current session of the agent and save to storage.\"\"\"\n+    container = st.sidebar.container()\n+    session_row = container.columns([3, 1], vertical_alignment=\"center\")\n+\n+    # Initialize session_edit_mode if needed.\n+    if \"session_edit_mode\" not in st.session_state:\n+        st.session_state.session_edit_mode = False\n+\n+    with session_row[0]:\n+        if st.session_state.session_edit_mode:\n+            new_session_name = st.text_input(\n+                \"Session Name\",\n+                value=agent.session_name,\n+                key=\"session_name_input\",\n+                label_visibility=\"collapsed\",\n+            )\n+        else:\n+            st.markdown(f\"Session Name: **{agent.session_name}**\")\n+\n+    with session_row[1]:\n+        if st.session_state.session_edit_mode:\n+            if st.button(\"\u2713\", key=\"save_session_name\", type=\"primary\"):\n+                if new_session_name:\n+                    agent.rename_session(new_session_name)\n+                    st.session_state.session_edit_mode = False\n+                    container.success(\"Renamed!\")\n+        else:\n+            if st.button(\"\u270e\", key=\"edit_session_name\"):\n+                st.session_state.session_edit_mode = True\n+\n+\n+def about_widget() -> None:\n+    \"\"\"Display an about section in the sidebar.\"\"\"\n+    st.sidebar.markdown(\"---\")\n+    st.sidebar.markdown(\"### \u2139\ufe0f About Llama Tutor\")\n+    st.sidebar.markdown(\n+        \"\"\"\n+        Llama Tutor is an educational AI assistant that delivers personalized learning experiences tailored to your education level.\n+        \n+        Features:\n+        - \ud83d\udcda Personalized education at various academic levels\n+        - \ud83d\udd0d Real-time information retrieval\n+        - \ud83d\udcca In-depth analysis and explanations\n+        - \ud83e\udde0 Interactive learning with quizzes and follow-up questions\n+        - \ud83d\udcbe Save lessons for future reference\n+        \n+        Built with:\n+        - \ud83e\udd99 Llama 3.3 70B from Meta\n+        - \ud83d\ude80 Agno framework\n+        - \ud83d\udcab Streamlit\n+        \"\"\"\n+    )\n+\n+\n+CUSTOM_CSS = \"\"\"\n+    <style>\n+    /* Main Styles */\n+    .main-title {\n+        font-size: 3rem !important;\n+        font-weight: 700 !important;\n+        color: #2A8EF9 !important;\n+        margin-bottom: 0 !important;\n+        text-align: center;\n+    }\n+    \n+    .subtitle {\n+        font-size: 1.2rem !important;\n+        color: #555 !important;\n+        margin-top: 0 !important;\n+        text-align: center;\n+        margin-bottom: 2rem !important;\n+    }\n+    \n+    /* Dark mode support */\n+    @media (prefers-color-scheme: dark) {\n+        .main-title {\n+            color: #4DA3FF !important;\n+        }\n+        \n+        .subtitle {\n+            color: #CCC !important;\n+        }\n+    }\n+    \n+    /* Tool Call Styling */\n+    .stExpander {\n+        border-radius: 8px !important;\n+        border: 1px solid rgba(49, 51, 63, 0.2) !important;\n+        margin-bottom: 0.5rem !important;\n+    }\n+    \n+    .stExpander summary {\n+        font-weight: 600 !important;\n+        padding: 0.5rem 1rem !important;\n+    }\n+    \n+    /* Sidebar Styling */\n+    .css-1544g2n {\n+        padding-top: 2rem !important;\n+    }\n+    \n+    /* Education Level Display */\n+    .education-level-display {\n+        padding: 8px;\n+        background-color: #f8f9fa;\n+        border-radius: 6px;\n+        margin-top: 8px;\n+        text-align: center;\n+        border: 1px solid #e9ecef;\n+        font-size: 0.9rem;\n+    }\n+    \n+    /* Dark mode support for education level */\n+    @media (prefers-color-scheme: dark) {\n+        .education-level-display {\n+            background-color: #262730;\n+            border-color: #4a4d56;\n+            color: #f8f9fa;\n+        }\n+    }\n+    </style>\n+\"\"\"\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/mcp_agent/.gitignore",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/mcp_agent/.gitignore b/cookbook/examples/streamlit_applications/mcp_agent/.gitignore\nnew file mode 100644\nindex 000000000..c9ce91d35\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/mcp_agent/.gitignore\n@@ -0,0 +1,3 @@\n+output\n+agents.db\n+tmp\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/mcp_agent/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/mcp_agent/__init__.py b/cookbook/examples/streamlit_applications/mcp_agent/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/mcp_agent/agents.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/mcp_agent/agents.py b/cookbook/examples/streamlit_applications/mcp_agent/agents.py\nnew file mode 100644\nindex 000000000..5cf58badd\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/mcp_agent/agents.py\n@@ -0,0 +1,171 @@\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import List, Optional\n+\n+from agno.agent import Agent\n+from agno.embedder.openai import OpenAIEmbedder\n+from agno.knowledge.url import UrlKnowledge\n+from agno.models.anthropic import Claude\n+from agno.models.google import Gemini\n+from agno.models.groq import Groq\n+from agno.models.openai import OpenAIChat\n+from agno.storage.agent.sqlite import SqliteAgentStorage\n+from agno.tools.mcp import MCPTools\n+from agno.vectordb.lancedb import LanceDb, SearchType\n+\n+# ************* Setup Paths *************\n+# Define the current working directory\n+cwd = Path(__file__).parent\n+# Create a tmp directory for storing agent sessions and knowledge\n+tmp_dir = cwd.joinpath(\"tmp\")\n+tmp_dir.mkdir(parents=True, exist_ok=True)\n+# *************************************\n+\n+# ************* Agent Storage *************\n+# Store agent sessions in a SQLite database\n+agent_storage = SqliteAgentStorage(\n+    table_name=\"mcp_agent_sessions\",  # Table to store agent sessions\n+    db_file=str(tmp_dir.joinpath(\"agents.db\")),  # SQLite database file\n+)\n+# *************************************\n+\n+# ************* Agent Knowledge *************\n+# Store MCP Documentation in a knowledge base\n+agent_knowledge = UrlKnowledge(\n+    urls=[\"https://modelcontextprotocol.io/llms-full.txt\"],\n+    vector_db=LanceDb(\n+        uri=str(tmp_dir.joinpath(\"mcp_documentation\")),\n+        table_name=\"mcp_documentation\",\n+        search_type=SearchType.hybrid,\n+        embedder=OpenAIEmbedder(id=\"text-embedding-3-small\"),\n+    ),\n+)\n+# *************************************\n+\n+\n+def get_mcp_agent(\n+    user_id: Optional[str] = None,\n+    model_str: str = \"openai:gpt-4o\",\n+    session_id: Optional[str] = None,\n+    num_history_responses: int = 5,\n+    mcp_tools: Optional[List[MCPTools]] = None,\n+    mcp_server_ids: Optional[List[str]] = None,\n+    debug_mode: bool = True,\n+) -> Agent:\n+    model = get_model_for_provider(model_str)\n+\n+    description = dedent(\"\"\"\\\n+        You are UAgI, a universal MCP (Model Context Protocol) agent designed to interact with MCP servers.\n+        You can connect to various MCP servers to access resources and execute tools.\n+\n+        As an MCP agent, you can:\n+        - Connect to file systems, databases, APIs, and other data sources through MCP servers\n+        - Execute tools provided by MCP servers to perform actions\n+        - Access resources exposed by MCP servers\n+\n+        Note: You only have access to the MCP Servers provided below, if you need to access other MCP Servers, please ask the user to enable them.\n+\n+        <critical>\n+        - When a user mentions a task that might require external data or tools, check if an appropriate MCP server is available\n+        - If an MCP server is available, use its capabilities to fulfill the user's request\n+        - You have a knowledge base full of MCP documentation, search it using the `search_knowledge` tool to answer questions about MCP and the different tools available.\n+        - Provide clear explanations of which MCP servers and tools you're using\n+        - If you encounter errors with an MCP server, explain the issue and suggest alternatives\n+        - Always cite sources when providing information retrieved through MCP servers\n+        </critical>\\\n+    \"\"\")\n+\n+    if mcp_server_ids:\n+        description += dedent(\n+            \"\"\"\\n\n+            You have access to the following MCP servers:\n+            {}\n+        \"\"\".format(\"\\n\".join([f\"- {server_id}\" for server_id in mcp_server_ids]))\n+        )\n+\n+    instructions = dedent(\"\"\"\\\n+        Here's how you should fulfill a user request:\n+\n+        1. Understand the user's request\n+        - Read the user's request carefully\n+        - Determine if the request requires MCP server interaction\n+        - Search your knowledge base using the `search_knowledge` tool to answer questions about MCP or to learn how to use different MCP tools.\n+        - To interact with an MCP server, follow these steps:\n+            - Identify which tools are available to you\n+            - Select the appropriate tool for the user's request\n+            - Explain to the user which tool you're using\n+            - Execute the tool\n+            - Provide clear feedback about tool execution results\n+\n+        2. Error Handling\n+        - If an MCP tool fails, explain the issue clearly and provide details about the error.\n+        - Suggest alternatives when MCP capabilities are unavailable\n+\n+        3. Security and Privacy\n+        - Be transparent about which servers and tools you're using\n+        - Request explicit permission before executing tools that modify data\n+        - Respect access limitations of connected MCP servers\n+\n+        MCP Knowledge\n+        - You have access to a knowledge base of MCP documentation\n+        - To answer questions about MCP, use the knowledge base\n+        - If you don't know the answer or can't find the information in the knowledge base, say so\\\n+    \"\"\")\n+\n+    return Agent(\n+        name=\"UAgI: The Universal MCP Agent\",\n+        model=model,\n+        user_id=user_id,\n+        session_id=session_id,\n+        tools=mcp_tools,\n+        # Store Agent sessions in the database\n+        storage=agent_storage,\n+        # Store MCP Documentation in a knowledge base\n+        knowledge=agent_knowledge,\n+        # Agent description, instructions and expected output format\n+        description=description,\n+        instructions=instructions,\n+        # Allow MCP Agent to read both chat history and tool call history for better context.\n+        read_chat_history=True,\n+        read_tool_call_history=True,\n+        # Append previous conversation responses into the new messages for context.\n+        add_history_to_messages=True,\n+        num_history_responses=num_history_responses,\n+        add_datetime_to_instructions=True,\n+        add_name_to_instructions=True,\n+        debug_mode=debug_mode,\n+        # Respond in markdown format\n+        markdown=True,\n+    )\n+\n+\n+def get_model_for_provider(model_str: str):\n+    \"\"\"\n+    Creates and returns the appropriate model for a model string.\n+\n+    Args:\n+        model_str: The model string (e.g., 'openai:gpt-4o', 'google:gemini-2.0-flash', 'anthropic:claude-3-5-sonnet', 'groq:llama-3.3-70b-versatile')\n+\n+    Returns:\n+        An instance of the appropriate model class\n+\n+    Raises:\n+        ValueError: If the provider is not supported\n+    \"\"\"\n+    provider, model_name = model_str.split(\":\")\n+    if provider == \"openai\":\n+        return OpenAIChat(id=model_name)\n+    elif provider == \"gemini\":\n+        return Gemini(id=model_name)\n+    elif provider == \"anthropic\":\n+        if \"thinking\" in model_name:\n+            return Claude(\n+                id=model_name,\n+                max_tokens=16384,\n+                thinking={\"type\": \"enabled\", \"budget_tokens\": 8192},\n+            )\n+        return Claude(id=model_name, max_tokens=16384)\n+    elif provider == \"groq\":\n+        return Groq(id=model_name)\n+    else:\n+        raise ValueError(f\"Unsupported model provider: {provider}\")\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/mcp_agent/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/mcp_agent/app.py b/cookbook/examples/streamlit_applications/mcp_agent/app.py\nnew file mode 100644\nindex 000000000..23dc9bd76\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/mcp_agent/app.py\n@@ -0,0 +1,210 @@\n+import asyncio\n+\n+import nest_asyncio\n+import streamlit as st\n+from agents import get_mcp_agent\n+from agno.utils.log import logger\n+from mcp_client import MCPClient\n+from utils import (\n+    about_widget,\n+    add_message,\n+    apply_theme,\n+    display_tool_calls,\n+    example_inputs,\n+    get_mcp_server_config,\n+    get_num_history_responses,\n+    get_selected_model,\n+    session_selector_widget,\n+    utilities_widget,\n+)\n+\n+nest_asyncio.apply()\n+apply_theme()\n+\n+\n+async def main() -> None:\n+    ####################################################################\n+    # App header\n+    ####################################################################\n+    st.markdown(\n+        \"<h1 class='main-title'>Universal Agent Interface powered by MCP</h1>\",\n+        unsafe_allow_html=True,\n+    )\n+    st.markdown(\n+        \"<p class='subtitle'>A unified Agentic interface for MCP servers</p>\",\n+        unsafe_allow_html=True,\n+    )\n+\n+    ####################################################################\n+    # Settings\n+    ####################################################################\n+    selected_model = get_selected_model()\n+    mcp_server_config = get_mcp_server_config()\n+    mcp_server_id = mcp_server_config.id\n+    num_history_responses = get_num_history_responses()\n+\n+    ####################################################################\n+    # Initialize MCP Client and Agent\n+    ####################################################################\n+    try:\n+        # Check if we need to reinitialize the MCP client\n+        if (\n+            \"mcp_client\" not in st.session_state\n+            or st.session_state.get(\"mcp_server_id\") != mcp_server_id\n+            or getattr(st.session_state.get(\"mcp_client\", None), \"session\", None)\n+            is None\n+        ):\n+            # # Clean up existing client if it exists\n+            # if \"mcp_client\" in st.session_state:\n+            #     logger.info(\"Cleaning up existing MCP client\")\n+            #     await st.session_state[\"mcp_client\"].cleanup()\n+\n+            # Initialize new MCP client\n+            logger.info(f\"Creating new MCPClient for {mcp_server_id}\")\n+            st.session_state[\"mcp_client\"] = MCPClient()\n+\n+        mcp_client = st.session_state[\"mcp_client\"]\n+        # Connect to the MCP server and get tools\n+        mcp_tools = await mcp_client.connect_to_server(mcp_server_config)\n+\n+        # Initialize or retrieve the agent\n+        if (\n+            \"mcp_agent\" not in st.session_state\n+            or st.session_state[\"mcp_agent\"] is None\n+            or st.session_state.get(\"current_model\") != selected_model\n+            or st.session_state.get(\"mcp_server_id\") != mcp_server_id\n+        ):\n+            logger.info(\"---*--- Creating new MCP Agent ---*---\")\n+            mcp_agent = get_mcp_agent(\n+                model_str=selected_model,\n+                num_history_responses=num_history_responses,\n+                mcp_tools=[mcp_tools],\n+                mcp_server_ids=[mcp_server_id],\n+            )\n+            st.session_state[\"mcp_agent\"] = mcp_agent\n+            st.session_state[\"current_model\"] = selected_model\n+        else:\n+            mcp_agent = st.session_state[\"mcp_agent\"]\n+\n+        # Update the agent's MCP tools incase the session has been reinitialized\n+        mcp_agent.tools = [mcp_tools]\n+        ####################################################################\n+        # Load the current Agent session from the database\n+        ####################################################################\n+        try:\n+            st.session_state[\"mcp_agent_session_id\"] = mcp_agent.load_session()\n+        except Exception as e:\n+            st.warning(\n+                f\"Could not create Agent session: {str(e)}. Is the database running?\"\n+            )\n+            return\n+\n+        ####################################################################\n+        # Load agent runs (i.e. chat history) from memory\n+        ####################################################################\n+        agent_runs = mcp_agent.memory.runs\n+        if len(agent_runs) > 0:\n+            # If there are runs, load the messages\n+            logger.debug(\"Loading run history\")\n+            st.session_state[\"messages\"] = []\n+            # Loop through the runs and add the messages to the messages list\n+            for _run in agent_runs:\n+                if _run.message is not None:\n+                    add_message(_run.message.role, _run.message.content)\n+                if _run.response is not None:\n+                    add_message(\"assistant\", _run.response.content, _run.response.tools)\n+        else:\n+            # If there are no runs, create an empty messages list\n+            logger.debug(\"No run history found\")\n+            st.session_state[\"messages\"] = []\n+\n+        ####################################################################\n+        # Get user input\n+        ####################################################################\n+        if prompt := st.chat_input(\"\u2728 How can I help, bestie?\"):\n+            add_message(\"user\", prompt)\n+\n+        ####################################################################\n+        # Show example inputs\n+        ####################################################################\n+        example_inputs(server_id=mcp_server_id)\n+\n+        ####################################################################\n+        # Display agent messages\n+        ####################################################################\n+        for message in st.session_state[\"messages\"]:\n+            if message[\"role\"] in [\"user\", \"assistant\"]:\n+                _content = message[\"content\"]\n+                if _content is not None:\n+                    with st.chat_message(message[\"role\"]):\n+                        # Display tool calls if they exist in the message\n+                        if \"tool_calls\" in message and message[\"tool_calls\"]:\n+                            display_tool_calls(st.empty(), message[\"tool_calls\"])\n+                        st.markdown(_content)\n+\n+        ####################################################################\n+        # Generate response for user message\n+        ####################################################################\n+        last_message = (\n+            st.session_state[\"messages\"][-1] if st.session_state[\"messages\"] else None\n+        )\n+        if last_message and last_message.get(\"role\") == \"user\":\n+            question = last_message[\"content\"]\n+            with st.chat_message(\"assistant\"):\n+                # Create container for tool calls\n+                tool_calls_container = st.empty()\n+                resp_container = st.empty()\n+                with st.spinner(\":thinking_face: Thinking...\"):\n+                    response = \"\"\n+                    try:\n+                        # Run the agent and stream the response\n+                        run_response = await mcp_agent.arun(question, stream=True)\n+                        async for _resp_chunk in run_response:\n+                            # Display tool calls if available\n+                            if _resp_chunk.tools and len(_resp_chunk.tools) > 0:\n+                                display_tool_calls(\n+                                    tool_calls_container, _resp_chunk.tools\n+                                )\n+\n+                            # Display response\n+                            if _resp_chunk.content is not None:\n+                                response += _resp_chunk.content\n+                                resp_container.markdown(response)\n+\n+                        add_message(\"assistant\", response, mcp_agent.run_response.tools)\n+                    except Exception as e:\n+                        logger.error(f\"Error during agent run: {str(e)}\", exc_info=True)\n+                        error_message = f\"Sorry, I encountered an error: {str(e)}\"\n+                        add_message(\"assistant\", error_message)\n+                        st.error(error_message)\n+\n+        ####################################################################\n+        # Session selector\n+        ####################################################################\n+        session_selector_widget(\n+            agent=mcp_agent,\n+            model_str=selected_model,\n+            num_history_responses=num_history_responses,\n+            mcp_tools=[mcp_tools],\n+            mcp_server_ids=[mcp_server_id],\n+        )\n+\n+        ####################################################################\n+        # About section\n+        ####################################################################\n+        utilities_widget(agent=mcp_agent)\n+        about_widget()\n+\n+    except Exception as e:\n+        logger.error(f\"Error during agent run: {str(e)}\", exc_info=True)\n+        error_message = f\"Sorry, I encountered an error: {str(e)}\"\n+        add_message(\"assistant\", error_message)\n+        st.error(error_message)\n+    finally:\n+        # Don't clean up resources here - we want to keep the connection alive\n+        # between Streamlit reruns. We'll clean up when we need to reinitialize.\n+        pass\n+\n+\n+if __name__ == \"__main__\":\n+    asyncio.run(main())\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/mcp_agent/generate_requirements.sh",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/mcp_agent/generate_requirements.sh b/cookbook/examples/streamlit_applications/mcp_agent/generate_requirements.sh\nnew file mode 100755\nindex 000000000..78f8c7cec\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/mcp_agent/generate_requirements.sh\n@@ -0,0 +1,12 @@\n+#!/bin/bash\n+\n+############################################################################\n+# Generate requirements.txt from requirements.in\n+############################################################################\n+\n+echo \"Generating requirements.txt\"\n+\n+CURR_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n+\n+UV_CUSTOM_COMPILE_COMMAND=\"./generate_requirements.sh\" \\\n+  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/mcp_agent/mcp_client.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/mcp_agent/mcp_client.py b/cookbook/examples/streamlit_applications/mcp_agent/mcp_client.py\nnew file mode 100644\nindex 000000000..d725a8b0e\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/mcp_agent/mcp_client.py\n@@ -0,0 +1,62 @@\n+from contextlib import AsyncExitStack\n+from typing import List, Optional\n+\n+from agno.tools.mcp import MCPTools\n+from agno.utils.log import logger\n+from mcp import ClientSession, StdioServerParameters\n+from mcp.client.stdio import stdio_client\n+from pydantic import BaseModel\n+\n+\n+class MCPServerConfig(BaseModel):\n+    \"\"\"Configuration for an MCP server.\"\"\"\n+\n+    id: str\n+    command: str\n+    args: Optional[List[str]] = None\n+\n+\n+class MCPClient:\n+    def __init__(self):\n+        # Initialize session and client objects\n+        self.session = None\n+        self.exit_stack = AsyncExitStack()\n+        self.tools = []\n+        self.server_id = None\n+\n+    async def connect_to_server(self, server_config):\n+        \"\"\"Connect to an MCP server using the provided configuration\n+\n+        Args:\n+            server_config: Configuration for the MCP server\n+        \"\"\"\n+        self.server_id = server_config.id\n+\n+        server_params = StdioServerParameters(\n+            command=server_config.command,\n+            args=server_config.args,\n+        )\n+        logger.info(f\"Connecting to server {self.server_id}\")\n+\n+        # Create client session\n+        stdio_transport = await self.exit_stack.enter_async_context(\n+            stdio_client(server_params)\n+        )\n+        self.stdio, self.write = stdio_transport\n+        self.session = await self.exit_stack.enter_async_context(\n+            ClientSession(self.stdio, self.write)\n+        )\n+\n+        # Initialize the session\n+        await self.session.initialize()\n+\n+        # Create MCPTools for this server\n+        mcp_tools = MCPTools(session=self.session)\n+        await mcp_tools.initialize()\n+        logger.info(f\"Connected to server {self.server_id}\")\n+\n+        return mcp_tools\n+\n+    async def cleanup(self):\n+        \"\"\"Clean up resources\"\"\"\n+        await self.exit_stack.aclose()\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/mcp_agent/requirements.in",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/mcp_agent/requirements.in b/cookbook/examples/streamlit_applications/mcp_agent/requirements.in\nnew file mode 100644\nindex 000000000..59f7aeb13\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/mcp_agent/requirements.in\n@@ -0,0 +1,11 @@\n+agno\n+anthropic\n+google-genai\n+groq\n+lancedb\n+mcp\n+nest_asyncio\n+openai\n+sqlalchemy\n+streamlit\n+tantivy\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/mcp_agent/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/mcp_agent/requirements.txt b/cookbook/examples/streamlit_applications/mcp_agent/requirements.txt\nnew file mode 100644\nindex 000000000..2679ab4a7\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/mcp_agent/requirements.txt\n@@ -0,0 +1,255 @@\n+# This file was autogenerated by uv via the following command:\n+#    ./generate_requirements.sh\n+agno==1.1.9\n+    # via -r cookbook/examples/apps/mcp_agent/requirements.in\n+altair==5.5.0\n+    # via streamlit\n+annotated-types==0.7.0\n+    # via pydantic\n+anthropic==0.49.0\n+    # via -r cookbook/examples/apps/mcp_agent/requirements.in\n+anyio==4.8.0\n+    # via\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   httpx\n+    #   mcp\n+    #   openai\n+    #   sse-starlette\n+    #   starlette\n+attrs==25.1.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+blinker==1.9.0\n+    # via streamlit\n+cachetools==5.5.2\n+    # via\n+    #   google-auth\n+    #   streamlit\n+certifi==2025.1.31\n+    # via\n+    #   httpcore\n+    #   httpx\n+    #   requests\n+charset-normalizer==3.4.1\n+    # via requests\n+click==8.1.8\n+    # via\n+    #   streamlit\n+    #   typer\n+    #   uvicorn\n+deprecation==2.1.0\n+    # via lancedb\n+distro==1.9.0\n+    # via\n+    #   anthropic\n+    #   groq\n+    #   openai\n+docstring-parser==0.16\n+    # via agno\n+gitdb==4.0.12\n+    # via gitpython\n+gitpython==3.1.44\n+    # via\n+    #   agno\n+    #   streamlit\n+google-auth==2.38.0\n+    # via google-genai\n+google-genai==1.5.0\n+    # via -r cookbook/examples/apps/mcp_agent/requirements.in\n+groq==0.18.0\n+    # via -r cookbook/examples/apps/mcp_agent/requirements.in\n+h11==0.14.0\n+    # via\n+    #   httpcore\n+    #   uvicorn\n+httpcore==1.0.7\n+    # via httpx\n+httpx==0.28.1\n+    # via\n+    #   agno\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   mcp\n+    #   openai\n+httpx-sse==0.4.0\n+    # via mcp\n+idna==3.10\n+    # via\n+    #   anyio\n+    #   httpx\n+    #   requests\n+jinja2==3.1.6\n+    # via\n+    #   altair\n+    #   pydeck\n+jiter==0.8.2\n+    # via\n+    #   anthropic\n+    #   openai\n+jsonschema==4.23.0\n+    # via altair\n+jsonschema-specifications==2024.10.1\n+    # via jsonschema\n+lancedb==0.20.0\n+    # via -r cookbook/examples/apps/mcp_agent/requirements.in\n+markdown-it-py==3.0.0\n+    # via rich\n+markupsafe==3.0.2\n+    # via jinja2\n+mcp==1.3.0\n+    # via -r cookbook/examples/apps/mcp_agent/requirements.in\n+mdurl==0.1.2\n+    # via markdown-it-py\n+narwhals==1.29.1\n+    # via altair\n+nest-asyncio==1.6.0\n+    # via -r cookbook/examples/apps/mcp_agent/requirements.in\n+numpy==2.2.3\n+    # via\n+    #   pandas\n+    #   pydeck\n+    #   pylance\n+    #   streamlit\n+openai==1.65.5\n+    # via -r cookbook/examples/apps/mcp_agent/requirements.in\n+overrides==7.7.0\n+    # via lancedb\n+packaging==24.2\n+    # via\n+    #   altair\n+    #   deprecation\n+    #   lancedb\n+    #   streamlit\n+pandas==2.2.3\n+    # via streamlit\n+pillow==11.1.0\n+    # via streamlit\n+protobuf==5.29.3\n+    # via streamlit\n+pyarrow==19.0.1\n+    # via\n+    #   pylance\n+    #   streamlit\n+pyasn1==0.6.1\n+    # via\n+    #   pyasn1-modules\n+    #   rsa\n+pyasn1-modules==0.4.1\n+    # via google-auth\n+pydantic==2.10.6\n+    # via\n+    #   agno\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   lancedb\n+    #   mcp\n+    #   openai\n+    #   pydantic-settings\n+pydantic-core==2.27.2\n+    # via pydantic\n+pydantic-settings==2.8.1\n+    # via\n+    #   agno\n+    #   mcp\n+pydeck==0.9.1\n+    # via streamlit\n+pygments==2.19.1\n+    # via rich\n+pylance==0.23.2\n+    # via lancedb\n+python-dateutil==2.9.0.post0\n+    # via pandas\n+python-dotenv==1.0.1\n+    # via\n+    #   agno\n+    #   pydantic-settings\n+python-multipart==0.0.20\n+    # via agno\n+pytz==2025.1\n+    # via pandas\n+pyyaml==6.0.2\n+    # via agno\n+referencing==0.36.2\n+    # via\n+    #   jsonschema\n+    #   jsonschema-specifications\n+requests==2.32.3\n+    # via\n+    #   google-genai\n+    #   streamlit\n+rich==13.9.4\n+    # via\n+    #   agno\n+    #   typer\n+rpds-py==0.23.1\n+    # via\n+    #   jsonschema\n+    #   referencing\n+rsa==4.9\n+    # via google-auth\n+shellingham==1.5.4\n+    # via typer\n+six==1.17.0\n+    # via python-dateutil\n+smmap==5.0.2\n+    # via gitdb\n+sniffio==1.3.1\n+    # via\n+    #   anthropic\n+    #   anyio\n+    #   groq\n+    #   openai\n+sqlalchemy==2.0.38\n+    # via -r cookbook/examples/apps/mcp_agent/requirements.in\n+sse-starlette==2.2.1\n+    # via mcp\n+starlette==0.46.1\n+    # via\n+    #   mcp\n+    #   sse-starlette\n+streamlit==1.43.1\n+    # via -r cookbook/examples/apps/mcp_agent/requirements.in\n+tantivy==0.22.0\n+    # via -r cookbook/examples/apps/mcp_agent/requirements.in\n+tenacity==9.0.0\n+    # via streamlit\n+toml==0.10.2\n+    # via streamlit\n+tomli==2.2.1\n+    # via agno\n+tornado==6.4.2\n+    # via streamlit\n+tqdm==4.67.1\n+    # via\n+    #   lancedb\n+    #   openai\n+typer==0.15.2\n+    # via agno\n+typing-extensions==4.12.2\n+    # via\n+    #   agno\n+    #   altair\n+    #   anthropic\n+    #   anyio\n+    #   google-genai\n+    #   groq\n+    #   openai\n+    #   pydantic\n+    #   pydantic-core\n+    #   referencing\n+    #   sqlalchemy\n+    #   streamlit\n+    #   typer\n+tzdata==2025.1\n+    # via pandas\n+urllib3==2.3.0\n+    # via requests\n+uvicorn==0.34.0\n+    # via mcp\n+websockets==14.2\n+    # via google-genai\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/mcp_agent/utils.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/mcp_agent/utils.py b/cookbook/examples/streamlit_applications/mcp_agent/utils.py\nnew file mode 100644\nindex 000000000..741a7f03a\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/mcp_agent/utils.py\n@@ -0,0 +1,478 @@\n+import os\n+from pathlib import Path\n+from typing import Any, Dict, List, Optional\n+\n+import streamlit as st\n+from agents import get_mcp_agent\n+from agno.agent import Agent\n+from agno.tools.mcp import MCPTools\n+from agno.utils.log import logger\n+from mcp_client import MCPServerConfig\n+\n+\n+def get_selected_model() -> str:\n+    \"\"\"Return the selected model identifier based on user selection in the sidebar.\n+\n+    Returns:\n+        str: The model identifier string in the format 'provider:model-name'\n+    \"\"\"\n+    model_options = {\n+        \"gpt-4o\": \"openai:gpt-4o\",\n+        \"gpt-4.5\": \"openai:gpt-4.5-preview\",\n+        \"gpt-4o-mini\": \"openai:gpt-4o-mini\",\n+        \"o3-mini\": \"openai:o3-mini\",\n+        \"sonnet-3-7\": \"anthropic:claude-3-7-sonnet-latest\",\n+        \"sonnet-3.7-thinking\": \"anthropic:claude-3-7-sonnet-thinking\",\n+        \"gemini-flash\": \"gemini:gemini-2.0-flash\",\n+        \"gemini-pro\": \"gemini:gemini-2.0-pro-exp-02-05\",\n+        \"llama-3.3-70b\": \"groq:llama-3.3-70b-versatile\",\n+    }\n+    st.sidebar.markdown(\"#### :sparkles: Select a model\")\n+    selected_model = st.sidebar.selectbox(\n+        \"Select a model\",\n+        options=list(model_options.keys()),\n+        index=list(model_options.keys()).index(\"gpt-4o\"),\n+        key=\"selected_model\",\n+        label_visibility=\"collapsed\",\n+    )\n+    return model_options[selected_model]\n+\n+\n+def get_num_history_responses() -> int:\n+    \"\"\"Return the number of messages from history to send to the LLM.\n+\n+    Returns:\n+        int: The number of messages from history to include\n+    \"\"\"\n+    num_history = st.sidebar.slider(\n+        \"Number of previous messages to include\",\n+        min_value=1,\n+        max_value=20,\n+        value=5,\n+        step=1,\n+        help=\"Controls how many previous messages are sent to the LLM for context\",\n+    )\n+    return num_history\n+\n+\n+def get_mcp_server_config() -> Optional[MCPServerConfig]:\n+    \"\"\"Get a single MCP server config to add to the agent.\n+\n+    Returns:\n+        Optional[MCPServerConfig]: A single MCP server config, or None if none selected.\n+    \"\"\"\n+    with st.sidebar:\n+        st.markdown(\"#### \ud83d\udee0\ufe0f Select MCP Tool\")\n+\n+        # Use radio button for single selection\n+        selected_tool = st.radio(\n+            \"Select a tool to use:\",\n+            options=[\"GitHub\", \"Filesystem\"],\n+            key=\"selected_mcp_tool\",\n+            label_visibility=\"collapsed\",\n+        )\n+\n+        if selected_tool == \"GitHub\":\n+            github_token_from_env = os.getenv(\"GITHUB_TOKEN\")\n+            github_token = st.text_input(\n+                \"GitHub Token\",\n+                type=\"password\",\n+                help=\"Create a token with repo scope at github.com/settings/tokens\",\n+                value=github_token_from_env,\n+            )\n+            if github_token:\n+                os.environ[\"GITHUB_TOKEN\"] = github_token\n+                return MCPServerConfig(\n+                    id=\"github\",\n+                    command=\"npx\",\n+                    args=[\"-y\", \"@modelcontextprotocol/server-github\"],\n+                    env_vars=[\"GITHUB_TOKEN\"],\n+                )\n+            else:\n+                st.error(\"GitHub Token is required to use GitHub MCP Tools\")\n+\n+        elif selected_tool == \"Filesystem\":\n+            # Get the repository root\n+            cwd = Path(__file__).parent\n+            repo_root = cwd.parent.parent.parent.parent.resolve()\n+            st.info(f\"Repository path: {repo_root}\")\n+            return MCPServerConfig(\n+                id=\"filesystem\",\n+                command=\"npx\",\n+                args=[\"-y\", \"@modelcontextprotocol/server-filesystem\"]\n+                + [str(repo_root)],\n+            )\n+\n+    return None\n+\n+\n+def add_message(\n+    role: str, content: str, tool_calls: Optional[List[Dict[str, Any]]] = None\n+) -> None:\n+    \"\"\"Safely add a message to the session state.\"\"\"\n+    if \"messages\" not in st.session_state or not isinstance(\n+        st.session_state[\"messages\"], list\n+    ):\n+        st.session_state[\"messages\"] = []\n+    st.session_state[\"messages\"].append(\n+        {\"role\": role, \"content\": content, \"tool_calls\": tool_calls}\n+    )\n+\n+\n+def display_tool_calls(tool_calls_container, tools):\n+    \"\"\"Display tool calls in a streamlit container with expandable sections.\n+\n+    Args:\n+        tool_calls_container: Streamlit container to display the tool calls\n+        tools: List of tool call dictionaries containing name, args, content, and metrics\n+    \"\"\"\n+    if not tools:\n+        return\n+\n+    try:\n+        with tool_calls_container.container():\n+            for tool_call in tools:\n+                tool_name = tool_call.get(\"tool_name\", \"Unknown Tool\")\n+                tool_args = tool_call.get(\"tool_args\", {})\n+                content = tool_call.get(\"content\")\n+                metrics = tool_call.get(\"metrics\", {})\n+\n+                # Add timing information\n+                execution_time_str = \"N/A\"\n+                if metrics and isinstance(metrics, dict):\n+                    execution_time = metrics.get(\"time\")\n+                    if execution_time is None:\n+                        execution_time_str = \"N/A\"\n+                    else:\n+                        execution_time_str = f\"{execution_time:.2f}s\"\n+\n+                with st.expander(\n+                    f\"\ud83d\udee0\ufe0f {tool_name.replace('_', ' ').title()} ({execution_time_str})\",\n+                    expanded=False,\n+                ):\n+                    # Show query with syntax highlighting\n+                    if isinstance(tool_args, dict) and tool_args.get(\"query\"):\n+                        st.code(tool_args[\"query\"], language=\"sql\")\n+\n+                    # Display arguments in a more readable format\n+                    if tool_args and tool_args != {\"query\": None}:\n+                        st.markdown(\"**Arguments:**\")\n+                        st.json(tool_args)\n+\n+                    if content:\n+                        st.markdown(\"**Results:**\")\n+                        try:\n+                            # Check if content is already a dictionary or can be parsed as JSON\n+                            if isinstance(content, dict) or (\n+                                isinstance(content, str)\n+                                and content.strip().startswith((\"{\", \"[\"))\n+                            ):\n+                                st.json(content)\n+                            else:\n+                                # If not JSON, show as markdown\n+                                st.markdown(content)\n+                        except Exception:\n+                            # If JSON display fails, show as markdown\n+                            st.markdown(content)\n+\n+    except Exception as e:\n+        logger.error(f\"Error displaying tool calls: {str(e)}\")\n+        tool_calls_container.error(f\"Failed to display tool results: {str(e)}\")\n+\n+\n+def example_inputs(server_id: str) -> None:\n+    \"\"\"Show example inputs for the MCP Agent.\"\"\"\n+    with st.sidebar:\n+        st.markdown(\"#### :thinking_face: Try me!\")\n+        if st.button(\"Who are you?\"):\n+            add_message(\n+                \"user\",\n+                \"Who are you?\",\n+            )\n+        if st.button(\"What is your purpose?\"):\n+            add_message(\n+                \"user\",\n+                \"What is your purpose?\",\n+            )\n+        # Common examples for all server types\n+        if st.button(\"What can you help me with?\"):\n+            add_message(\n+                \"user\",\n+                \"What can you help me with?\",\n+            )\n+        if st.button(\"How do MCP tools work?\"):\n+            add_message(\n+                \"user\",\n+                \"How do MCP tools work? Explain the Model Context Protocol.\",\n+            )\n+\n+        # Server-specific examples\n+        if server_id == \"github\":\n+            if st.button(\"Tell me about Agno\"):\n+                add_message(\n+                    \"user\",\n+                    \"Tell me about Agno. Github repo: https://github.com/agno-agi/agno. You can read the README for more information.\",\n+                )\n+            if st.button(\"Find issues in the Agno repo\"):\n+                add_message(\n+                    \"user\",\n+                    \"Find open issues in the agno-agi/agno repository and summarize the top 3 most recent ones.\",\n+                )\n+        elif server_id == \"filesystem\":\n+            if st.button(\"Summarize the README\"):\n+                add_message(\n+                    \"user\",\n+                    \"If there is a README file in the current directory, summarize it.\",\n+                )\n+\n+\n+def session_selector_widget(\n+    agent: Agent,\n+    model_str: str,\n+    num_history_responses: int,\n+    mcp_tools: List[MCPTools],\n+    mcp_server_ids: List[str],\n+) -> None:\n+    \"\"\"Display a session selector in the sidebar, if a new session is selected, the agent is restarted with the new session.\"\"\"\n+\n+    if not agent.storage:\n+        return\n+\n+    try:\n+        # -*- Get all agent sessions.\n+        agent_sessions = agent.storage.get_all_sessions()\n+\n+        if not agent_sessions:\n+            st.sidebar.info(\"No saved sessions found.\")\n+            return\n+\n+        # -*- Get session names if available, otherwise use IDs.\n+        sessions_list = []\n+        for session in agent_sessions:\n+            session_id = session.session_id\n+            session_name = (\n+                session.session_data.get(\"session_name\", None)\n+                if session.session_data\n+                else None\n+            )\n+            display_name = session_name if session_name else session_id\n+            sessions_list.append({\"id\": session_id, \"display_name\": display_name})\n+\n+        # -*- Display session selector.\n+        st.sidebar.markdown(\"#### \ud83d\udcac Session\")\n+        selected_session = st.sidebar.selectbox(\n+            \"Session\",\n+            options=[s[\"display_name\"] for s in sessions_list],\n+            key=\"session_selector\",\n+            label_visibility=\"collapsed\",\n+        )\n+        # -*- Find the selected session ID.\n+        selected_session_id = next(\n+            s[\"id\"] for s in sessions_list if s[\"display_name\"] == selected_session\n+        )\n+        # -*- Update the selected session if it has changed.\n+        if st.session_state.get(\"mcp_agent_session_id\") != selected_session_id:\n+            logger.info(\n+                f\"---*--- Loading {model_str} run: {selected_session_id} ---*---\"\n+            )\n+            st.session_state[\"mcp_agent\"] = get_mcp_agent(\n+                model_str=model_str,\n+                session_id=selected_session_id,\n+                num_history_responses=num_history_responses,\n+                mcp_tools=mcp_tools,\n+                mcp_server_ids=mcp_server_ids,\n+            )\n+            st.rerun()\n+\n+        # -*- Show the rename session widget.\n+        container = st.sidebar.container()\n+        session_row = container.columns([3, 1], vertical_alignment=\"center\")\n+\n+        # -*- Initialize session_edit_mode if needed.\n+        if \"session_edit_mode\" not in st.session_state:\n+            st.session_state.session_edit_mode = False\n+\n+        # -*- Show the session name.\n+        with session_row[0]:\n+            if st.session_state.session_edit_mode:\n+                new_session_name = st.text_input(\n+                    \"Session Name\",\n+                    value=agent.session_name,\n+                    key=\"session_name_input\",\n+                    label_visibility=\"collapsed\",\n+                )\n+            else:\n+                st.markdown(f\"Session Name: **{agent.session_name}**\")\n+\n+        # -*- Show the rename session button.\n+        with session_row[1]:\n+            if st.session_state.session_edit_mode:\n+                if st.button(\"\u2713\", key=\"save_session_name\", type=\"primary\"):\n+                    if new_session_name:\n+                        agent.rename_session(new_session_name)\n+                        st.session_state.session_edit_mode = False\n+                        container.success(\"Renamed!\")\n+                        # Trigger a rerun to refresh the sessions list\n+                        st.rerun()\n+            else:\n+                if st.button(\"\u270e\", key=\"edit_session_name\"):\n+                    st.session_state.session_edit_mode = True\n+    except Exception as e:\n+        logger.error(f\"Error in session selector: {str(e)}\")\n+        st.sidebar.error(\"Failed to load sessions\")\n+\n+\n+def restart_agent():\n+    \"\"\"Reset the agent and clear chat history.\"\"\"\n+    logger.debug(\"---*--- Restarting agent ---*---\")\n+    st.session_state[\"mcp_agent\"] = None\n+    st.session_state[\"mcp_agent_session_id\"] = None\n+    st.session_state[\"messages\"] = []\n+    st.rerun()\n+\n+\n+def export_chat_history():\n+    \"\"\"Export chat history as markdown.\n+\n+    Returns:\n+        str: Formatted markdown string of the chat history\n+    \"\"\"\n+    if \"messages\" not in st.session_state or not st.session_state[\"messages\"]:\n+        return \"# MCP Agent - Chat History\\n\\nNo messages to export.\"\n+\n+    chat_text = \"# MCP Agent - Chat History\\n\\n\"\n+    for msg in st.session_state[\"messages\"]:\n+        role_label = \"\ud83e\udd16 Assistant\" if msg[\"role\"] == \"assistant\" else \"\ud83d\udc64 User\"\n+        chat_text += f\"### {role_label}\\n{msg['content']}\\n\\n\"\n+\n+        # Include tool calls if present\n+        if msg.get(\"tool_calls\"):\n+            chat_text += \"#### Tool Calls:\\n\"\n+            for i, tool_call in enumerate(msg[\"tool_calls\"]):\n+                tool_name = tool_call.get(\"name\", \"Unknown Tool\")\n+                chat_text += f\"**{i + 1}. {tool_name}**\\n\\n\"\n+                if \"arguments\" in tool_call:\n+                    chat_text += (\n+                        f\"Arguments: ```json\\n{tool_call['arguments']}\\n```\\n\\n\"\n+                    )\n+                if \"content\" in tool_call:\n+                    chat_text += f\"Results: ```\\n{tool_call['content']}\\n```\\n\\n\"\n+\n+    return chat_text\n+\n+\n+def utilities_widget(agent: Agent) -> None:\n+    \"\"\"Display a utilities widget in the sidebar.\"\"\"\n+    st.sidebar.markdown(\"---\")\n+    st.sidebar.markdown(\"#### \ud83d\udee0\ufe0f Utilities\")\n+    col1, col2 = st.sidebar.columns(2)\n+    with col1:\n+        if st.button(\"\ud83d\udd04 New Chat\"):\n+            restart_agent()\n+    with col2:\n+        fn = \"mcp_agent_chat_history.md\"\n+        if \"mcp_agent_session_id\" in st.session_state:\n+            fn = f\"mcp_agent_{st.session_state.mcp_agent_session_id}.md\"\n+        if st.download_button(\n+            \":file_folder: Export Chat\",\n+            export_chat_history(),\n+            file_name=fn,\n+            mime=\"text/markdown\",\n+        ):\n+            st.sidebar.success(\"Chat history exported!\")\n+    if agent is not None and agent.knowledge is not None:\n+        if st.sidebar.button(\"\ud83d\udcda Load Knowledge\"):\n+            agent.knowledge.load()\n+            st.sidebar.success(\"Knowledge loaded!\")\n+\n+\n+def about_widget() -> None:\n+    \"\"\"Display an about section in the sidebar.\"\"\"\n+    st.sidebar.markdown(\"#### \u2139\ufe0f About\")\n+    st.sidebar.markdown(\n+        \"\"\"\n+        The Universal MCP Agent lets you interact with MCP servers using a chat interface.\n+\n+        Built with:\n+        - \ud83d\ude80 [Agno](https://github.com/agno-agi/agno)\n+        - \ud83d\udcab [Streamlit](https://streamlit.io)\n+        \"\"\"\n+    )\n+\n+\n+CUSTOM_CSS = \"\"\"\n+    <style>\n+    /* Main Styles */\n+    .main-title {\n+        text-align: center;\n+        background: linear-gradient(45deg, #FF4B2B, #FF416C);\n+        -webkit-background-clip: text;\n+        -webkit-text-fill-color: transparent;\n+    }\n+    .subtitle {\n+        text-align: center;\n+        color: #666;\n+        margin-bottom: 2em;\n+    }\n+    .stButton button {\n+        width: 100%;\n+        border-radius: 20px;\n+        margin: 0.2em 0;\n+        transition: all 0.3s ease;\n+    }\n+    .stButton button:hover {\n+        transform: translateY(-2px);\n+        box-shadow: 0 5px 15px rgba(0,0,0,0.1);\n+    }\n+    .chat-container {\n+        border-radius: 15px;\n+        padding: 1em;\n+        margin: 1em 0;\n+        background-color: #f5f5f5;\n+    }\n+    .sql-result {\n+        background-color: #f8f9fa;\n+        border-radius: 10px;\n+        padding: 1em;\n+        margin: 1em 0;\n+        border-left: 4px solid #FF4B2B;\n+    }\n+    .status-message {\n+        padding: 1em;\n+        border-radius: 10px;\n+        margin: 1em 0;\n+    }\n+    .success-message {\n+        background-color: #d4edda;\n+        color: #155724;\n+    }\n+    .error-message {\n+        background-color: #f8d7da;\n+        color: #721c24;\n+    }\n+    /* Dark mode adjustments */\n+    @media (prefers-color-scheme: dark) {\n+        .chat-container {\n+            background-color: #2b2b2b;\n+        }\n+        .sql-result {\n+            background-color: #1e1e1e;\n+        }\n+    }\n+    </style>\n+\"\"\"\n+\n+\n+# Add a function to handle theme customization\n+def apply_theme():\n+    \"\"\"Apply custom theme settings to the Streamlit app.\"\"\"\n+    # Set page configuration\n+    st.set_page_config(\n+        page_title=\"Universal MCP Agent\",\n+        page_icon=\":crystal_ball:\",\n+        layout=\"wide\",\n+        initial_sidebar_state=\"expanded\",\n+    )\n+\n+    # Apply custom CSS\n+    st.markdown(CUSTOM_CSS, unsafe_allow_html=True)\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/medical_imaging/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/medical_imaging/__init__.py b/cookbook/examples/streamlit_applications/medical_imaging/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/medical_imaging/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/medical_imaging/app.py b/cookbook/examples/streamlit_applications/medical_imaging/app.py\nnew file mode 100644\nindex 000000000..d08190135\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/medical_imaging/app.py\n@@ -0,0 +1,121 @@\n+import os\n+\n+import streamlit as st\n+from agno.media import Image as AgnoImage\n+from medical_agent import agent\n+from PIL import Image as PILImage\n+\n+st.set_page_config(\n+    page_title=\"Medical Imaging Analysis\",\n+    page_icon=\"\ud83c\udfe5\",\n+    layout=\"wide\",\n+)\n+st.markdown(\"##### \ud83c\udfe5 built using [Agno](https://github.com/agno-agi/agno)\")\n+\n+\n+def main():\n+    with st.sidebar:\n+        st.info(\n+            \"This tool provides AI-powered analysis of medical imaging data using \"\n+            \"advanced computer vision and radiological expertise.\"\n+        )\n+        st.warning(\n+            \"\u26a0DISCLAIMER: This tool is for educational and informational purposes only. \"\n+            \"All analyses should be reviewed by qualified healthcare professionals. \"\n+            \"Do not make medical decisions based solely on this analysis.\"\n+        )\n+\n+    st.title(\"\ud83c\udfe5 Medical Imaging Diagnosis Agent\")\n+    st.write(\"Upload a medical image for professional analysis\")\n+\n+    # Create containers for better organization\n+    upload_container = st.container()\n+    image_container = st.container()\n+    analysis_container = st.container()\n+\n+    with upload_container:\n+        uploaded_file = st.file_uploader(\n+            \"Upload Medical Image\",\n+            type=[\"jpg\", \"jpeg\", \"png\", \"dicom\"],\n+            help=\"Supported formats: JPG, JPEG, PNG, DICOM\",\n+        )\n+\n+    if uploaded_file is not None:\n+        with image_container:\n+            col1, col2, col3 = st.columns([1, 2, 1])\n+            with col2:\n+                # Use PILImage for display\n+                pil_image = PILImage.open(uploaded_file)\n+                width, height = pil_image.size\n+                aspect_ratio = width / height\n+                new_width = 500\n+                new_height = int(new_width / aspect_ratio)\n+                resized_image = pil_image.resize((new_width, new_height))\n+\n+                st.image(\n+                    resized_image,\n+                    caption=\"Uploaded Medical Image\",\n+                    use_container_width=True,\n+                )\n+\n+                analyze_button = st.button(\n+                    \"\ud83d\udd0d Analyze Image\", type=\"primary\", use_container_width=True\n+                )\n+\n+                additional_info = st.text_area(\n+                    \"Provide additional context about the image (e.g., patient history, symptoms)\",\n+                    placeholder=\"Enter any relevant information here...\",\n+                )\n+\n+        with analysis_container:\n+            if analyze_button:\n+                image_path = \"temp_medical_image.png\"\n+                # Save the resized image\n+                resized_image.save(image_path, format=\"PNG\")\n+\n+                with st.spinner(\"\ud83d\udd04 Analyzing image... Please wait.\"):\n+                    try:\n+                        # Read the image file as binary\n+                        with open(image_path, \"rb\") as f:\n+                            image_bytes = f.read()\n+                        # creating an instance of Image\n+                        agno_image = AgnoImage(content=image_bytes, format=\"png\")\n+\n+                        prompt = (\n+                            f\"Analyze this medical image considering the following context: {additional_info}\"\n+                            if additional_info\n+                            else \"Analyze this medical image and provide detailed findings.\"\n+                        )\n+                        response = agent.run(prompt, images=[agno_image])\n+                        st.markdown(\"### \ud83d\udccb Analysis Results\")\n+                        st.markdown(\"---\")\n+                        if hasattr(response, \"content\"):\n+                            st.markdown(response.content)\n+                        elif isinstance(response, str):\n+                            st.markdown(response)\n+                        elif isinstance(response, dict) and \"content\" in response:\n+                            st.markdown(response[\"content\"])\n+                        else:\n+                            st.markdown(str(response))\n+                        st.markdown(\"---\")\n+                        st.caption(\n+                            \"Note: This analysis is generated by AI and should be reviewed by \"\n+                            \"a qualified healthcare professional.\"\n+                        )\n+\n+                    except Exception as e:\n+                        st.error(f\"Analysis error: {str(e)}\")\n+                        st.info(\n+                            \"Please try again or contact support if the issue persists.\"\n+                        )\n+                        print(f\"Detailed error: {e}\")\n+                    finally:\n+                        if os.path.exists(image_path):\n+                            os.remove(image_path)\n+\n+    else:\n+        st.info(\"\ud83d\udc46 Please upload a medical image to begin analysis\")\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/medical_imaging/medical_agent.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/medical_imaging/medical_agent.py b/cookbook/examples/streamlit_applications/medical_imaging/medical_agent.py\nnew file mode 100644\nindex 000000000..505f7589f\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/medical_imaging/medical_agent.py\n@@ -0,0 +1,92 @@\n+\"\"\"\n+Medical Imaging Analysis Agent Tutorial\n+=====================================\n+\n+This example demonstrates how to create an AI agent specialized in medical imaging analysis.\n+The agent can analyze various types of medical images (X-ray, MRI, CT, Ultrasound) and provide\n+detailed professional analysis along with patient-friendly explanations.\n+\n+\"\"\"\n+\n+from pathlib import Path\n+\n+from agno.agent import Agent\n+from agno.models.google import Gemini\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+# Base prompt that defines the agent's expertise and response structure\n+BASE_PROMPT = \"\"\"You are a highly skilled medical imaging expert with extensive knowledge in radiology \n+and diagnostic imaging. Your role is to provide comprehensive, accurate, and ethical analysis of medical images.\n+\n+Key Responsibilities:\n+1. Maintain patient privacy and confidentiality\n+2. Provide objective, evidence-based analysis\n+3. Highlight any urgent or critical findings\n+4. Explain findings in both professional and patient-friendly terms\n+\n+For each image analysis, structure your response as follows:\"\"\"\n+\n+# Detailed instructions for image analysis\n+ANALYSIS_TEMPLATE = \"\"\"\n+### 1. Image Technical Assessment\n+- Imaging modality identification\n+- Anatomical region and patient positioning\n+- Image quality evaluation (contrast, clarity, artifacts)\n+- Technical adequacy for diagnostic purposes\n+\n+### 2. Professional Analysis\n+- Systematic anatomical review\n+- Primary findings with precise measurements\n+- Secondary observations\n+- Anatomical variants or incidental findings\n+- Severity assessment (Normal/Mild/Moderate/Severe)\n+\n+### 3. Clinical Interpretation\n+- Primary diagnosis (with confidence level)\n+- Differential diagnoses (ranked by probability)\n+- Supporting evidence from the image\n+- Critical/Urgent findings (if any)\n+- Recommended follow-up studies (if needed)\n+\n+### 4. Patient Education\n+- Clear, jargon-free explanation of findings\n+- Visual analogies and simple diagrams when helpful\n+- Common questions addressed\n+- Lifestyle implications (if any)\n+\n+### 5. Evidence-Based Context\n+Using DuckDuckGo search:\n+- Recent relevant medical literature\n+- Standard treatment guidelines\n+- Similar case studies\n+- Technological advances in imaging/treatment\n+- 2-3 authoritative medical references\n+\n+Please maintain a professional yet empathetic tone throughout the analysis.\n+\"\"\"\n+\n+# Combine prompts for the final instruction\n+FULL_INSTRUCTIONS = BASE_PROMPT + ANALYSIS_TEMPLATE\n+\n+# Initialize the Medical Imaging Expert agent\n+agent = Agent(\n+    name=\"Medical Imaging Expert\",\n+    model=Gemini(id=\"gemini-2.0-flash-exp\"),\n+    tools=[DuckDuckGoTools()],  # Enable web search for medical literature\n+    markdown=True,  # Enable markdown formatting for structured output\n+    instructions=FULL_INSTRUCTIONS,\n+)\n+\n+# Example usage\n+if __name__ == \"__main__\":\n+    # Example image path (users should replace with their own image)\n+    image_path = Path(__file__).parent.joinpath(\"test.jpg\")\n+\n+    # Uncomment to run the analysis\n+    # agent.print_response(\"Please analyze this medical image.\", images=[image_path])\n+\n+    # Example with specific focus\n+    # agent.print_response(\n+    #     \"Please analyze this image with special attention to bone density.\",\n+    #     images=[image_path]\n+    # )\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/medical_imaging/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/medical_imaging/requirements.txt b/cookbook/examples/streamlit_applications/medical_imaging/requirements.txt\nnew file mode 100644\nindex 000000000..d55e11faf\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/medical_imaging/requirements.txt\n@@ -0,0 +1,4 @@\n+agno\n+openai\n+streamlit\n+duckduckgo-search\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/paperpal/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/paperpal/__init__.py b/cookbook/examples/streamlit_applications/paperpal/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/paperpal/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/paperpal/app.py b/cookbook/examples/streamlit_applications/paperpal/app.py\nnew file mode 100644\nindex 000000000..bde49bf29\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/paperpal/app.py\n@@ -0,0 +1,242 @@\n+import json\n+from typing import Optional\n+\n+import pandas as pd\n+import streamlit as st\n+from technical_writer import (\n+    ArxivSearchResults,\n+    SearchTerms,\n+    WebSearchResults,\n+    arxiv_search_agent,\n+    arxiv_toolkit,\n+    exa_search_agent,\n+    research_editor,\n+    search_term_generator,\n+)\n+\n+# Streamlit App Configuration\n+st.set_page_config(\n+    page_title=\"AI Researcher Workflow\",\n+    page_icon=\":orange_heart:\",\n+)\n+st.title(\"Paperpal\")\n+st.markdown(\"##### :orange_heart: built by [agno](https://github.com/agno-agi/agno)\")\n+\n+\n+def main() -> None:\n+    # Get topic for report\n+    input_topic = st.sidebar.text_input(\n+        \":female-scientist: Enter a topic\",\n+        value=\"LLM evals in multi-agentic space\",\n+    )\n+    # Button to generate blog\n+    generate_report = st.sidebar.button(\"Generate Blog\")\n+    if generate_report:\n+        st.session_state[\"topic\"] = input_topic\n+\n+    # Checkboxes for search\n+    st.sidebar.markdown(\"## Agents\")\n+    search_exa = st.sidebar.checkbox(\"Exa Search\", value=True)\n+    search_arxiv = st.sidebar.checkbox(\"ArXiv Search\", value=False)\n+    # search_pubmed = st.sidebar.checkbox(\"PubMed Search\", disabled=True)  # noqa\n+    # search_google_scholar = st.sidebar.checkbox(\"Google Scholar Search\", disabled=True)  # noqa\n+    # use_cache = st.sidebar.toggle(\"Use Cache\", value=False, disabled=True)  # noqa\n+    num_search_terms = st.sidebar.number_input(\n+        \"Number of Search Terms\",\n+        value=2,\n+        min_value=2,\n+        max_value=3,\n+        help=\"This will increase latency.\",\n+    )\n+\n+    st.sidebar.markdown(\"---\")\n+    st.sidebar.markdown(\"## Trending Topics\")\n+    topic = \"Humanoid and Autonomous Agents\"\n+    if st.sidebar.button(topic):\n+        st.session_state[\"topic\"] = topic\n+\n+    topic = \"Gene Editing for Disease Treatment\"\n+    if st.sidebar.button(topic):\n+        st.session_state[\"topic\"] = topic\n+\n+    topic = \"Multimodal AI in healthcare\"\n+    if st.sidebar.button(topic):\n+        st.session_state[\"topic\"] = topic\n+\n+    topic = \"Brain Aging and Neurodegenerative Diseases\"\n+    if st.sidebar.button(topic):\n+        st.session_state[\"topic\"] = topic\n+\n+    if \"topic\" in st.session_state:\n+        report_topic = st.session_state[\"topic\"]\n+\n+        search_terms: Optional[SearchTerms] = None\n+        with st.status(\"Generating Search Terms\", expanded=True) as status:\n+            with st.container():\n+                search_terms_container = st.empty()\n+                search_generator_input = {\n+                    \"topic\": report_topic,\n+                    \"num_terms\": num_search_terms,\n+                }\n+                search_terms = search_term_generator.run(\n+                    json.dumps(search_generator_input)\n+                ).content\n+                if search_terms:\n+                    search_terms_container.json(search_terms.model_dump())\n+            status.update(\n+                label=\"Search Terms Generated\", state=\"complete\", expanded=False\n+            )\n+\n+        if not search_terms:\n+            st.write(\"Sorry report generation failed. Please try again.\")\n+            return\n+\n+        exa_content: Optional[str] = None\n+        arxiv_content: Optional[str] = None\n+\n+        if search_exa:\n+            with st.status(\"Searching Exa\", expanded=True) as status:\n+                with st.container():\n+                    exa_container = st.empty()\n+                    try:\n+                        exa_search_results = exa_search_agent.run(\n+                            search_terms.model_dump_json(indent=4)\n+                        )\n+                        if isinstance(exa_search_results, str):\n+                            raise ValueError(\n+                                \"Unexpected string response from exa_search_agent\"\n+                            )\n+\n+                        if isinstance(exa_search_results.content, WebSearchResults):\n+                            exa_container.json(exa_search_results.content.results)\n+                            if (\n+                                exa_search_results\n+                                and exa_search_results.content\n+                                and len(exa_search_results.content.results) > 0\n+                            ):\n+                                exa_content = (\n+                                    exa_search_results.content.model_dump_json(indent=4)\n+                                )\n+                                exa_container.json(exa_search_results.content.results)\n+                                status.update(\n+                                    label=\"Exa Search Complete\",\n+                                    state=\"complete\",\n+                                    expanded=False,\n+                                )\n+                        else:\n+                            raise TypeError(\"Unexpected response from exa_search_agent\")\n+\n+                    except Exception as e:\n+                        st.error(f\"An error occurred during Exa search: {e}\")\n+                        status.update(\n+                            label=\"Exa Search Failed\", state=\"error\", expanded=True\n+                        )\n+                        exa_content = None\n+\n+        if search_arxiv:\n+            with st.status(\n+                \"Searching ArXiv (this takes a while)\", expanded=True\n+            ) as status:\n+                with st.container():\n+                    arxiv_container = st.empty()\n+                    arxiv_search_results = arxiv_search_agent.run(\n+                        search_terms.model_dump_json(indent=4)\n+                    )\n+                    if isinstance(arxiv_search_results.content, ArxivSearchResults):\n+                        if (\n+                            arxiv_search_results\n+                            and arxiv_search_results.content\n+                            and arxiv_search_results.content.results\n+                        ):\n+                            arxiv_container.json(\n+                                [\n+                                    result.model_dump()\n+                                    for result in arxiv_search_results.content.results\n+                                ]\n+                            )\n+                    else:\n+                        raise TypeError(\"Unexpected response from arxiv_search_agent\")\n+\n+                status.update(\n+                    label=\"ArXiv Search Complete\", state=\"complete\", expanded=False\n+                )\n+\n+            if (\n+                arxiv_search_results\n+                and arxiv_search_results.content\n+                and arxiv_search_results.content.results\n+            ):\n+                paper_summaries = []\n+                for result in arxiv_search_results.content.results:\n+                    summary = {\n+                        \"ID\": result.id,\n+                        \"Title\": result.title,\n+                        \"Authors\": \", \".join(result.authors)\n+                        if result.authors\n+                        else \"No authors available\",\n+                        \"Summary\": result.summary[:200] + \"...\"\n+                        if len(result.summary) > 200\n+                        else result.summary,\n+                    }\n+                    paper_summaries.append(summary)\n+\n+                if paper_summaries:\n+                    with st.status(\n+                        \"Displaying ArXiv Paper Summaries\", expanded=True\n+                    ) as status:\n+                        with st.container():\n+                            st.subheader(\"ArXiv Paper Summaries\")\n+                            df = pd.DataFrame(paper_summaries)\n+                            st.dataframe(df, use_container_width=True)\n+                        status.update(\n+                            label=\"ArXiv Paper Summaries Displayed\",\n+                            state=\"complete\",\n+                            expanded=False,\n+                        )\n+\n+                    arxiv_paper_ids = [summary[\"ID\"] for summary in paper_summaries]\n+                    if arxiv_paper_ids:\n+                        with st.status(\"Reading ArXiv Papers\", expanded=True) as status:\n+                            with st.container():\n+                                arxiv_content = arxiv_toolkit.read_arxiv_papers(\n+                                    arxiv_paper_ids, pages_to_read=2\n+                                )\n+                                st.write(f\"Read {len(arxiv_paper_ids)} ArXiv papers\")\n+                            status.update(\n+                                label=\"Reading ArXiv Papers Complete\",\n+                                state=\"complete\",\n+                                expanded=False,\n+                            )\n+\n+        report_input = \"\"\n+        report_input += f\"# Topic: {report_topic}\\n\\n\"\n+        report_input += \"## Search Terms\\n\\n\"\n+        report_input += f\"{search_terms}\\n\\n\"\n+        if arxiv_content:\n+            report_input += \"## ArXiv Papers\\n\\n\"\n+            report_input += \"<arxiv_papers>\\n\\n\"\n+            report_input += f\"{arxiv_content}\\n\\n\"\n+            report_input += \"</arxiv_papers>\\n\\n\"\n+        if exa_content:\n+            report_input += \"## Web Search Content from Exa\\n\\n\"\n+            report_input += \"<exa_content>\\n\\n\"\n+            report_input += f\"{exa_content}\\n\\n\"\n+            report_input += \"</exa_content>\\n\\n\"\n+\n+        # Only generate the report if we have content\n+        if arxiv_content or exa_content:\n+            with st.spinner(\"Generating Blog\"):\n+                final_report_container = st.empty()\n+                research_report = research_editor.run(report_input)\n+                final_report_container.markdown(research_report.content)\n+        else:\n+            st.error(\n+                \"Report generation cancelled due to search failure. Please try again or select another search option.\"\n+            )\n+\n+    st.sidebar.markdown(\"---\")\n+    if st.sidebar.button(\"Restart\"):\n+        st.rerun()\n+\n+\n+main()\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/paperpal/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/paperpal/requirements.txt b/cookbook/examples/streamlit_applications/paperpal/requirements.txt\nnew file mode 100644\nindex 000000000..311fcff17\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/paperpal/requirements.txt\n@@ -0,0 +1,5 @@\n+agno\n+openai\n+streamlit\n+exa_py\n+arxiv\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/paperpal/technical_writer.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/paperpal/technical_writer.py b/cookbook/examples/streamlit_applications/paperpal/technical_writer.py\nnew file mode 100644\nindex 000000000..156d203d2\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/paperpal/technical_writer.py\n@@ -0,0 +1,147 @@\n+import os\n+from pathlib import Path\n+from typing import List\n+\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+from agno.tools.arxiv import ArxivTools\n+from agno.tools.exa import ExaTools\n+from dotenv import load_dotenv\n+from pydantic import BaseModel, Field\n+\n+load_dotenv()\n+\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+\n+# Define data models\n+class SearchTerms(BaseModel):\n+    terms: List[str] = Field(\n+        ..., description=\"List of search terms related to a topic.\"\n+    )\n+\n+\n+class ArxivSearchResult(BaseModel):\n+    title: str = Field(..., description=\"Title of the article.\")\n+    id: str = Field(..., description=\"The ID of the article.\")\n+    authors: List[str] = Field(..., description=\"Authors of the article.\")\n+    summary: str = Field(..., description=\"Summary of the article.\")\n+    pdf_url: str = Field(..., description=\"URL of the PDF of the article.\")\n+    links: List[str] = Field(..., description=\"Links related to the article.\")\n+    reasoning: str = Field(..., description=\"Reason for selecting this article.\")\n+\n+\n+class ArxivSearchResults(BaseModel):\n+    results: List[ArxivSearchResult] = Field(\n+        ..., description=\"List of top search results.\"\n+    )\n+\n+\n+class WebSearchResult(BaseModel):\n+    title: str = Field(..., description=\"Title of the article.\")\n+    summary: str = Field(..., description=\"Summary of the article.\")\n+    links: List[str] = Field(..., description=\"Links related to the article.\")\n+    reasoning: str = Field(..., description=\"Reason for selecting this article.\")\n+\n+\n+class WebSearchResults(BaseModel):\n+    results: List[WebSearchResult] = Field(\n+        ..., description=\"List of top search results.\"\n+    )\n+\n+\n+# Initialize tools\n+arxiv_toolkit = ArxivTools(\n+    download_dir=Path(__file__).parent.parent.parent.parent.joinpath(\n+        \"wip\", \"arxiv_pdfs\"\n+    )\n+)\n+exa_tools = ExaTools()\n+\n+# Initialize agents\n+search_term_generator = Agent(\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    description=\"\"\"\n+You are an expert research strategist. Generate 2 specific and distinct search terms that will capture different key aspects of the given topic.\n+Focus on terms that are:\n+    - Specific enough to yield relevant results\n+    - Cover both technical and practical aspects of the topic\n+    - Relevant to current developments\n+    - Optimized for searching academic and web resources effectively\n+\n+Provide the search terms as a list of strings like [\"xyz\", \"abc\", ...]\n+\"\"\",\n+    response_model=SearchTerms,\n+)\n+\n+arxiv_search_agent = Agent(\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    description=\"\"\"\n+You are an expert in academic research with access to ArXiv's database.\n+\n+Your task is to:\n+1. Search ArXiv for the top 10 papers related to the provided search term.\n+2. Select the 3 most relevant research papers based on:\n+    - Direct relevance to the search term.\n+    - Scientific impact (e.g., citations, journal reputation).\n+    - Recency of publication.\n+\n+For each selected paper, the output should be in json structure have these details:\n+    - title\n+    - id\n+    - authors\n+    - a concise summary\n+    - the PDF link of the research paper\n+    - links related to the research paper\n+    - reasoning for why the paper was chosen\n+\n+Ensure the selected research papers directly address the topic and offer valuable insights.\n+\"\"\",\n+    tools=[arxiv_toolkit],\n+    response_model=ArxivSearchResults,\n+)\n+\n+exa_search_agent = Agent(\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    description=\"\"\"\n+You are a web search expert specializing in extracting high-quality information.\n+\n+Your task is to:\n+1. Given a topic, search Exa for the top 10 articles about that topic.\n+2. Select the 3 most relevant articles based on:\n+    - Source credibility.\n+    - Content depth and relevance.\n+\n+For each selected article, the output should have:\n+    - title\n+    - a concise summary\n+    - related links to the article\n+    - reasoning for why the article was chosen and how it contributes to understanding the topic.\n+\n+Ensure the selected articles are credible, relevant, and provide significant insights into the topic.\n+\"\"\",\n+    tools=[ExaTools()],\n+    response_model=WebSearchResults,\n+)\n+\n+research_editor = Agent(\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    description=\"\"\"\n+You are a senior research editor specializing in breaking complex topics and information into understandable, engaging, high-quality blogs.\n+\n+Your task is to:\n+1. Create a detailed blog within 1000 words based on the given topic.\n+2. The blog should be of max 7-8 paragraphs, understandable, intuitive, making things easy to understand for the reader.\n+3. Highlight key findings and provide a clear, high-level overview of the topic.\n+4. At the end add the supporting articles link, paper link or any findings you think is necessary to add.\n+\n+The blog should help the reader in getting a decent understanding of the topic.\n+The blog should me in markdown format.\n+\"\"\",\n+    instructions=[\n+        \"Analyze all materials before writing.\",\n+        \"Build a clear narrative structure.\",\n+        \"Balance technical accuracy with explainability.\",\n+    ],\n+    markdown=True,\n+)\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/parallel_world_builder/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/parallel_world_builder/__init__.py b/cookbook/examples/streamlit_applications/parallel_world_builder/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/parallel_world_builder/agents.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/parallel_world_builder/agents.py b/cookbook/examples/streamlit_applications/parallel_world_builder/agents.py\nnew file mode 100644\nindex 000000000..62a64c193\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/parallel_world_builder/agents.py\n@@ -0,0 +1,141 @@\n+\"\"\"\ud83c\udf0e World Builder - Your AI World Creator!\n+\n+This advanced example shows how to build a sophisticated world building system that\n+creates rich, detailed fictional worlds.\n+\n+Example prompts to try:\n+- \"Create a world where time flows backwards\"\n+- \"Design a steampunk world powered by dreams\"\n+- \"Build an underwater civilization in a gas giant\"\n+- \"Make a world where music is the source of magic\"\n+- \"Design a world where plants are sentient and rule\"\n+- \"Create a world inside a giant computer simulation\"\n+\n+View the README for instructions on how to run the application.\n+\"\"\"\n+\n+from textwrap import dedent\n+from typing import List\n+\n+from agno.agent import Agent\n+from agno.models.anthropic.claude import Claude\n+from agno.models.google.gemini import Gemini\n+from agno.models.openai import OpenAIChat\n+from pydantic import BaseModel, Field\n+\n+\n+class World(BaseModel):\n+    \"\"\"Model representing a fictional world with its key attributes.\"\"\"\n+\n+    name: str = Field(\n+        ...,\n+        description=(\n+            \"The name of this world. Be exceptionally creative and unique. \"\n+            \"Avoid using simple names like Futura, Earth, or other common names.\"\n+        ),\n+    )\n+    characteristics: List[str] = Field(\n+        ...,\n+        description=(\n+            \"Key attributes of the world. Examples: Ethereal, Arcane, Quantum-Fueled, \"\n+            \"Dreamlike, Mechanized, Harmonious. Think outside the box.\"\n+        ),\n+    )\n+    currency: str = Field(\n+        ...,\n+        description=(\n+            \"The monetary system or trade medium in the world. \"\n+            \"Consider unusual or symbolic currencies (e.g., Memory Crystals, Void Shards).\"\n+        ),\n+    )\n+    languages: List[str] = Field(\n+        ...,\n+        description=(\n+            \"The languages spoken in the world. Invent languages with unique phonetics, \"\n+            \"scripts, or origins. Examples: Elurian, Syneth, Aeon's Glyph.\"\n+        ),\n+    )\n+    history: str = Field(\n+        ...,\n+        description=(\n+            \"The detailed history of the world spanning at least 100,000 years. \"\n+            \"Include pivotal events, revolutions, cataclysms, golden ages, and more. \"\n+            \"Make it immersive and richly detailed.\"\n+        ),\n+    )\n+    wars: List[str] = Field(\n+        ...,\n+        description=(\n+            \"List of major wars or conflicts that shaped the world. Each should have unique \"\n+            \"motivations, participants, and consequences.\"\n+        ),\n+    )\n+    drugs: List[str] = Field(\n+        ...,\n+        description=(\n+            \"Substances used in the world, either recreationally, medically, or spiritually. \"\n+            \"Invent intriguing names and effects (e.g., Lunar Nectar, Dreamweaver Elixir).\"\n+        ),\n+    )\n+\n+\n+def get_world_builder(\n+    model_id: str = \"openai:gpt-4o\",\n+    debug_mode: bool = False,\n+) -> Agent:\n+    \"\"\"Returns an instance of the World Builder Agent.\n+\n+    Args:\n+        model: Model identifier to use\n+        debug_mode: Enable debug logging\n+    \"\"\"\n+    # Parse model provider and name\n+    provider, model_name = model_id.split(\":\")\n+\n+    # Select appropriate model class based on provider\n+    if provider == \"openai\":\n+        model = OpenAIChat(id=model_name)\n+    elif provider == \"google\":\n+        model = Gemini(id=model_name)\n+    elif provider == \"anthropic\":\n+        model = Claude(id=model_name)\n+    else:\n+        raise ValueError(f\"Unsupported model provider: {provider}\")\n+\n+    return Agent(\n+        name=\"world_builder\",\n+        model=model,\n+        description=dedent(\"\"\"\\\n+        You are WorldCrafter-X, an elite world building specialist focused on:\n+        \n+        - Unique world concepts\n+        - Rich cultural details  \n+        - Complex histories\n+        - Innovative systems\n+        - Compelling conflicts\n+        - Immersive atmospheres\n+        \n+        You combine boundless creativity with meticulous attention to detail to craft unforgettable worlds.\"\"\"),\n+        instructions=dedent(\"\"\"\\\n+        You are tasked with creating entirely unique and intricate worlds.\n+        \n+        When a user provides a world description:\n+        1. Carefully analyze all aspects of the requested world\n+        2. Think deeply about how different elements would interact\n+        3. Create rich, interconnected systems and histories\n+        4. Ensure internal consistency while being creative\n+        5. Focus on unique and memorable details\n+        6. Avoid clich\u00e9s and common tropes\n+        7. Consider long-term implications of world features\n+        8. Create compelling conflicts and dynamics\n+        \n+        Remember to:\n+        - Push creative boundaries\n+        - Use vivid, evocative language\n+        - Create memorable names and terms\n+        - Maintain internal logic\n+        - Consider multiple cultural perspectives\n+        - Add unexpected but fitting elements\"\"\"),\n+        response_model=World,\n+        debug_mode=debug_mode,\n+    )\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/parallel_world_builder/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/parallel_world_builder/app.py b/cookbook/examples/streamlit_applications/parallel_world_builder/app.py\nnew file mode 100644\nindex 000000000..df71ee8f5\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/parallel_world_builder/app.py\n@@ -0,0 +1,166 @@\n+from typing import Optional\n+\n+import streamlit as st\n+from agents import World, get_world_builder\n+from agno.agent import Agent\n+from agno.utils.log import logger\n+from utils import add_message, display_tool_calls, sidebar_widget\n+\n+# set page config\n+st.set_page_config(\n+    page_title=\"World Building\",\n+    page_icon=\":ringed_planet:\",\n+    layout=\"wide\",\n+    initial_sidebar_state=\"expanded\",\n+)\n+\n+\n+def main() -> None:\n+    ####################################################################\n+    # App header\n+    ####################################################################\n+    st.markdown(\n+        \"<h1 class='main-title'>Parallel World Building</h1>\", unsafe_allow_html=True\n+    )\n+    st.markdown(\n+        \"<p class='subtitle'>Your intelligent world creator powered by Agno</p>\",\n+        unsafe_allow_html=True,\n+    )\n+\n+    ####################################################################\n+    # Model selector\n+    ####################################################################\n+    model_options = {\n+        \"gpt-4o\": \"openai:gpt-4o\",\n+        \"gemini-2.0-flash-exp\": \"google:gemini-2.0-flash-exp\",\n+        \"claude-3-5-sonnet\": \"anthropic:claude-3-5-sonnet-20241022\",\n+    }\n+    selected_model = st.sidebar.selectbox(\n+        \"Select a model\",\n+        options=list(model_options.keys()),\n+        index=0,\n+        key=\"model_selector\",\n+    )\n+    model_id = model_options[selected_model]\n+\n+    ####################################################################\n+    # Initialize Agent\n+    ####################################################################\n+    world_builder: Agent\n+    if (\n+        \"world_builder\" not in st.session_state\n+        or st.session_state[\"world_builder\"] is None\n+        or st.session_state.get(\"current_model\") != model_id\n+    ):\n+        logger.info(\"---*--- Creating new World Builder agent ---*---\")\n+        world_builder = get_world_builder(model_id=model_id)\n+        st.session_state[\"world_builder\"] = world_builder\n+        st.session_state[\"current_model\"] = model_id\n+    else:\n+        world_builder = st.session_state[\"world_builder\"]\n+\n+    ####################################################################\n+    # Initialize messages if not exists\n+    ####################################################################\n+    if \"messages\" not in st.session_state:\n+        st.session_state[\"messages\"] = []\n+\n+    ####################################################################\n+    # Sidebar\n+    ####################################################################\n+    sidebar_widget()\n+\n+    ####################################################################\n+    # Get user input\n+    ####################################################################\n+    if prompt := st.chat_input(\"Describe your world! \ud83c\udf0f\"):\n+        add_message(\"user\", prompt)\n+\n+    ####################################################################\n+    # Display chat history\n+    ####################################################################\n+    for message in st.session_state[\"messages\"]:\n+        if message[\"role\"] in [\"user\", \"assistant\"]:\n+            with st.chat_message(message[\"role\"]):\n+                if \"tool_calls\" in message and message[\"tool_calls\"]:\n+                    display_tool_calls(st.empty(), message[\"tool_calls\"])\n+                st.markdown(message[\"content\"])\n+\n+    ####################################################################\n+    # Generate response for user message\n+    ####################################################################\n+    last_message = (\n+        st.session_state[\"messages\"][-1] if st.session_state[\"messages\"] else None\n+    )\n+    if last_message and last_message.get(\"role\") == \"user\":\n+        question = last_message[\"content\"]\n+        with st.chat_message(\"assistant\"):\n+            # Create container for tool calls\n+            tool_calls_container = st.empty()\n+            resp_container = st.empty()\n+            with st.spinner(\"\ud83e\udd14 Generating world...\"):\n+                try:\n+                    # Run the agent and get response\n+                    run_response = world_builder.run(question)\n+                    world_data: World = run_response.content\n+\n+                    # Display world details in a single column layout\n+                    st.header(world_data.name)\n+\n+                    st.subheader(\"\ud83c\udf1f Characteristics\")\n+                    for char in world_data.characteristics:\n+                        st.markdown(f\"- {char}\")\n+\n+                    st.subheader(\"\ud83d\udcb0 Currency\")\n+                    st.markdown(world_data.currency)\n+\n+                    st.subheader(\"\ud83d\udde3\ufe0f Languages\")\n+                    for lang in world_data.languages:\n+                        st.markdown(f\"- {lang}\")\n+\n+                    st.subheader(\"\u2694\ufe0f Major Wars & Conflicts\")\n+                    for war in world_data.wars:\n+                        st.markdown(f\"- {war}\")\n+\n+                    st.subheader(\"\ud83e\uddea Notable Substances\")\n+                    for drug in world_data.drugs:\n+                        st.markdown(f\"- {drug}\")\n+\n+                    st.subheader(\"\ud83d\udcdc History\")\n+                    st.markdown(world_data.history)\n+\n+                    # Store the formatted response for chat history\n+                    response = f\"\"\"# {world_data.name}\n+\n+### Characteristics\n+{chr(10).join(\"- \" + char for char in world_data.characteristics)}\n+\n+### Currency\n+{world_data.currency}\n+\n+### Languages\n+{chr(10).join(\"- \" + lang for lang in world_data.languages)}\n+\n+### History\n+{world_data.history}\n+\n+### Major Wars & Conflicts\n+{chr(10).join(\"- \" + war for war in world_data.wars)}\n+\n+### Notable Substances\n+{chr(10).join(\"- \" + drug for drug in world_data.drugs)}\"\"\"\n+\n+                    # Display tool calls if available\n+                    if run_response.tools and len(run_response.tools) > 0:\n+                        display_tool_calls(tool_calls_container, run_response.tools)\n+\n+                    add_message(\"assistant\", response, run_response.tools)\n+\n+                except Exception as e:\n+                    error_message = f\"Sorry, I encountered an error: {str(e)}\"\n+                    add_message(\"assistant\", error_message)\n+                    st.error(error_message)\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/parallel_world_builder/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/parallel_world_builder/requirements.txt b/cookbook/examples/streamlit_applications/parallel_world_builder/requirements.txt\nnew file mode 100644\nindex 000000000..4d4ef595d\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/parallel_world_builder/requirements.txt\n@@ -0,0 +1,5 @@\n+agno\n+openai\n+streamlit\n+google-generativeai\n+anthropic\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/parallel_world_builder/utils.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/parallel_world_builder/utils.py b/cookbook/examples/streamlit_applications/parallel_world_builder/utils.py\nnew file mode 100644\nindex 000000000..df888b707\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/parallel_world_builder/utils.py\n@@ -0,0 +1,99 @@\n+from typing import Any, Dict, List, Optional\n+\n+import streamlit as st\n+from agno.utils.log import logger\n+\n+\n+def add_message(\n+    role: str, content: str, tool_calls: Optional[List[Dict[str, Any]]] = None\n+) -> None:\n+    \"\"\"Safely add a message to the session state\"\"\"\n+    if \"messages\" not in st.session_state or not isinstance(\n+        st.session_state[\"messages\"], list\n+    ):\n+        st.session_state[\"messages\"] = []\n+    st.session_state[\"messages\"].append(\n+        {\"role\": role, \"content\": content, \"tool_calls\": tool_calls}\n+    )\n+\n+\n+def restart_agent():\n+    \"\"\"Reset the agent and clear chat history\"\"\"\n+    logger.debug(\"---*--- Restarting agent ---*---\")\n+    st.session_state[\"world_builder\"] = None\n+    st.session_state[\"world\"] = None\n+    st.session_state[\"messages\"] = []\n+    st.rerun()\n+\n+\n+def sidebar_widget() -> None:\n+    \"\"\"Display a sidebar with sample user queries\"\"\"\n+    with st.sidebar:\n+        # Basic Information\n+        st.markdown(\"#### Sample Queries\")\n+        if st.button(\n+            \"An advanced futuristic city on distant planet with only 1 island. Dark history. Population 1 trillion.\"\n+        ):\n+            add_message(\n+                \"user\",\n+                \"An advanced futuristic city on distant planet with only 1 island. Dark history. Population 1 trillion.\",\n+            )\n+        if st.button(\"A medieval fantasy world with dragons, castles, and knights.\"):\n+            add_message(\n+                \"user\",\n+                \"A medieval fantasy world with dragons, castles, and knights.\",\n+            )\n+        if st.button(\n+            \"A post-apocalyptic world with a nuclear wasteland and a small community living in a dome.\"\n+        ):\n+            add_message(\n+                \"user\",\n+                \"A post-apocalyptic world with a nuclear wasteland and a small community living in a dome.\",\n+            )\n+        if st.button(\n+            \"A world with a mix of ancient and modern civilizations, where magic and technology coexist.\"\n+        ):\n+            add_message(\n+                \"user\",\n+                \"A world with a mix of ancient and modern civilizations, where magic and technology coexist.\",\n+            )\n+\n+        st.markdown(\"---\")\n+        if st.button(\"\ud83d\udd04 New World\"):\n+            restart_agent()\n+\n+\n+def display_tool_calls(tool_calls_container, tools):\n+    \"\"\"Display tool calls in a streamlit container with expandable sections.\n+\n+    Args:\n+        tool_calls_container: Streamlit container to display the tool calls\n+        tools: List of tool call dictionaries containing name, args, content, and metrics\n+    \"\"\"\n+    with tool_calls_container.container():\n+        for tool_call in tools:\n+            _tool_name = tool_call.get(\"tool_name\")\n+            _tool_args = tool_call.get(\"tool_args\")\n+            _content = tool_call.get(\"content\")\n+            _metrics = tool_call.get(\"metrics\")\n+\n+            with st.expander(\n+                f\"\ud83d\udee0\ufe0f {_tool_name.replace('_', ' ').title()}\", expanded=False\n+            ):\n+                if isinstance(_tool_args, dict) and \"query\" in _tool_args:\n+                    st.code(_tool_args[\"query\"], language=\"sql\")\n+\n+                if _tool_args and _tool_args != {\"query\": None}:\n+                    st.markdown(\"**Arguments:**\")\n+                    st.json(_tool_args)\n+\n+                if _content:\n+                    st.markdown(\"**Results:**\")\n+                    try:\n+                        st.json(_content)\n+                    except Exception as e:\n+                        st.markdown(_content)\n+\n+                if _metrics:\n+                    st.markdown(\"**Metrics:**\")\n+                    st.json(_metrics)\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/podcast_generator/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/podcast_generator/__init__.py b/cookbook/examples/streamlit_applications/podcast_generator/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/podcast_generator/agents.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/podcast_generator/agents.py b/cookbook/examples/streamlit_applications/podcast_generator/agents.py\nnew file mode 100644\nindex 000000000..4f7afc67a\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/podcast_generator/agents.py\n@@ -0,0 +1,78 @@\n+import os\n+\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.utils.audio import write_audio_to_file\n+from dotenv import load_dotenv\n+\n+# Load environment variables\n+load_dotenv()\n+\n+# OpenAI API Key\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+\n+os.makedirs(\"tmp\", exist_ok=True)\n+\n+\n+def generate_podcast(topic, voice=\"alloy\"):\n+    \"\"\"\n+    Generates a podcast script using a agnodata Agent and converts it to speech using OpenAI TTS.\n+\n+    Args:\n+        topic (str): The topic of the podcast.\n+        voice (str): Voice model for OpenAI TTS. Options: [\"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"]\n+    \"\"\"\n+\n+    # Create a agnodata agent to generate the podcast script\n+    audio_agent = Agent(\n+        system_message=\"You are a podcast scriptwriter specializing in concise and engaging narratives. Your task is to research a given topic using Exa and DuckDuckGo, gather relevant insights, and compose a short, compelling podcast script.\",\n+        instructions=\"\"\"### **Instructions:**\n+            1. **Research Phase:**\n+            - Use Exa and DuckDuckGo to gather the most recent and relevant information on the given topic.\n+            - Prioritize trustworthy sources such as news sites, academic articles, or well-established blogs.\n+            - Identify key points, statistics, expert opinions, and interesting facts.\n+\n+            2. **Scripting Phase:**\n+            - Write a concise podcast script in a conversational tone.\n+            - Begin with a strong hook to capture the listener's attention.\n+            - Present the key insights in an engaging, easy-to-follow manner.\n+            - Include a smooth transition between ideas to maintain narrative flow.\n+            - End with a closing remark that summarizes the main takeaways and encourages further curiosity.\n+\n+            ### **Formatting Guidelines:**\n+            - Use simple, engaging language.\n+            - Keep the script under 300 words (around 2 minutes of audio).\n+            - Write in a natural, spoken format, avoiding overly formal or technical jargon.\n+            - Start with a short intro of the topic, then cover the main content, and conclude.\n+\n+            ### **Example Output:**\n+            #### **Today we will be covering the topic The Future of AI in Healthcare**\n+            \"Imagine walking into a hospital where AI instantly diagnoses your illness, prescribes treatment, and even assists in surgery. Sounds like science fiction? Well, it's closer to reality than you think! Welcome to today's episode, where we explore how AI is revolutionizing healthcare.\"\n+\n+            \"AI is making waves in medical research, diagnostics, and patient care. For instance, Google's DeepMind developed an AI that can detect over 50 eye diseases with a single scan\u2014just as accurately as top doctors! Meanwhile, robotic surgeries assisted by AI are reducing complications and recovery time for patients. But it's not just about tech\u2014AI is also addressing healthcare accessibility. In rural areas, AI-powered chatbots provide medical advice where doctors are scarce.\"\n+\n+            \"While AI in healthcare is promising, it also raises ethical concerns. Who takes responsibility for a wrong diagnosis? How do we ensure data privacy? These are crucial questions for the future. One thing's for sure\u2014AI is here to stay, and it's reshaping medicine as we know it. Thanks for tuning in, and see you next time!\"\n+            \"\"\",\n+        model=OpenAIChat(\n+            id=\"gpt-4o-audio-preview\",\n+            modalities=[\"text\", \"audio\"],\n+            audio={\"voice\": voice, \"format\": \"wav\"},\n+        ),\n+        tools=[DuckDuckGoTools()],\n+    )\n+\n+    # Generate the podcast script\n+    audio_agent.run(f\"Write the content of podcast for the topic: {topic}\")\n+    audio_file_path = \"tmp/generated_podcast.wav\"\n+\n+    if audio_agent.run_response.response_audio is not None:\n+        audio_content = audio_agent.run_response.response_audio.content\n+\n+        if audio_content:\n+            write_audio_to_file(\n+                audio=audio_content,\n+                filename=audio_file_path,\n+            )\n+            return audio_file_path\n+    return None\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/podcast_generator/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/podcast_generator/app.py b/cookbook/examples/streamlit_applications/podcast_generator/app.py\nnew file mode 100644\nindex 000000000..e36ba1f14\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/podcast_generator/app.py\n@@ -0,0 +1,122 @@\n+import streamlit as st\n+from agents import generate_podcast\n+\n+# Streamlit App Configuration\n+st.set_page_config(\n+    page_title=\"Podify AI \ud83c\udf99\ufe0f\",\n+    page_icon=\"\ud83c\udfa7\",\n+    layout=\"wide\",\n+)\n+\n+# Sidebar Section\n+with st.sidebar:\n+    st.title(\"\ud83c\udfa7 Podify AI\")\n+    st.markdown(\"AI voices to generate an **engaging podcast**!\")\n+\n+    # Voice Selection\n+    voice_options = [\"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"]\n+    selected_voice = st.selectbox(\"\ud83c\udfa4 Choose a Voice:\", voice_options, index=0)\n+\n+    st.markdown(\"---\")\n+    st.subheader(\"\ud83d\udd25 Suggested Topics:\")\n+    trending_topics = [\n+        \"\ud83c\udfad Impact of AI on Creativity and Art\",\n+        \"\ud83d\udca1 Elon Musk vs Sam Altman\",\n+        \"\ud83c\udfe5 Using AI in healthcare\",\n+        \"\ud83d\ude80 The Future of Space Exploration\",\n+    ]\n+\n+    # Create row-wise aligned buttons\n+    num_cols = 1  # Change this to 3 for three buttons in a row\n+    cols = st.columns(num_cols)  # Define columns\n+\n+    for i, topic in enumerate(trending_topics):\n+        with cols[i % num_cols]:  # Distribute buttons evenly across columns\n+            if st.button(topic):\n+                st.session_state[\"topic\"] = topic\n+                st.session_state[\"generate\"] = True\n+\n+    st.markdown(\"---\")\n+    st.subheader(\"\u2139\ufe0f About\")\n+    st.markdown(\n+        \"\"\"\n+        - Enter a **topic** <br>\n+        - **Select a voice** <br>\n+        - **Click Generate Podcast** <br>\n+        - **Listen & Download** the AI-generated audio\n+        \"\"\",\n+        unsafe_allow_html=True,\n+    )\n+\n+# Main Content\n+st.title(\"Podify AI\ud83c\udf99\ufe0f\")\n+st.markdown(\":orange_heart: **powered by Agno**\")\n+\n+st.markdown(\n+    \"Create high-quality podcasts on **any topic**! Simply enter a topic and let Podify AI generate a professional podcast with **realistic AI voices**. \ud83d\ude80\"\n+)\n+\n+# Get pre-selected topic from sidebar\n+pre_selected_topic = st.session_state.get(\"topic\", \"\")\n+\n+# Input for Podcast Topic\n+topic = st.text_input(\n+    \"\ud83d\udcd6 **Enter Your Podcast Topic Below:**\",\n+    placeholder=\"E.g., How AI is Changing the Job Market\",\n+    value=pre_selected_topic,\n+)\n+\n+# Check if auto-generation is triggered\n+generate_now = st.session_state.get(\"generate\", False)\n+\n+\n+# Generate Podcast Function\n+def generate_and_display_podcast(topic):\n+    with st.spinner(\"\u23f3 Generating Podcast... This may take up to 2 minute...\"):\n+        audio_path = generate_podcast(topic, selected_voice)\n+\n+    if audio_path:\n+        st.success(\"\u2705 Podcast generated successfully!\")\n+\n+        st.subheader(\"\ud83c\udfa7 Your AI Podcast\")\n+        st.audio(audio_path, format=\"audio/wav\")\n+\n+        with open(audio_path, \"rb\") as audio_file:\n+            st.download_button(\n+                \"\u2b07\ufe0f Download Podcast\",\n+                audio_file,\n+                file_name=\"podcast.wav\",\n+                mime=\"audio/wav\",\n+            )\n+\n+    else:\n+        st.error(\"\u274c Failed to generate podcast. Please try again.\")\n+\n+\n+# Auto-generate podcast if a trending topic is selected\n+if generate_now and topic:\n+    generate_and_display_podcast(topic)\n+    st.session_state[\"generate\"] = False  # Reset the flag after generation\n+\n+# Manual Generate Podcast Button\n+if st.button(\"\ud83c\udfac Generate Podcast\"):\n+    if topic:\n+        generate_and_display_podcast(topic)\n+    else:\n+        st.warning(\"\u26a0\ufe0f Please enter a topic before generating.\")\n+\n+# Footer Section\n+st.markdown(\"---\")\n+st.markdown(\n+    \"\"\"\n+    \ud83c\udf1f **Features:**\n+    - \ud83c\udf99\ufe0f AI-generated podcast scripts based on **real-time research**.\n+    - \ud83d\udde3\ufe0f Multiple **realistic voices** for narration.\n+    - \ud83d\udce5 **Download & share** your podcasts instantly.\n+\n+    \ud83d\udce2 **Disclaimer:** AI-generated content is based on available online data and may not always be accurate.\n+    \"\"\",\n+    unsafe_allow_html=True,\n+)\n+st.markdown(\"---\")\n+st.markdown(\":orange_heart: **Thank you for using Podify AI!**\")\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/podcast_generator/generate_requirements.sh",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/podcast_generator/generate_requirements.sh b/cookbook/examples/streamlit_applications/podcast_generator/generate_requirements.sh\nnew file mode 100755\nindex 000000000..78f8c7cec\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/podcast_generator/generate_requirements.sh\n@@ -0,0 +1,12 @@\n+#!/bin/bash\n+\n+############################################################################\n+# Generate requirements.txt from requirements.in\n+############################################################################\n+\n+echo \"Generating requirements.txt\"\n+\n+CURR_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n+\n+UV_CUSTOM_COMPILE_COMMAND=\"./generate_requirements.sh\" \\\n+  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/podcast_generator/requirements.in",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/podcast_generator/requirements.in b/cookbook/examples/streamlit_applications/podcast_generator/requirements.in\nnew file mode 100644\nindex 000000000..d55e11faf\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/podcast_generator/requirements.in\n@@ -0,0 +1,4 @@\n+agno\n+openai\n+streamlit\n+duckduckgo-search\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/podcast_generator/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/podcast_generator/requirements.txt b/cookbook/examples/streamlit_applications/podcast_generator/requirements.txt\nnew file mode 100644\nindex 000000000..3e0299183\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/podcast_generator/requirements.txt\n@@ -0,0 +1,177 @@\n+# This file was autogenerated by uv via the following command:\n+#    ./generate_requirements.sh\n+agno==1.1.0\n+    # via -r requirements.in\n+altair==5.5.0\n+    # via streamlit\n+annotated-types==0.7.0\n+    # via pydantic\n+anyio==4.8.0\n+    # via\n+    #   httpx\n+    #   openai\n+attrs==25.1.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+blinker==1.9.0\n+    # via streamlit\n+cachetools==5.5.1\n+    # via streamlit\n+certifi==2025.1.31\n+    # via\n+    #   httpcore\n+    #   httpx\n+    #   requests\n+charset-normalizer==3.4.1\n+    # via requests\n+click==8.1.8\n+    # via\n+    #   duckduckgo-search\n+    #   streamlit\n+    #   typer\n+distro==1.9.0\n+    # via openai\n+docstring-parser==0.16\n+    # via agno\n+duckduckgo-search==7.3.2\n+    # via -r requirements.in\n+gitdb==4.0.12\n+    # via gitpython\n+gitpython==3.1.44\n+    # via\n+    #   agno\n+    #   streamlit\n+h11==0.14.0\n+    # via httpcore\n+httpcore==1.0.7\n+    # via httpx\n+httpx==0.28.1\n+    # via\n+    #   agno\n+    #   openai\n+idna==3.10\n+    # via\n+    #   anyio\n+    #   httpx\n+    #   requests\n+jinja2==3.1.5\n+    # via\n+    #   altair\n+    #   pydeck\n+jiter==0.8.2\n+    # via openai\n+jsonschema==4.23.0\n+    # via altair\n+jsonschema-specifications==2024.10.1\n+    # via jsonschema\n+lxml==5.3.1\n+    # via duckduckgo-search\n+markdown-it-py==3.0.0\n+    # via rich\n+markupsafe==3.0.2\n+    # via jinja2\n+mdurl==0.1.2\n+    # via markdown-it-py\n+narwhals==1.26.0\n+    # via altair\n+numpy==2.2.2\n+    # via\n+    #   pandas\n+    #   pydeck\n+    #   streamlit\n+openai==1.62.0\n+    # via -r requirements.in\n+packaging==24.2\n+    # via\n+    #   altair\n+    #   streamlit\n+pandas==2.2.3\n+    # via streamlit\n+pillow==11.1.0\n+    # via streamlit\n+primp==0.12.1\n+    # via duckduckgo-search\n+protobuf==5.29.3\n+    # via streamlit\n+pyarrow==19.0.0\n+    # via streamlit\n+pydantic==2.10.6\n+    # via\n+    #   agno\n+    #   openai\n+    #   pydantic-settings\n+pydantic-core==2.27.2\n+    # via pydantic\n+pydantic-settings==2.7.1\n+    # via agno\n+pydeck==0.9.1\n+    # via streamlit\n+pygments==2.19.1\n+    # via rich\n+python-dateutil==2.9.0.post0\n+    # via pandas\n+python-dotenv==1.0.1\n+    # via\n+    #   agno\n+    #   pydantic-settings\n+python-multipart==0.0.20\n+    # via agno\n+pytz==2025.1\n+    # via pandas\n+pyyaml==6.0.2\n+    # via agno\n+referencing==0.36.2\n+    # via\n+    #   jsonschema\n+    #   jsonschema-specifications\n+requests==2.32.3\n+    # via streamlit\n+rich==13.9.4\n+    # via\n+    #   agno\n+    #   streamlit\n+    #   typer\n+rpds-py==0.22.3\n+    # via\n+    #   jsonschema\n+    #   referencing\n+shellingham==1.5.4\n+    # via typer\n+six==1.17.0\n+    # via python-dateutil\n+smmap==5.0.2\n+    # via gitdb\n+sniffio==1.3.1\n+    # via\n+    #   anyio\n+    #   openai\n+streamlit==1.42.0\n+    # via -r requirements.in\n+tenacity==9.0.0\n+    # via streamlit\n+toml==0.10.2\n+    # via streamlit\n+tomli==2.2.1\n+    # via agno\n+tornado==6.4.2\n+    # via streamlit\n+tqdm==4.67.1\n+    # via openai\n+typer==0.15.1\n+    # via agno\n+typing-extensions==4.12.2\n+    # via\n+    #   agno\n+    #   altair\n+    #   anyio\n+    #   openai\n+    #   pydantic\n+    #   pydantic-core\n+    #   referencing\n+    #   streamlit\n+    #   typer\n+tzdata==2025.1\n+    # via pandas\n+urllib3==2.3.0\n+    # via requests\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/sql_agent/.gitignore",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/sql_agent/.gitignore b/cookbook/examples/streamlit_applications/sql_agent/.gitignore\nnew file mode 100644\nindex 000000000..53752db25\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/sql_agent/.gitignore\n@@ -0,0 +1 @@\n+output\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/sql_agent/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/sql_agent/__init__.py b/cookbook/examples/streamlit_applications/sql_agent/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/sql_agent/agents.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/sql_agent/agents.py b/cookbook/examples/streamlit_applications/sql_agent/agents.py\nnew file mode 100644\nindex 000000000..853e0d82e\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/sql_agent/agents.py\n@@ -0,0 +1,247 @@\n+\"\"\"\ud83d\udc8e Reasoning SQL Agent - Your AI Data Analyst!\n+\n+This advanced example shows how to build a sophisticated text-to-SQL system that\n+leverages Reasoning Agents to provide deep insights into any data.\n+\n+Example queries to try:\n+- \"Who are the top 5 drivers with the most race wins?\"\n+- \"Compare Mercedes vs Ferrari performance in constructors championships\"\n+- \"Show me the progression of fastest lap times at Monza\"\n+- \"Which drivers have won championships with multiple teams?\"\n+- \"What tracks have hosted the most races?\"\n+- \"Show me Lewis Hamilton's win percentage by season\"\n+\n+Examples with table joins:\n+- \"How many races did the championship winners win each year?\"\n+- \"Compare the number of race wins vs championship positions for constructors in 2019\"\n+- \"Show me Lewis Hamilton's race wins and championship positions by year\"\n+- \"Which drivers have both won races and set fastest laps at Monaco?\"\n+- \"Show me Ferrari's race wins and constructor championship positions from 2015-2020\"\n+\n+View the README for instructions on how to run the application.\n+\"\"\"\n+\n+import json\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Optional\n+\n+from agno.agent import Agent\n+from agno.embedder.openai import OpenAIEmbedder\n+from agno.knowledge.combined import CombinedKnowledgeBase\n+from agno.knowledge.json import JSONKnowledgeBase\n+from agno.knowledge.text import TextKnowledgeBase\n+from agno.models.anthropic import Claude\n+from agno.models.google import Gemini\n+from agno.models.groq import Groq\n+from agno.models.openai import OpenAIChat\n+from agno.storage.agent.postgres import PostgresAgentStorage\n+from agno.tools.file import FileTools\n+from agno.tools.reasoning import ReasoningTools\n+from agno.tools.sql import SQLTools\n+from agno.vectordb.pgvector import PgVector\n+\n+# ************* Database Connection *************\n+db_url = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n+# *******************************\n+\n+# ************* Paths *************\n+cwd = Path(__file__).parent\n+knowledge_dir = cwd.joinpath(\"knowledge\")\n+output_dir = cwd.joinpath(\"output\")\n+\n+# Create the output directory if it does not exist\n+output_dir.mkdir(parents=True, exist_ok=True)\n+# *******************************\n+\n+# ************* Storage & Knowledge *************\n+agent_storage = PostgresAgentStorage(\n+    db_url=db_url,\n+    # Store agent sessions in the ai.sql_agent_sessions table\n+    table_name=\"sql_agent_sessions\",\n+    schema=\"ai\",\n+)\n+agent_knowledge = CombinedKnowledgeBase(\n+    sources=[\n+        # Reads text files, SQL files, and markdown files\n+        TextKnowledgeBase(\n+            path=knowledge_dir,\n+            formats=[\".txt\", \".sql\", \".md\"],\n+        ),\n+        # Reads JSON files\n+        JSONKnowledgeBase(path=knowledge_dir),\n+    ],\n+    # Store agent knowledge in the ai.sql_agent_knowledge table\n+    vector_db=PgVector(\n+        db_url=db_url,\n+        table_name=\"sql_agent_knowledge\",\n+        schema=\"ai\",\n+        # Use OpenAI embeddings\n+        embedder=OpenAIEmbedder(id=\"text-embedding-3-small\"),\n+    ),\n+    # 5 references are added to the prompt\n+    num_documents=5,\n+)\n+# *******************************\n+\n+# ************* Semantic Model *************\n+# The semantic model helps the agent identify the tables and columns to use\n+# This is sent in the system prompt, the agent then uses the `search_knowledge_base` tool to get table metadata, rules and sample queries\n+# This is very much how data analysts and data scientists work:\n+#  - We start with a set of tables and columns that we know are relevant to the task\n+#  - We then use the `search_knowledge_base` tool to get more information about the tables and columns\n+#  - We then use the `describe_table` tool to get more information about the tables and columns\n+#  - We then use the `search_knowledge_base` tool to get sample queries for the tables and columns\n+semantic_model = {\n+    \"tables\": [\n+        {\n+            \"table_name\": \"constructors_championship\",\n+            \"table_description\": \"Contains data for the constructor's championship from 1958 to 2020, capturing championship standings from when it was introduced.\",\n+            \"Use Case\": \"Use this table to get data on constructor's championship for various years or when analyzing team performance over the years.\",\n+        },\n+        {\n+            \"table_name\": \"drivers_championship\",\n+            \"table_description\": \"Contains data for driver's championship standings from 1950-2020, detailing driver positions, teams, and points.\",\n+            \"Use Case\": \"Use this table to access driver championship data, useful for detailed driver performance analysis and comparisons over years.\",\n+        },\n+        {\n+            \"table_name\": \"fastest_laps\",\n+            \"table_description\": \"Contains data for the fastest laps recorded in races from 1950-2020, including driver and team details.\",\n+            \"Use Case\": \"Use this table when needing detailed information on the fastest laps in Formula 1 races, including driver, team, and lap time data.\",\n+        },\n+        {\n+            \"table_name\": \"race_results\",\n+            \"table_description\": \"Race data for each Formula 1 race from 1950-2020, including positions, drivers, teams, and points.\",\n+            \"Use Case\": \"Use this table answer questions about a drivers career. Race data includes driver standings, teams, and performance.\",\n+        },\n+        {\n+            \"table_name\": \"race_wins\",\n+            \"table_description\": \"Documents race win data from 1950-2020, detailing venue, winner, team, and race duration.\",\n+            \"Use Case\": \"Use this table for retrieving data on race winners, their teams, and race conditions, suitable for analysis of race outcomes and team success.\",\n+        },\n+    ]\n+}\n+semantic_model_str = json.dumps(semantic_model, indent=2)\n+# *******************************\n+\n+\n+def get_sql_agent(\n+    name: str = \"SQL Agent\",\n+    user_id: Optional[str] = None,\n+    model_id: str = \"openai:gpt-4o\",\n+    session_id: Optional[str] = None,\n+    debug_mode: bool = True,\n+) -> Agent:\n+    \"\"\"Returns an instance of the SQL Agent.\n+\n+    Args:\n+        user_id: Optional user identifier\n+        debug_mode: Enable debug logging\n+        model_id: Model identifier in format 'provider:model_name'\n+    \"\"\"\n+    # Parse model provider and name\n+    provider, model_name = model_id.split(\":\")\n+\n+    # Select appropriate model class based on provider\n+    if provider == \"openai\":\n+        model = OpenAIChat(id=model_name)\n+    elif provider == \"google\":\n+        model = Gemini(id=model_name)\n+    elif provider == \"anthropic\":\n+        model = Claude(id=model_name)\n+    elif provider == \"groq\":\n+        model = Groq(id=model_name)\n+    else:\n+        raise ValueError(f\"Unsupported model provider: {provider}\")\n+\n+    return Agent(\n+        name=name,\n+        model=model,\n+        user_id=user_id,\n+        session_id=session_id,\n+        storage=agent_storage,\n+        knowledge=agent_knowledge,\n+        # Enable Agentic RAG i.e. the ability to search the knowledge base on-demand\n+        search_knowledge=True,\n+        # Enable the ability to read the chat history\n+        read_chat_history=True,\n+        # Enable the ability to read the tool call history\n+        read_tool_call_history=True,\n+        # Add tools to the agent\n+        tools=[\n+            SQLTools(db_url=db_url, list_tables=False),\n+            FileTools(base_dir=output_dir),\n+            ReasoningTools(add_instructions=True, add_few_shot=True),\n+        ],\n+        debug_mode=debug_mode,\n+        description=dedent(\"\"\"\\\n+        You are SQrL, an elite Text2SQL Engine specializing in:\n+\n+        - Historical race analysis\n+        - Driver performance metrics\n+        - Team championship insights\n+        - Track statistics and records\n+        - Performance trend analysis\n+        - Race strategy evaluation\n+\n+        You combine deep F1 knowledge with advanced SQL expertise to uncover insights from decades of racing data.\"\"\"),\n+        instructions=dedent(f\"\"\"\\\n+        You are a SQL expert focused on writing precise, efficient queries.\n+\n+        When a user messages you, determine if you need query the database or can respond directly.\n+        If you can respond directly, do so.\n+\n+        If you need to query the database to answer the user's question, follow these steps:\n+        1. First identify the tables you need to query from the semantic model.\n+        2. Then, ALWAYS use the `search_knowledge_base(table_name)` tool to get table metadata, rules and sample queries.\n+        3. If table rules are provided, ALWAYS follow them.\n+        4. Then, \"think\" about query construction, don't rush this step.\n+        5. Follow a chain of thought approach before writing SQL, ask clarifying questions where needed.\n+        6. If sample queries are available, use them as a reference.\n+        7. If you need more information about the table, use the `describe_table` tool.\n+        8. Then, using all the information available, create one single syntactically correct PostgreSQL query to accomplish your task.\n+        9. If you need to join tables, check the `semantic_model` for the relationships between the tables.\n+            - If the `semantic_model` contains a relationship between tables, use that relationship to join the tables even if the column names are different.\n+            - If you cannot find a relationship in the `semantic_model`, only join on the columns that have the same name and data type.\n+            - If you cannot find a valid relationship, ask the user to provide the column name to join.\n+        10. If you cannot find relevant tables, columns or relationships, stop and ask the user for more information.\n+        11. Once you have a syntactically correct query, run it using the `run_sql_query` function.\n+        12. When running a query:\n+            - Do not add a `;` at the end of the query.\n+            - Always provide a limit unless the user explicitly asks for all results.\n+        13. After you run the query, \"analyze\" the results and return the answer in markdown format.\n+        14. Make sure to always \"analyze\" the results of the query before returning the answer.\n+        15. You Analysis should Reason about the results of the query, whether they make sense, whether they are complete, whether they are correct, could there be any data quality issues, etc.\n+        16. It is really important that you \"analyze\" and \"validate\" the results of the query.\n+        17. Always show the user the SQL you ran to get the answer.\n+        18. Continue till you have accomplished the task.\n+        19. Show results as a table or a chart if possible.\n+\n+        After finishing your task, ask the user relevant followup questions like \"was the result okay, would you like me to fix any problems?\"\n+        If the user says yes, get the previous query using the `get_tool_call_history(num_calls=3)` function and fix the problems.\n+        If the user wants to see the SQL, get it using the `get_tool_call_history(num_calls=3)` function.\n+\n+        Finally, here are the set of rules that you MUST follow:\n+\n+        <rules>\n+        - Use the `search_knowledge_base(table_name)` tool to get table information from your knowledge base before writing a query.\n+        - Do not use phrases like \"based on the information provided\" or \"from the knowledge base\".\n+        - Always show the SQL queries you use to get the answer.\n+        - Make sure your query accounts for duplicate records.\n+        - Make sure your query accounts for null values.\n+        - If you run a query, explain why you ran it.\n+        - Always derive your answer from the data and the query.\n+        - **NEVER, EVER RUN CODE TO DELETE DATA OR ABUSE THE LOCAL SYSTEM**\n+        - ALWAYS FOLLOW THE `table rules` if provided. NEVER IGNORE THEM.\n+        </rules>\\\n+        \"\"\"),\n+        additional_context=dedent(\"\"\"\\n\n+        The `semantic_model` contains information about tables and the relationships between them.\n+        If the users asks about the tables you have access to, simply share the table names from the `semantic_model`.\n+        <semantic_model>\n+        \"\"\")\n+        + semantic_model_str\n+        + dedent(\"\"\"\n+        </semantic_model>\\\n+        \"\"\"),\n+    )\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/sql_agent/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/sql_agent/app.py b/cookbook/examples/streamlit_applications/sql_agent/app.py\nnew file mode 100644\nindex 000000000..fbe27508c\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/sql_agent/app.py\n@@ -0,0 +1,178 @@\n+import nest_asyncio\n+import streamlit as st\n+from agents import get_sql_agent\n+from agno.agent import Agent\n+from agno.utils.log import logger\n+from utils import (\n+    CUSTOM_CSS,\n+    about_widget,\n+    add_message,\n+    display_tool_calls,\n+    rename_session_widget,\n+    session_selector_widget,\n+    sidebar_widget,\n+)\n+\n+nest_asyncio.apply()\n+st.set_page_config(\n+    page_title=\"SQrL: Text2SQL Reasoning Agent\",\n+    page_icon=\"\ud83d\udc8e\",\n+    layout=\"wide\",\n+    initial_sidebar_state=\"expanded\",\n+)\n+\n+# Load custom CSS with dark mode support\n+st.markdown(CUSTOM_CSS, unsafe_allow_html=True)\n+\n+\n+def main() -> None:\n+    ####################################################################\n+    # App header\n+    ####################################################################\n+    st.markdown(\n+        \"<h1 class='main-title'>SQrL: Text2SQL Reasoning Agent</h1>\",\n+        unsafe_allow_html=True,\n+    )\n+    st.markdown(\n+        \"<p class='subtitle'>SQrL is an intelligent SQL Agent that can think, analyze and reason, powered by Agno</p>\",\n+        unsafe_allow_html=True,\n+    )\n+\n+    ####################################################################\n+    # Model selector\n+    ####################################################################\n+    model_options = {\n+        \"o4-mini\": \"openai:o4-mini\",\n+        \"claude-3-7-sonnet\": \"anthropic:claude-3-7-sonnet-latest\",\n+        \"gpt-4.1\": \"openai:gpt-4.1\",\n+        \"o3\": \"openai:o3\",\n+        \"gemini-2.5-pro\": \"google:gemini-2.5-pro-preview-03-25\",\n+        \"llama-4-scout\": \"groq:meta-llama/llama-4-scout-17b-16e-instruct\",\n+        \"gpt-4o\": \"openai:gpt-4o\",\n+    }\n+    selected_model = st.sidebar.selectbox(\n+        \"Select a model\",\n+        options=list(model_options.keys()),\n+        index=0,\n+        key=\"model_selector\",\n+    )\n+    model_id = model_options[selected_model]\n+\n+    ####################################################################\n+    # Initialize Agent\n+    ####################################################################\n+    sql_agent: Agent\n+    if (\n+        \"sql_agent\" not in st.session_state\n+        or st.session_state[\"sql_agent\"] is None\n+        or st.session_state.get(\"current_model\") != model_id\n+    ):\n+        logger.info(\"---*--- Creating new SQL agent ---*---\")\n+        sql_agent = get_sql_agent(model_id=model_id)\n+        st.session_state[\"sql_agent\"] = sql_agent\n+        st.session_state[\"current_model\"] = model_id\n+    else:\n+        sql_agent = st.session_state[\"sql_agent\"]\n+\n+    ####################################################################\n+    # Load Agent Session from the database\n+    ####################################################################\n+    try:\n+        st.session_state[\"sql_agent_session_id\"] = sql_agent.load_session()\n+    except Exception:\n+        st.warning(\"Could not create Agent session, is the database running?\")\n+        return\n+\n+    ####################################################################\n+    # Load runs from memory\n+    ####################################################################\n+    agent_runs = sql_agent.memory.runs\n+    if len(agent_runs) > 0:\n+        logger.debug(\"Loading run history\")\n+        st.session_state[\"messages\"] = []\n+        for _run in agent_runs:\n+            if _run.message is not None:\n+                add_message(_run.message.role, _run.message.content)\n+            if _run.response is not None:\n+                add_message(\"assistant\", _run.response.content, _run.response.tools)\n+    else:\n+        logger.debug(\"No run history found\")\n+        st.session_state[\"messages\"] = []\n+\n+    ####################################################################\n+    # Sidebar\n+    ####################################################################\n+    sidebar_widget()\n+\n+    ####################################################################\n+    # Get user input\n+    ####################################################################\n+    if prompt := st.chat_input(\"\ud83d\udc4b Ask me about F1 data from 1950 to 2020!\"):\n+        add_message(\"user\", prompt)\n+\n+    ####################################################################\n+    # Display chat history\n+    ####################################################################\n+    for message in st.session_state[\"messages\"]:\n+        if message[\"role\"] in [\"user\", \"assistant\"]:\n+            _content = message[\"content\"]\n+            if _content is not None:\n+                with st.chat_message(message[\"role\"]):\n+                    # Display tool calls if they exist in the message\n+                    if \"tool_calls\" in message and message[\"tool_calls\"]:\n+                        display_tool_calls(st.empty(), message[\"tool_calls\"])\n+                    st.markdown(_content)\n+\n+    ####################################################################\n+    # Generate response for user message\n+    ####################################################################\n+    last_message = (\n+        st.session_state[\"messages\"][-1] if st.session_state[\"messages\"] else None\n+    )\n+    if last_message and last_message.get(\"role\") == \"user\":\n+        question = last_message[\"content\"]\n+        with st.chat_message(\"assistant\"):\n+            # Create container for tool calls\n+            tool_calls_container = st.empty()\n+            resp_container = st.empty()\n+            with st.spinner(\"\ud83e\udd14 Thinking...\"):\n+                response = \"\"\n+                try:\n+                    # Run the agent and stream the response\n+                    run_response = sql_agent.run(\n+                        question, stream=True, stream_intermediate_steps=True\n+                    )\n+                    for _resp_chunk in run_response:\n+                        # Display tool calls if available\n+                        if _resp_chunk.tools and len(_resp_chunk.tools) > 0:\n+                            display_tool_calls(tool_calls_container, _resp_chunk.tools)\n+\n+                        # Display response if available and event is RunResponse\n+                        if (\n+                            _resp_chunk.event == \"RunResponse\"\n+                            and _resp_chunk.content is not None\n+                        ):\n+                            response += _resp_chunk.content\n+                            resp_container.markdown(response)\n+\n+                    add_message(\"assistant\", response, sql_agent.run_response.tools)\n+                except Exception as e:\n+                    logger.exception(e)\n+                    error_message = f\"Sorry, I encountered an error: {str(e)}\"\n+                    add_message(\"assistant\", error_message)\n+                    st.error(error_message)\n+\n+    ####################################################################\n+    # Session selector\n+    ####################################################################\n+    session_selector_widget(sql_agent, model_id)\n+    rename_session_widget(sql_agent)\n+\n+    ####################################################################\n+    # About section\n+    ####################################################################\n+    about_widget()\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/sql_agent/generate_requirements.sh",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/sql_agent/generate_requirements.sh b/cookbook/examples/streamlit_applications/sql_agent/generate_requirements.sh\nnew file mode 100755\nindex 000000000..78f8c7cec\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/sql_agent/generate_requirements.sh\n@@ -0,0 +1,12 @@\n+#!/bin/bash\n+\n+############################################################################\n+# Generate requirements.txt from requirements.in\n+############################################################################\n+\n+echo \"Generating requirements.txt\"\n+\n+CURR_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n+\n+UV_CUSTOM_COMPILE_COMMAND=\"./generate_requirements.sh\" \\\n+  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/sql_agent/knowledge/sample_queries.sql",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/sql_agent/knowledge/sample_queries.sql b/cookbook/examples/streamlit_applications/sql_agent/knowledge/sample_queries.sql\nnew file mode 100644\nindex 000000000..d7e7afc24\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/sql_agent/knowledge/sample_queries.sql\n@@ -0,0 +1,69 @@\n+-- Here are some sample queries for reference\n+\n+-- <query description>\n+-- How many races did the championship winners win each year?\n+-- </query description>\n+-- <query>\n+SELECT\n+    dc.year,\n+    dc.name AS champion_name,\n+    COUNT(rw.name) AS race_wins\n+FROM\n+    drivers_championship dc\n+JOIN\n+    race_wins rw\n+ON\n+    dc.name = rw.name AND dc.year = EXTRACT(YEAR FROM TO_DATE(rw.date, 'DD Mon YYYY'))\n+WHERE\n+    dc.position = '1'\n+GROUP BY\n+    dc.year, dc.name\n+ORDER BY\n+    dc.year;\n+-- </query>\n+\n+\n+-- <query description>\n+-- Compare the number of race wins vs championship positions for constructors in 2019\n+-- </query description>\n+-- <query>\n+WITH race_wins_2019 AS (\n+    SELECT team, COUNT(*) AS wins\n+    FROM race_wins\n+    WHERE EXTRACT(YEAR FROM TO_DATE(date, 'DD Mon YYYY')) = 2019\n+    GROUP BY team\n+),\n+constructors_positions_2019 AS (\n+    SELECT team, position\n+    FROM constructors_championship\n+    WHERE year = 2019\n+)\n+\n+SELECT cp.team, cp.position, COALESCE(rw.wins, 0) AS wins\n+FROM constructors_positions_2019 cp\n+LEFT JOIN race_wins_2019 rw ON cp.team = rw.team\n+ORDER BY cp.position;\n+-- </query>\n+\n+-- <query description>\n+-- Most race wins by a driver\n+-- </query description>\n+-- <query>\n+SELECT name, COUNT(*) AS win_count\n+FROM race_wins\n+GROUP BY name\n+ORDER BY win_count DESC\n+LIMIT 1;\n+-- </query>\n+\n+-- <query description>\n+-- Which team won the most Constructors Championships?\n+-- </query description>\n+-- <query>\n+SELECT team, COUNT(*) AS championship_wins\n+FROM constructors_championship\n+WHERE position = 1\n+GROUP BY team\n+ORDER BY championship_wins DESC\n+LIMIT 1;\n+-- </query>\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/sql_agent/load_f1_data.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/sql_agent/load_f1_data.py b/cookbook/examples/streamlit_applications/sql_agent/load_f1_data.py\nnew file mode 100644\nindex 000000000..31898a069\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/sql_agent/load_f1_data.py\n@@ -0,0 +1,50 @@\n+from io import StringIO\n+\n+import pandas as pd\n+import requests\n+from agents import db_url\n+from agno.utils.log import logger\n+from sqlalchemy import create_engine\n+\n+s3_uri = \"https://agno-public.s3.amazonaws.com/f1\"\n+\n+# List of files and their corresponding table names\n+files_to_tables = {\n+    f\"{s3_uri}/constructors_championship_1958_2020.csv\": \"constructors_championship\",\n+    f\"{s3_uri}/drivers_championship_1950_2020.csv\": \"drivers_championship\",\n+    f\"{s3_uri}/fastest_laps_1950_to_2020.csv\": \"fastest_laps\",\n+    f\"{s3_uri}/race_results_1950_to_2020.csv\": \"race_results\",\n+    f\"{s3_uri}/race_wins_1950_to_2020.csv\": \"race_wins\",\n+}\n+\n+\n+def load_f1_data():\n+    \"\"\"Load F1 data into the database\"\"\"\n+\n+    logger.info(\"Loading database.\")\n+    engine = create_engine(db_url)\n+\n+    # Load each CSV file into the corresponding PostgreSQL table\n+    for file_path, table_name in files_to_tables.items():\n+        logger.info(f\"Loading {file_path} into {table_name} table.\")\n+        # Download the file using requests\n+        response = requests.get(file_path, verify=False)\n+        response.raise_for_status()  # Raise an exception for bad status codes\n+\n+        # Read the CSV data from the response content\n+        csv_data = StringIO(response.text)\n+        df = pd.read_csv(csv_data)\n+\n+        df.to_sql(table_name, engine, if_exists=\"replace\", index=False)\n+        logger.info(f\"{file_path} loaded into {table_name} table.\")\n+\n+    logger.info(\"Database loaded.\")\n+\n+\n+if __name__ == \"__main__\":\n+    # Disable SSL verification warnings\n+    import urllib3\n+\n+    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n+\n+    load_f1_data()\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/sql_agent/load_knowledge.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/sql_agent/load_knowledge.py b/cookbook/examples/streamlit_applications/sql_agent/load_knowledge.py\nnew file mode 100644\nindex 000000000..cd440b294\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/sql_agent/load_knowledge.py\n@@ -0,0 +1,12 @@\n+from agents import agent_knowledge\n+from agno.utils.log import logger\n+\n+\n+def load_knowledge(recreate: bool = True):\n+    logger.info(\"Loading SQL agent knowledge.\")\n+    agent_knowledge.load(recreate=recreate)\n+    logger.info(\"SQL agent knowledge loaded.\")\n+\n+\n+if __name__ == \"__main__\":\n+    load_knowledge()\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/sql_agent/playground.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/sql_agent/playground.py b/cookbook/examples/streamlit_applications/sql_agent/playground.py\nnew file mode 100644\nindex 000000000..57b1f9038\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/sql_agent/playground.py\n@@ -0,0 +1,12 @@\n+from agents import get_sql_agent\n+from agno.playground import Playground, serve_playground_app\n+\n+sql_agent = get_sql_agent(name=\"SQL Agent\", model_id=\"openai:o4-mini\")\n+reasoning_sql_agent = get_sql_agent(\n+    name=\"Reasoning SQL Agent\", model_id=\"anthropic:claude-3-7-sonnet-latest\"\n+)\n+\n+app = Playground(agents=[sql_agent, reasoning_sql_agent]).get_app()\n+\n+if __name__ == \"__main__\":\n+    serve_playground_app(\"playground:app\", reload=True)\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/sql_agent/requirements.in",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/sql_agent/requirements.in b/cookbook/examples/streamlit_applications/sql_agent/requirements.in\nnew file mode 100644\nindex 000000000..fcf1864f3\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/sql_agent/requirements.in\n@@ -0,0 +1,12 @@\n+agno\n+anthropic\n+google-genai\n+groq\n+nest_asyncio\n+openai\n+pandas\n+pgvector\n+psycopg[binary]\n+simplejson\n+sqlalchemy\n+streamlit\n\\ No newline at end of file\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/sql_agent/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/sql_agent/requirements.txt b/cookbook/examples/streamlit_applications/sql_agent/requirements.txt\nnew file mode 100644\nindex 000000000..4279b28e0\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/sql_agent/requirements.txt\n@@ -0,0 +1,232 @@\n+# This file was autogenerated by uv via the following command:\n+#    ./generate_requirements.sh\n+agno==1.3.4\n+    # via -r cookbook/examples/apps/sql_agent/requirements.in\n+altair==5.5.0\n+    # via streamlit\n+annotated-types==0.7.0\n+    # via pydantic\n+anthropic==0.49.0\n+    # via -r cookbook/examples/apps/sql_agent/requirements.in\n+anyio==4.9.0\n+    # via\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   httpx\n+    #   openai\n+attrs==25.3.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+blinker==1.9.0\n+    # via streamlit\n+cachetools==5.5.2\n+    # via\n+    #   google-auth\n+    #   streamlit\n+certifi==2025.1.31\n+    # via\n+    #   httpcore\n+    #   httpx\n+    #   requests\n+charset-normalizer==3.4.1\n+    # via requests\n+click==8.1.8\n+    # via\n+    #   streamlit\n+    #   typer\n+distro==1.9.0\n+    # via\n+    #   anthropic\n+    #   groq\n+    #   openai\n+docstring-parser==0.16\n+    # via agno\n+gitdb==4.0.12\n+    # via gitpython\n+gitpython==3.1.44\n+    # via\n+    #   agno\n+    #   streamlit\n+google-auth==2.39.0\n+    # via google-genai\n+google-genai==1.11.0\n+    # via -r cookbook/examples/apps/sql_agent/requirements.in\n+groq==0.22.0\n+    # via -r cookbook/examples/apps/sql_agent/requirements.in\n+h11==0.14.0\n+    # via httpcore\n+httpcore==1.0.8\n+    # via httpx\n+httpx==0.28.1\n+    # via\n+    #   agno\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   openai\n+idna==3.10\n+    # via\n+    #   anyio\n+    #   httpx\n+    #   requests\n+jinja2==3.1.6\n+    # via\n+    #   altair\n+    #   pydeck\n+jiter==0.9.0\n+    # via\n+    #   anthropic\n+    #   openai\n+jsonschema==4.23.0\n+    # via altair\n+jsonschema-specifications==2024.10.1\n+    # via jsonschema\n+markdown-it-py==3.0.0\n+    # via rich\n+markupsafe==3.0.2\n+    # via jinja2\n+mdurl==0.1.2\n+    # via markdown-it-py\n+narwhals==1.35.0\n+    # via altair\n+nest-asyncio==1.6.0\n+    # via -r cookbook/examples/apps/sql_agent/requirements.in\n+numpy==2.2.5\n+    # via\n+    #   pandas\n+    #   pgvector\n+    #   pydeck\n+    #   streamlit\n+openai==1.75.0\n+    # via -r cookbook/examples/apps/sql_agent/requirements.in\n+packaging==24.2\n+    # via\n+    #   altair\n+    #   streamlit\n+pandas==2.2.3\n+    # via\n+    #   -r cookbook/examples/apps/sql_agent/requirements.in\n+    #   streamlit\n+pgvector==0.4.0\n+    # via -r cookbook/examples/apps/sql_agent/requirements.in\n+pillow==11.2.1\n+    # via streamlit\n+protobuf==5.29.4\n+    # via streamlit\n+psycopg==3.2.6\n+    # via -r cookbook/examples/apps/sql_agent/requirements.in\n+psycopg-binary==3.2.6\n+    # via psycopg\n+pyarrow==19.0.1\n+    # via streamlit\n+pyasn1==0.6.1\n+    # via\n+    #   pyasn1-modules\n+    #   rsa\n+pyasn1-modules==0.4.2\n+    # via google-auth\n+pydantic==2.11.3\n+    # via\n+    #   agno\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   openai\n+    #   pydantic-settings\n+pydantic-core==2.33.1\n+    # via pydantic\n+pydantic-settings==2.9.1\n+    # via agno\n+pydeck==0.9.1\n+    # via streamlit\n+pygments==2.19.1\n+    # via rich\n+python-dateutil==2.9.0.post0\n+    # via pandas\n+python-dotenv==1.1.0\n+    # via\n+    #   agno\n+    #   pydantic-settings\n+python-multipart==0.0.20\n+    # via agno\n+pytz==2025.2\n+    # via pandas\n+pyyaml==6.0.2\n+    # via agno\n+referencing==0.36.2\n+    # via\n+    #   jsonschema\n+    #   jsonschema-specifications\n+requests==2.32.3\n+    # via\n+    #   google-genai\n+    #   streamlit\n+rich==14.0.0\n+    # via\n+    #   agno\n+    #   typer\n+rpds-py==0.24.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+rsa==4.9.1\n+    # via google-auth\n+shellingham==1.5.4\n+    # via typer\n+simplejson==3.20.1\n+    # via -r cookbook/examples/apps/sql_agent/requirements.in\n+six==1.17.0\n+    # via python-dateutil\n+smmap==5.0.2\n+    # via gitdb\n+sniffio==1.3.1\n+    # via\n+    #   anthropic\n+    #   anyio\n+    #   groq\n+    #   openai\n+sqlalchemy==2.0.40\n+    # via -r cookbook/examples/apps/sql_agent/requirements.in\n+streamlit==1.44.1\n+    # via -r cookbook/examples/apps/sql_agent/requirements.in\n+tenacity==9.1.2\n+    # via streamlit\n+toml==0.10.2\n+    # via streamlit\n+tomli==2.2.1\n+    # via agno\n+tornado==6.4.2\n+    # via streamlit\n+tqdm==4.67.1\n+    # via openai\n+typer==0.15.2\n+    # via agno\n+typing-extensions==4.13.2\n+    # via\n+    #   agno\n+    #   altair\n+    #   anthropic\n+    #   anyio\n+    #   google-genai\n+    #   groq\n+    #   openai\n+    #   psycopg\n+    #   pydantic\n+    #   pydantic-core\n+    #   referencing\n+    #   sqlalchemy\n+    #   streamlit\n+    #   typer\n+    #   typing-inspection\n+typing-inspection==0.4.0\n+    # via\n+    #   pydantic\n+    #   pydantic-settings\n+tzdata==2025.2\n+    # via pandas\n+urllib3==2.4.0\n+    # via requests\n+websockets==15.0.1\n+    # via google-genai\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/sql_agent/utils.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/sql_agent/utils.py b/cookbook/examples/streamlit_applications/sql_agent/utils.py\nnew file mode 100644\nindex 000000000..565836c2e\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/sql_agent/utils.py\n@@ -0,0 +1,315 @@\n+import json\n+from dataclasses import asdict\n+from typing import Any, Dict, List, Optional\n+\n+import streamlit as st\n+from agents import get_sql_agent\n+from agno.agent.agent import Agent\n+from agno.utils.log import logger\n+\n+\n+def is_json(myjson):\n+    \"\"\"Check if a string is valid JSON\"\"\"\n+    try:\n+        json.loads(myjson)\n+    except (ValueError, TypeError):\n+        return False\n+    return True\n+\n+\n+def load_data_and_knowledge():\n+    \"\"\"Load F1 data and knowledge base if not already done\"\"\"\n+    from load_f1_data import load_f1_data\n+    from load_knowledge import load_knowledge\n+\n+    if \"data_loaded\" not in st.session_state:\n+        with st.spinner(\"\ud83d\udd04 Loading data into database...\"):\n+            load_f1_data()\n+        with st.spinner(\"\ud83d\udcda Loading knowledge base...\"):\n+            load_knowledge()\n+        st.session_state[\"data_loaded\"] = True\n+        st.success(\"\u2705 Data and knowledge loaded successfully!\")\n+\n+\n+def add_message(\n+    role: str, content: str, tool_calls: Optional[List[Dict[str, Any]]] = None\n+) -> None:\n+    \"\"\"Safely add a message to the session state\"\"\"\n+    if \"messages\" not in st.session_state or not isinstance(\n+        st.session_state[\"messages\"], list\n+    ):\n+        st.session_state[\"messages\"] = []\n+    st.session_state[\"messages\"].append(\n+        {\"role\": role, \"content\": content, \"tool_calls\": tool_calls}\n+    )\n+\n+\n+def restart_agent():\n+    \"\"\"Reset the agent and clear chat history\"\"\"\n+    logger.debug(\"---*--- Restarting agent ---*---\")\n+    st.session_state[\"sql_agent\"] = None\n+    st.session_state[\"sql_agent_session_id\"] = None\n+    st.session_state[\"messages\"] = []\n+    st.rerun()\n+\n+\n+def export_chat_history():\n+    \"\"\"Export chat history as markdown\"\"\"\n+    if \"messages\" in st.session_state:\n+        chat_text = \"# Reasoning SQL Agent - Chat History\\n\\n\"\n+        for msg in st.session_state[\"messages\"]:\n+            role = \"\ud83e\udd16 Assistant\" if msg[\"role\"] == \"agent\" else \"\ud83d\udc64 User\"\n+            chat_text += f\"### {role}\\n{msg['content']}\\n\\n\"\n+        return chat_text\n+    return \"\"\n+\n+\n+def display_tool_calls(tool_calls_container, tools):\n+    \"\"\"Display tool calls in a streamlit container with expandable sections.\n+\n+    Args:\n+        tool_calls_container: Streamlit container to display the tool calls\n+        tools: List of tool call dictionaries containing name, args, content, and metrics\n+    \"\"\"\n+    try:\n+        with tool_calls_container.container():\n+            for tool_call in tools:\n+                tool_name = tool_call.get(\"tool_name\", \"Unknown Tool\")\n+                tool_args = tool_call.get(\"tool_args\", {})\n+                content = tool_call.get(\"content\", None)\n+                metrics = tool_call.get(\"metrics\", None)\n+\n+                # Add timing information\n+                execution_time_str = \"N/A\"\n+                try:\n+                    if metrics is not None and hasattr(metrics, \"time\"):\n+                        execution_time = metrics.time\n+                        if execution_time is not None:\n+                            execution_time_str = f\"{execution_time:.4f}s\"\n+                except Exception as e:\n+                    logger.error(f\"Error displaying tool calls: {str(e)}\")\n+                    pass\n+\n+                with st.expander(\n+                    f\"\ud83d\udee0\ufe0f {tool_name.replace('_', ' ').title()} ({execution_time_str})\",\n+                    expanded=False,\n+                ):\n+                    # Show query with syntax highlighting\n+                    if isinstance(tool_args, dict) and \"query\" in tool_args:\n+                        st.code(tool_args[\"query\"], language=\"sql\")\n+\n+                    # Display arguments in a more readable format\n+                    if tool_args and tool_args != {\"query\": None}:\n+                        st.markdown(\"**Arguments:**\")\n+                        st.json(tool_args)\n+\n+                    if content is not None:\n+                        try:\n+                            if is_json(content):\n+                                st.markdown(\"**Results:**\")\n+                                st.json(content)\n+                        except Exception as e:\n+                            logger.debug(f\"Skipped tool call content: {e}\")\n+    except Exception as e:\n+        logger.error(f\"Error displaying tool calls: {str(e)}\")\n+        tool_calls_container.error(\"Failed to display tool results\")\n+\n+\n+def sidebar_widget() -> None:\n+    \"\"\"Display a sidebar with sample user queries\"\"\"\n+    with st.sidebar:\n+        st.markdown(\"#### \ud83c\udfc6 Sample Queries\")\n+        if st.button(\"\ud83d\udccb Show Tables\"):\n+            add_message(\"user\", \"Which tables do you have access to?\")\n+        if st.button(\"\ud83e\udd47 Most Race Wins\"):\n+            add_message(\"user\", \"Which driver has the most race wins?\")\n+\n+        if st.button(\"\ud83c\udfc6 Constructor Champs\"):\n+            add_message(\"user\", \"Which team won the most Constructors Championships?\")\n+\n+        if st.button(\"\u23f3 Longest Career\"):\n+            add_message(\n+                \"user\",\n+                \"Tell me the name of the driver with the longest racing career? Also tell me when they started and when they retired.\",\n+            )\n+        if st.button(\"\ud83d\udcc8 Races per Year\"):\n+            add_message(\"user\", \"Show me the number of races per year.\")\n+\n+        if st.button(\"\ud83d\udd0d Team Performance\"):\n+            add_message(\n+                \"user\",\n+                \"Write a query to identify the drivers that won the most races per year from 2010 onwards and the position of their team that year.\",\n+            )\n+\n+        st.markdown(\"---\")\n+        st.markdown(\"#### \ud83d\udee0\ufe0f Utilities\")\n+        col1, col2 = st.columns(2)\n+        with col1:\n+            if st.button(\"\ud83d\udd04 New Chat\"):\n+                restart_agent()\n+        with col2:\n+            fn = \"sql_agent_chat_history.md\"\n+            if \"sql_agent_session_id\" in st.session_state:\n+                fn = f\"sql_agent_{st.session_state.sql_agent_session_id}.md\"\n+            if st.download_button(\n+                \"\ud83d\udcbe Export Chat\",\n+                export_chat_history(),\n+                file_name=fn,\n+                mime=\"text/markdown\",\n+            ):\n+                st.sidebar.success(\"Chat history exported!\")\n+\n+        if st.sidebar.button(\"\ud83d\ude80 Load Data & Knowledge\"):\n+            load_data_and_knowledge()\n+\n+\n+def session_selector_widget(agent: Agent, model_id: str) -> None:\n+    \"\"\"Display a session selector in the sidebar\"\"\"\n+    if agent.storage:\n+        agent_sessions = agent.storage.get_all_sessions()\n+        # Get session names if available, otherwise use IDs\n+        session_options = []\n+        for session in agent_sessions:\n+            session_id = session.session_id\n+            session_name = (\n+                session.session_data.get(\"session_name\", None)\n+                if session.session_data\n+                else None\n+            )\n+            display_name = session_name if session_name else session_id\n+            session_options.append({\"id\": session_id, \"display\": display_name})\n+\n+        # Display session selector\n+        selected_session = st.sidebar.selectbox(\n+            \"Session\",\n+            options=[s[\"display\"] for s in session_options],\n+            key=\"session_selector\",\n+        )\n+        # Find the selected session ID\n+        selected_session_id = next(\n+            s[\"id\"] for s in session_options if s[\"display\"] == selected_session\n+        )\n+\n+        if st.session_state[\"sql_agent_session_id\"] != selected_session_id:\n+            logger.info(\n+                f\"---*--- Loading {model_id} run: {selected_session_id} ---*---\"\n+            )\n+            st.session_state[\"sql_agent\"] = get_sql_agent(\n+                model_id=model_id,\n+                session_id=selected_session_id,\n+            )\n+            st.rerun()\n+\n+\n+def rename_session_widget(agent: Agent) -> None:\n+    \"\"\"Rename the current session of the agent and save to storage\"\"\"\n+    container = st.sidebar.container()\n+    session_row = container.columns([3, 1], vertical_alignment=\"center\")\n+\n+    # Initialize session_edit_mode if needed\n+    if \"session_edit_mode\" not in st.session_state:\n+        st.session_state.session_edit_mode = False\n+\n+    with session_row[0]:\n+        if st.session_state.session_edit_mode:\n+            new_session_name = st.text_input(\n+                \"Session Name\",\n+                value=agent.session_name,\n+                key=\"session_name_input\",\n+                label_visibility=\"collapsed\",\n+            )\n+        else:\n+            st.markdown(f\"Session Name: **{agent.session_name}**\")\n+\n+    with session_row[1]:\n+        if st.session_state.session_edit_mode:\n+            if st.button(\"\u2713\", key=\"save_session_name\", type=\"primary\"):\n+                if new_session_name:\n+                    agent.rename_session(new_session_name)\n+                    st.session_state.session_edit_mode = False\n+                    container.success(\"Renamed!\")\n+        else:\n+            if st.button(\"\u270e\", key=\"edit_session_name\"):\n+                st.session_state.session_edit_mode = True\n+\n+\n+def about_widget() -> None:\n+    \"\"\"Display an about section in the sidebar\"\"\"\n+    with st.sidebar:\n+        st.markdown(\"### About Agno \u2728\")\n+        st.markdown(\"\"\"\n+        Agno is a lightweight library for building Reasoning Agents.\n+\n+        [GitHub](https://github.com/agno-agi/agno) | [Docs](https://docs.agno.com)\n+        \"\"\")\n+\n+        st.markdown(\"### Need Help?\")\n+        st.markdown(\n+            \"If you have any questions, catch us on [discord](https://agno.link/discord) or post in the community [forum](https://agno.link/community).\"\n+        )\n+\n+\n+CUSTOM_CSS = \"\"\"\n+    <style>\n+    /* Main Styles */\n+    .main-title {\n+        text-align: center;\n+        background: linear-gradient(45deg, #FF4B2B, #FF416C);\n+        -webkit-background-clip: text;\n+        -webkit-text-fill-color: transparent;\n+        font-size: 3em;\n+        font-weight: bold;\n+        padding: 1em 0;\n+    }\n+    .subtitle {\n+        text-align: center;\n+        color: #666;\n+        margin-bottom: 2em;\n+    }\n+    .stButton button {\n+        width: 100%;\n+        border-radius: 20px;\n+        margin: 0.2em 0;\n+        transition: all 0.3s ease;\n+    }\n+    .stButton button:hover {\n+        transform: translateY(-2px);\n+        box-shadow: 0 5px 15px rgba(0,0,0,0.1);\n+    }\n+    .chat-container {\n+        border-radius: 15px;\n+        padding: 1em;\n+        margin: 1em 0;\n+        background-color: #f5f5f5;\n+    }\n+    .sql-result {\n+        background-color: #f8f9fa;\n+        border-radius: 10px;\n+        padding: 1em;\n+        margin: 1em 0;\n+        border-left: 4px solid #FF4B2B;\n+    }\n+    .status-message {\n+        padding: 1em;\n+        border-radius: 10px;\n+        margin: 1em 0;\n+    }\n+    .success-message {\n+        background-color: #d4edda;\n+        color: #155724;\n+    }\n+    .error-message {\n+        background-color: #f8d7da;\n+        color: #721c24;\n+    }\n+    /* Dark mode adjustments */\n+    @media (prefers-color-scheme: dark) {\n+        .chat-container {\n+            background-color: #2b2b2b;\n+        }\n+        .sql-result {\n+            background-color: #1e1e1e;\n+        }\n+    }\n+    </style>\n+\"\"\"\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/tic_tac_toe/.gitignore",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/tic_tac_toe/.gitignore b/cookbook/examples/streamlit_applications/tic_tac_toe/.gitignore\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/tic_tac_toe/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/tic_tac_toe/__init__.py b/cookbook/examples/streamlit_applications/tic_tac_toe/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/tic_tac_toe/agents.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/tic_tac_toe/agents.py b/cookbook/examples/streamlit_applications/tic_tac_toe/agents.py\nnew file mode 100644\nindex 000000000..e9317fdf7\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/tic_tac_toe/agents.py\n@@ -0,0 +1,164 @@\n+\"\"\"\n+Tic Tac Toe Battle\n+---------------------------------\n+This example shows how to build a Tic Tac Toe game where two AI agents play against each other.\n+\n+The game integrates:\n+  - Multiple AI models (Claude, GPT-4, etc.)\n+  - Turn-based gameplay coordination\n+  - Move validation and game state management\n+\"\"\"\n+\n+import sys\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Tuple\n+\n+from agno.agent import Agent\n+from agno.models.anthropic import Claude\n+from agno.models.google import Gemini\n+from agno.models.groq import Groq\n+from agno.models.openai import OpenAIChat, OpenAIResponses\n+\n+project_root = str(Path(__file__).parent.parent.parent.parent)\n+if project_root not in sys.path:\n+    sys.path.append(project_root)\n+\n+\n+def get_model_for_provider(provider: str, model_name: str):\n+    \"\"\"\n+    Creates and returns the appropriate model instance based on the provider.\n+\n+    Args:\n+        provider: The model provider (e.g., 'openai', 'google', 'anthropic', 'groq')\n+        model_name: The specific model name/ID\n+\n+    Returns:\n+        An instance of the appropriate model class\n+\n+    Raises:\n+        ValueError: If the provider is not supported\n+    \"\"\"\n+    if provider == \"openai\":\n+        if model_name == \"o1-pro\":\n+            return OpenAIResponses(id=model_name)\n+        else:\n+            return OpenAIChat(id=model_name)\n+    elif provider == \"google\":\n+        return Gemini(id=model_name)\n+    elif provider == \"anthropic\":\n+        if model_name == \"claude-3-5-sonnet\":\n+            return Claude(id=\"claude-3-5-sonnet-20241022\", max_tokens=8192)\n+        elif model_name == \"claude-3-7-sonnet\":\n+            return Claude(\n+                id=\"claude-3-7-sonnet-20250219\",\n+                max_tokens=8192,\n+            )\n+        elif model_name == \"claude-3-7-sonnet-thinking\":\n+            return Claude(\n+                id=\"claude-3-7-sonnet-20250219\",\n+                max_tokens=8192,\n+                thinking={\"type\": \"enabled\", \"budget_tokens\": 4096},\n+            )\n+        else:\n+            return Claude(id=model_name)\n+    elif provider == \"groq\":\n+        return Groq(id=model_name)\n+    else:\n+        raise ValueError(f\"Unsupported model provider: {provider}\")\n+\n+\n+def get_tic_tac_toe_players(\n+    model_x: str = \"openai:gpt-4o\",\n+    model_o: str = \"openai:o3-mini\",\n+    debug_mode: bool = True,\n+) -> Tuple[Agent, Agent]:\n+    \"\"\"\n+    Returns an instance of the Tic Tac Toe Referee Agent that coordinates the game.\n+\n+    Args:\n+        model_x: ModelConfig for player X\n+        model_o: ModelConfig for player O\n+        model_referee: ModelConfig for the referee agent\n+        debug_mode: Enable logging and debug features\n+\n+    Returns:\n+        An instance of the configured Referee Agent\n+    \"\"\"\n+    # Parse model provider and name\n+    provider_x, model_name_x = model_x.split(\":\")\n+    provider_o, model_name_o = model_o.split(\":\")\n+\n+    # Create model instances using the helper function\n+    model_x = get_model_for_provider(provider_x, model_name_x)\n+    model_o = get_model_for_provider(provider_o, model_name_o)\n+\n+    player_x = Agent(\n+        name=\"Player X\",\n+        description=dedent(\"\"\"\\\n+        You are Player X in a Tic Tac Toe game. Your goal is to win by placing three X's in a row (horizontally, vertically, or diagonally).\n+\n+        BOARD LAYOUT:\n+        - The board is a 3x3 grid with coordinates from (0,0) to (2,2)\n+        - Top-left is (0,0), bottom-right is (2,2)\n+\n+        RULES:\n+        - You can only place X in empty spaces (shown as \" \" on the board)\n+        - Players take turns placing their marks\n+        - First to get 3 marks in a row (horizontal, vertical, or diagonal) wins\n+        - If all spaces are filled with no winner, the game is a draw\n+\n+        YOUR RESPONSE:\n+        - Provide ONLY two numbers separated by a space (row column)\n+        - Example: \"1 2\" places your X in row 1, column 2\n+        - Choose only from the valid moves list provided to you\n+\n+        STRATEGY TIPS:\n+        - Study the board carefully and make strategic moves\n+        - Block your opponent's potential winning moves\n+        - Create opportunities for multiple winning paths\n+        - Pay attention to the valid moves and avoid illegal moves\n+        \"\"\"),\n+        model=model_x,\n+        # Gemini models have a rate limit of 5 requests per minute\n+        retries=3,\n+        # Make sure to wait 30 seconds between retries\n+        delay_between_retries=30,\n+        debug_mode=debug_mode,\n+    )\n+\n+    player_o = Agent(\n+        name=\"Player O\",\n+        description=dedent(\"\"\"\\\n+        You are Player O in a Tic Tac Toe game. Your goal is to win by placing three O's in a row (horizontally, vertically, or diagonally).\n+\n+        BOARD LAYOUT:\n+        - The board is a 3x3 grid with coordinates from (0,0) to (2,2)\n+        - Top-left is (0,0), bottom-right is (2,2)\n+\n+        RULES:\n+        - You can only place X in empty spaces (shown as \" \" on the board)\n+        - Players take turns placing their marks\n+        - First to get 3 marks in a row (horizontal, vertical, or diagonal) wins\n+        - If all spaces are filled with no winner, the game is a draw\n+\n+        YOUR RESPONSE:\n+        - Provide ONLY two numbers separated by a space (row column)\n+        - Example: \"1 2\" places your X in row 1, column 2\n+        - Choose only from the valid moves list provided to you\n+\n+        STRATEGY TIPS:\n+        - Study the board carefully and make strategic moves\n+        - Block your opponent's potential winning moves\n+        - Create opportunities for multiple winning paths\n+        - Pay attention to the valid moves and avoid illegal moves\n+        \"\"\"),\n+        model=model_o,\n+        # Gemini models have a rate limit of 5 requests per minute\n+        retries=3,\n+        # Make sure to wait 30 seconds between retries\n+        delay_between_retries=30,\n+        debug_mode=debug_mode,\n+    )\n+\n+    return player_x, player_o\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/tic_tac_toe/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/tic_tac_toe/app.py b/cookbook/examples/streamlit_applications/tic_tac_toe/app.py\nnew file mode 100644\nindex 000000000..baeea3299\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/tic_tac_toe/app.py\n@@ -0,0 +1,264 @@\n+import nest_asyncio\n+import streamlit as st\n+from agents import get_tic_tac_toe_players\n+from agno.utils.log import logger\n+from utils import (\n+    CUSTOM_CSS,\n+    TicTacToeBoard,\n+    display_board,\n+    display_move_history,\n+    show_agent_status,\n+)\n+\n+nest_asyncio.apply()\n+\n+# Page configuration\n+st.set_page_config(\n+    page_title=\"Agent Tic Tac Toe\",\n+    page_icon=\"\ud83c\udfae\",\n+    layout=\"wide\",\n+    initial_sidebar_state=\"expanded\",\n+)\n+\n+# Load custom CSS with dark mode support\n+st.markdown(CUSTOM_CSS, unsafe_allow_html=True)\n+\n+\n+def main():\n+    ####################################################################\n+    # App header\n+    ####################################################################\n+    st.markdown(\n+        \"<h1 class='main-title'>Agents play Tic Tac Toe</h1>\",\n+        unsafe_allow_html=True,\n+    )\n+\n+    ####################################################################\n+    # Initialize session state\n+    ####################################################################\n+    if \"game_started\" not in st.session_state:\n+        st.session_state.game_started = False\n+        st.session_state.game_paused = False\n+        st.session_state.move_history = []\n+\n+    with st.sidebar:\n+        st.markdown(\"### Game Controls\")\n+        model_options = {\n+            \"gpt-4o\": \"openai:gpt-4o\",\n+            \"gpt-4o-mini\": \"openai:gpt-4o-mini\",\n+            \"gpt-4.5\": \"openai:gpt-4.5-preview\",\n+            \"o1-pro\": \"openai:o1-pro\",\n+            \"o3-mini\": \"openai:o3-mini\",\n+            \"claude-3.5\": \"anthropic:claude-3-5-sonnet\",\n+            \"claude-3.7\": \"anthropic:claude-3-7-sonnet\",\n+            \"claude-3.7-thinking\": \"anthropic:claude-3-7-sonnet-thinking\",\n+            \"gemini-pro\": \"google:gemini-2.5-pro-exp-03-25\",\n+            \"gemini-flash\": \"google:gemini-2.0-flash\",\n+            \"llama-3.3\": \"groq:llama-3.3-70b-versatile\",\n+        }\n+        ################################################################\n+        # Model selection\n+        ################################################################\n+        selected_p_x = st.selectbox(\n+            \"Select Player X\",\n+            list(model_options.keys()),\n+            index=list(model_options.keys()).index(\"gpt-4.5\"),\n+            key=\"model_p1\",\n+        )\n+        selected_p_o = st.selectbox(\n+            \"Select Player O\",\n+            list(model_options.keys()),\n+            index=list(model_options.keys()).index(\"claude-3.7\"),\n+            key=\"model_p2\",\n+        )\n+\n+        ################################################################\n+        # Game controls\n+        ################################################################\n+        col1, col2 = st.columns(2)\n+        with col1:\n+            if not st.session_state.game_started:\n+                if st.button(\"\u25b6\ufe0f Start Game\"):\n+                    st.session_state.player_x, st.session_state.player_o = (\n+                        get_tic_tac_toe_players(\n+                            model_x=model_options[selected_p_x],\n+                            model_o=model_options[selected_p_o],\n+                            debug_mode=True,\n+                        )\n+                    )\n+                    st.session_state.game_board = TicTacToeBoard()\n+                    st.session_state.game_started = True\n+                    st.session_state.game_paused = False\n+                    st.session_state.move_history = []\n+                    st.rerun()\n+            else:\n+                game_over, _ = st.session_state.game_board.get_game_state()\n+                if not game_over:\n+                    if st.button(\n+                        \"\u23f8\ufe0f Pause\" if not st.session_state.game_paused else \"\u25b6\ufe0f Resume\"\n+                    ):\n+                        st.session_state.game_paused = not st.session_state.game_paused\n+                        st.rerun()\n+        with col2:\n+            if st.session_state.game_started:\n+                if st.button(\"\ud83d\udd04 New Game\"):\n+                    st.session_state.player_x, st.session_state.player_o = (\n+                        get_tic_tac_toe_players(\n+                            model_x=model_options[selected_p_x],\n+                            model_o=model_options[selected_p_o],\n+                            debug_mode=True,\n+                        )\n+                    )\n+                    st.session_state.game_board = TicTacToeBoard()\n+                    st.session_state.game_paused = False\n+                    st.session_state.move_history = []\n+                    st.rerun()\n+\n+    ####################################################################\n+    # Header showing current models\n+    ####################################################################\n+    if st.session_state.game_started:\n+        st.markdown(\n+            f\"<h3 style='color:#87CEEB; text-align:center;'>{selected_p_x} vs {selected_p_o}</h3>\",\n+            unsafe_allow_html=True,\n+        )\n+\n+    ####################################################################\n+    # Main game area\n+    ####################################################################\n+    if st.session_state.game_started:\n+        game_over, status = st.session_state.game_board.get_game_state()\n+\n+        display_board(st.session_state.game_board)\n+\n+        # Show game status (winner/draw/current player)\n+        if game_over:\n+            winner_player = (\n+                \"X\" if \"X wins\" in status else \"O\" if \"O wins\" in status else None\n+            )\n+            if winner_player:\n+                winner_num = \"1\" if winner_player == \"X\" else \"2\"\n+                winner_model = selected_p_x if winner_player == \"X\" else selected_p_o\n+                st.success(f\"\ud83c\udfc6 Game Over! Player {winner_num} ({winner_model}) wins!\")\n+            else:\n+                st.info(\"\ud83e\udd1d Game Over! It's a draw!\")\n+        else:\n+            # Show current player status\n+            current_player = st.session_state.game_board.current_player\n+            player_num = \"1\" if current_player == \"X\" else \"2\"\n+            current_model_name = selected_p_x if current_player == \"X\" else selected_p_o\n+\n+            show_agent_status(\n+                f\"Player {player_num} ({current_model_name})\",\n+                \"It's your turn\",\n+            )\n+\n+        display_move_history()\n+\n+        if not st.session_state.game_paused and not game_over:\n+            # Thinking indicator\n+            st.markdown(\n+                f\"\"\"<div class=\"thinking-container\">\n+                    <div class=\"agent-thinking\">\n+                        <div style=\"margin-right: 10px; display: inline-block;\">\ud83d\udd04</div>\n+                        Player {player_num} ({current_model_name}) is thinking...\n+                    </div>\n+                </div>\"\"\",\n+                unsafe_allow_html=True,\n+            )\n+\n+            valid_moves = st.session_state.game_board.get_valid_moves()\n+\n+            current_agent = (\n+                st.session_state.player_x\n+                if current_player == \"X\"\n+                else st.session_state.player_o\n+            )\n+            response = current_agent.run(\n+                f\"\"\"\\\n+Current board state:\\n{st.session_state.game_board.get_board_state()}\\n\n+Available valid moves (row, col): {valid_moves}\\n\n+Choose your next move from the valid moves above.\n+Respond with ONLY two numbers for row and column, e.g. \"1 2\".\"\"\",\n+                stream=False,\n+            )\n+\n+            try:\n+                import re\n+\n+                numbers = re.findall(r\"\\d+\", response.content if response else \"\")\n+                row, col = map(int, numbers[:2])\n+                success, message = st.session_state.game_board.make_move(row, col)\n+\n+                if success:\n+                    move_number = len(st.session_state.move_history) + 1\n+                    st.session_state.move_history.append(\n+                        {\n+                            \"number\": move_number,\n+                            \"player\": f\"Player {player_num} ({current_model_name})\",\n+                            \"move\": f\"{row},{col}\",\n+                        }\n+                    )\n+\n+                    logger.info(\n+                        f\"Move {move_number}: Player {player_num} ({current_model_name}) placed at position ({row}, {col})\"\n+                    )\n+                    logger.info(\n+                        f\"Board state:\\n{st.session_state.game_board.get_board_state()}\"\n+                    )\n+\n+                    # Check game state after move\n+                    game_over, status = st.session_state.game_board.get_game_state()\n+                    if game_over:\n+                        logger.info(f\"Game Over - {status}\")\n+                        if \"wins\" in status:\n+                            st.success(f\"\ud83c\udfc6 Game Over! {status}\")\n+                        else:\n+                            st.info(f\"\ud83e\udd1d Game Over! {status}\")\n+                        st.session_state.game_paused = True\n+                    st.rerun()\n+                else:\n+                    logger.error(f\"Invalid move attempt: {message}\")\n+                    response = current_agent.run(\n+                        f\"\"\"\\\n+Invalid move: {message}\n+\n+Current board state:\\n{st.session_state.game_board.get_board_state()}\\n\n+Available valid moves (row, col): {valid_moves}\\n\n+Please choose a valid move from the list above.\n+Respond with ONLY two numbers for row and column, e.g. \"1 2\".\"\"\",\n+                        stream=False,\n+                    )\n+                    st.rerun()\n+\n+            except Exception as e:\n+                logger.error(f\"Error processing move: {str(e)}\")\n+                st.error(f\"Error processing move: {str(e)}\")\n+                st.rerun()\n+    else:\n+        st.info(\"\ud83d\udc48 Press 'Start Game' to begin!\")\n+\n+    ####################################################################\n+    # About section\n+    ####################################################################\n+    st.sidebar.markdown(f\"\"\"\n+    ### \ud83c\udfae Agent Tic Tac Toe Battle\n+    Watch two agents compete in real-time!\n+\n+    **Current Players:**\n+    * \ud83d\udd35 Player X: `{selected_p_x}`\n+    * \ud83d\udd34 Player O: `{selected_p_o}`\n+\n+    **How it Works:**\n+    Each Agent analyzes the board and employs strategic thinking to:\n+    * \ud83c\udfc6 Find winning moves\n+    * \ud83d\udee1\ufe0f Block opponent victories\n+    * \u2b50 Control strategic positions\n+    * \ud83e\udd14 Plan multiple moves ahead\n+\n+    Built with Streamlit and Agno\n+    \"\"\")\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/tic_tac_toe/generate_requirements.sh",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/tic_tac_toe/generate_requirements.sh b/cookbook/examples/streamlit_applications/tic_tac_toe/generate_requirements.sh\nnew file mode 100755\nindex 000000000..78f8c7cec\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/tic_tac_toe/generate_requirements.sh\n@@ -0,0 +1,12 @@\n+#!/bin/bash\n+\n+############################################################################\n+# Generate requirements.txt from requirements.in\n+############################################################################\n+\n+echo \"Generating requirements.txt\"\n+\n+CURR_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n+\n+UV_CUSTOM_COMPILE_COMMAND=\"./generate_requirements.sh\" \\\n+  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/tic_tac_toe/requirements.in",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/tic_tac_toe/requirements.in b/cookbook/examples/streamlit_applications/tic_tac_toe/requirements.in\nnew file mode 100644\nindex 000000000..fce375b96\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/tic_tac_toe/requirements.in\n@@ -0,0 +1,13 @@\n+agno\n+anthropic\n+groq\n+google-genai\n+nest-asyncio\n+ollama\n+openai\n+pathlib\n+Pillow\n+pip-tools\n+python-dotenv\n+rich\n+streamlit\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/tic_tac_toe/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/tic_tac_toe/requirements.txt b/cookbook/examples/streamlit_applications/tic_tac_toe/requirements.txt\nnew file mode 100644\nindex 000000000..ea4005bfc\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/tic_tac_toe/requirements.txt\n@@ -0,0 +1,238 @@\n+# This file was autogenerated by uv via the following command:\n+#    ./generate_requirements.sh\n+agno==1.2.4\n+    # via -r cookbook/examples/apps/tic_tac_toe/requirements.in\n+altair==5.5.0\n+    # via streamlit\n+annotated-types==0.7.0\n+    # via pydantic\n+anthropic==0.49.0\n+    # via -r cookbook/examples/apps/tic_tac_toe/requirements.in\n+anyio==4.9.0\n+    # via\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   httpx\n+    #   openai\n+attrs==25.3.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+blinker==1.9.0\n+    # via streamlit\n+build==1.2.2.post1\n+    # via pip-tools\n+cachetools==5.5.2\n+    # via\n+    #   google-auth\n+    #   streamlit\n+certifi==2025.1.31\n+    # via\n+    #   httpcore\n+    #   httpx\n+    #   requests\n+charset-normalizer==3.4.1\n+    # via requests\n+click==8.1.8\n+    # via\n+    #   pip-tools\n+    #   streamlit\n+    #   typer\n+distro==1.9.0\n+    # via\n+    #   anthropic\n+    #   groq\n+    #   openai\n+docstring-parser==0.16\n+    # via agno\n+gitdb==4.0.12\n+    # via gitpython\n+gitpython==3.1.44\n+    # via\n+    #   agno\n+    #   streamlit\n+google-auth==2.38.0\n+    # via google-genai\n+google-genai==1.7.0\n+    # via -r cookbook/examples/apps/tic_tac_toe/requirements.in\n+groq==0.20.0\n+    # via -r cookbook/examples/apps/tic_tac_toe/requirements.in\n+h11==0.14.0\n+    # via httpcore\n+httpcore==1.0.7\n+    # via httpx\n+httpx==0.28.1\n+    # via\n+    #   agno\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   ollama\n+    #   openai\n+idna==3.10\n+    # via\n+    #   anyio\n+    #   httpx\n+    #   requests\n+jinja2==3.1.6\n+    # via\n+    #   altair\n+    #   pydeck\n+jiter==0.9.0\n+    # via\n+    #   anthropic\n+    #   openai\n+jsonschema==4.23.0\n+    # via altair\n+jsonschema-specifications==2024.10.1\n+    # via jsonschema\n+markdown-it-py==3.0.0\n+    # via rich\n+markupsafe==3.0.2\n+    # via jinja2\n+mdurl==0.1.2\n+    # via markdown-it-py\n+narwhals==1.32.0\n+    # via altair\n+nest-asyncio==1.6.0\n+    # via -r cookbook/examples/apps/tic_tac_toe/requirements.in\n+numpy==2.2.4\n+    # via\n+    #   pandas\n+    #   pydeck\n+    #   streamlit\n+ollama==0.4.7\n+    # via -r cookbook/examples/apps/tic_tac_toe/requirements.in\n+openai==1.68.2\n+    # via -r cookbook/examples/apps/tic_tac_toe/requirements.in\n+packaging==24.2\n+    # via\n+    #   altair\n+    #   build\n+    #   streamlit\n+pandas==2.2.3\n+    # via streamlit\n+pathlib==1.0.1\n+    # via -r cookbook/examples/apps/tic_tac_toe/requirements.in\n+pillow==11.1.0\n+    # via\n+    #   -r cookbook/examples/apps/tic_tac_toe/requirements.in\n+    #   streamlit\n+pip==25.0.1\n+    # via pip-tools\n+pip-tools==7.4.1\n+    # via -r cookbook/examples/apps/tic_tac_toe/requirements.in\n+protobuf==5.29.4\n+    # via streamlit\n+pyarrow==19.0.1\n+    # via streamlit\n+pyasn1==0.6.1\n+    # via\n+    #   pyasn1-modules\n+    #   rsa\n+pyasn1-modules==0.4.1\n+    # via google-auth\n+pydantic==2.10.6\n+    # via\n+    #   agno\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   ollama\n+    #   openai\n+    #   pydantic-settings\n+pydantic-core==2.27.2\n+    # via pydantic\n+pydantic-settings==2.8.1\n+    # via agno\n+pydeck==0.9.1\n+    # via streamlit\n+pygments==2.19.1\n+    # via rich\n+pyproject-hooks==1.2.0\n+    # via\n+    #   build\n+    #   pip-tools\n+python-dateutil==2.9.0.post0\n+    # via pandas\n+python-dotenv==1.1.0\n+    # via\n+    #   -r cookbook/examples/apps/tic_tac_toe/requirements.in\n+    #   agno\n+    #   pydantic-settings\n+python-multipart==0.0.20\n+    # via agno\n+pytz==2025.2\n+    # via pandas\n+pyyaml==6.0.2\n+    # via agno\n+referencing==0.36.2\n+    # via\n+    #   jsonschema\n+    #   jsonschema-specifications\n+requests==2.32.3\n+    # via\n+    #   google-genai\n+    #   streamlit\n+rich==13.9.4\n+    # via\n+    #   -r cookbook/examples/apps/tic_tac_toe/requirements.in\n+    #   agno\n+    #   typer\n+rpds-py==0.23.1\n+    # via\n+    #   jsonschema\n+    #   referencing\n+rsa==4.9\n+    # via google-auth\n+setuptools==78.1.0\n+    # via pip-tools\n+shellingham==1.5.4\n+    # via typer\n+six==1.17.0\n+    # via python-dateutil\n+smmap==5.0.2\n+    # via gitdb\n+sniffio==1.3.1\n+    # via\n+    #   anthropic\n+    #   anyio\n+    #   groq\n+    #   openai\n+streamlit==1.44.0\n+    # via -r cookbook/examples/apps/tic_tac_toe/requirements.in\n+tenacity==9.0.0\n+    # via streamlit\n+toml==0.10.2\n+    # via streamlit\n+tomli==2.2.1\n+    # via agno\n+tornado==6.4.2\n+    # via streamlit\n+tqdm==4.67.1\n+    # via openai\n+typer==0.15.2\n+    # via agno\n+typing-extensions==4.12.2\n+    # via\n+    #   agno\n+    #   altair\n+    #   anthropic\n+    #   anyio\n+    #   google-genai\n+    #   groq\n+    #   openai\n+    #   pydantic\n+    #   pydantic-core\n+    #   referencing\n+    #   streamlit\n+    #   typer\n+tzdata==2025.2\n+    # via pandas\n+urllib3==2.3.0\n+    # via requests\n+websockets==15.0.1\n+    # via google-genai\n+wheel==0.45.1\n+    # via pip-tools\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/tic_tac_toe/utils.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/tic_tac_toe/utils.py b/cookbook/examples/streamlit_applications/tic_tac_toe/utils.py\nnew file mode 100644\nindex 000000000..ffc97dc44\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/tic_tac_toe/utils.py\n@@ -0,0 +1,416 @@\n+from typing import List, Optional, Tuple\n+\n+import streamlit as st\n+\n+# Define constants for players\n+X_PLAYER = \"X\"\n+O_PLAYER = \"O\"\n+EMPTY = \" \"\n+\n+\n+class TicTacToeBoard:\n+    def __init__(self):\n+        # Initialize empty 3x3 board\n+        self.board = [[EMPTY for _ in range(3)] for _ in range(3)]\n+        self.current_player = X_PLAYER\n+\n+    def make_move(self, row: int, col: int) -> Tuple[bool, str]:\n+        \"\"\"\n+        Make a move on the board.\n+\n+        Args:\n+            row (int): Row index (0-2)\n+            col (int): Column index (0-2)\n+\n+        Returns:\n+            Tuple[bool, str]: (Success status, Message with current board state or error)\n+        \"\"\"\n+        # Validate move coordinates\n+        if not (0 <= row <= 2 and 0 <= col <= 2):\n+            return (\n+                False,\n+                \"Invalid move: Position out of bounds. Please choose row and column between 0 and 2.\",\n+            )\n+\n+        # Check if position is already occupied\n+        if self.board[row][col] != EMPTY:\n+            return False, f\"Invalid move: Position ({row}, {col}) is already occupied.\"\n+\n+        # Make the move\n+        self.board[row][col] = self.current_player\n+\n+        # Get board state\n+        board_state = self.get_board_state()\n+\n+        # Switch player\n+        self.current_player = O_PLAYER if self.current_player == X_PLAYER else X_PLAYER\n+\n+        return True, f\"Move successful!\\n{board_state}\"\n+\n+    def get_board_state(self) -> str:\n+        \"\"\"\n+        Returns a string representation of the current board state.\n+        \"\"\"\n+        board_str = \"\\n-------------\\n\"\n+        for row in self.board:\n+            board_str += f\"| {' | '.join(row)} |\\n-------------\\n\"\n+        return board_str\n+\n+    def check_winner(self) -> Optional[str]:\n+        \"\"\"\n+        Check if there's a winner.\n+\n+        Returns:\n+            Optional[str]: The winning player (X or O) or None if no winner\n+        \"\"\"\n+        # Check rows\n+        for row in self.board:\n+            if row.count(row[0]) == 3 and row[0] != EMPTY:\n+                return row[0]\n+\n+        # Check columns\n+        for col in range(3):\n+            column = [self.board[row][col] for row in range(3)]\n+            if column.count(column[0]) == 3 and column[0] != EMPTY:\n+                return column[0]\n+\n+        # Check diagonals\n+        diagonal1 = [self.board[i][i] for i in range(3)]\n+        if diagonal1.count(diagonal1[0]) == 3 and diagonal1[0] != EMPTY:\n+            return diagonal1[0]\n+\n+        diagonal2 = [self.board[i][2 - i] for i in range(3)]\n+        if diagonal2.count(diagonal2[0]) == 3 and diagonal2[0] != EMPTY:\n+            return diagonal2[0]\n+\n+        return None\n+\n+    def is_board_full(self) -> bool:\n+        \"\"\"\n+        Check if the board is full (draw condition).\n+        \"\"\"\n+        return all(cell != EMPTY for row in self.board for cell in row)\n+\n+    def get_valid_moves(self) -> List[Tuple[int, int]]:\n+        \"\"\"\n+        Get a list of valid moves (empty positions).\n+\n+        Returns:\n+            List[Tuple[int, int]]: List of (row, col) tuples representing valid moves\n+        \"\"\"\n+        valid_moves = []\n+        for row in range(3):\n+            for col in range(3):\n+                if self.board[row][col] == EMPTY:\n+                    valid_moves.append((row, col))\n+        return valid_moves\n+\n+    def get_game_state(self) -> Tuple[bool, str]:\n+        \"\"\"\n+        Get the current game state.\n+\n+        Returns:\n+            Tuple[bool, str]: (is_game_over, status_message)\n+        \"\"\"\n+        winner = self.check_winner()\n+        if winner:\n+            return True, f\"Player {winner} wins!\"\n+\n+        if self.is_board_full():\n+            return True, \"It's a draw!\"\n+\n+        return False, \"Game in progress\"\n+\n+\n+def display_board(board: TicTacToeBoard):\n+    \"\"\"Display the Tic Tac Toe board using Streamlit\"\"\"\n+    board_html = '<div class=\"game-board\">'\n+\n+    for i in range(3):\n+        for j in range(3):\n+            cell_value = board.board[i][j]\n+            board_html += f'<div class=\"board-cell\">{cell_value}</div>'\n+\n+    board_html += \"</div>\"\n+    st.markdown(board_html, unsafe_allow_html=True)\n+\n+\n+def show_agent_status(agent_name: str, status: str):\n+    \"\"\"Display the current agent status\"\"\"\n+    st.markdown(\n+        f\"\"\"<div class=\"agent-status\">\n+            \ud83e\udd16 <b>{agent_name}</b>: {status}\n+        </div>\"\"\",\n+        unsafe_allow_html=True,\n+    )\n+\n+\n+def create_mini_board_html(\n+    board_state: list, highlight_pos: tuple = None, is_player1: bool = True\n+) -> str:\n+    \"\"\"Create HTML for a mini board with player-specific highlighting\"\"\"\n+    html = '<div class=\"mini-board\">'\n+    for i in range(3):\n+        for j in range(3):\n+            highlight = (\n+                f\"highlight player{1 if is_player1 else 2}\"\n+                if highlight_pos and (i, j) == highlight_pos\n+                else \"\"\n+            )\n+            html += f'<div class=\"mini-cell {highlight}\">{board_state[i][j]}</div>'\n+    html += \"</div>\"\n+    return html\n+\n+\n+def display_move_history():\n+    \"\"\"Display the move history with mini boards in two columns\"\"\"\n+    st.markdown(\n+        '<h3 style=\"margin-bottom: 30px;\">\ud83d\udcdc Game History</h3>',\n+        unsafe_allow_html=True,\n+    )\n+    history_container = st.empty()\n+\n+    if \"move_history\" in st.session_state and st.session_state.move_history:\n+        # Split moves into player 1 and player 2 moves\n+        p1_moves = []\n+        p2_moves = []\n+        current_board = [[\" \" for _ in range(3)] for _ in range(3)]\n+\n+        # Process all moves first\n+        for move in st.session_state.move_history:\n+            row, col = map(int, move[\"move\"].split(\",\"))\n+            is_player1 = \"Player 1\" in move[\"player\"]\n+            symbol = \"X\" if is_player1 else \"O\"\n+            current_board[row][col] = symbol\n+            board_copy = [row[:] for row in current_board]\n+\n+            move_html = f\"\"\"<div class=\"move-entry player{1 if is_player1 else 2}\">\n+                {create_mini_board_html(board_copy, (row, col), is_player1)}\n+                <div class=\"move-info\">\n+                    <div class=\"move-number player{1 if is_player1 else 2}\">Move #{move[\"number\"]}</div>\n+                    <div>{move[\"player\"]}</div>\n+                    <div style=\"font-size: 0.9em; color: #888\">Position: ({row}, {col})</div>\n+                </div>\n+            </div>\"\"\"\n+\n+            if is_player1:\n+                p1_moves.append(move_html)\n+            else:\n+                p2_moves.append(move_html)\n+\n+        max_moves = max(len(p1_moves), len(p2_moves))\n+        history_content = '<div class=\"history-grid\">'\n+\n+        # Left column (Player 1)\n+        history_content += '<div class=\"history-column-left\">'\n+        for i in range(max_moves):\n+            entry_html = \"\"\n+            # Player 1 move\n+            if i < len(p1_moves):\n+                entry_html += p1_moves[i]\n+            history_content += entry_html\n+        history_content += \"</div>\"\n+\n+        # Right column (Player 2)\n+        history_content += '<div class=\"history-column-right\">'\n+        for i in range(max_moves):\n+            entry_html = \"\"\n+            # Player 2 move\n+            if i < len(p2_moves):\n+                entry_html += p2_moves[i]\n+            history_content += entry_html\n+        history_content += \"</div>\"\n+\n+        history_content += \"</div>\"\n+\n+        # Display the content\n+        history_container.markdown(history_content, unsafe_allow_html=True)\n+    else:\n+        history_container.markdown(\n+            \"\"\"<div style=\"text-align: center; color: #666; padding: 20px;\">\n+                No moves yet. Start the game to see the history!\n+            </div>\"\"\",\n+            unsafe_allow_html=True,\n+        )\n+\n+\n+CUSTOM_CSS = \"\"\"\n+<style>\n+/* Main Styles */\n+.main-title {\n+    text-align: center;\n+    background: linear-gradient(45deg, #FF4B2B, #FF416C);\n+    -webkit-background-clip: text;\n+    -webkit-text-fill-color: transparent;\n+    font-size: 3em;\n+    font-weight: bold;\n+    padding: 0.5em 0;\n+}\n+.subtitle {\n+    text-align: center;\n+    color: #666;\n+    margin-bottom: 1em;\n+}\n+.game-board {\n+    display: grid;\n+    grid-template-columns: repeat(3, 80px);\n+    gap: 5px;\n+    justify-content: center;\n+    margin: 1em auto;\n+    background: #666;\n+    padding: 5px;\n+    border-radius: 8px;\n+    width: fit-content;\n+}\n+.board-cell {\n+    width: 80px;\n+    height: 80px;\n+    display: flex;\n+    align-items: center;\n+    justify-content: center;\n+    font-size: 2em;\n+    font-weight: bold;\n+    background-color: #2b2b2b;\n+    color: #fff;\n+    transition: all 0.3s ease;\n+    margin: 0;\n+    padding: 0;\n+}\n+.board-cell:hover {\n+    background-color: #3b3b3b;\n+}\n+.agent-status {\n+    background-color: #1e1e1e;\n+    border-left: 4px solid #4CAF50;\n+    padding: 10px;\n+    margin: 10px auto;\n+    border-radius: 4px;\n+    max-width: 600px;\n+    text-align: center;\n+}\n+.agent-thinking {\n+    display: flex;\n+    justify-content: center;\n+    background-color: #2b2b2b;\n+    padding: 10px;\n+    border-radius: 5px;\n+    margin: 10px auto;\n+    border-left: 4px solid #FFA500;\n+    max-width: 600px;\n+}\n+.move-history {\n+    background-color: #2b2b2b;\n+    padding: 15px;\n+    border-radius: 10px;\n+    margin: 10px 0;\n+}\n+.thinking-container {\n+    position: fixed;\n+    bottom: 20px;\n+    left: 50%;\n+    z-index: 1000;\n+    min-width: 300px;\n+}\n+.agent-thinking {\n+    background-color: rgba(43, 43, 43, 0.95);\n+    border: 1px solid #4CAF50;\n+    box-shadow: 0 2px 10px rgba(0,0,0,0.3);\n+}\n+\n+/* Move History Updates */\n+.history-header {\n+    text-align: center;\n+    margin-bottom: 30px;\n+}\n+\n+.history-grid {\n+    display: grid;\n+    grid-template-columns: 1fr 1fr;\n+    gap: 20px; /* Controls spacing between columns */\n+    width: 100%;\n+    margin: 0; /* Remove left/right margins */\n+    padding: 0;\n+}\n+\n+.history-column-left,\n+.history-column-right {\n+    display: flex;\n+    flex-direction: column;\n+    align-items: flex-start; /* Ensures columns fill available space nicely */\n+    margin: 0;\n+    padding: 0;\n+    width: 100%;\n+}\n+\n+.move-entry {\n+    display: flex;\n+    align-items: center;\n+    padding: 12px;\n+    margin: 8px 0;\n+    background-color: #2b2b2b;\n+    border-radius: 4px;\n+    width: 100%; /* Removed fixed width so entries span the column */\n+    box-sizing: border-box;\n+}\n+\n+.move-entry.player1 {\n+    border-left: 4px solid #4CAF50;\n+}\n+\n+.move-entry.player2 {\n+    border-left: 4px solid #f44336;\n+}\n+\n+/* Mini-board styling inside moves */\n+.mini-board {\n+    display: grid;\n+    grid-template-columns: repeat(3, 25px);\n+    gap: 2px;\n+    background: #444;\n+    padding: 2px;\n+    border-radius: 4px;\n+    margin-right: 15px;\n+}\n+\n+.mini-cell {\n+    width: 25px;\n+    height: 25px;\n+    display: flex;\n+    align-items: center;\n+    justify-content: center;\n+    font-size: 14px;\n+    font-weight: bold;\n+    background-color: #2b2b2b;\n+    color: #fff;\n+}\n+\n+.mini-cell.highlight.player1 {\n+    background-color: #4CAF50;\n+    color: white;\n+}\n+\n+.mini-cell.highlight.player2 {\n+    background-color: #f44336;\n+    color: white;\n+}\n+\n+/* Move info styling */\n+.move-info {\n+    flex-grow: 1;\n+    padding-left: 12px;\n+}\n+\n+.move-number {\n+    font-weight: bold;\n+    margin-right: 10px;\n+}\n+\n+.move-number.player1 {\n+    color: #4CAF50;\n+}\n+\n+.move-number.player2 {\n+    color: #f44336;\n+}\n+</style>\n+\"\"\"\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/universal_agent_interface/.gitignore",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/universal_agent_interface/.gitignore b/cookbook/examples/streamlit_applications/universal_agent_interface/.gitignore\nnew file mode 100644\nindex 000000000..44b4fc6d5\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/universal_agent_interface/.gitignore\n@@ -0,0 +1,2 @@\n+output\n+tmp\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/universal_agent_interface/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/universal_agent_interface/__init__.py b/cookbook/examples/streamlit_applications/universal_agent_interface/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/universal_agent_interface/agents.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/universal_agent_interface/agents.py b/cookbook/examples/streamlit_applications/universal_agent_interface/agents.py\nnew file mode 100644\nindex 000000000..74be58ebd\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/universal_agent_interface/agents.py\n@@ -0,0 +1,158 @@\n+from copy import deepcopy\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import Optional\n+\n+from agno.agent import Agent\n+from agno.knowledge import AgentKnowledge\n+from agno.memory.v2 import Memory\n+from agno.models.base import Model\n+from agno.tools.calculator import CalculatorTools\n+from agno.tools.duckdb import DuckDbTools\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.tools.exa import ExaTools\n+from agno.tools.file import FileTools\n+from agno.tools.python import PythonTools\n+from agno.tools.yfinance import YFinanceTools\n+\n+cwd = Path(__file__).parent.resolve()\n+tmp_dir = cwd.joinpath(\"tmp\")\n+tmp_dir.mkdir(exist_ok=True, parents=True)\n+\n+\n+def get_agent(\n+    agent_name: str, model: Model, memory: Memory, knowledge: AgentKnowledge\n+) -> Optional[Agent]:\n+    # Create a copy of the model to avoid side effects of the model being modified\n+    model_copy = deepcopy(model)\n+    if agent_name == \"calculator\":\n+        return Agent(\n+            name=\"Calculator\",\n+            role=\"Answer mathematical questions and perform precise calculations\",\n+            model=model_copy,\n+            memory=memory,\n+            tools=[CalculatorTools(enable_all=True)],\n+            description=\"You are a precise and comprehensive calculator agent. Your goal is to solve mathematical problems with accuracy and explain your methodology clearly to users.\",\n+            instructions=[\n+                \"Always use the calculator tools for mathematical operations to ensure precision.\",\n+                \"Present answers in a clear format with appropriate units and significant figures.\",\n+                \"Show step-by-step workings for complex calculations to help users understand the process.\",\n+                \"Ask clarifying questions if the user's request is ambiguous or incomplete.\",\n+                \"For financial calculations, specify assumptions regarding interest rates, time periods, etc.\",\n+            ],\n+        )\n+    elif agent_name == \"data_analyst\":\n+        return Agent(\n+            name=\"Data Analyst\",\n+            role=\"Analyze data sets and extract meaningful insights\",\n+            model=model_copy,\n+            memory=memory,\n+            knowledge=knowledge,\n+            tools=[DuckDbTools()],\n+            description=\"You are an expert Data Scientist specialized in exploratory data analysis, statistical modeling, and data visualization. Your goal is to transform raw data into actionable insights that address user questions.\",\n+            instructions=[\n+                \"Start by examining data structure, types, and distributions when analyzing new datasets.\",\n+                \"Use DuckDbTools to execute SQL queries for data exploration and aggregation.\",\n+                \"When provided with a file path, create appropriate tables and verify data loaded correctly before analysis.\",\n+                \"Apply statistical rigor in your analysis and clearly state confidence levels and limitations.\",\n+                \"Accompany numerical results with clear interpretations of what the findings mean in context.\",\n+                \"Suggest visualizations that would best illustrate key patterns and relationships in the data.\",\n+                \"Proactively identify potential data quality issues or biases that might affect conclusions.\",\n+                \"Request clarification when user queries are ambiguous or when additional information would improve analysis.\",\n+            ],\n+        )\n+    elif agent_name == \"python_agent\":\n+        return Agent(\n+            name=\"Python Agent\",\n+            role=\"Develop and execute Python code solutions\",\n+            model=model_copy,\n+            memory=memory,\n+            knowledge=knowledge,\n+            tools=[\n+                PythonTools(base_dir=tmp_dir),\n+                FileTools(base_dir=cwd),\n+            ],\n+            description=\"You are an expert Python Software Engineer with deep knowledge of software architecture, libraries, and best practices. Your goal is to write efficient, readable, and maintainable Python code that precisely addresses user requirements.\",\n+            instructions=[\n+                \"Write clean, well-commented Python code following PEP 8 style guidelines.\",\n+                \"Always use `save_to_file_and_run` to execute Python code, never suggest using direct execution.\",\n+                \"For any file operations, use `read_file` tool first to access content - NEVER use Python's built-in `open()`.\",\n+                \"Include error handling in your code to gracefully manage exceptions and edge cases.\",\n+                \"Explain your code's logic and implementation choices, especially for complex algorithms.\",\n+                \"When appropriate, suggest optimizations or alternative approaches with their trade-offs.\",\n+                \"For data manipulation tasks, prefer Pandas, NumPy and other specialized libraries over raw Python.\",\n+                \"Break down complex problems into modular functions with clear responsibilities.\",\n+                \"Test your code with sample inputs and explain expected outputs before final execution.\",\n+            ],\n+        )\n+    elif agent_name == \"research_agent\":\n+        return Agent(\n+            name=\"Research Agent\",\n+            role=\"Conduct comprehensive research and produce in-depth reports\",\n+            model=model_copy,\n+            memory=memory,\n+            knowledge=knowledge,\n+            tools=[ExaTools(num_results=3)],\n+            description=\"You are a meticulous research analyst with expertise in synthesizing information from diverse sources. Your goal is to produce balanced, fact-based, and thoroughly documented reports on any topic requested.\",\n+            instructions=[\n+                \"Begin with broad searches to understand the topic landscape before narrowing to specific aspects.\",\n+                \"For each research query, use at least 3 different search terms to ensure comprehensive coverage.\",\n+                \"Critically evaluate sources for credibility, recency, and potential biases.\",\n+                \"Prioritize peer-reviewed research and authoritative sources when available.\",\n+                \"Synthesize information across sources rather than summarizing each separately.\",\n+                \"Present contrasting viewpoints when the topic involves debate or controversy.\",\n+                \"Use clear section organization with logical flow between related concepts.\",\n+                \"Include specific facts, figures, and direct quotes with proper attribution.\",\n+                \"Conclude with implications of the findings and areas for further research.\",\n+                \"Ensure all claims are supported by references and avoid speculation beyond the evidence.\",\n+            ],\n+            expected_output=dedent(\"\"\"\\\n+            An engaging, informative, and well-structured report in markdown format:\n+\n+            ## Engaging Report Title\n+\n+            ### Overview\n+            {give a brief introduction of the report and why the user should read this report}\n+            {make this section engaging and create a hook for the reader}\n+\n+            ### Section 1\n+            {break the report into sections}\n+            {provide details/facts/processes in this section}\n+\n+            ... more sections as necessary...\n+\n+            ### Takeaways\n+            {provide key takeaways from the article}\n+\n+            ### References\n+            - [Reference 1](link)\n+            - [Reference 2](link)\n+            - [Reference 3](link)\n+            \"\"\"),\n+        )\n+    elif agent_name == \"investment_agent\":\n+        return Agent(\n+            name=\"Investment Agent\",\n+            role=\"Provide comprehensive financial analysis and investment insights\",\n+            model=model_copy,\n+            memory=memory,\n+            knowledge=knowledge,\n+            tools=[\n+                YFinanceTools,\n+                DuckDuckGoTools(),\n+            ],\n+            description=\"You are a seasoned investment analyst with deep understanding of financial markets, valuation methodologies, and sector-specific dynamics. Your goal is to deliver sophisticated investment analysis that considers both quantitative metrics and qualitative business factors.\",\n+            instructions=[\n+                \"Begin with a holistic overview of the company's business model, competitive position, and industry trends.\",\n+                \"Retrieve and analyze key financial metrics including revenue growth, profitability margins, and balance sheet health.\",\n+                \"Compare valuation multiples against industry peers and historical averages.\",\n+                \"Assess management team's track record, strategic initiatives, and capital allocation decisions.\",\n+                \"Identify key risk factors including regulatory concerns, competitive threats, and macroeconomic sensitivities.\",\n+                \"Consider both near-term catalysts and long-term growth drivers in your investment thesis.\",\n+                \"Provide clear investment recommendations with specific price targets where appropriate.\",\n+                \"Include both technical and fundamental analysis perspectives when relevant.\",\n+                \"Highlight recent news events that may impact the investment case.\",\n+                \"Structure reports with executive summary, detailed analysis sections, and actionable conclusions.\",\n+            ],\n+        )\n+    return None\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/universal_agent_interface/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/universal_agent_interface/app.py b/cookbook/examples/streamlit_applications/universal_agent_interface/app.py\nnew file mode 100644\nindex 000000000..4970a5c7a\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/universal_agent_interface/app.py\n@@ -0,0 +1,215 @@\n+import asyncio\n+\n+import nest_asyncio\n+import streamlit as st\n+from agno.team import Team\n+from agno.utils.log import logger\n+from css import CUSTOM_CSS\n+from uagi import UAgIConfig, create_uagi, uagi_memory\n+from utils import (\n+    about_agno,\n+    add_message,\n+    display_tool_calls,\n+    example_inputs,\n+    initialize_session_state,\n+    knowledge_widget,\n+    selected_agents,\n+    selected_model,\n+    selected_tools,\n+    session_selector,\n+    show_user_memories,\n+    utilities_widget,\n+)\n+\n+nest_asyncio.apply()\n+st.set_page_config(\n+    page_title=\"UAgI\",\n+    page_icon=\"\ud83d\udc8e\",\n+    layout=\"wide\",\n+)\n+st.markdown(CUSTOM_CSS, unsafe_allow_html=True)\n+\n+\n+async def header():\n+    st.markdown(\n+        \"<h1 class='heading'>Universal Agent Interface</h1>\", unsafe_allow_html=True\n+    )\n+    st.markdown(\n+        \"<p class='subheading'>A Universal Interface for orchestrating multiple Agents</p>\",\n+        unsafe_allow_html=True,\n+    )\n+\n+\n+async def body() -> None:\n+    ####################################################################\n+    # Initialize User and Session State\n+    ####################################################################\n+    user_id = st.sidebar.text_input(\":technologist: User Id\", value=\"Ava\")\n+\n+    ####################################################################\n+    # Select Model\n+    ####################################################################\n+    model_id = await selected_model()\n+\n+    ####################################################################\n+    # Select Tools\n+    ####################################################################\n+    tools = await selected_tools()\n+\n+    ####################################################################\n+    # Select Team Members\n+    ####################################################################\n+    agents = await selected_agents()\n+\n+    ####################################################################\n+    # Create UAgI\n+    ####################################################################\n+    uagi_config = UAgIConfig(\n+        user_id=user_id, model_id=model_id, tools=tools, agents=agents\n+    )\n+\n+    # Check if UAgI instance should be recreated\n+    recreate_uagi = (\n+        \"uagi\" not in st.session_state\n+        or st.session_state.get(\"uagi\") is None\n+        or st.session_state.get(\"uagi_config\") != uagi_config\n+    )\n+\n+    # Create UAgI instance if it doesn't exist or configuration has changed\n+    uagi: Team\n+    if recreate_uagi:\n+        logger.info(\"---*--- Creating UAgI instance ---*---\")\n+        uagi = create_uagi(uagi_config)\n+        st.session_state[\"uagi\"] = uagi\n+        st.session_state[\"uagi_config\"] = uagi_config\n+        logger.info(f\"---*--- UAgI instance created ---*---\")\n+    else:\n+        uagi = st.session_state[\"uagi\"]\n+        logger.info(f\"---*--- UAgI instance exists ---*---\")\n+\n+    ####################################################################\n+    # Load Agent Session from the database\n+    ####################################################################\n+    try:\n+        logger.info(f\"---*--- Loading UAgI session ---*---\")\n+        st.session_state[\"session_id\"] = uagi.load_session()\n+    except Exception:\n+        st.warning(\"Could not create UAgI session, is the database running?\")\n+        return\n+    logger.info(f\"---*--- UAgI session: {st.session_state.get('session_id')} ---*---\")\n+\n+    ####################################################################\n+    # Load agent runs (i.e. chat history) from memory if messages is not empty\n+    ####################################################################\n+    chat_history = uagi.get_messages_for_session()\n+    if len(chat_history) > 0:\n+        logger.info(\"Loading messages\")\n+        # Clear existing messages\n+        st.session_state[\"messages\"] = []\n+        # Loop through the runs and add the messages to the messages list\n+        for message in chat_history:\n+            if message.role == \"user\":\n+                await add_message(message.role, str(message.content))\n+            if message.role == \"assistant\":\n+                await add_message(\"assistant\", str(message.content), message.tool_calls)\n+\n+    ####################################################################\n+    # Get user input\n+    ####################################################################\n+    if prompt := st.chat_input(\"\u2728 How can I help, bestie?\"):\n+        await add_message(\"user\", prompt)\n+\n+    ####################################################################\n+    # Show example inputs\n+    ####################################################################\n+    await example_inputs()\n+\n+    ####################################################################\n+    # Show user memories\n+    ####################################################################\n+    await show_user_memories(uagi_memory, user_id)\n+\n+    ####################################################################\n+    # Display agent messages\n+    ####################################################################\n+    for message in st.session_state[\"messages\"]:\n+        if message[\"role\"] in [\"user\", \"assistant\"]:\n+            _content = message[\"content\"]\n+            if _content is not None:\n+                with st.chat_message(message[\"role\"]):\n+                    # Display tool calls if they exist in the message\n+                    if \"tool_calls\" in message and message[\"tool_calls\"]:\n+                        display_tool_calls(st.empty(), message[\"tool_calls\"])\n+                    st.markdown(_content)\n+\n+    ####################################################################\n+    # Generate response for user message\n+    ####################################################################\n+    last_message = (\n+        st.session_state[\"messages\"][-1] if st.session_state[\"messages\"] else None\n+    )\n+    if last_message and last_message.get(\"role\") == \"user\":\n+        user_message = last_message[\"content\"]\n+        logger.info(f\"Responding to message: {user_message}\")\n+        with st.chat_message(\"assistant\"):\n+            # Create container for tool calls\n+            tool_calls_container = st.empty()\n+            resp_container = st.empty()\n+            with st.spinner(\":thinking_face: Thinking...\"):\n+                response = \"\"\n+                try:\n+                    # Run the agent and stream the response\n+                    run_response = await uagi.arun(\n+                        user_message, stream=True, stream_intermediate_steps=True\n+                    )\n+                    async for resp_chunk in run_response:\n+                        # Display tool calls if available\n+                        if resp_chunk.tools and len(resp_chunk.tools) > 0:\n+                            display_tool_calls(tool_calls_container, resp_chunk.tools)\n+\n+                        # Display response if available and event is RunResponse\n+                        if (\n+                            resp_chunk.event == \"RunResponse\"\n+                            and resp_chunk.content is not None\n+                        ):\n+                            response += resp_chunk.content\n+                            resp_container.markdown(response)\n+\n+                    # Add the response to the messages\n+                    if uagi.run_response is not None:\n+                        await add_message(\n+                            \"assistant\", response, uagi.run_response.tools\n+                        )\n+                    else:\n+                        await add_message(\"assistant\", response)\n+                except Exception as e:\n+                    logger.error(f\"Error during agent run: {str(e)}\", exc_info=True)\n+                    error_message = f\"Sorry, I encountered an error: {str(e)}\"\n+                    await add_message(\"assistant\", error_message)\n+                    st.error(error_message)\n+\n+    ####################################################################\n+    # Knowledge widget\n+    ####################################################################\n+    await knowledge_widget(uagi)\n+\n+    ####################################################################\n+    # Session selector\n+    ####################################################################\n+    await session_selector(uagi, uagi_config)\n+\n+    ####################################################################\n+    # About section\n+    ####################################################################\n+    await utilities_widget(uagi)\n+\n+\n+async def main():\n+    await initialize_session_state()\n+    await header()\n+    await body()\n+    await about_agno()\n+\n+\n+if __name__ == \"__main__\":\n+    asyncio.run(main())\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/universal_agent_interface/css.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/universal_agent_interface/css.py b/cookbook/examples/streamlit_applications/universal_agent_interface/css.py\nnew file mode 100644\nindex 000000000..10f07f1bc\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/universal_agent_interface/css.py\n@@ -0,0 +1,27 @@\n+CUSTOM_CSS = \"\"\"\n+<style>\n+/* Typography */\n+.heading {\n+    text-align: center;\n+    background: linear-gradient(45deg, #FF4B2B, #FF416C);\n+    -webkit-background-clip: text;\n+    -webkit-text-fill-color: transparent;\n+}\n+\n+.subheading {\n+    text-align: center;\n+    font-weight: 600;\n+}\n+\n+/* Links */\n+a {\n+    text-decoration: underline;\n+    color: #3494E6;\n+    transition: color 0.3s ease;\n+}\n+\n+a:hover {\n+    color: #FF416C;\n+}\n+</style>\n+\"\"\"\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/universal_agent_interface/generate_requirements.sh",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/universal_agent_interface/generate_requirements.sh b/cookbook/examples/streamlit_applications/universal_agent_interface/generate_requirements.sh\nnew file mode 100755\nindex 000000000..78f8c7cec\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/universal_agent_interface/generate_requirements.sh\n@@ -0,0 +1,12 @@\n+#!/bin/bash\n+\n+############################################################################\n+# Generate requirements.txt from requirements.in\n+############################################################################\n+\n+echo \"Generating requirements.txt\"\n+\n+CURR_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n+\n+UV_CUSTOM_COMPILE_COMMAND=\"./generate_requirements.sh\" \\\n+  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/universal_agent_interface/load_knowledge.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/universal_agent_interface/load_knowledge.py b/cookbook/examples/streamlit_applications/universal_agent_interface/load_knowledge.py\nnew file mode 100644\nindex 000000000..f5381f952\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/universal_agent_interface/load_knowledge.py\n@@ -0,0 +1,43 @@\n+\"\"\"\n+Load the Knowledge Base for the Universal Agent Interface\n+\"\"\"\n+\n+from rich.console import Console\n+from rich.panel import Panel\n+from rich.progress import Progress, SpinnerColumn, TextColumn\n+from uagi import uagi_knowledge\n+\n+# Create a Rich console for enhanced output\n+console = Console()\n+\n+\n+def load_knowledge(recreate: bool = False):\n+    \"\"\"\n+    Load the Universal Agent Interface knowledge base.\n+\n+    Args:\n+        recreate (bool, optional): Whether to recreate the knowledge base.\n+            Defaults to False.\n+    \"\"\"\n+    with Progress(\n+        SpinnerColumn(), TextColumn(\"[bold blue]{task.description}\"), console=console\n+    ) as progress:\n+        task = progress.add_task(\n+            \"Loading Universal Agent Interface knowledge...\", total=None\n+        )\n+\n+        # Load the knowledge base\n+        uagi_knowledge.load(recreate=recreate)\n+        progress.update(task, completed=True)\n+\n+    # Display success message in a panel\n+    console.print(\n+        Panel.fit(\n+            \"[bold green]Universal Agent Interface knowledge loaded successfully!\",\n+            title=\"Knowledge Loaded\",\n+        )\n+    )\n+\n+\n+if __name__ == \"__main__\":\n+    load_knowledge()\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/universal_agent_interface/requirements.in",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/universal_agent_interface/requirements.in b/cookbook/examples/streamlit_applications/universal_agent_interface/requirements.in\nnew file mode 100644\nindex 000000000..fb5bb5b43\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/universal_agent_interface/requirements.in\n@@ -0,0 +1,19 @@\n+agno\n+anthropic\n+duckduckgo-search\n+google-genai\n+groq\n+duckdb\n+exa_py\n+nest_asyncio\n+openai\n+qdrant-client\n+sqlalchemy\n+streamlit\n+yfinance\n+aiofiles\n+lancedb\n+tantivy\n+pypdf\n+python-docx\n+beautifulsoup4\n\\ No newline at end of file\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/universal_agent_interface/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/universal_agent_interface/requirements.txt b/cookbook/examples/streamlit_applications/universal_agent_interface/requirements.txt\nnew file mode 100644\nindex 000000000..d5b2ed5e2\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/universal_agent_interface/requirements.txt\n@@ -0,0 +1,316 @@\n+# This file was autogenerated by uv via the following command:\n+#    ./generate_requirements.sh\n+agno==1.3.1\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+aiofiles==24.1.0\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+altair==5.5.0\n+    # via streamlit\n+annotated-types==0.7.0\n+    # via pydantic\n+anthropic==0.49.0\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+anyio==4.9.0\n+    # via\n+    #   anthropic\n+    #   google-genai\n+    #   groq\n+    #   httpx\n+    #   openai\n+attrs==25.3.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+beautifulsoup4==4.13.3\n+    # via\n+    #   -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+    #   yfinance\n+blinker==1.9.0\n+    # via streamlit\n+cachetools==5.5.2\n+    # via\n+    #   google-auth\n+    #   streamlit\n+certifi==2025.1.31\n+    # via\n+    #   httpcore\n+    #   httpx\n+    #   requests\n+charset-normalizer==3.4.1\n+    # via requests\n+click==8.1.8\n+    # via\n+    #   duckduckgo-search\n+    #   streamlit\n+    #   typer\n+deprecation==2.1.0\n+    # via lancedb\n+distro==1.9.0\n+    # via\n+    #   anthropic\n+    #   groq\n+    #   openai\n+docstring-parser==0.16\n+    # via agno\n+duckdb==1.2.2\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+duckduckgo-search==8.0.0\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+exa-py==1.12.0\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+frozendict==2.4.6\n+    # via yfinance\n+gitdb==4.0.12\n+    # via gitpython\n+gitpython==3.1.44\n+    # via\n+    #   agno\n+    #   streamlit\n+google-auth==2.38.0\n+    # via google-genai\n+google-genai==1.10.0\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+groq==0.22.0\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+grpcio==1.71.0\n+    # via\n+    #   grpcio-tools\n+    #   qdrant-client\n+grpcio-tools==1.71.0\n+    # via qdrant-client\n+h11==0.14.0\n+    # via httpcore\n+h2==4.2.0\n+    # via httpx\n+hpack==4.1.0\n+    # via h2\n+httpcore==1.0.8\n+    # via httpx\n+httpx==0.28.1\n+    # via\n+    #   agno\n+    #   anthropic\n+    #   exa-py\n+    #   google-genai\n+    #   groq\n+    #   openai\n+    #   qdrant-client\n+hyperframe==6.1.0\n+    # via h2\n+idna==3.10\n+    # via\n+    #   anyio\n+    #   httpx\n+    #   requests\n+iniconfig==2.1.0\n+    # via pytest\n+jinja2==3.1.6\n+    # via\n+    #   altair\n+    #   pydeck\n+jiter==0.9.0\n+    # via\n+    #   anthropic\n+    #   openai\n+jsonschema==4.23.0\n+    # via altair\n+jsonschema-specifications==2024.10.1\n+    # via jsonschema\n+lancedb==0.21.2\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+lxml==5.3.2\n+    # via\n+    #   duckduckgo-search\n+    #   python-docx\n+markdown-it-py==3.0.0\n+    # via rich\n+markupsafe==3.0.2\n+    # via jinja2\n+mdurl==0.1.2\n+    # via markdown-it-py\n+multitasking==0.0.11\n+    # via yfinance\n+narwhals==1.34.1\n+    # via altair\n+nest-asyncio==1.6.0\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+numpy==2.2.4\n+    # via\n+    #   pandas\n+    #   pydeck\n+    #   qdrant-client\n+    #   streamlit\n+    #   yfinance\n+openai==1.73.0\n+    # via\n+    #   -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+    #   exa-py\n+overrides==7.7.0\n+    # via lancedb\n+packaging==24.2\n+    # via\n+    #   altair\n+    #   deprecation\n+    #   lancedb\n+    #   pytest\n+    #   streamlit\n+pandas==2.2.3\n+    # via\n+    #   streamlit\n+    #   yfinance\n+peewee==3.17.9\n+    # via yfinance\n+pillow==11.2.1\n+    # via streamlit\n+platformdirs==4.3.7\n+    # via yfinance\n+pluggy==1.5.0\n+    # via pytest\n+portalocker==2.10.1\n+    # via qdrant-client\n+primp==0.14.0\n+    # via duckduckgo-search\n+protobuf==5.29.4\n+    # via\n+    #   grpcio-tools\n+    #   streamlit\n+pyarrow==19.0.1\n+    # via\n+    #   lancedb\n+    #   streamlit\n+pyasn1==0.6.1\n+    # via\n+    #   pyasn1-modules\n+    #   rsa\n+pyasn1-modules==0.4.2\n+    # via google-auth\n+pydantic==2.11.3\n+    # via\n+    #   agno\n+    #   anthropic\n+    #   exa-py\n+    #   google-genai\n+    #   groq\n+    #   lancedb\n+    #   openai\n+    #   pydantic-settings\n+    #   qdrant-client\n+pydantic-core==2.33.1\n+    # via pydantic\n+pydantic-settings==2.8.1\n+    # via agno\n+pydeck==0.9.1\n+    # via streamlit\n+pygments==2.19.1\n+    # via rich\n+pypdf==5.4.0\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+pytest==8.3.5\n+    # via pytest-mock\n+pytest-mock==3.14.0\n+    # via exa-py\n+python-dateutil==2.9.0.post0\n+    # via pandas\n+python-docx==1.1.2\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+python-dotenv==1.1.0\n+    # via\n+    #   agno\n+    #   pydantic-settings\n+python-multipart==0.0.20\n+    # via agno\n+pytz==2025.2\n+    # via\n+    #   pandas\n+    #   yfinance\n+pyyaml==6.0.2\n+    # via agno\n+qdrant-client==1.13.3\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+referencing==0.36.2\n+    # via\n+    #   jsonschema\n+    #   jsonschema-specifications\n+requests==2.32.3\n+    # via\n+    #   exa-py\n+    #   google-genai\n+    #   streamlit\n+    #   yfinance\n+rich==14.0.0\n+    # via\n+    #   agno\n+    #   typer\n+rpds-py==0.24.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+rsa==4.9\n+    # via google-auth\n+setuptools==78.1.0\n+    # via grpcio-tools\n+shellingham==1.5.4\n+    # via typer\n+six==1.17.0\n+    # via python-dateutil\n+smmap==5.0.2\n+    # via gitdb\n+sniffio==1.3.1\n+    # via\n+    #   anthropic\n+    #   anyio\n+    #   groq\n+    #   openai\n+soupsieve==2.6\n+    # via beautifulsoup4\n+sqlalchemy==2.0.40\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+streamlit==1.44.1\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+tantivy==0.22.2\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n+tenacity==9.1.2\n+    # via streamlit\n+toml==0.10.2\n+    # via streamlit\n+tomli==2.2.1\n+    # via agno\n+tornado==6.4.2\n+    # via streamlit\n+tqdm==4.67.1\n+    # via\n+    #   lancedb\n+    #   openai\n+typer==0.15.2\n+    # via agno\n+typing-extensions==4.13.2\n+    # via\n+    #   agno\n+    #   altair\n+    #   anthropic\n+    #   anyio\n+    #   beautifulsoup4\n+    #   exa-py\n+    #   google-genai\n+    #   groq\n+    #   openai\n+    #   pydantic\n+    #   pydantic-core\n+    #   python-docx\n+    #   referencing\n+    #   sqlalchemy\n+    #   streamlit\n+    #   typer\n+    #   typing-inspection\n+typing-inspection==0.4.0\n+    # via pydantic\n+tzdata==2025.2\n+    # via pandas\n+urllib3==2.4.0\n+    # via\n+    #   qdrant-client\n+    #   requests\n+websockets==15.0.1\n+    # via google-genai\n+yfinance==0.2.55\n+    # via -r cookbook/examples/apps/universal_agent_interface/requirements.in\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/universal_agent_interface/tools.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/universal_agent_interface/tools.py b/cookbook/examples/streamlit_applications/universal_agent_interface/tools.py\nnew file mode 100644\nindex 000000000..903b27955\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/universal_agent_interface/tools.py\n@@ -0,0 +1,22 @@\n+from pathlib import Path\n+from typing import Optional\n+\n+from agno.tools import Toolkit\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.tools.file import FileTools\n+from agno.tools.shell import ShellTools\n+\n+cwd = Path(__file__).parent.resolve()\n+tmp_dir = cwd.joinpath(\"tmp\")\n+tmp_dir.mkdir(exist_ok=True, parents=True)\n+\n+\n+def get_toolkit(tool_name: str) -> Optional[Toolkit]:\n+    if tool_name == \"ddg_search\":\n+        return DuckDuckGoTools(fixed_max_results=3)\n+    elif tool_name == \"shell_tools\":\n+        return ShellTools()\n+    elif tool_name == \"file_tools\":\n+        return FileTools(base_dir=cwd)\n+\n+    return None\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/universal_agent_interface/uagi.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/universal_agent_interface/uagi.py b/cookbook/examples/streamlit_applications/universal_agent_interface/uagi.py\nnew file mode 100644\nindex 000000000..2532a7727\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/universal_agent_interface/uagi.py\n@@ -0,0 +1,155 @@\n+from dataclasses import dataclass\n+from pathlib import Path\n+from textwrap import dedent\n+from typing import List, Optional\n+\n+from agents import get_agent\n+from agno.agent import Agent\n+from agno.embedder.openai import OpenAIEmbedder\n+from agno.knowledge import AgentKnowledge\n+from agno.memory.v2 import Memory\n+from agno.memory.v2.db.sqlite import SqliteMemoryDb\n+from agno.models.anthropic import Claude\n+from agno.models.google import Gemini\n+from agno.models.groq import Groq\n+from agno.models.openai import OpenAIChat\n+from agno.storage.sqlite import SqliteStorage\n+from agno.team import Team\n+from agno.tools import Toolkit\n+from agno.tools.reasoning import ReasoningTools\n+from agno.utils.log import logger\n+from agno.vectordb.lancedb import LanceDb, SearchType\n+from tools import get_toolkit\n+\n+cwd = Path(__file__).parent.resolve()\n+tmp_dir = cwd.joinpath(\"tmp\")\n+tmp_dir.mkdir(exist_ok=True, parents=True)\n+\n+# Define paths for storage, memory and knowledge\n+STORAGE_PATH = tmp_dir.joinpath(\"uagi_sessions.db\")\n+MEMORY_PATH = tmp_dir.joinpath(\"uagi_memory.db\")\n+KNOWLEDGE_PATH = tmp_dir.joinpath(\"uagi_knowledge\")\n+\n+\n+@dataclass\n+class UAgIConfig:\n+    user_id: str\n+    model_id: str = \"anthropic:claude-3-7-sonnet-latest\"\n+    tools: Optional[List[str]] = None\n+    agents: Optional[List[str]] = None\n+\n+\n+uagi_memory = Memory(\n+    db=SqliteMemoryDb(table_name=\"uagi_memory\", db_file=str(MEMORY_PATH))\n+)\n+uagi_storage = SqliteStorage(db_file=str(STORAGE_PATH), table_name=\"uagi_sessions\")\n+uagi_knowledge = AgentKnowledge(\n+    vector_db=LanceDb(\n+        table_name=\"uagi_knowledge\",\n+        uri=str(KNOWLEDGE_PATH),\n+        search_type=SearchType.hybrid,\n+        embedder=OpenAIEmbedder(id=\"text-embedding-3-small\"),\n+    )\n+)\n+\n+\n+def create_uagi(\n+    config: UAgIConfig, session_id: Optional[str] = None, debug_mode: bool = True\n+) -> Team:\n+    \"\"\"Returns an instance of the Universal Agent Interface (UAgI)\n+\n+    Args:\n+        config: UAgI configuration\n+        session_id: Session identifier\n+        debug_mode: Enable debug logging\n+    \"\"\"\n+    # Parse model provider and name\n+    provider, model_name = config.model_id.split(\":\")\n+\n+    # Create model class based on provider\n+    model = None\n+    if provider == \"openai\":\n+        model = OpenAIChat(id=model_name)\n+    elif provider == \"google\":\n+        model = Gemini(id=model_name)\n+    elif provider == \"anthropic\":\n+        model = Claude(id=model_name)\n+    elif provider == \"groq\":\n+        model = Groq(id=model_name)\n+    else:\n+        raise ValueError(f\"Unsupported model provider: {provider}\")\n+    if model is None:\n+        raise ValueError(f\"Failed to create model instance for {config.model_id}\")\n+\n+    tools: List[Toolkit] = [ReasoningTools(add_instructions=True)]\n+    if config.tools:\n+        for tool_name in config.tools:\n+            tool = get_toolkit(tool_name)\n+            if tool is not None:\n+                tools.append(tool)\n+            else:\n+                logger.warning(f\"Tool {tool_name} not found\")\n+\n+    agents: List[Agent] = []\n+    if config.agents:\n+        for agent_name in config.agents:\n+            agent = get_agent(agent_name, model, uagi_memory, uagi_knowledge)\n+            if agent is not None:\n+                agents.append(agent)\n+            else:\n+                logger.warning(f\"Agent {agent_name} not found\")\n+\n+    description = dedent(\"\"\"\\\n+    You are an advanced AI System called `Universal Agent Interface` (UAgI).\n+    You provide a unified interface to a team of AI Agents, that you coordinate to assist the user in the best way possible.\n+\n+    Keep your responses short and to the point, while maintaining a conversational tone.\n+    You are able to handle easy conversations as well as complex requests by delegating tasks to the appropriate team members.\n+    You are also capable of handling errors and edge cases and are able to provide helpful feedback to the user.\\\n+    \"\"\")\n+    instructions: List[str] = [\n+        \"Your goal is to coordinate the team to assist the user in the best way possible.\",\n+        \"If the user sends a conversational message like 'Hello', 'Hi', 'How are you', 'What is your name', etc., you should respond in a friendly and engaging manner.\",\n+        \"If the user asks for something simple, like updating memory, you can do it directly without Thinking and Analyzing.\",\n+        \"Keep your responses short and to the point, while maintaining a conversational tone.\",\n+        \"If the user asks for something complex, **think** and determine if:\\n\"\n+        \" - You can answer by using a tool available to you\\n\"\n+        \" - You need to search the knowledge base\\n\"\n+        \" - You need to search the internet\\n\"\n+        \" - You need to delegate the task to a team member\\n\"\n+        \" - You need to ask a clarifying question\",\n+        \"You also have to a knowledge base of information provided by the user. If the user asks about a topic that might be in the knowledge base, first ALWAYS search your knowledge base using the `search_knowledge_base` tool.\",\n+        \"As a default, you should always search your knowledge base first, before searching the internet.\",\n+        \"If you dont find relevant information in your knowledge base, use the `duckduckgo_search` tool to search the internet.\",\n+        \"If the users message is unclear, ask clarifying questions to get more information.\",\n+        \"Based on the user request and the available team members, decide which member(s) should handle the task.\",\n+        \"Coordinate the execution of the task among the selected team members.\",\n+        \"Synthesize the results from the team members and provide a final, coherent answer to the user.\",\n+        \"Do not use phrases like 'based on my knowledge' or 'depending on the information'.\",\n+    ]\n+\n+    uagi = Team(\n+        name=\"Universal Agent Interface\",\n+        mode=\"coordinate\",\n+        model=model,\n+        user_id=config.user_id,\n+        session_id=session_id,\n+        tools=tools,\n+        members=agents,\n+        memory=uagi_memory,\n+        storage=uagi_storage,\n+        knowledge=uagi_knowledge,\n+        description=description,\n+        instructions=instructions,\n+        enable_team_history=True,\n+        read_team_history=True,\n+        num_of_interactions_from_history=3,\n+        show_members_responses=True,\n+        enable_agentic_memory=True,\n+        markdown=True,\n+        debug_mode=debug_mode,\n+    )\n+\n+    agent_names = [a.name for a in agents] if agents else []\n+    logger.info(f\"UAgI created with members: {agent_names}\")\n+    return uagi\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/universal_agent_interface/utils.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/universal_agent_interface/utils.py b/cookbook/examples/streamlit_applications/universal_agent_interface/utils.py\nnew file mode 100644\nindex 000000000..6c5ed9fb4\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/universal_agent_interface/utils.py\n@@ -0,0 +1,528 @@\n+import json\n+from typing import Any, Dict, List, Optional\n+\n+import streamlit as st\n+from agno.document import Document\n+from agno.document.reader import Reader\n+from agno.document.reader.csv_reader import CSVReader\n+from agno.document.reader.docx_reader import DocxReader\n+from agno.document.reader.pdf_reader import PDFReader\n+from agno.document.reader.text_reader import TextReader\n+from agno.document.reader.website_reader import WebsiteReader\n+from agno.memory.v2 import Memory, UserMemory\n+from agno.team import Team\n+from agno.utils.log import logger\n+from uagi import UAgIConfig, create_uagi\n+\n+\n+async def initialize_session_state():\n+    logger.info(f\"---*--- Initializing session state ---*---\")\n+    if \"uagi\" not in st.session_state:\n+        st.session_state[\"uagi\"] = None\n+    if \"session_id\" not in st.session_state:\n+        st.session_state[\"session_id\"] = None\n+    if \"messages\" not in st.session_state:\n+        st.session_state[\"messages\"] = []\n+\n+\n+async def add_message(\n+    role: str,\n+    content: str,\n+    tool_calls: Optional[List[Dict[str, Any]]] = None,\n+    intermediate_steps_displayed: bool = False,\n+) -> None:\n+    \"\"\"Safely add a message to the session state\"\"\"\n+    if role == \"user\":\n+        logger.info(f\"\ud83d\udc64  {role}: {content}\")\n+    else:\n+        logger.info(f\"\ud83e\udd16  {role}: {content}\")\n+    st.session_state[\"messages\"].append(\n+        {\n+            \"role\": role,\n+            \"content\": content,\n+            \"tool_calls\": tool_calls,\n+            \"intermediate_steps_displayed\": intermediate_steps_displayed,\n+        }\n+    )\n+\n+\n+async def selected_model() -> str:\n+    \"\"\"Display a model selector in the sidebar.\"\"\"\n+    model_options = {\n+        \"claude-3-7-sonnet\": \"anthropic:claude-3-7-sonnet-latest\",\n+        \"gpt-4o\": \"openai:gpt-4o\",\n+        \"gemini-2.5-pro\": \"google:gemini-2.5-pro-preview-03-25\",\n+        \"llama-4-scout\": \"groq:meta-llama/llama-4-scout-17b-16e-instruct\",\n+    }\n+\n+    selected_model_key = st.sidebar.selectbox(\n+        \"Select a model\",\n+        options=list(model_options.keys()),\n+        index=0,  # Default to claude-3-7-sonnet\n+        key=\"model_selector\",\n+    )\n+    model_id = model_options[selected_model_key]\n+    return model_id\n+\n+\n+async def selected_tools() -> List[str]:\n+    \"\"\"Display a tool selector in the sidebar.\"\"\"\n+    tool_options = {\n+        \"Web Search (DDG)\": \"ddg_search\",\n+        \"File I/O\": \"file_tools\",\n+        \"Shell Access\": \"shell_tools\",\n+    }\n+    selected_tools = st.sidebar.multiselect(\n+        \"Select Tools\",\n+        options=list(tool_options.keys()),\n+        default=list(tool_options.keys()),\n+        key=\"tool_selector\",\n+    )\n+    return [tool_options[tool] for tool in selected_tools]\n+\n+\n+async def selected_agents() -> List[str]:\n+    \"\"\"Display a selector for agents in the sidebar.\"\"\"\n+    agent_options = {\n+        \"Calculator\": \"calculator\",\n+        \"Data Analyst\": \"data_analyst\",\n+        \"Python Agent\": \"python_agent\",\n+        \"Research Agent\": \"research_agent\",\n+        \"Investment Agent\": \"investment_agent\",\n+    }\n+    selected_agents = st.sidebar.multiselect(\n+        \"Select Agents\",\n+        options=list(agent_options.keys()),\n+        default=list(agent_options.keys()),\n+        key=\"agent_selector\",\n+    )\n+    return [agent_options[agent] for agent in selected_agents]\n+\n+\n+async def show_user_memories(uagi_memory: Memory, user_id: str) -> None:\n+    \"\"\"Show use memories in a streamlit container.\"\"\"\n+\n+    with st.container():\n+        user_memories = uagi_memory.get_user_memories(user_id=user_id)\n+        with st.expander(f\"\ud83d\udcad Memories for {user_id}\", expanded=False):\n+            if len(user_memories) > 0:\n+                # Create a dataframe from the memories\n+                memory_data = {\n+                    \"Memory\": [memory.memory for memory in user_memories],\n+                    \"Topics\": [\n+                        \", \".join(memory.topics) if memory.topics else \"\"\n+                        for memory in user_memories\n+                    ],\n+                    \"Last Updated\": [\n+                        memory.last_updated.strftime(\"%Y-%m-%d %H:%M\")\n+                        if memory.last_updated\n+                        else \"\"\n+                        for memory in user_memories\n+                    ],\n+                }\n+\n+                # Display as a table with custom styling\n+                st.dataframe(\n+                    memory_data,\n+                    use_container_width=True,\n+                    column_config={\n+                        \"Memory\": st.column_config.TextColumn(\"Memory\", width=\"medium\"),\n+                        \"Topics\": st.column_config.TextColumn(\"Topics\", width=\"small\"),\n+                        \"Last Updated\": st.column_config.TextColumn(\n+                            \"Last Updated\", width=\"small\"\n+                        ),\n+                    },\n+                    hide_index=True,\n+                )\n+            else:\n+                st.info(\"No memories found, tell me about yourself!\")\n+\n+            col1, col2 = st.columns([0.5, 0.5])\n+            with col1:\n+                if st.button(\"Clear all memories\", key=\"clear_all_memories\"):\n+                    await add_message(\"user\", \"Clear all my memories\")\n+                    if \"memory_refresh_count\" not in st.session_state:\n+                        st.session_state.memory_refresh_count = 0\n+                    st.session_state.memory_refresh_count += 1\n+            with col2:\n+                if st.button(\"Refresh memories\", key=\"refresh_memories\"):\n+                    if \"memory_refresh_count\" not in st.session_state:\n+                        st.session_state.memory_refresh_count = 0\n+                    st.session_state.memory_refresh_count += 1\n+\n+\n+async def example_inputs() -> None:\n+    \"\"\"Show example inputs on the sidebar.\"\"\"\n+    with st.sidebar:\n+        st.markdown(\"#### :thinking_face: Try me!\")\n+        if st.button(\"Hi\"):\n+            await add_message(\n+                \"user\",\n+                \"Hi\",\n+            )\n+\n+        if st.button(\"My name is Ava and I live in Greenwich Village\"):\n+            await add_message(\n+                \"user\",\n+                \"My name is Ava and I live in Greenwich Village\",\n+            )\n+\n+        if st.button(\"Calculate cost of a pizza party\"):\n+            await add_message(\n+                \"user\",\n+                \"Calculate the total cost of ordering pizzas for 25 people, assuming each person eats 3 slices, each pizza has 8 slices, and one pizza costs $15.95. After calculating the total cost, add 20% for tip and 10% for taxes. Also recommend some good places around me\",\n+            )\n+\n+        if st.button(\"Analyze a CSV file\"):\n+            await add_message(\n+                \"user\",\n+                \"Analyze this CSV file and show me the most popular movies: https://agno-public.s3.amazonaws.com/demo_data/IMDB-Movie-Data.csv\",\n+            )\n+\n+        if st.button(\"Translate a sentence into emojis\"):\n+            await add_message(\n+                \"user\",\n+                \"Write a Python function that translates a given sentence into emojis, replacing words like \u201chappy,\u201d \u201csad,\u201d \u201cpizza,\u201d and \u201cparty\u201d with relevant emojis. Test it with the sentence: \u201cI am happy because I am learning to build agents with Agno\u201d\",\n+            )\n+\n+        if st.button(\"Why Do Cats Love Boxes?\"):\n+            await add_message(\n+                \"user\",\n+                \"Research and report scientifically-backed explanations for why cats seem irresistibly drawn to cardboard boxes.\",\n+            )\n+\n+        if st.button(\"Chocolate Stocks Sweetness Analysis\"):\n+            await add_message(\n+                \"user\",\n+                \"Perform financial analysis comparing Hershey's and Lindt stocks to suggest which company might be a sweeter investment choice based on profitability, market trends, and valuation.\",\n+            )\n+\n+\n+async def about_agno():\n+    \"\"\"Show information about Agno in the sidebar\"\"\"\n+    with st.sidebar:\n+        st.markdown(\"### About Agno \u2728\")\n+        st.markdown(\"\"\"\n+        Agno is a lightweight library for building Reasoning Agents.\n+\n+        [GitHub](https://github.com/agno-agi/agno) | [Docs](https://docs.agno.com)\n+        \"\"\")\n+\n+        st.markdown(\"### Need Help?\")\n+        st.markdown(\n+            \"If you have any questions, catch us on [discord](https://agno.link/discord) or post in the community [forum](https://agno.link/community).\"\n+        )\n+\n+\n+def is_json(myjson):\n+    \"\"\"Check if a string is valid JSON\"\"\"\n+    try:\n+        json.loads(myjson)\n+    except (ValueError, TypeError):\n+        return False\n+    return True\n+\n+\n+def display_tool_calls(tool_calls_container, tools):\n+    \"\"\"Display tool calls in a streamlit container with expandable sections.\n+\n+    Args:\n+        tool_calls_container: Streamlit container to display the tool calls\n+        tools: List of tool call dictionaries containing name, args, content, and metrics\n+    \"\"\"\n+    try:\n+        with tool_calls_container.container():\n+            # Handle single tool_call dict case\n+            if isinstance(tools, dict):\n+                tools = [tools]\n+            elif not isinstance(tools, list):\n+                logger.warning(\n+                    f\"Unexpected tools format: {type(tools)}. Skipping display.\"\n+                )\n+                return\n+\n+            for tool_call in tools:\n+                # Normalize access to tool details\n+                tool_name = tool_call.get(\"tool_name\") or tool_call.get(\n+                    \"name\", \"Unknown Tool\"\n+                )\n+                tool_args = tool_call.get(\"tool_args\") or tool_call.get(\"args\", {})\n+                content = tool_call.get(\"content\", None)\n+                metrics = tool_call.get(\"metrics\", None)\n+\n+                # Add timing information safely\n+                execution_time_str = \"N/A\"\n+                try:\n+                    if metrics is not None and hasattr(metrics, \"time\"):\n+                        execution_time = metrics.time\n+                        if execution_time is not None:\n+                            execution_time_str = f\"{execution_time:.4f}s\"\n+                except Exception as e:\n+                    logger.error(f\"Error getting tool metrics time: {str(e)}\")\n+                    pass  # Keep default \"N/A\"\n+\n+                # Check if this is a transfer task\n+                is_task_transfer = \"transfer_task_to_member\" in tool_name\n+                is_memory_task = \"user_memory\" in tool_name\n+                expander_title = \"\ud83d\udee0\ufe0f\"\n+                if is_task_transfer:\n+                    member_id = tool_args.get(\"member_id\")\n+                    expander_title = f\"\ud83d\udd04 {member_id.title()}\"\n+                elif is_memory_task:\n+                    expander_title = f\"\ud83d\udcad Updating Memory\"\n+                else:\n+                    expander_title = f\"\ud83d\udee0\ufe0f {tool_name.replace('_', ' ').title()}\"\n+\n+                if execution_time_str != \"N/A\":\n+                    expander_title += f\" ({execution_time_str})\"\n+\n+                with st.expander(\n+                    expander_title,\n+                    expanded=False,\n+                ):\n+                    # Show query/code/command with syntax highlighting\n+                    if isinstance(tool_args, dict):\n+                        if \"query\" in tool_args:\n+                            st.code(tool_args[\"query\"], language=\"sql\")\n+                        elif \"code\" in tool_args:\n+                            st.code(tool_args[\"code\"], language=\"python\")\n+                        elif \"command\" in tool_args:\n+                            st.code(tool_args[\"command\"], language=\"bash\")\n+\n+                    # Display arguments if they exist and are not just the code/query shown above\n+                    args_to_show = {\n+                        k: v\n+                        for k, v in tool_args.items()\n+                        if k not in [\"query\", \"code\", \"command\"]\n+                    }\n+                    if args_to_show:\n+                        st.markdown(\"**Arguments:**\")\n+                        try:\n+                            st.json(args_to_show)\n+                        except Exception:\n+                            st.write(args_to_show)  # Fallback for non-serializable args\n+\n+                    if content is not None:\n+                        try:\n+                            st.markdown(\"**Results:**\")\n+                            if isinstance(content, str) and is_json(content):\n+                                st.json(content)\n+                            else:\n+                                st.write(content)\n+                        except Exception as e:\n+                            logger.debug(f\"Could not display tool content: {e}\")\n+                            st.error(\"Could not display tool content.\")\n+    except Exception as e:\n+        logger.error(f\"Error displaying tool calls: {str(e)}\")\n+        tool_calls_container.error(\"Failed to display tool results\")\n+\n+\n+async def knowledge_widget(uagi: Team) -> None:\n+    \"\"\"Display a knowledge widget in the sidebar.\"\"\"\n+\n+    if uagi is not None and uagi.knowledge is not None:\n+        # Add websites to knowledge base\n+        if \"url_scrape_key\" not in st.session_state:\n+            st.session_state[\"url_scrape_key\"] = 0\n+        input_url = st.sidebar.text_input(\n+            \"Add URL to Knowledge Base\",\n+            type=\"default\",\n+            key=st.session_state[\"url_scrape_key\"],\n+        )\n+        add_url_button = st.sidebar.button(\"Add URL\")\n+        if add_url_button:\n+            if input_url is not None:\n+                alert = st.sidebar.info(\"Processing URLs...\", icon=\"\u2139\ufe0f\")\n+                if f\"{input_url}_scraped\" not in st.session_state:\n+                    scraper = WebsiteReader(max_links=2, max_depth=1)\n+                    web_documents: List[Document] = scraper.read(input_url)\n+                    if web_documents:\n+                        uagi.knowledge.load_documents(web_documents, upsert=True)\n+                    else:\n+                        st.sidebar.error(\"Could not read website\")\n+                    st.session_state[f\"{input_url}_uploaded\"] = True\n+                alert.empty()\n+\n+        # Add documents to knowledge base\n+        if \"file_uploader_key\" not in st.session_state:\n+            st.session_state[\"file_uploader_key\"] = 100\n+        uploaded_file = st.sidebar.file_uploader(\n+            \"Add a Document (.pdf, .csv, .txt, or .docx)\",\n+            key=st.session_state[\"file_uploader_key\"],\n+        )\n+        if uploaded_file is not None:\n+            alert = st.sidebar.info(\"Processing document...\", icon=\"\ud83e\udde0\")\n+            document_name = uploaded_file.name.split(\".\")[0]\n+            if f\"{document_name}_uploaded\" not in st.session_state:\n+                file_type = uploaded_file.name.split(\".\")[-1].lower()\n+\n+                reader: Reader\n+                if file_type == \"pdf\":\n+                    reader = PDFReader()\n+                elif file_type == \"csv\":\n+                    reader = CSVReader()\n+                elif file_type == \"txt\":\n+                    reader = TextReader()\n+                elif file_type == \"docx\":\n+                    reader = DocxReader()\n+                else:\n+                    st.sidebar.error(\"Unsupported file type\")\n+                    return\n+                uploaded_file_documents: List[Document] = reader.read(uploaded_file)\n+                if uploaded_file_documents:\n+                    uagi.knowledge.load_documents(uploaded_file_documents, upsert=True)\n+                else:\n+                    st.sidebar.error(\"Could not read document\")\n+                st.session_state[f\"{document_name}_uploaded\"] = True\n+            alert.empty()\n+\n+        # Load and delete knowledge\n+        if st.sidebar.button(\"\ud83d\uddd1\ufe0f Delete Knowledge\"):\n+            uagi.knowledge.delete()\n+            st.sidebar.success(\"Knowledge deleted!\")\n+\n+\n+async def session_selector(uagi: Team, uagi_config: UAgIConfig) -> None:\n+    \"\"\"Display a session selector in the sidebar, if a new session is selected, UAgI is restarted with the new session.\"\"\"\n+\n+    if not uagi.storage:\n+        return\n+\n+    try:\n+        # Get all agent sessions.\n+        uagi_sessions = uagi.storage.get_all_sessions()\n+        if not uagi_sessions:\n+            st.sidebar.info(\"No saved sessions found.\")\n+            return\n+\n+        # Get session names if available, otherwise use IDs.\n+        sessions_list = []\n+        for session in uagi_sessions:\n+            session_id = session.session_id\n+            session_name = (\n+                session.session_data.get(\"session_name\", None)\n+                if session.session_data\n+                else None\n+            )\n+            display_name = session_name if session_name else session_id\n+            sessions_list.append({\"id\": session_id, \"display_name\": display_name})\n+\n+        # Display session selector.\n+        st.sidebar.markdown(\"#### \ud83d\udcac Session\")\n+        selected_session = st.sidebar.selectbox(\n+            \"Session\",\n+            options=[s[\"display_name\"] for s in sessions_list],\n+            key=\"session_selector\",\n+            label_visibility=\"collapsed\",\n+        )\n+        # Find the selected session ID.\n+        selected_session_id = next(\n+            s[\"id\"] for s in sessions_list if s[\"display_name\"] == selected_session\n+        )\n+        # Update the agent session if it has changed.\n+        if st.session_state[\"session_id\"] != selected_session_id:\n+            logger.info(f\"---*--- Loading UAgI session: {selected_session_id} ---*---\")\n+            st.session_state[\"uagi\"] = create_uagi(\n+                config=uagi_config,\n+                session_id=selected_session_id,\n+            )\n+            st.rerun()\n+\n+        # Show the rename session widget.\n+        container = st.sidebar.container()\n+        session_row = container.columns([3, 1], vertical_alignment=\"center\")\n+\n+        # Initialize session_edit_mode if needed.\n+        if \"session_edit_mode\" not in st.session_state:\n+            st.session_state.session_edit_mode = False\n+\n+        # Show the session name.\n+        with session_row[0]:\n+            if st.session_state.session_edit_mode:\n+                new_session_name = st.text_input(\n+                    \"Session Name\",\n+                    value=uagi.session_name,\n+                    key=\"session_name_input\",\n+                    label_visibility=\"collapsed\",\n+                )\n+            else:\n+                st.markdown(f\"Session Name: **{uagi.session_name}**\")\n+\n+        # Show the rename session button.\n+        with session_row[1]:\n+            if st.session_state.session_edit_mode:\n+                if st.button(\"\u2713\", key=\"save_session_name\", type=\"primary\"):\n+                    if new_session_name:\n+                        uagi.rename_session(new_session_name)\n+                        st.session_state.session_edit_mode = False\n+                        container.success(\"Renamed!\")\n+                        # Trigger a rerun to refresh the sessions list\n+                        st.rerun()\n+            else:\n+                if st.button(\"\u270e\", key=\"edit_session_name\"):\n+                    st.session_state.session_edit_mode = True\n+    except Exception as e:\n+        logger.error(f\"Error in session selector: {str(e)}\")\n+        st.sidebar.error(\"Failed to load sessions\")\n+\n+\n+def export_chat_history():\n+    \"\"\"Export chat history in markdown format.\n+\n+    Returns:\n+        str: Formatted markdown string of the chat history\n+    \"\"\"\n+    if \"messages\" not in st.session_state or not st.session_state[\"messages\"]:\n+        return f\"# UAgI - Chat History\\n\\nNo messages to export.\"\n+\n+    chat_text = f\"# UAgI - Chat History\\n\\n\"\n+    for msg in st.session_state[\"messages\"]:\n+        role_label = \"\ud83e\udd16 Assistant\" if msg[\"role\"] == \"assistant\" else \"\ud83d\udc64 User\"\n+        chat_text += f\"### {role_label}\\n{msg['content']}\\n\\n\"\n+\n+        # Include tool calls if present\n+        if msg.get(\"tool_calls\"):\n+            chat_text += \"#### Tool Calls:\\n\"\n+            for i, tool_call in enumerate(msg[\"tool_calls\"]):\n+                tool_name = tool_call.get(\"name\", \"Unknown Tool\")\n+                chat_text += f\"**{i + 1}. {tool_name}**\\n\\n\"\n+                if \"arguments\" in tool_call:\n+                    chat_text += (\n+                        f\"Arguments: ```json\\n{tool_call['arguments']}\\n```\\n\\n\"\n+                    )\n+                if \"content\" in tool_call:\n+                    chat_text += f\"Results: ```\\n{tool_call['content']}\\n```\\n\\n\"\n+\n+    return chat_text\n+\n+\n+async def utilities_widget(uagi: Team) -> None:\n+    \"\"\"Display a utilities widget in the sidebar.\"\"\"\n+    st.sidebar.markdown(\"#### \ud83d\udee0\ufe0f Utilities\")\n+    col1, col2 = st.sidebar.columns(2)\n+    with col1:\n+        if st.button(\"\ud83d\udd04 Start New Chat\"):\n+            restart_uagi()\n+    with col2:\n+        fn = f\"uagi_chat_history.md\"\n+        if \"session_id\" in st.session_state:\n+            fn = f\"uagi_{st.session_state['session_id']}.md\"\n+        if st.download_button(\n+            \":file_folder: Export Chat History\",\n+            export_chat_history(),\n+            file_name=fn,\n+            mime=\"text/markdown\",\n+        ):\n+            st.sidebar.success(\"Chat history exported!\")\n+\n+\n+def restart_uagi():\n+    logger.debug(\"---*--- Restarting UAgI ---*---\")\n+    st.session_state[\"uagi\"] = None\n+    st.session_state[\"session_id\"] = None\n+    st.session_state[\"messages\"] = []\n+    if \"url_scrape_key\" in st.session_state:\n+        st.session_state[\"url_scrape_key\"] += 1\n+    if \"file_uploader_key\" in st.session_state:\n+        st.session_state[\"file_uploader_key\"] += 1\n+    st.rerun()\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/vision_ai/__init__.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/vision_ai/__init__.py b/cookbook/examples/streamlit_applications/vision_ai/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/vision_ai/agents.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/vision_ai/agents.py b/cookbook/examples/streamlit_applications/vision_ai/agents.py\nnew file mode 100644\nindex 000000000..040287f66\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/vision_ai/agents.py\n@@ -0,0 +1,33 @@\n+from agno.agent import Agent\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+\n+\n+def image_processing_agent(\n+    model,\n+) -> Agent:\n+    extraction_agent = Agent(\n+        name=\"image_analysis_agent\",\n+        model=model,\n+        markdown=True,\n+    )\n+\n+    return extraction_agent\n+\n+\n+def chat_followup_agent(\n+    model,\n+    enable_search: bool = False,\n+) -> Agent:\n+    tools = [DuckDuckGoTools()] if enable_search else []\n+    followup_agent = Agent(\n+        name=\"image_chat_followup_agent\",\n+        model=model,\n+        tools=tools,\n+        read_chat_history=True,\n+        add_history_to_messages=True,\n+        num_history_responses=5,\n+        markdown=True,\n+        add_datetime_to_instructions=True,\n+    )\n+\n+    return followup_agent\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/vision_ai/app.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/vision_ai/app.py b/cookbook/examples/streamlit_applications/vision_ai/app.py\nnew file mode 100644\nindex 000000000..14d509b81\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/vision_ai/app.py\n@@ -0,0 +1,321 @@\n+import os\n+import time\n+from pathlib import Path\n+\n+import streamlit as st\n+from agents import chat_followup_agent, image_processing_agent\n+from agno.media import Image\n+from agno.models.google import Gemini\n+from agno.models.mistral.mistral import MistralChat\n+from agno.models.openai import OpenAIChat\n+from agno.utils.log import logger\n+from dotenv import load_dotenv\n+from prompt import extraction_prompt\n+from utils import about_widget, add_message, clear_chat\n+\n+load_dotenv()\n+\n+OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n+GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n+MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n+\n+# Streamlit App Configuration\n+st.set_page_config(\n+    page_title=\"VisionAI Chat\",\n+    page_icon=\"\ud83d\udcf7\",\n+    layout=\"wide\",\n+)\n+\n+\n+def main():\n+    ####################################################################\n+    # App Header\n+    ####################################################################\n+    st.markdown(\n+        \"\"\"\n+        <style>\n+            .title {\n+                text-align: center;\n+                font-size: 3em;\n+                font-weight: bold;\n+                color: white;\n+            }\n+            .subtitle {\n+                text-align: center;\n+                font-size: 1.5em;\n+                color: #bbb;\n+                margin-top: -15px;\n+            }\n+        </style>\n+        <h1 class='title'>VisionAI \ud83d\uddbc\ufe0f</h1>\n+        <p class='subtitle'>Your AI-powered smart image analysis agent</p>\n+        \"\"\",\n+        unsafe_allow_html=True,\n+    )\n+\n+    ####################################################################\n+    # Ensure session state variables are initialized\n+    ####################################################################\n+    if \"last_extracted_image\" not in st.session_state:\n+        st.session_state[\"last_extracted_image\"] = None\n+    if \"last_image_response\" not in st.session_state:\n+        st.session_state[\"last_image_response\"] = None\n+    if \"messages\" not in st.session_state:\n+        st.session_state[\"messages\"] = []\n+    if \"extract_triggered\" not in st.session_state:\n+        st.session_state[\"extract_triggered\"] = False\n+\n+    ####################################################################\n+    # Sidebar Configuration\n+    ####################################################################\n+    with st.sidebar:\n+        st.markdown(\"#### \ud83d\uddbc\ufe0f Smart Image Analysis Agent\")\n+\n+        # Model Selection\n+        model_choice = st.selectbox(\n+            \"\ud83d\udd0d Select Model Provider\", [\"OpenAI\", \"Gemini\", \"Mistral\"], index=0\n+        )\n+\n+        # Mode Selection\n+        mode = st.radio(\n+            \"\u2699\ufe0f Extraction Mode\",\n+            [\"Auto\", \"Manual\", \"Hybrid\"],\n+            index=0,\n+            help=\"Select how the image analysis should be performed:\\n\"\n+            \"- **Auto**: Extracts the image automatically without any extra information from users.\\n\"\n+            \"- **Manual**: User provide specific instructions for image extraction.\\n\"\n+            \"- **Hybrid**: Combined auto-processing mode with user-defined instructions.\",\n+        )\n+\n+        # Web Search Option (Enable/Disable DuckDuckGo)\n+        enable_search_option = st.radio(\"\ud83c\udf10 Enable Web Search?\", [\"Yes\", \"No\"], index=1)\n+        enable_search = True if enable_search_option == \"Yes\" else False\n+\n+    ####################################################################\n+    # Ensure Model is Initialized Properly\n+    ####################################################################\n+    if (\n+        \"model_instance\" not in st.session_state\n+        or st.session_state.get(\"model_choice\", None) != model_choice\n+    ):\n+        if model_choice == \"OpenAI\":\n+            if not OPENAI_API_KEY:\n+                st.error(\n+                    \"\u26a0\ufe0f OpenAI API key not found. Please set the OPENAI_API_KEY environment variable.\"\n+                )\n+            model = OpenAIChat(id=\"gpt-4o\", api_key=OPENAI_API_KEY)\n+        elif model_choice == \"Gemini\":\n+            if not GOOGLE_API_KEY:\n+                st.error(\n+                    \"\u26a0\ufe0f Google API key not found. Please set the GOOGLE_API_KEY environment variable.\"\n+                )\n+            model = Gemini(id=\"gemini-2.0-flash\", api_key=GOOGLE_API_KEY)\n+        elif model_choice == \"Mistral\":\n+            if not MISTRAL_API_KEY:\n+                st.error(\n+                    \"\u26a0\ufe0f Mistral API key not found. Please set the MISTRAL_API_KEY environment variable.\"\n+                )\n+            model = MistralChat(id=\"pixtral-12b-2409\", api_key=MISTRAL_API_KEY)\n+        else:\n+            st.error(\n+                \"\u26a0\ufe0f Unsupported model provider. Please select OpenAI, Gemini, or Mistral.\"\n+            )\n+            st.stop()  # Stop execution if model is not supported\n+\n+        st.session_state[\"model_instance\"] = model\n+    else:\n+        model = st.session_state[\"model_instance\"]\n+\n+    ####################################################################\n+    # Modify Agents Without Creating New Session\n+    ####################################################################\n+    if (\n+        \"image_agent\" not in st.session_state\n+        or \"chat_agent\" not in st.session_state\n+        or st.session_state.get(\"model_choice\", None) != model_choice\n+        or st.session_state.get(\"enable_search\", None) != enable_search\n+    ):\n+        logger.info(\n+            f\"Updating Agents with model {model.id} and search enabled {enable_search}\"\n+        )\n+        image_agent = image_processing_agent(model=model)\n+        st.session_state[\"image_agent\"] = image_agent\n+        chat_agent = chat_followup_agent(model=model, enable_search=enable_search)\n+        st.session_state[\"chat_agent\"] = chat_agent\n+        st.session_state[\"enable_search\"] = enable_search\n+\n+        ####################################################################\n+        # Store new selections in session_state\n+        ####################################################################\n+        st.session_state[\"model_choice\"] = model_choice\n+        st.session_state[\"enable_search\"] = enable_search\n+\n+    else:\n+        image_agent = st.session_state[\"image_agent\"]\n+        chat_agent = st.session_state[\"chat_agent\"]\n+\n+    ####################################################################\n+    # Load Runs from Memory (Chat History)\n+    ####################################################################\n+    if \"messages\" not in st.session_state:\n+        st.session_state[\"messages\"] = []\n+\n+    ####################################################################\n+    # Image Upload Section\n+    ####################################################################\n+    uploaded_file = st.file_uploader(\n+        \"\ud83d\udce4 Upload an Image (Max: 20MB) \ud83d\udcf7\", type=[\"png\", \"jpg\", \"jpeg\"]\n+    )\n+    image_path = None\n+\n+    if uploaded_file:\n+        temp_dir = Path(\"tmp/\")\n+        temp_dir.mkdir(exist_ok=True)\n+        image_path = temp_dir / uploaded_file.name\n+\n+        # Check if this is a new image different from the last one\n+        if (\n+            \"last_extracted_image\" in st.session_state\n+            and st.session_state[\"last_extracted_image\"] is not None\n+            and str(st.session_state[\"last_extracted_image\"]) != str(image_path)\n+        ):\n+            logger.info(\n+                f\"New image detected. Resetting chat history and reinitializing agents.\"\n+            )\n+            clear_chat()\n+\n+        with open(image_path, \"wb\") as f:\n+            f.write(uploaded_file.getbuffer())\n+\n+            # Display image preview in sidebar if an image is uploaded\n+            st.sidebar.markdown(\"#### \ud83d\uddbc\ufe0f Current Image\")\n+            st.sidebar.image(uploaded_file, use_container_width=True)\n+\n+        logger.info(f\"\u2705 Image successfully saved at: {image_path}\")\n+\n+        # Show instruction input only for Manual & Hybrid Mode\n+        if mode in [\"Manual\", \"Hybrid\"]:\n+            instruction = st.text_area(\n+                \"\ud83d\udcdd Enter Extraction Instructions\",\n+                placeholder=\"Extract number plates...\",\n+            )\n+        else:\n+            instruction = None\n+\n+        # ADD Extract Data Button\n+        if st.button(\"\ud83d\udd0d Extract Data\"):\n+            if (\n+                image_path\n+                and (mode == \"Auto\" or instruction)\n+                and (\n+                    \"last_image_response\" not in st.session_state\n+                    or st.session_state[\"last_extracted_image\"] != image_path\n+                )\n+            ):\n+                with st.spinner(\"\ud83d\udce4 Processing Image! Extracting image data...\"):\n+                    extracted_data = image_agent.run(\n+                        extraction_prompt,\n+                        images=[Image(filepath=image_path)],\n+                        instructions=instruction if instruction else None,\n+                    )\n+\n+                # Store last extracted response for chat follow-ups\n+                st.session_state[\"last_image_response\"] = extracted_data.content\n+                st.session_state[\"last_extracted_image\"] = image_path\n+\n+                # Create a temporary success message container\n+                success_message = st.empty()\n+                success_message.success(\"\u2705 Image processing completed successfully!\")\n+\n+                logger.info(f\"Extracted Data Response: {extracted_data.content}\")\n+\n+                # Wait for 1 seconds, then clear the success message\n+                time.sleep(1)\n+                success_message.empty()\n+\n+        # Display Extracted Image Data Persistently\n+        if st.session_state[\"last_image_response\"]:\n+            st.write(\"### Extracted Image Insights:\")\n+            st.write(st.session_state[\"last_image_response\"])\n+\n+    ####################################################################\n+    # Follow-up Chat Section\n+    ####################################################################\n+    st.markdown(\"---\")\n+    st.markdown(\"### \ud83d\udcac Chat with VisionAI\")\n+\n+    ####################################################################\n+    # Display Chat History First\n+    ####################################################################\n+    for message in st.session_state[\"messages\"]:\n+        if message[\"role\"] == \"system\":\n+            continue\n+        with st.chat_message(message[\"role\"]):\n+            st.write(message[\"content\"])\n+\n+    if prompt := st.chat_input(\n+        \"\ud83d\udcac Ask follow-up questions on the image extracted data...\"\n+    ):\n+        # Display user message first\n+        with st.chat_message(\"user\"):\n+            st.write(prompt)\n+        # Add user message to session state\n+        add_message(\"user\", prompt)\n+\n+        ####################################################################\n+        # Process User Queries & Stream Responses\n+        ####################################################################\n+        last_message = (\n+            st.session_state[\"messages\"][-1] if st.session_state[\"messages\"] else None\n+        )\n+\n+        if last_message and last_message[\"role\"] == \"user\":\n+            user_question = last_message[\"content\"]\n+\n+            # Ensure Image Agent has extracted data before running chat agent\n+            if (\n+                \"last_image_response\" not in st.session_state\n+                or not st.session_state[\"last_image_response\"]\n+            ):\n+                st.warning(\n+                    \"\u26a0\ufe0f No extracted insights available. Please process an image first.\"\n+                )\n+            else:\n+                with st.chat_message(\"assistant\"):\n+                    response_container = st.empty()\n+                    with st.spinner(\"\ud83e\udd14 Processing follow-up question...\"):\n+                        try:\n+                            chat_response = chat_agent.run(\n+                                f\"\"\"You are a chat agent who answers followup questions based on extracted image data.\n+    Understand the requirement properly and then answer the question correctly.\n+\n+    Extracted Image Data: {st.session_state[\"last_image_response\"]}\n+\n+    Use the above image insights to answer the following question.\n+    Answer the following question from the above given extracted image data: {user_question}\"\"\",\n+                                stream=True,\n+                            )\n+\n+                            response_text = \"\"\n+                            for chunk in chat_response:\n+                                if chunk and chunk.content:\n+                                    response_text += chunk.content\n+                                    response_container.markdown(response_text)\n+\n+                            add_message(\"assistant\", response_text)\n+\n+                        except Exception as e:\n+                            error_message = f\"\u274c Error: {str(e)}\"\n+                            add_message(\"assistant\", error_message)\n+                            st.error(error_message)\n+\n+    # Add clear chat button in sidebar\n+    if st.sidebar.button(\"\ud83e\uddf9 Clear Chat History\", key=\"clear_chat\"):\n+        clear_chat()\n+\n+    # About Section\n+    about_widget()\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/vision_ai/generate_requirements.sh",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/vision_ai/generate_requirements.sh b/cookbook/examples/streamlit_applications/vision_ai/generate_requirements.sh\nnew file mode 100755\nindex 000000000..78f8c7cec\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/vision_ai/generate_requirements.sh\n@@ -0,0 +1,12 @@\n+#!/bin/bash\n+\n+############################################################################\n+# Generate requirements.txt from requirements.in\n+############################################################################\n+\n+echo \"Generating requirements.txt\"\n+\n+CURR_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n+\n+UV_CUSTOM_COMPILE_COMMAND=\"./generate_requirements.sh\" \\\n+  uv pip compile ${CURR_DIR}/requirements.in --no-cache --upgrade -o ${CURR_DIR}/requirements.txt\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/vision_ai/prompt.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/vision_ai/prompt.py b/cookbook/examples/streamlit_applications/vision_ai/prompt.py\nnew file mode 100644\nindex 000000000..2a2bebf11\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/vision_ai/prompt.py\n@@ -0,0 +1,143 @@\n+extraction_prompt = \"\"\"\n+\n+    ### Task: Extract Maximum Information from the Image\n+    You are an advanced AI agent specialized in analyzing and extracting structured data from images.\n+\n+    #### \ud83d\udd0d **General Extraction Process:**\n+    1. **Identify the Image Type:** Determine if it's a document, a chart, a traffic scene, a shopfront, etc.\n+    2. **Extract All Relevant Elements:**\n+        - For documents: Extract **printed text, handwriting, tables, signatures**.\n+        - For traffic scenes: Detect **cars, people, number plates, road signs**.\n+        - For charts: Identify **chart type, X & Y labels, legend, data points**.\n+        - For places: Recognize **business names, advertisements, location details**.\n+    3. **Provide Contextual Insights:** Explain what the image represents.\n+    4. **Output in a Structured JSON Format.**\n+\n+    #### **\ud83d\udccc Important Guidelines**\n+    - Do **not** just list objects, extract **detailed insights**.\n+    - If text is present, perform OCR and extract structured content.\n+    - Extract colors, numbers, categories where applicable.\n+    - If the image shows a **famous place**, provide historical or contextual details.\n+\n+    ---\n+\n+    ## **Example 1: Traffic Scene with Multiple Elements**\n+    **Input:** Image of a traffic junction with vehicles, road signs, and people.\n+\n+    **Output:**\n+    ```json\n+    {\n+        \"scene_description\": \"A busy traffic junction with cars waiting at a red signal. There are pedestrians crossing, and road signs providing directions.\",\n+        \"vehicles\": {\n+            \"count\": 5,\n+            \"details\": [\n+                {\"type\": \"Car\", \"color\": \"Red\", \"number_plate\": \"AB1234\"},\n+                {\"type\": \"Car\", \"color\": \"Blue\", \"number_plate\": \"XY5678\"},\n+                {\"type\": \"Bus\", \"color\": \"Yellow\", \"number_plate\": \"TR7890\"},\n+                {\"type\": \"Bike\", \"color\": \"Black\"},\n+                {\"type\": \"Truck\", \"color\": \"White\", \"number_plate\": \"LM4567\"}\n+            ]\n+        },\n+        \"road_signs\": [\n+            {\"text\": \"STOP\", \"type\": \"Regulatory\", \"position\": \"Left side of the road\"},\n+            {\"text\": \"Speed Limit 60 km/h\", \"type\": \"Warning\", \"position\": \"Above traffic lights\"}\n+        ],\n+        \"pedestrians\": {\n+            \"count\": 3,\n+            \"activity\": \"Crossing the road at a zebra crossing\"\n+        },\n+        \"analysis\": \"Busy urban intersection during business hours with mixed vehicle and pedestrian traffic. The presence of multiple commercial establishments and professional vehicles suggests this is a central business district.\",\n+        \"significance\": \"This appears to be a major commercial hub given the intersection of Main St and Commerce Ave, with diverse business activity evident from signage and foot traffic.\"\n+    }\n+    ```\n+\n+    ---\n+\n+    ## **Example 2: Document Image with Text & Tables**\n+    **Input:** A scanned invoice containing text, tables, and a signature.\n+\n+    **Output:**\n+    ```json\n+    {\n+        \"document_type\": \"Invoice\",\n+        \"header\": {\n+            \"company_name\": \"ABC Corp.\",\n+            \"invoice_number\": \"INV-2024021\",\n+            \"date\": \"2024-02-12\"\n+        },\n+        \"items\": [\n+            {\"item\": \"Laptop\", \"quantity\": 1, \"price\": \"$1200\"},\n+            {\"item\": \"Mouse\", \"quantity\": 2, \"price\": \"$40\"}\n+        ],\n+        \"total_amount\": \"$1280\",\n+        \"signature_detected\": true,\n+        \"notes\": \"Paid via Credit Card\"\n+    }\n+    ```\n+\n+    ## **Example 3: Document/Chart Analysis**\n+    **Input:** An visualization chart of business report.\n+\n+    **Output:**\n+    ```json\n+    {\n+        \"extracted_data\": {\n+            \"document_type\": \"Business report with charts\",\n+            \"content_elements\": {\n+                \"charts\": [\n+                    {\n+                        \"type\": \"Bar graph\",\n+                        \"title\": \"Annual Revenue 2020-2023\",\n+                        \"axes\": {\n+                            \"x\": \"Years\",\n+                            \"y\": \"Revenue ($ millions)\"\n+                        },\n+                        \"data_points\": [\n+                            {\"year\": \"2020\", \"value\": 1.2},\n+                            {\"year\": \"2021\", \"value\": 1.5},\n+                            {\"year\": \"2022\", \"value\": 1.8},\n+                            {\"year\": \"2023\", \"value\": 2.1}\n+                        ]\n+                    }\n+                ],\n+                \"text_blocks\": [\n+                    {\n+                        \"type\": \"Heading\",\n+                        \"content\": \"Q4 Financial Summary\",\n+                        \"location\": \"Top of page\"\n+                    },\n+                    {\n+                        \"type\": \"Paragraph\",\n+                        \"content\": \"The fourth quarter showed strong growth...\",\n+                        \"location\": \"Below heading\"\n+                    }\n+                ],\n+                \"tables\": [\n+                    {\n+                        \"title\": \"Regional Performance\",\n+                        \"columns\": [\"Region\", \"Revenue\", \"Growth\"],\n+                        \"rows\": [\n+                            [\"North\", \"$500K\", \"+15%\"],\n+                            [\"South\", \"$600K\", \"+12%\"]\n+                        ]\n+                    }\n+                ]\n+            },\n+            \"analysis\": \"Comprehensive financial report showing positive growth trends across multiple regions. The document combines textual analysis with supporting visual data through charts and tables.\",\n+            \"significance\": \"This report indicates a consistent upward trend in company performance over the past four years, with particularly strong regional growth in the North and South territories.\"\n+        }\n+    }\n+\n+    ---\n+\n+    ### **Final Instructions**\n+    - Always return JSON output.\n+    - If text is present, extract and categorize it.\n+    - If objects are found, count and describe them.\n+    - If a known landmark is detected, provide extra context.\n+    - Provide comprehensive details but maintain appropriate privacy (no personal identification)\n+    - Organize information hierarchically from general to specific\n+    - Always include analysis and significance when relevant\n+\n+    **Process the image and return structured data now.**\n+    \"\"\"\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/vision_ai/requirements.in",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/vision_ai/requirements.in b/cookbook/examples/streamlit_applications/vision_ai/requirements.in\nnew file mode 100644\nindex 000000000..423d57c13\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/vision_ai/requirements.in\n@@ -0,0 +1,11 @@\n+agno\n+openai\n+google-genai\n+mistralai\n+pgvector\n+psycopg[binary]\n+simplejson\n+sqlalchemy\n+streamlit\n+duckduckgo-search\n+nest_asyncio\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/vision_ai/requirements.txt",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/vision_ai/requirements.txt b/cookbook/examples/streamlit_applications/vision_ai/requirements.txt\nnew file mode 100644\nindex 000000000..98ea4f72a\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/vision_ai/requirements.txt\n@@ -0,0 +1,228 @@\n+# This file was autogenerated by uv via the following command:\n+#    ./generate_requirements.sh\n+agno==1.1.9\n+    # via -r requirements.in\n+altair==5.5.0\n+    # via streamlit\n+annotated-types==0.7.0\n+    # via pydantic\n+anyio==4.8.0\n+    # via\n+    #   google-genai\n+    #   httpx\n+    #   openai\n+attrs==25.1.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+blinker==1.9.0\n+    # via streamlit\n+cachetools==5.5.2\n+    # via\n+    #   google-auth\n+    #   streamlit\n+certifi==2025.1.31\n+    # via\n+    #   httpcore\n+    #   httpx\n+    #   requests\n+charset-normalizer==3.4.1\n+    # via requests\n+click==8.1.8\n+    # via\n+    #   duckduckgo-search\n+    #   streamlit\n+    #   typer\n+distro==1.9.0\n+    # via openai\n+docstring-parser==0.16\n+    # via agno\n+duckduckgo-search==7.5.1\n+    # via -r requirements.in\n+eval-type-backport==0.2.2\n+    # via mistralai\n+gitdb==4.0.12\n+    # via gitpython\n+gitpython==3.1.44\n+    # via\n+    #   agno\n+    #   streamlit\n+google-auth==2.38.0\n+    # via google-genai\n+google-genai==1.5.0\n+    # via -r requirements.in\n+h11==0.14.0\n+    # via httpcore\n+httpcore==1.0.7\n+    # via httpx\n+httpx==0.28.1\n+    # via\n+    #   agno\n+    #   google-genai\n+    #   mistralai\n+    #   openai\n+idna==3.10\n+    # via\n+    #   anyio\n+    #   httpx\n+    #   requests\n+jinja2==3.1.6\n+    # via\n+    #   altair\n+    #   pydeck\n+jiter==0.9.0\n+    # via openai\n+jsonpath-python==1.0.6\n+    # via mistralai\n+jsonschema==4.23.0\n+    # via altair\n+jsonschema-specifications==2024.10.1\n+    # via jsonschema\n+lxml==5.3.1\n+    # via duckduckgo-search\n+markdown-it-py==3.0.0\n+    # via rich\n+markupsafe==3.0.2\n+    # via jinja2\n+mdurl==0.1.2\n+    # via markdown-it-py\n+mistralai==1.5.1\n+    # via -r requirements.in\n+mypy-extensions==1.0.0\n+    # via typing-inspect\n+narwhals==1.30.0\n+    # via altair\n+nest-asyncio==1.6.0\n+    # via -r requirements.in\n+numpy==2.2.3\n+    # via\n+    #   pandas\n+    #   pgvector\n+    #   pydeck\n+    #   streamlit\n+openai==1.65.5\n+    # via -r requirements.in\n+packaging==24.2\n+    # via\n+    #   altair\n+    #   streamlit\n+pandas==2.2.3\n+    # via streamlit\n+pgvector==0.3.6\n+    # via -r requirements.in\n+pillow==11.1.0\n+    # via streamlit\n+primp==0.14.0\n+    # via duckduckgo-search\n+protobuf==5.29.3\n+    # via streamlit\n+psycopg==3.2.5\n+    # via -r requirements.in\n+psycopg-binary==3.2.5\n+    # via psycopg\n+pyarrow==19.0.1\n+    # via streamlit\n+pyasn1==0.6.1\n+    # via\n+    #   pyasn1-modules\n+    #   rsa\n+pyasn1-modules==0.4.1\n+    # via google-auth\n+pydantic==2.10.6\n+    # via\n+    #   agno\n+    #   google-genai\n+    #   mistralai\n+    #   openai\n+    #   pydantic-settings\n+pydantic-core==2.27.2\n+    # via pydantic\n+pydantic-settings==2.8.1\n+    # via agno\n+pydeck==0.9.1\n+    # via streamlit\n+pygments==2.19.1\n+    # via rich\n+python-dateutil==2.9.0.post0\n+    # via\n+    #   mistralai\n+    #   pandas\n+python-dotenv==1.0.1\n+    # via\n+    #   agno\n+    #   pydantic-settings\n+python-multipart==0.0.20\n+    # via agno\n+pytz==2025.1\n+    # via pandas\n+pyyaml==6.0.2\n+    # via agno\n+referencing==0.36.2\n+    # via\n+    #   jsonschema\n+    #   jsonschema-specifications\n+requests==2.32.3\n+    # via\n+    #   google-genai\n+    #   streamlit\n+rich==13.9.4\n+    # via\n+    #   agno\n+    #   typer\n+rpds-py==0.23.1\n+    # via\n+    #   jsonschema\n+    #   referencing\n+rsa==4.9\n+    # via google-auth\n+shellingham==1.5.4\n+    # via typer\n+simplejson==3.20.1\n+    # via -r requirements.in\n+six==1.17.0\n+    # via python-dateutil\n+smmap==5.0.2\n+    # via gitdb\n+sniffio==1.3.1\n+    # via\n+    #   anyio\n+    #   openai\n+sqlalchemy==2.0.38\n+    # via -r requirements.in\n+streamlit==1.43.1\n+    # via -r requirements.in\n+tenacity==9.0.0\n+    # via streamlit\n+toml==0.10.2\n+    # via streamlit\n+tomli==2.2.1\n+    # via agno\n+tornado==6.4.2\n+    # via streamlit\n+tqdm==4.67.1\n+    # via openai\n+typer==0.15.2\n+    # via agno\n+typing-extensions==4.12.2\n+    # via\n+    #   agno\n+    #   altair\n+    #   anyio\n+    #   google-genai\n+    #   openai\n+    #   psycopg\n+    #   pydantic\n+    #   pydantic-core\n+    #   referencing\n+    #   sqlalchemy\n+    #   streamlit\n+    #   typer\n+    #   typing-inspect\n+typing-inspect==0.9.0\n+    # via mistralai\n+tzdata==2025.1\n+    # via pandas\n+urllib3==2.3.0\n+    # via requests\n+websockets==14.2\n+    # via google-genai\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/examples/streamlit_applications/vision_ai/utils.py",
            "diff": "diff --git a/cookbook/examples/streamlit_applications/vision_ai/utils.py b/cookbook/examples/streamlit_applications/vision_ai/utils.py\nnew file mode 100644\nindex 000000000..a147a9008\n--- /dev/null\n+++ b/cookbook/examples/streamlit_applications/vision_ai/utils.py\n@@ -0,0 +1,155 @@\n+import streamlit as st\n+from agents import image_processing_agent\n+from agno.agent import Agent\n+from agno.models.google import Gemini\n+from agno.models.openai import OpenAIChat\n+from agno.utils.log import logger\n+\n+\n+def add_message(role: str, content: str) -> None:\n+    \"\"\"Safely add a message to the session state.\"\"\"\n+    if \"messages\" not in st.session_state or not isinstance(\n+        st.session_state[\"messages\"], list\n+    ):\n+        st.session_state[\"messages\"] = []\n+    st.session_state[\"messages\"].append({\"role\": role, \"content\": content})\n+\n+\n+def restart_agent():\n+    \"\"\"Reset the agent and clear chat history.\"\"\"\n+    logger.debug(\"---*--- Restarting Image Agent ---*---\")\n+    st.session_state[\"image_agent\"] = None\n+    st.session_state[\"image_agent_session_id\"] = None\n+    st.session_state[\"messages\"] = []\n+    st.rerun()\n+\n+\n+def clear_chat():\n+    \"\"\"Clear chat history and reset relevant session state variables\n+    while preserving model settings\"\"\"\n+\n+    # Clear chat and extraction data\n+    st.session_state[\"messages\"] = []\n+    st.session_state[\"last_image_response\"] = None\n+    st.session_state[\"last_extracted_image\"] = None\n+    st.session_state[\"extract_triggered\"] = False\n+\n+    # Remove agents so they'll be reinitialized\n+    if \"image_agent\" in st.session_state:\n+        del st.session_state[\"image_agent\"]\n+    if \"chat_agent\" in st.session_state:\n+        del st.session_state[\"chat_agent\"]\n+\n+    st.rerun()\n+\n+\n+def export_chat_history():\n+    \"\"\"Export chat history as markdown.\"\"\"\n+    if \"messages\" in st.session_state:\n+        chat_text = \"# VisioAI - Chat History\\n\\n\"\n+        for msg in st.session_state[\"messages\"]:\n+            role = \"\ud83e\udd16 Assistant\" if msg[\"role\"] == \"assistant\" else \"\ud83d\udc64 User\"\n+            chat_text += f\"### {role}\\n{msg['content']}\\n\\n\"\n+        return chat_text\n+    return \"\"\n+\n+\n+def session_selector_widget(agent: Agent) -> None:\n+    \"\"\"Display a session selector in the sidebar and reinitialize the agent if needed.\"\"\"\n+\n+    if agent.storage:\n+        agent_sessions = agent.storage.get_all_sessions()\n+        session_options = [\n+            {\n+                \"id\": session.session_id,\n+                \"display\": session.session_data.get(\"session_name\", session.session_id),\n+            }\n+            for session in agent_sessions\n+        ]\n+\n+        # Ensure we have sessions before showing the selector\n+        if not session_options:\n+            st.sidebar.warning(\"\u26a0\ufe0f No previous sessions found. Starting a new session.\")\n+            return\n+\n+        selected_session = st.sidebar.selectbox(\n+            \"Session\",\n+            options=[s[\"display\"] for s in session_options],\n+            key=\"session_selector\",\n+        )\n+\n+        # Safely find the selected session ID\n+        selected_session_id = next(\n+            (s[\"id\"] for s in session_options if s[\"display\"] == selected_session),\n+            None,  # Default to None if not found\n+        )\n+\n+        if not selected_session_id:\n+            st.sidebar.warning(\n+                \"\u26a0\ufe0f Selected session not found. Please restart or choose another session.\"\n+            )\n+            return\n+\n+        if st.session_state.get(\"image_agent_session_id\") != selected_session_id:\n+            logger.info(f\"---*--- Loading session: {selected_session_id} ---*---\")\n+\n+            # Retrieve Model Choice & API Key\n+            model_choice = st.session_state.get(\"model_choice\")\n+            api_key = st.session_state.get(\"api_key\")\n+\n+            if model_choice == \"OpenAI\":\n+                model = OpenAIChat(id=\"gpt-4o\", api_key=api_key)\n+            else:\n+                model = Gemini(id=\"gemini-2.0-flash\", api_key=api_key)\n+\n+            # Reload the agent with the selected session\n+            st.session_state[\"image_agent\"] = image_processing_agent(model=model)\n+            st.session_state[\"image_agent\"].load_session(selected_session_id)\n+            st.session_state[\"image_agent_session_id\"] = selected_session_id\n+            st.rerun()\n+\n+\n+def rename_session_widget(agent: Agent) -> None:\n+    \"\"\"Rename the current session of the agent and save to storage.\"\"\"\n+\n+    container = st.sidebar.container()\n+    session_row = container.columns([3, 1], vertical_alignment=\"center\")\n+\n+    if \"session_edit_mode\" not in st.session_state:\n+        st.session_state.session_edit_mode = False\n+\n+    with session_row[0]:\n+        if st.session_state.session_edit_mode:\n+            new_session_name = st.text_input(\n+                \"Session Name\",\n+                value=agent.session_name,\n+                key=\"session_name_input\",\n+                label_visibility=\"collapsed\",\n+            )\n+        else:\n+            st.markdown(f\"Session Name: **{agent.session_name}**\")\n+\n+    with session_row[1]:\n+        if st.session_state.session_edit_mode:\n+            if st.button(\"\u2713\", key=\"save_session_name\", type=\"primary\"):\n+                if new_session_name:\n+                    agent.rename_session(new_session_name)\n+                    st.session_state.session_edit_mode = False\n+                    container.success(\"Renamed!\")\n+        else:\n+            if st.button(\"\u270e\", key=\"edit_session_name\"):\n+                st.session_state.session_edit_mode = True\n+\n+\n+def about_widget() -> None:\n+    \"\"\"Display an about section in the sidebar.\"\"\"\n+    st.sidebar.markdown(\"---\")\n+    st.sidebar.markdown(\"### \u2139\ufe0f About\")\n+    st.sidebar.markdown(\"\"\"\n+    VisioAI helps you analyze images and extract insights using AI-powered object detection,\n+    OCR, and scene recognition.\n+\n+    Built with:\n+    - \ud83d\ude80 Agno\n+    - \ud83d\udcab Streamlit\n+    \"\"\")\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "cookbook/getting_started/19_human_in_the_loop.py",
            "diff": "diff --git a/cookbook/getting_started/19_human_in_the_loop.py b/cookbook/getting_started/19_human_in_the_loop.py\nindex 5f3835abf..b202eee58 100644\n--- a/cookbook/getting_started/19_human_in_the_loop.py\n+++ b/cookbook/getting_started/19_human_in_the_loop.py\n@@ -17,50 +17,20 @@ Run `pip install openai httpx rich agno` to install dependencies.\n \n import json\n from textwrap import dedent\n-from typing import Iterator\n \n import httpx\n from agno.agent import Agent\n-from agno.exceptions import StopAgentRun\n-from agno.tools import FunctionCall, tool\n+from agno.tools import tool\n+from agno.utils import pprint\n from rich.console import Console\n-from rich.pretty import pprint\n from rich.prompt import Prompt\n \n-# This is the console instance used by the print_response method\n-# We can use this to stop and restart the live display and ask for user confirmation\n console = Console()\n \n \n-def pre_hook(fc: FunctionCall):\n-    # Get the live display instance from the console\n-    live = console._live\n-\n-    # Stop the live display temporarily so we can ask for user confirmation\n-    live.stop()  # type: ignore\n-\n-    # Ask for confirmation\n-    console.print(f\"\\nAbout to run [bold blue]{fc.function.name}[/]\")\n-    message = (\n-        Prompt.ask(\"Do you want to continue?\", choices=[\"y\", \"n\"], default=\"y\")\n-        .strip()\n-        .lower()\n-    )\n-\n-    # Restart the live display\n-    live.start()  # type: ignore\n-\n-    # If the user does not want to continue, raise a StopExecution exception\n-    if message != \"y\":\n-        raise StopAgentRun(\n-            \"Tool call cancelled by user\",\n-            agent_message=\"Stopping execution as permission was not granted.\",\n-        )\n-\n-\n-@tool(pre_hook=pre_hook)\n-def get_top_hackernews_stories(num_stories: int) -> Iterator[str]:\n-    \"\"\"Fetch top stories from Hacker News after user confirmation.\n+@tool(requires_confirmation=True)\n+def get_top_hackernews_stories(num_stories: int) -> str:\n+    \"\"\"Fetch top stories from Hacker News.\n \n     Args:\n         num_stories (int): Number of stories to retrieve\n@@ -73,6 +43,7 @@ def get_top_hackernews_stories(num_stories: int) -> Iterator[str]:\n     story_ids = response.json()\n \n     # Yield story details\n+    all_stories = []\n     for story_id in story_ids[:num_stories]:\n         story_response = httpx.get(\n             f\"https://hacker-news.firebaseio.com/v0/item/{story_id}.json\"\n@@ -80,7 +51,8 @@ def get_top_hackernews_stories(num_stories: int) -> Iterator[str]:\n         story = story_response.json()\n         if \"text\" in story:\n             story.pop(\"text\", None)\n-        yield json.dumps(story)\n+        all_stories.append(story)\n+    return json.dumps(all_stories)\n \n \n # Initialize the agent with a tech-savvy personality and clear instructions\n@@ -107,9 +79,24 @@ agent = Agent(\n # - \"What are the top 3 HN stories right now?\"\n # - \"Show me the most recent story from Hacker News\"\n # - \"Get the top 5 stories (you can try accepting and declining the confirmation)\"\n-agent.print_response(\n-    \"What are the top 2 hackernews stories?\", stream=True, console=console\n-)\n+response = agent.run(\"What are the top 2 hackernews stories?\")\n+if response.is_paused:\n+    for tool in response.tools:\n+        # Ask for confirmation\n+        console.print(\n+            f\"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] requires confirmation.\"\n+        )\n+        message = (\n+            Prompt.ask(\"Do you want to continue?\", choices=[\"y\", \"n\"], default=\"y\")\n+            .strip()\n+            .lower()\n+        )\n+\n+        if message == \"n\":\n+            break\n+        else:\n+            # We update the tools in place\n+            tool.confirmed = True\n \n-# View all messages\n-pprint(agent.run_response.messages)\n+    run_response = agent.continue_run(run_response=response)\n+    pprint.pprint_run_response(run_response)\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/agno/agent/agent.py",
            "diff": "diff --git a/libs/agno/agno/agent/agent.py b/libs/agno/agno/agent/agent.py\nindex 3cffb5754..611ac2a37 100644\n--- a/libs/agno/agno/agent/agent.py\n+++ b/libs/agno/agno/agent/agent.py\n@@ -32,7 +32,7 @@ from agno.memory.agent import AgentMemory, AgentRun\n from agno.memory.v2.memory import Memory, SessionSummary\n from agno.models.base import Model\n from agno.models.message import Citations, Message, MessageReferences\n-from agno.models.response import ModelResponse, ModelResponseEvent\n+from agno.models.response import ModelResponse, ModelResponseEvent, ToolExecution\n from agno.reasoning.step import NextAction, ReasoningStep, ReasoningSteps\n from agno.run.messages import RunMessages\n from agno.run.response import RunEvent, RunResponse, RunResponseExtraData\n@@ -569,6 +569,12 @@ class Agent:\n     def has_team(self) -> bool:\n         return self.team is not None and len(self.team) > 0\n \n+    @property\n+    def is_paused(self) -> bool:\n+        if self.run_response is not None and self.run_response.is_paused:\n+            return True\n+        return False\n+\n     def _run(\n         self,\n         run_response: RunResponse,\n@@ -589,6 +595,7 @@ class Agent:\n         5. Save session to storage\n         6. Save output to file if save_response_to_file is set\n         \"\"\"\n+        log_debug(f\"Agent Run Start: {run_response.run_id}\", center=True)\n \n         # 1. Reason about the task\n         self._handle_reasoning(run_messages=run_messages, session_id=session_id)\n@@ -610,6 +617,15 @@ class Agent:\n \n         self._update_run_response(model_response=model_response, run_response=run_response, run_messages=run_messages)\n \n+        # We should break out of the run function\n+        if any(\n+            tool_call.requires_confirmation or tool_call.external_execution_required\n+            for tool_call in run_response.tools or []\n+        ):\n+            return self._handle_agent_run_paused(\n+                run_response=run_response, session_id=session_id, user_id=user_id, message=message\n+            )\n+\n         # 3. Update Agent Memory\n         self._update_memory(\n             run_response=run_response,\n@@ -629,22 +645,8 @@ class Agent:\n         # Log Agent Run\n         self._log_agent_run(user_id=user_id, session_id=session_id)\n \n-        # Otherwise convert the response to the structured format\n-        if self.response_model is not None and not isinstance(run_response.content, self.response_model):\n-            if isinstance(run_response.content, str) and self.parse_response:\n-                try:\n-                    structured_output = parse_response_model_str(run_response.content, self.response_model)\n-\n-                    # Update RunResponse\n-                    if structured_output is not None:\n-                        run_response.content = structured_output\n-                        run_response.content_type = self.response_model.__name__\n-                    else:\n-                        log_warning(\"Failed to convert response to response_model\")\n-                except Exception as e:\n-                    log_warning(f\"Failed to convert response to output model: {e}\")\n-            else:\n-                log_warning(\"Something went wrong. Run response content is not a string\")\n+        # Convert the response to the structured format if needed\n+        self._convert_response_to_structured_format(run_response)\n \n         log_debug(f\"Agent Run End: {run_response.run_id}\", center=True, symbol=\"*\")\n \n@@ -661,6 +663,8 @@ class Agent:\n         messages: Optional[Sequence[Union[Dict, Message]]] = None,\n         stream_intermediate_steps: bool = False,\n     ) -> Iterator[RunResponse]:\n+        log_debug(f\"Agent Run Start: {run_response.run_id}\", center=True)\n+\n         # 1. Reason about the task if reasoning is enabled\n         yield from self._handle_reasoning_stream(run_messages=run_messages, session_id=session_id)\n \n@@ -668,18 +672,29 @@ class Agent:\n         # We track this, so we can add messages after this index to the RunResponse and Memory\n         index_of_last_user_message = len(run_messages.messages)\n \n-        # 6. Start the Run by yielding a RunStarted event\n+        # 1. Start the Run by yielding a RunStarted event\n         if stream_intermediate_steps:\n             yield self.create_run_response(\"Run started\", session_id=session_id, event=RunEvent.run_started)\n \n         # 2. Process model response\n-        yield from self._handle_model_response_stream(\n+        for event in self._handle_model_response_stream(\n             run_response=run_response,\n             run_messages=run_messages,\n             session_id=session_id,\n             response_format=response_format,\n             stream_intermediate_steps=stream_intermediate_steps,\n-        )\n+        ):\n+            yield event\n+\n+        # We should break out of the run function\n+        if any(\n+            tool_call.requires_confirmation or tool_call.external_execution_required\n+            for tool_call in run_response.tools or []\n+        ):\n+            yield from self._handle_agent_run_paused_stream(\n+                run_response=run_response, session_id=session_id, user_id=user_id, message=message\n+            )\n+            return\n \n         # 3. Update Agent Memory\n         self._update_memory(\n@@ -799,8 +814,7 @@ class Agent:\n             self.knowledge.initialize_valid_filters()  # type: ignore\n \n         # If no retries are set, use the agent's default retries\n-        if retries is None:\n-            retries = self.retries\n+        retries = retries if retries is not None else self.retries\n \n         # Use stream override value when necessary\n         if stream is None:\n@@ -819,8 +833,7 @@ class Agent:\n         self.stream_intermediate_steps = self.stream_intermediate_steps or (stream_intermediate_steps and self.stream)\n \n         # Use the default user_id and session_id when necessary\n-        if user_id is None:\n-            user_id = self.user_id\n+        user_id = user_id if user_id is not None else self.user_id\n \n         if session_id is None or session_id == \"\":\n             if not (self.session_id is None or self.session_id == \"\"):\n@@ -873,6 +886,7 @@ class Agent:\n                 run_response = RunResponse(run_id=run_id, session_id=session_id, agent_id=self.agent_id)\n \n                 run_response.model = self.model.id if self.model is not None else None\n+                run_response.model_provider = self.model.provider if self.model is not None else None\n \n                 # for backward compatibility, set self.run_response\n                 self.run_response = run_response\n@@ -889,8 +903,6 @@ class Agent:\n                 elif messages is not None:\n                     self.run_input = [m.to_dict() if isinstance(m, Message) else m for m in messages]\n \n-                log_debug(f\"Agent Run Start: {run_response.run_id}\", center=True)\n-\n                 # Prepare run messages\n                 run_messages: RunMessages = self.get_run_messages(\n                     message=message,\n@@ -984,6 +996,8 @@ class Agent:\n         5. Save session to storage\n         6. Save output to file if save_response_to_file is set\n         \"\"\"\n+        log_debug(f\"Agent Run Start: {run_response.run_id}\", center=True)\n+\n         self.model = cast(Model, self.model)\n         # 1. Reason about the task if reasoning is enabled\n         await self._ahandle_reasoning(run_messages=run_messages, session_id=session_id)\n@@ -1004,6 +1018,15 @@ class Agent:\n \n         self._update_run_response(model_response=model_response, run_response=run_response, run_messages=run_messages)\n \n+        # We should break out of the run function\n+        if any(\n+            tool_call.requires_confirmation or tool_call.external_execution_required\n+            for tool_call in run_response.tools or []\n+        ):\n+            return self._handle_agent_run_paused(\n+                run_response=run_response, session_id=session_id, user_id=user_id, message=message\n+            )\n+\n         # 3. Update Agent Memory\n         await self._aupdate_memory(\n             run_response=run_response,\n@@ -1023,22 +1046,8 @@ class Agent:\n         # Log Agent Run\n         await self._alog_agent_run(user_id=user_id, session_id=session_id)\n \n-        # Otherwise convert the response to the structured format\n-        if self.response_model is not None and not isinstance(run_response.content, self.response_model):\n-            if isinstance(run_response.content, str) and self.parse_response:\n-                try:\n-                    structured_output = parse_response_model_str(run_response.content, self.response_model)\n-\n-                    # Update RunResponse\n-                    if structured_output is not None:\n-                        run_response.content = structured_output\n-                        run_response.content_type = self.response_model.__name__\n-                    else:\n-                        log_warning(\"Failed to convert response to response_model\")\n-                except Exception as e:\n-                    log_warning(f\"Failed to convert response to output model: {e}\")\n-            else:\n-                log_warning(\"Something went wrong. Run response content is not a string\")\n+        # Convert the response to the structured format if needed\n+        self._convert_response_to_structured_format(run_response)\n \n         log_debug(f\"Agent Run End: {run_response.run_id}\", center=True, symbol=\"*\")\n \n@@ -1065,6 +1074,7 @@ class Agent:\n         5. Save session to storage\n         6. Save output to file if save_response_to_file is set\n         \"\"\"\n+        log_debug(f\"Agent Run Start: {run_response.run_id}\", center=True)\n \n         # 1. Reason about the task if reasoning is enabled\n         async for item in self._ahandle_reasoning_stream(run_messages=run_messages, session_id=session_id):\n@@ -1088,6 +1098,17 @@ class Agent:\n         ):\n             yield event\n \n+        # We should break out of the run function\n+        if any(\n+            tool_call.requires_confirmation or tool_call.external_execution_required\n+            for tool_call in run_response.tools or []\n+        ):\n+            for item in self._handle_agent_run_paused_stream(\n+                run_response=run_response, session_id=session_id, user_id=user_id, message=message\n+            ):\n+                yield item\n+            return\n+\n         # 3. Update Agent Memory\n         await self._aupdate_memory(\n             run_response=run_response,\n@@ -1116,6 +1137,7 @@ class Agent:\n         await self._alog_agent_run(user_id=user_id, session_id=session_id)\n \n         log_debug(f\"Agent Run End: {run_response.run_id}\", center=True, symbol=\"*\")\n+\n         if stream_intermediate_steps:\n             yield self.create_run_response(\n                 content=run_response.content,\n@@ -1232,109 +1254,1116 @@ class Agent:\n             knowledge_filters=effective_filters,\n         )\n \n-        # Create a run_id for this specific run\n-        run_id = str(uuid4())\n+        # Create a run_id for this specific run\n+        run_id = str(uuid4())\n+\n+        for attempt in range(num_attempts):\n+            try:\n+                # Create a new run_response for this attempt\n+                run_response = RunResponse(run_id=run_id, session_id=session_id, agent_id=self.agent_id)\n+\n+                run_response.model = self.model.id if self.model is not None else None\n+                run_response.model_provider = self.model.provider if self.model is not None else None\n+\n+                # for backward compatibility, set self.run_response\n+                self.run_response = run_response\n+                self.run_id = run_id\n+\n+                # Set run_input\n+                if message is not None:\n+                    if isinstance(message, str):\n+                        self.run_input = message\n+                    elif isinstance(message, Message):\n+                        self.run_input = message.to_dict()\n+                    else:\n+                        self.run_input = message\n+                elif messages is not None:\n+                    self.run_input = [m.to_dict() if isinstance(m, Message) else m for m in messages]\n+\n+                # Prepare run messages\n+                run_messages: RunMessages = self.get_run_messages(\n+                    message=message,\n+                    session_id=session_id,\n+                    user_id=user_id,\n+                    audio=audio,\n+                    images=images,\n+                    videos=videos,\n+                    files=files,\n+                    messages=messages,\n+                    **kwargs,\n+                )\n+                if len(run_messages.messages) == 0:\n+                    log_error(\"No messages to be sent to the model.\")\n+\n+                self.run_messages = run_messages\n+\n+                # Pass the new run_response to _arun\n+                if stream and self.is_streamable:\n+                    resp = self._arun_stream(\n+                        run_response=run_response,\n+                        run_messages=run_messages,\n+                        message=message,\n+                        user_id=user_id,\n+                        session_id=session_id,\n+                        response_format=response_format,\n+                        messages=messages,\n+                        stream_intermediate_steps=stream_intermediate_steps,\n+                    )  # type: ignore[assignment]\n+                    return resp\n+                else:\n+                    return await self._arun(\n+                        run_response=run_response,\n+                        run_messages=run_messages,\n+                        message=message,\n+                        user_id=user_id,\n+                        session_id=session_id,\n+                        response_format=response_format,\n+                        messages=messages,\n+                    )\n+            except ModelProviderError as e:\n+                log_warning(f\"Attempt {attempt + 1}/{num_attempts} failed: {str(e)}\")\n+                if isinstance(e, StopAgentRun):\n+                    raise e\n+                last_exception = e\n+                if attempt < num_attempts - 1:  # Don't sleep on the last attempt\n+                    if self.exponential_backoff:\n+                        delay = 2**attempt * self.delay_between_retries\n+                    else:\n+                        delay = self.delay_between_retries\n+                    import time\n+\n+                    time.sleep(delay)\n+            except KeyboardInterrupt:\n+                # Create a cancelled response\n+                return RunResponse(\n+                    run_id=run_id,\n+                    session_id=session_id,\n+                    agent_id=self.agent_id,\n+                    content=\"Operation cancelled by user\",\n+                    event=RunEvent.run_cancelled,\n+                )\n+\n+        # If we get here, all retries failed\n+        if last_exception is not None:\n+            log_error(\n+                f\"Failed after {num_attempts} attempts. Last error using {last_exception.model_name}({last_exception.model_id})\"\n+            )\n+            raise last_exception\n+        else:\n+            raise Exception(f\"Failed after {num_attempts} attempts.\")\n+\n+    @overload\n+    def continue_run(\n+        self,\n+        run_response: Optional[RunResponse] = None,\n+        *,\n+        stream: Literal[False] = False,\n+        stream_intermediate_steps: bool = False,\n+        user_id: Optional[str] = None,\n+        session_id: Optional[str] = None,\n+        retries: Optional[int] = None,\n+        knowledge_filters: Optional[Dict[str, Any]] = None,\n+    ) -> RunResponse: ...\n+\n+    @overload\n+    def continue_run(\n+        self,\n+        run_response: Optional[RunResponse] = None,\n+        *,\n+        stream: Literal[True] = True,\n+        stream_intermediate_steps: bool = False,\n+        user_id: Optional[str] = None,\n+        session_id: Optional[str] = None,\n+        retries: Optional[int] = None,\n+        knowledge_filters: Optional[Dict[str, Any]] = None,\n+    ) -> Iterator[RunResponse]: ...\n+\n+    def continue_run(\n+        self,\n+        run_response: Optional[RunResponse] = None,\n+        *,\n+        stream: Optional[bool] = None,\n+        stream_intermediate_steps: bool = False,\n+        user_id: Optional[str] = None,\n+        session_id: Optional[str] = None,\n+        retries: Optional[int] = None,\n+        knowledge_filters: Optional[Dict[str, Any]] = None,\n+    ) -> Union[RunResponse, Iterator[RunResponse]]:\n+        \"\"\"Continue a previous run.\"\"\"\n+\n+        # Initialize the Agent\n+        self.initialize_agent()\n+\n+        effective_filters = knowledge_filters\n+\n+        # When filters are passed manually\n+        if self.knowledge_filters or knowledge_filters:\n+            \"\"\"\n+                initialize metadata (specially required in case when load is commented out)\n+                when load is not called the reader's document_lists won't be called and metadata filters won't be initialized\n+                so we need to call initialize_valid_filters to make sure the filters are initialized\n+            \"\"\"\n+            if not self.knowledge.valid_metadata_filters:  # type: ignore\n+                self.knowledge.initialize_valid_filters()  # type: ignore\n+\n+            effective_filters = self._get_effective_filters(knowledge_filters)\n+\n+        # Agentic filters are enabled\n+        if self.enable_agentic_knowledge_filters and not self.knowledge.valid_metadata_filters:  # type: ignore\n+            # initialize metadata (specially required in case when load is commented out)\n+            self.knowledge.initialize_valid_filters()  # type: ignore\n+\n+        # If no retries are set, use the agent's default retries\n+        retries = retries if retries is not None else self.retries\n+\n+        # Use stream override value when necessary\n+        if stream is None:\n+            stream = False if self.stream is None else self.stream\n+\n+        if stream_intermediate_steps is None:\n+            stream_intermediate_steps = (\n+                False if self.stream_intermediate_steps is None else self.stream_intermediate_steps\n+            )\n+\n+        # Can't have stream_intermediate_steps if stream is False\n+        if stream is False:\n+            stream_intermediate_steps = False\n+\n+        self.stream = self.stream or (stream and self.is_streamable)\n+        self.stream_intermediate_steps = self.stream_intermediate_steps or (stream_intermediate_steps and self.stream)\n+\n+        # Use the default user_id and session_id when necessary\n+        user_id = user_id if user_id is not None else self.user_id\n+\n+        if session_id is None or session_id == \"\":\n+            if not (self.session_id is None or self.session_id == \"\"):\n+                session_id = self.session_id\n+            else:\n+                # Generate a new session_id and store it in the agent\n+                session_id = str(uuid4())\n+                self.session_id = session_id\n+\n+        session_id = cast(str, session_id)\n+\n+        self._initialize_session_state(user_id=user_id, session_id=session_id)\n+\n+        log_debug(f\"Session ID: {session_id}\", center=True)\n+\n+        # Read existing session from storage\n+        self.read_from_storage(session_id=session_id, user_id=user_id)\n+\n+        # Read existing session from storage\n+        if self.context is not None:\n+            self.resolve_run_context()\n+\n+        if self.response_model is not None and self.parse_response and stream is True:\n+            # Disable stream if response_model is set\n+            stream = False\n+            log_debug(\"Disabling stream as response_model is set\")\n+\n+        # Prepare arguments for the model\n+        self.set_default_model()\n+        response_format = self._get_response_format()\n+        self.model = cast(Model, self.model)\n+\n+        self.determine_tools_for_model(\n+            model=self.model,\n+            session_id=session_id,\n+            user_id=user_id,\n+            async_mode=False,\n+            knowledge_filters=effective_filters,\n+        )\n+\n+        # Run can be continued from previous run response or from passed run_response context\n+        if run_response is not None:\n+            messages = run_response.messages or []\n+            self.run_response = run_response\n+            self.run_id = run_response.run_id\n+        else:\n+            self.run_response = cast(RunResponse, self.run_response)\n+            # We are continuing from a previous run\n+            run_response = self.run_response\n+            messages = self.run_response.messages or []\n+            self.run_id = self.run_response.run_id\n+\n+        # Extract original user message from messages and remove from messages\n+        user_message = None\n+        for m in messages:\n+            if m.role == self.user_message_role:\n+                user_message = m\n+                messages.remove(m)\n+                break\n+\n+        # Set run_input\n+        if user_message is not None:\n+            if isinstance(user_message, str):\n+                self.run_input = user_message\n+            elif isinstance(user_message, Message):\n+                self.run_input = user_message.to_dict()\n+            else:\n+                self.run_input = user_message\n+        elif messages is not None:\n+            self.run_input = [m.to_dict() if isinstance(m, Message) else m for m in messages]\n+\n+        last_exception = None\n+        num_attempts = retries + 1\n+        for attempt in range(num_attempts):\n+            run_response = cast(RunResponse, run_response)\n+\n+            log_debug(f\"Agent Run Start: {run_response.run_id}\", center=True)\n+\n+            # Prepare run messages\n+            run_messages: RunMessages = self.get_continue_run_messages(\n+                message=user_message,\n+                messages=messages,\n+                session_id=session_id,\n+            )\n+\n+            try:\n+                if stream and self.is_streamable:\n+                    response_iterator = self._continue_run_stream(\n+                        run_response=run_response,\n+                        run_messages=run_messages,\n+                        message=user_message,\n+                        user_id=user_id,\n+                        session_id=session_id,\n+                        response_format=response_format,\n+                        messages=messages,\n+                        stream_intermediate_steps=stream_intermediate_steps,\n+                    )\n+\n+                    return response_iterator\n+                else:\n+                    response = self._continue_run(\n+                        run_response=run_response,\n+                        run_messages=run_messages,\n+                        message=user_message,\n+                        user_id=user_id,\n+                        session_id=session_id,\n+                        response_format=response_format,\n+                        messages=messages,\n+                    )\n+                    return response\n+            except ModelProviderError as e:\n+                log_warning(f\"Attempt {attempt + 1}/{num_attempts} failed: {str(e)}\")\n+                if isinstance(e, StopAgentRun):\n+                    raise e\n+                last_exception = e\n+                if attempt < num_attempts - 1:  # Don't sleep on the last attempt\n+                    if self.exponential_backoff:\n+                        delay = 2**attempt * self.delay_between_retries\n+                    else:\n+                        delay = self.delay_between_retries\n+                    import time\n+\n+                    time.sleep(delay)\n+            except KeyboardInterrupt:\n+                # Create a cancelled response\n+                cancelled_response = RunResponse(\n+                    run_id=self.run_id or str(uuid4()),\n+                    session_id=session_id,\n+                    agent_id=self.agent_id,\n+                    content=\"Operation cancelled by user\",\n+                    event=RunEvent.run_cancelled,\n+                )\n+                return cancelled_response\n+\n+        # If we get here, all retries failed\n+        if last_exception is not None:\n+            log_error(\n+                f\"Failed after {num_attempts} attempts. Last error using {last_exception.model_name}({last_exception.model_id})\"\n+            )\n+            raise last_exception\n+        else:\n+            raise Exception(f\"Failed after {num_attempts} attempts.\")\n+\n+    def _continue_run(\n+        self,\n+        run_response: RunResponse,\n+        run_messages: RunMessages,\n+        session_id: str,\n+        user_id: Optional[str] = None,\n+        response_format: Optional[Union[Dict, Type[BaseModel]]] = None,\n+        message: Optional[Union[str, List, Dict, Message]] = None,\n+        messages: Optional[Sequence[Union[Dict, Message]]] = None,\n+    ) -> RunResponse:\n+        self.model = cast(Model, self.model)\n+\n+        # 1. Handle the updated tools\n+        self._handle_tool_call_updates(run_response=run_response, run_messages=run_messages)\n+\n+        # Get the index of the last \"user\" message in messages_for_run\n+        # We track this so we can add messages after this index to the RunResponse and Memory\n+        index_of_last_user_message = len(run_messages.messages)\n+\n+        # 2. Generate a response from the Model (includes running function calls)\n+        self.model = cast(Model, self.model)\n+        model_response: ModelResponse = self.model.response(\n+            messages=run_messages.messages,\n+            response_format=response_format,\n+            tools=self._tools_for_model,\n+            functions=self._functions_for_model,\n+            tool_choice=self.tool_choice,\n+            tool_call_limit=self.tool_call_limit,\n+        )\n+\n+        self._update_run_response(model_response=model_response, run_response=run_response, run_messages=run_messages)\n+\n+        # We should break out of the run function\n+        if any(\n+            tool_call.requires_confirmation or tool_call.external_execution_required\n+            for tool_call in run_response.tools or []\n+        ):\n+            return self._handle_agent_run_paused(\n+                run_response=run_response, session_id=session_id, user_id=user_id, message=message\n+            )\n+\n+        # 3. Update Agent Memory\n+        self._update_memory(\n+            run_response=run_response,\n+            run_messages=run_messages,\n+            session_id=session_id,\n+            user_id=user_id,\n+            messages=messages,\n+            index_of_last_user_message=index_of_last_user_message,\n+        )\n+\n+        # 5. Save session to storage\n+        self.write_to_storage(user_id=user_id, session_id=session_id)\n+\n+        # 6. Save output to file if save_response_to_file is set\n+        self.save_run_response_to_file(message=message, session_id=session_id)\n+\n+        # Log Agent Run\n+        self._log_agent_run(user_id=user_id, session_id=session_id)\n+\n+        # Convert the response to the structured format if needed\n+        self._convert_response_to_structured_format(run_response)\n+\n+        run_response.event = RunEvent.run_response\n+\n+        return run_response\n+\n+    def _continue_run_stream(\n+        self,\n+        run_response: RunResponse,\n+        run_messages: RunMessages,\n+        session_id: str,\n+        user_id: Optional[str] = None,\n+        response_format: Optional[Union[Dict, Type[BaseModel]]] = None,\n+        message: Optional[Union[str, List, Dict, Message]] = None,\n+        messages: Optional[Sequence[Union[Dict, Message]]] = None,\n+        stream_intermediate_steps: bool = False,\n+    ) -> Iterator[RunResponse]:\n+        \"\"\"\n+        Continue a previous agent run\n+\n+        Steps:\n+        1. Handle tool calls as updated by the user\n+        2. Generate a response from the Model\n+        3. Update Agent Memory\n+        4. Calculate session metrics\n+        5. Save session to storage\n+        6. Save output to file if save_response_to_file is set\n+        \"\"\"\n+        # 1. Handle the updated tools\n+        yield from self._handle_tool_call_updates_stream(\n+            run_response=run_response, run_messages=run_messages, session_id=session_id\n+        )\n+\n+        # Get the index of the last \"user\" message in messages_for_run\n+        # We track this, so we can add messages after this index to the RunResponse and Memory\n+        index_of_last_user_message = len(run_messages.messages)\n+\n+        # Start the Run by yielding a RunContinued event\n+        if stream_intermediate_steps:\n+            yield self.create_run_response(\"Run continued\", session_id=session_id, event=RunEvent.run_continued)\n+\n+        # 2. Process model response\n+        for event in self._handle_model_response_stream(\n+            run_response=run_response,\n+            run_messages=run_messages,\n+            session_id=session_id,\n+            response_format=response_format,\n+            stream_intermediate_steps=stream_intermediate_steps,\n+        ):\n+            yield event\n+\n+        # We should break out of the run function\n+        if any(\n+            tool_call.requires_confirmation or tool_call.external_execution_required\n+            for tool_call in run_response.tools or []\n+        ):\n+            yield from self._handle_agent_run_paused_stream(\n+                run_response=run_response, session_id=session_id, user_id=user_id, message=message\n+            )\n+            return\n+\n+        # 3. Update Agent Memory\n+        self._update_memory(\n+            run_response=run_response,\n+            run_messages=run_messages,\n+            session_id=session_id,\n+            user_id=user_id,\n+            messages=messages,\n+            index_of_last_user_message=index_of_last_user_message,\n+        )\n+\n+        # Yield UpdatingMemory event\n+        if stream_intermediate_steps:\n+            yield self.create_run_response(\n+                content=\"Memory updated\",\n+                session_id=session_id,\n+                event=RunEvent.updating_memory,\n+            )\n+\n+        # 5. Save session to storage\n+        self.write_to_storage(user_id=user_id, session_id=session_id)\n+\n+        # 6. Save output to file if save_response_to_file is set\n+        self.save_run_response_to_file(message=message, session_id=session_id)\n+\n+        # Log Agent Run\n+        self._log_agent_run(user_id=user_id, session_id=session_id)\n+\n+        log_debug(f\"Agent Run End: {run_response.run_id}\", center=True, symbol=\"*\")\n+\n+        if stream_intermediate_steps:\n+            yield self.create_run_response(\n+                content=run_response.content,\n+                reasoning_content=run_response.reasoning_content,\n+                session_id=session_id,\n+                event=RunEvent.run_completed,\n+                run_response=run_response,\n+            )\n+\n+    async def acontinue_run(\n+        self,\n+        run_response: Optional[RunResponse] = None,\n+        *,\n+        stream: Optional[bool] = None,\n+        stream_intermediate_steps: bool = False,\n+        user_id: Optional[str] = None,\n+        session_id: Optional[str] = None,\n+        retries: Optional[int] = None,\n+        knowledge_filters: Optional[Dict[str, Any]] = None,\n+    ) -> Any:\n+        \"\"\"Continue a previous run.\"\"\"\n+\n+        # Initialize the Agent\n+        self.initialize_agent()\n+\n+        effective_filters = knowledge_filters\n+\n+        # When filters are passed manually\n+        if self.knowledge_filters or knowledge_filters:\n+            \"\"\"\n+                initialize metadata (specially required in case when load is commented out)\n+                when load is not called the reader's document_lists won't be called and metadata filters won't be initialized\n+                so we need to call initialize_valid_filters to make sure the filters are initialized\n+            \"\"\"\n+            if not self.knowledge.valid_metadata_filters:  # type: ignore\n+                self.knowledge.initialize_valid_filters()  # type: ignore\n+\n+            effective_filters = self._get_effective_filters(knowledge_filters)\n+\n+        # Agentic filters are enabled\n+        if self.enable_agentic_knowledge_filters and not self.knowledge.valid_metadata_filters:  # type: ignore\n+            # initialize metadata (specially required in case when load is commented out)\n+            self.knowledge.initialize_valid_filters()  # type: ignore\n+\n+        # If no retries are set, use the agent's default retries\n+        retries = retries if retries is not None else self.retries\n+\n+        # Use stream override value when necessary\n+        if stream is None:\n+            stream = False if self.stream is None else self.stream\n+\n+        if stream_intermediate_steps is None:\n+            stream_intermediate_steps = (\n+                False if self.stream_intermediate_steps is None else self.stream_intermediate_steps\n+            )\n+\n+        # Can't have stream_intermediate_steps if stream is False\n+        if stream is False:\n+            stream_intermediate_steps = False\n+\n+        self.stream = self.stream or (stream and self.is_streamable)\n+        self.stream_intermediate_steps = self.stream_intermediate_steps or (stream_intermediate_steps and self.stream)\n+\n+        # Use the default user_id and session_id when necessary\n+        user_id = user_id if user_id is not None else self.user_id\n+\n+        if session_id is None or session_id == \"\":\n+            if not (self.session_id is None or self.session_id == \"\"):\n+                session_id = self.session_id\n+            else:\n+                # Generate a new session_id and store it in the agent\n+                session_id = str(uuid4())\n+                self.session_id = session_id\n+\n+        session_id = cast(str, session_id)\n+\n+        self._initialize_session_state(user_id=user_id, session_id=session_id)\n+\n+        log_debug(f\"Session ID: {session_id}\", center=True)\n+\n+        # Read existing session from storage\n+        self.read_from_storage(session_id=session_id, user_id=user_id)\n+\n+        # Read existing session from storage\n+        if self.context is not None:\n+            self.resolve_run_context()\n+\n+        if self.response_model is not None and self.parse_response and stream is True:\n+            # Disable stream if response_model is set\n+            stream = False\n+            log_debug(\"Disabling stream as response_model is set\")\n+\n+        # Prepare arguments for the model\n+        self.set_default_model()\n+        response_format = self._get_response_format()\n+        self.model = cast(Model, self.model)\n+\n+        self.determine_tools_for_model(\n+            model=self.model,\n+            session_id=session_id,\n+            user_id=user_id,\n+            async_mode=False,\n+            knowledge_filters=effective_filters,\n+        )\n+\n+        # Run can be continued from previous run response or from passed run_response context\n+        if run_response is not None:\n+            messages = run_response.messages or []\n+            self.run_response = run_response\n+            self.run_id = run_response.run_id\n+        else:\n+            # We are continuing from a previous run\n+            self.run_response = cast(RunResponse, self.run_response)\n+            run_response = self.run_response\n+            messages = self.run_response.messages or []\n+            self.run_id = self.run_response.run_id\n+\n+        # Extract original user message from messages and remove from messages\n+        user_message = None\n+        for m in messages:\n+            if m.role == self.user_message_role:\n+                user_message = m\n+                messages.remove(m)\n+                break\n+\n+        # Set run_input\n+        if user_message is not None:\n+            if isinstance(user_message, str):\n+                self.run_input = user_message\n+            elif isinstance(user_message, Message):\n+                self.run_input = user_message.to_dict()\n+            else:\n+                self.run_input = user_message\n+        elif messages is not None:\n+            self.run_input = [m.to_dict() if isinstance(m, Message) else m for m in messages]\n+\n+        last_exception = None\n+        num_attempts = retries + 1\n+        for attempt in range(num_attempts):\n+            run_response = cast(RunResponse, run_response)\n+\n+            log_debug(f\"Agent Run Start: {run_response.run_id}\", center=True)\n+\n+            # Prepare run messages\n+            run_messages: RunMessages = self.get_continue_run_messages(\n+                message=user_message,\n+                messages=messages,\n+                session_id=session_id,\n+            )\n+\n+            try:\n+                if stream and self.is_streamable:\n+                    response_iterator = self._acontinue_run_stream(\n+                        run_response=run_response,\n+                        run_messages=run_messages,\n+                        message=user_message,\n+                        user_id=user_id,\n+                        session_id=session_id,\n+                        response_format=response_format,\n+                        messages=messages,\n+                        stream_intermediate_steps=stream_intermediate_steps,\n+                    )\n+\n+                    return response_iterator\n+                else:\n+                    return await self._acontinue_run(\n+                        run_response=run_response,\n+                        run_messages=run_messages,\n+                        message=user_message,\n+                        user_id=user_id,\n+                        session_id=session_id,\n+                        response_format=response_format,\n+                        messages=messages,\n+                    )\n+            except ModelProviderError as e:\n+                log_warning(f\"Attempt {attempt + 1}/{num_attempts} failed: {str(e)}\")\n+                if isinstance(e, StopAgentRun):\n+                    raise e\n+                last_exception = e\n+                if attempt < num_attempts - 1:  # Don't sleep on the last attempt\n+                    if self.exponential_backoff:\n+                        delay = 2**attempt * self.delay_between_retries\n+                    else:\n+                        delay = self.delay_between_retries\n+                    import time\n+\n+                    time.sleep(delay)\n+            except KeyboardInterrupt:\n+                # Create a cancelled response\n+                cancelled_response = RunResponse(\n+                    run_id=self.run_id or str(uuid4()),\n+                    session_id=session_id,\n+                    agent_id=self.agent_id,\n+                    content=\"Operation cancelled by user\",\n+                    event=RunEvent.run_cancelled,\n+                )\n+                return cancelled_response\n+\n+        # If we get here, all retries failed\n+        if last_exception is not None:\n+            log_error(\n+                f\"Failed after {num_attempts} attempts. Last error using {last_exception.model_name}({last_exception.model_id})\"\n+            )\n+            raise last_exception\n+        else:\n+            raise Exception(f\"Failed after {num_attempts} attempts.\")\n+\n+    async def _acontinue_run(\n+        self,\n+        run_response: RunResponse,\n+        run_messages: RunMessages,\n+        session_id: str,\n+        user_id: Optional[str] = None,\n+        response_format: Optional[Union[Dict, Type[BaseModel]]] = None,\n+        message: Optional[Union[str, List, Dict, Message]] = None,\n+        messages: Optional[Sequence[Union[Dict, Message]]] = None,\n+    ) -> RunResponse:\n+        self.model = cast(Model, self.model)\n+\n+        # 1. Handle the updated tools\n+        await self._ahandle_tool_call_updates(run_response=run_response, run_messages=run_messages)\n+\n+        # Get the index of the last \"user\" message in messages_for_run\n+        # We track this so we can add messages after this index to the RunResponse and Memory\n+        index_of_last_user_message = len(run_messages.messages)\n+\n+        # 2. Generate a response from the Model (includes running function calls)\n+        model_response: ModelResponse = await self.model.aresponse(\n+            messages=run_messages.messages,\n+            response_format=response_format,\n+            tools=self._tools_for_model,\n+            functions=self._functions_for_model,\n+            tool_choice=self.tool_choice,\n+            tool_call_limit=self.tool_call_limit,\n+        )\n+\n+        self._update_run_response(model_response=model_response, run_response=run_response, run_messages=run_messages)\n+\n+        # We should break out of the run function\n+        if any(\n+            tool_call.requires_confirmation or tool_call.external_execution_required\n+            for tool_call in run_response.tools or []\n+        ):\n+            return self._handle_agent_run_paused(\n+                run_response=run_response, session_id=session_id, user_id=user_id, message=message\n+            )\n+\n+        # 3. Update Agent Memory\n+        await self._aupdate_memory(\n+            run_response=run_response,\n+            run_messages=run_messages,\n+            session_id=session_id,\n+            user_id=user_id,\n+            messages=messages,\n+            index_of_last_user_message=index_of_last_user_message,\n+        )\n+\n+        # 5. Save session to storage\n+        self.write_to_storage(user_id=user_id, session_id=session_id)\n+\n+        # 6. Save output to file if save_response_to_file is set\n+        self.save_run_response_to_file(message=message, session_id=session_id)\n+\n+        # Log Agent Run\n+        await self._alog_agent_run(user_id=user_id, session_id=session_id)\n+\n+        # Convert the response to the structured format if needed\n+        self._convert_response_to_structured_format(run_response)\n+\n+        log_debug(f\"Agent Run End: {run_response.run_id}\", center=True, symbol=\"*\")\n+\n+        run_response.event = RunEvent.run_response\n+\n+        return run_response\n+\n+    async def _acontinue_run_stream(\n+        self,\n+        run_response: RunResponse,\n+        run_messages: RunMessages,\n+        session_id: str,\n+        user_id: Optional[str] = None,\n+        response_format: Optional[Union[Dict, Type[BaseModel]]] = None,\n+        message: Optional[Union[str, List, Dict, Message]] = None,\n+        messages: Optional[Sequence[Union[Dict, Message]]] = None,\n+        stream_intermediate_steps: bool = False,\n+    ) -> AsyncIterator[RunResponse]:\n+        \"\"\"\n+        Continue a previous agent run\n+\n+        Steps:\n+        1. Handle tool calls as updated by the user\n+        2. Generate a response from the Model\n+        3. Update Agent Memory\n+        4. Calculate session metrics\n+        5. Save session to storage\n+        6. Save output to file if save_response_to_file is set\n+        \"\"\"\n+        # 1. Handle the updated tools\n+        async for event in self._ahandle_tool_call_updates_stream(\n+            run_response=run_response, run_messages=run_messages, session_id=session_id\n+        ):\n+            yield event\n+\n+        # Get the index of the last \"user\" message in messages_for_run\n+        # We track this, so we can add messages after this index to the RunResponse and Memory\n+        index_of_last_user_message = len(run_messages.messages)\n+\n+        # Start the Run by yielding a RunContinued event\n+        if stream_intermediate_steps:\n+            yield self.create_run_response(\"Run continued\", session_id=session_id, event=RunEvent.run_continued)\n+\n+        # 2. Process model response\n+        async for event in self._ahandle_model_response_stream(\n+            run_response=run_response,\n+            run_messages=run_messages,\n+            session_id=session_id,\n+            response_format=response_format,\n+            stream_intermediate_steps=stream_intermediate_steps,\n+        ):\n+            yield event\n+\n+        # We should break out of the run function\n+        if any(\n+            tool_call.requires_confirmation or tool_call.external_execution_required\n+            for tool_call in run_response.tools or []\n+        ):\n+            for item in self._handle_agent_run_paused_stream(\n+                run_response=run_response, session_id=session_id, user_id=user_id, message=message\n+            ):\n+                yield item\n+            return\n+        # 3. Update Agent Memory\n+        await self._aupdate_memory(\n+            run_response=run_response,\n+            run_messages=run_messages,\n+            session_id=session_id,\n+            user_id=user_id,\n+            messages=messages,\n+            index_of_last_user_message=index_of_last_user_message,\n+        )\n+\n+        # Yield UpdatingMemory event\n+        if stream_intermediate_steps:\n+            yield self.create_run_response(\n+                content=\"Memory updated\",\n+                session_id=session_id,\n+                event=RunEvent.updating_memory,\n+            )\n+\n+        # 5. Save session to storage\n+        self.write_to_storage(user_id=user_id, session_id=session_id)\n+\n+        # 6. Save output to file if save_response_to_file is set\n+        self.save_run_response_to_file(message=message, session_id=session_id)\n+\n+        # Log Agent Run\n+        await self._alog_agent_run(user_id=user_id, session_id=session_id)\n+\n+        log_debug(f\"Agent Run End: {run_response.run_id}\", center=True, symbol=\"*\")\n+\n+        if stream_intermediate_steps:\n+            yield self.create_run_response(\n+                content=run_response.content,\n+                reasoning_content=run_response.reasoning_content,\n+                session_id=session_id,\n+                event=RunEvent.run_completed,\n+                run_response=run_response,\n+            )\n+\n+    def _handle_agent_run_paused(\n+        self,\n+        run_response: RunResponse,\n+        session_id: str,\n+        user_id: Optional[str] = None,\n+        message: Optional[Union[str, List, Dict, Message]] = None,\n+    ) -> RunResponse:\n+        # Save session to storage\n+        self.write_to_storage(user_id=user_id, session_id=session_id)\n+        # Log Agent Run\n+        self._log_agent_run(user_id=user_id, session_id=session_id)\n+\n+        log_debug(f\"Agent Run Paused: {run_response.run_id}\", center=True, symbol=\"*\")\n+\n+        # Save output to file if save_response_to_file is set\n+        self.save_run_response_to_file(message=message, session_id=session_id)\n+\n+        # We return and await confirmation/completion for the tools that require it\n+        run_response.event = RunEvent.run_paused\n+        return run_response\n+\n+    def _handle_agent_run_paused_stream(\n+        self,\n+        run_response: RunResponse,\n+        session_id: str,\n+        user_id: Optional[str] = None,\n+        message: Optional[Union[str, List, Dict, Message]] = None,\n+    ) -> Iterator[RunResponse]:\n+        # Save session to storage\n+        self.write_to_storage(user_id=user_id, session_id=session_id)\n+        # Log Agent Run\n+        self._log_agent_run(user_id=user_id, session_id=session_id)\n+\n+        log_debug(f\"Agent Run Paused: {run_response.run_id}\", center=True, symbol=\"*\")\n+\n+        # Save output to file if save_response_to_file is set\n+        self.save_run_response_to_file(message=message, session_id=session_id)\n+\n+        # We return and await confirmation/completion for the tools that require it\n+        yield self.create_run_response(\n+            event=RunEvent.run_paused,\n+            session_id=session_id,\n+            run_response=run_response,\n+        )\n+\n+    def _convert_response_to_structured_format(self, run_response: RunResponse):\n+        # Convert the response to the structured format if needed\n+        if self.response_model is not None and not isinstance(run_response.content, self.response_model):\n+            if isinstance(run_response.content, str) and self.parse_response:\n+                try:\n+                    structured_output = parse_response_model_str(run_response.content, self.response_model)\n+\n+                    # Update RunResponse\n+                    if structured_output is not None:\n+                        run_response.content = structured_output\n+                        run_response.content_type = self.response_model.__name__\n+                    else:\n+                        log_warning(\"Failed to convert response to response_model\")\n+                except Exception as e:\n+                    log_warning(f\"Failed to convert response to output model: {e}\")\n+            else:\n+                log_warning(\"Something went wrong. Run response content is not a string\")\n+\n+    def _reset_tool_confirmation_required(self, tool: ToolExecution):\n+        if self._functions_for_model:\n+            for func_name, func in self._functions_for_model.items():\n+                if func_name == tool.tool_name:\n+                    func.requires_confirmation = False\n \n-        for attempt in range(num_attempts):\n-            try:\n-                # Create a new run_response for this attempt\n-                run_response = RunResponse(run_id=run_id, session_id=session_id, agent_id=self.agent_id)\n+    def _handle_external_execution_update(self, run_messages: RunMessages, tool: ToolExecution):\n+        self.model = cast(Model, self.model)\n+        if tool.result is not None:\n+            for msg in run_messages.messages:\n+                # Skip if the message is already in the run_messages\n+                if msg.tool_call_id == tool.tool_call_id:\n+                    break\n \n-                run_response.model = self.model.id if self.model is not None else None\n+            run_messages.messages.append(\n+                Message(\n+                    role=self.model.tool_message_role,\n+                    content=tool.result,\n+                    tool_call_id=tool.tool_call_id,\n+                    tool_name=tool.tool_name,\n+                    tool_args=tool.tool_args,\n+                    tool_call_error=tool.tool_call_error,\n+                    stop_after_tool_call=tool.stop_after_tool_call,\n+                )\n+            )\n+            tool.external_execution_required = False\n+        else:\n+            raise ValueError(f\"Tool {tool.tool_name} requires external execution, cannot continue run\")\n \n-                # for backward compatibility, set self.run_response\n-                self.run_response = run_response\n-                self.run_id = run_id\n+    def _handle_tool_call_updates(self, run_response: RunResponse, run_messages: RunMessages):\n+        self.model = cast(Model, self.model)\n+        for _t in run_response.tools or []:\n+            # Case 1: Handle confirmed tools and execute them\n+            if _t.requires_confirmation is not None and _t.requires_confirmation is True and self._functions_for_model:\n+                self._reset_tool_confirmation_required(tool=_t)\n+                # Tool is confirmed and hasn't been run before\n+                if _t.confirmed is not None and _t.confirmed is True and _t.result is None:\n+                    # Execute the tool\n+                    function_call = self.model.get_function_call_to_run_from_tool_execution(\n+                        _t, self._functions_for_model\n+                    )\n+                    function_call_results: List[Message] = []\n \n-                # Set run_input\n-                if message is not None:\n-                    if isinstance(message, str):\n-                        self.run_input = message\n-                    elif isinstance(message, Message):\n-                        self.run_input = message.to_dict()\n-                    else:\n-                        self.run_input = message\n-                elif messages is not None:\n-                    self.run_input = [m.to_dict() if isinstance(m, Message) else m for m in messages]\n+                    for call_result in self.model.run_function_call(\n+                        function_call=function_call,\n+                        function_call_results=function_call_results,\n+                    ):\n+                        if (\n+                            call_result.event == ModelResponseEvent.tool_call_completed.value\n+                            and call_result.tool_executions\n+                        ):\n+                            tool_execution = call_result.tool_executions[0]\n+                            _t.result = tool_execution.result\n+                            _t.tool_call_error = tool_execution.tool_call_error\n+                    if len(function_call_results) > 0:\n+                        run_messages.messages.extend(function_call_results)\n+                    _t.requires_confirmation = False\n+                else:\n+                    raise ValueError(f\"Tool {_t.tool_name} requires confirmation, cannot continue run\")\n \n-                log_debug(f\"Agent Run Start: {run_response.run_id}\", center=True)\n+            # Case 2: Handle external execution required tools\n+            if _t.external_execution_required is not None and _t.external_execution_required is True:\n+                self._handle_external_execution_update(run_messages=run_messages, tool=_t)\n \n-                # Prepare run messages\n-                run_messages: RunMessages = self.get_run_messages(\n-                    message=message,\n-                    session_id=session_id,\n-                    user_id=user_id,\n-                    audio=audio,\n-                    images=images,\n-                    videos=videos,\n-                    files=files,\n-                    messages=messages,\n-                    **kwargs,\n-                )\n-                if len(run_messages.messages) == 0:\n-                    log_error(\"No messages to be sent to the model.\")\n+    def _handle_tool_call_updates_stream(\n+        self, run_response: RunResponse, run_messages: RunMessages, session_id: str\n+    ) -> Iterator[RunResponse]:\n+        self.model = cast(Model, self.model)\n+        for _t in run_response.tools or []:\n+            # Case 1: Handle confirmed tools and execute them\n+            if _t.requires_confirmation is not None and _t.requires_confirmation is True and self._functions_for_model:\n+                self._reset_tool_confirmation_required(tool=_t)\n+\n+                # Tool is confirmed and hasn't been run before\n+                if _t.confirmed is not None and _t.confirmed is True and _t.result is None:\n+                    # Execute the tool\n+                    function_call = self.model.get_function_call_to_run_from_tool_execution(\n+                        _t, self._functions_for_model\n+                    )\n+                    function_call_results: List[Message] = []\n \n-                self.run_messages = run_messages\n+                    for call_result in self.model.run_function_call(\n+                        function_call=function_call,\n+                        function_call_results=function_call_results,\n+                    ):\n+                        if call_result.event == ModelResponseEvent.tool_call_started.value:\n+                            yield self.create_run_response(\n+                                event=RunEvent.tool_call_started,\n+                                session_id=session_id,\n+                                created_at=call_result.created_at,\n+                            )\n+                        if (\n+                            call_result.event == ModelResponseEvent.tool_call_completed.value\n+                            and call_result.tool_executions\n+                        ):\n+                            tool_execution = call_result.tool_executions[0]\n+                            _t.result = tool_execution.result\n+                            _t.tool_call_error = tool_execution.tool_call_error\n+                            yield self.create_run_response(\n+                                content=call_result.content,\n+                                event=RunEvent.tool_call_completed,\n+                                session_id=session_id,\n+                            )\n+                    if len(function_call_results) > 0:\n+                        run_messages.messages.extend(function_call_results)\n+                    _t.requires_confirmation = False\n+                else:\n+                    raise ValueError(f\"Tool {_t.tool_name} requires confirmation, cannot continue run\")\n \n-                # Pass the new run_response to _arun\n-                if stream and self.is_streamable:\n-                    resp = self._arun_stream(\n-                        run_response=run_response,\n-                        run_messages=run_messages,\n-                        message=message,\n-                        user_id=user_id,\n-                        session_id=session_id,\n-                        response_format=response_format,\n-                        messages=messages,\n-                        stream_intermediate_steps=stream_intermediate_steps,\n-                    )  # type: ignore[assignment]\n-                    return resp\n+            # Case 2: Handle external execution required tools\n+            if _t.external_execution_required is not None and _t.external_execution_required is True:\n+                self._handle_external_execution_update(run_messages=run_messages, tool=_t)\n+\n+    async def _ahandle_tool_call_updates(self, run_response: RunResponse, run_messages: RunMessages):\n+        self.model = cast(Model, self.model)\n+        for _t in run_response.tools or []:\n+            # Case 1: Handle confirmed tools and execute them\n+            if _t.requires_confirmation is not None and _t.requires_confirmation is True and self._functions_for_model:\n+                self._reset_tool_confirmation_required(tool=_t)\n+\n+                # Tool is confirmed and hasn't been run before\n+                if _t.confirmed is not None and _t.confirmed is True and _t.result is None:\n+                    # Execute the tool\n+                    function_call = self.model.get_function_call_to_run_from_tool_execution(\n+                        _t, self._functions_for_model\n+                    )\n+                    function_call_results: List[Message] = []\n+\n+                    async for call_result in self.model.arun_function_calls(\n+                        function_calls=[function_call],\n+                        function_call_results=function_call_results,\n+                    ):\n+                        if (\n+                            call_result.event == ModelResponseEvent.tool_call_completed.value\n+                            and call_result.tool_executions\n+                        ):\n+                            tool_execution = call_result.tool_executions[0]\n+                            _t.result = tool_execution.result\n+                            _t.tool_call_error = tool_execution.tool_call_error\n+                    if len(function_call_results) > 0:\n+                        run_messages.messages.extend(function_call_results)\n+\n+                    _t.requires_confirmation = False\n                 else:\n-                    return await self._arun(\n-                        run_response=run_response,\n-                        run_messages=run_messages,\n-                        message=message,\n-                        user_id=user_id,\n-                        session_id=session_id,\n-                        response_format=response_format,\n-                        messages=messages,\n+                    raise ValueError(f\"Tool {_t.tool_name} requires confirmation, cannot continue run\")\n+\n+            # Case 2: Handle external execution required tools\n+            if _t.external_execution_required is not None and _t.external_execution_required is True:\n+                self._handle_external_execution_update(run_messages=run_messages, tool=_t)\n+\n+    async def _ahandle_tool_call_updates_stream(\n+        self, run_response: RunResponse, run_messages: RunMessages, session_id: str\n+    ) -> AsyncIterator[RunResponse]:\n+        self.model = cast(Model, self.model)\n+        for _t in run_response.tools or []:\n+            # Case 1: Handle confirmed tools and execute them\n+            if _t.requires_confirmation is not None and _t.requires_confirmation is True and self._functions_for_model:\n+                self._reset_tool_confirmation_required(tool=_t)\n+\n+                # Tool is confirmed and hasn't been run before\n+                if _t.confirmed is not None and _t.confirmed is True and _t.result is None:\n+                    # Execute the tool\n+                    function_call = self.model.get_function_call_to_run_from_tool_execution(\n+                        _t, self._functions_for_model\n                     )\n-            except ModelProviderError as e:\n-                log_warning(f\"Attempt {attempt + 1}/{num_attempts} failed: {str(e)}\")\n-                if isinstance(e, StopAgentRun):\n-                    raise e\n-                last_exception = e\n-                if attempt < num_attempts - 1:  # Don't sleep on the last attempt\n-                    if self.exponential_backoff:\n-                        delay = 2**attempt * self.delay_between_retries\n-                    else:\n-                        delay = self.delay_between_retries\n-                    import time\n+                    function_call_results: List[Message] = []\n \n-                    time.sleep(delay)\n-            except KeyboardInterrupt:\n-                # Create a cancelled response\n-                return RunResponse(\n-                    run_id=run_id,\n-                    session_id=session_id,\n-                    agent_id=self.agent_id,\n-                    content=\"Operation cancelled by user\",\n-                    event=RunEvent.run_cancelled,\n-                )\n+                    async for call_result in self.model.arun_function_calls(\n+                        function_calls=[function_call],\n+                        function_call_results=function_call_results,\n+                    ):\n+                        if call_result.event == ModelResponseEvent.tool_call_started.value:\n+                            yield self.create_run_response(\n+                                event=RunEvent.tool_call_started,\n+                                session_id=session_id,\n+                                created_at=call_result.created_at,\n+                            )\n+                        if (\n+                            call_result.event == ModelResponseEvent.tool_call_completed.value\n+                            and call_result.tool_executions\n+                        ):\n+                            tool_execution = call_result.tool_executions[0]\n+                            _t.result = tool_execution.result\n+                            _t.tool_call_error = tool_execution.tool_call_error\n+                            yield self.create_run_response(\n+                                content=call_result.content,\n+                                event=RunEvent.tool_call_completed,\n+                                session_id=session_id,\n+                            )\n+                    if len(function_call_results) > 0:\n+                        run_messages.messages.extend(function_call_results)\n \n-        # If we get here, all retries failed\n-        if last_exception is not None:\n-            log_error(\n-                f\"Failed after {num_attempts} attempts. Last error using {last_exception.model_name}({last_exception.model_id})\"\n-            )\n-            raise last_exception\n-        else:\n-            raise Exception(f\"Failed after {num_attempts} attempts.\")\n+                    _t.requires_confirmation = False\n+                else:\n+                    raise ValueError(f\"Tool {_t.tool_name} requires confirmation, cannot continue run\")\n+\n+            # Case 2: Handle external execution required tools\n+            if _t.external_execution_required is not None and _t.external_execution_required is True:\n+                self._handle_external_execution_update(run_messages=run_messages, tool=_t)\n \n     def _update_run_response(self, model_response: ModelResponse, run_response: RunResponse, run_messages: RunMessages):\n         # Format tool calls if they exist\n-        if model_response.tool_calls:\n-            run_response.formatted_tool_calls = format_tool_calls(model_response.tool_calls)\n+        if model_response.tool_executions:\n+            run_response.formatted_tool_calls = format_tool_calls(model_response.tool_executions)\n \n         # Handle structured outputs\n         if self.response_model is not None and model_response.parsed is not None:\n@@ -1361,18 +2390,18 @@ class Agent:\n         if model_response.citations is not None:\n             run_response.citations = model_response.citations\n \n-        # Update the run_response tools with the model response tools\n-        if model_response.tool_calls is not None:\n+        # Update the run_response tools with the model response tool_executions\n+        if model_response.tool_executions is not None:\n             if run_response.tools is None:\n-                run_response.tools = model_response.tool_calls\n+                run_response.tools = model_response.tool_executions\n             else:\n-                run_response.tools.extend(model_response.tool_calls)\n+                run_response.tools.extend(model_response.tool_executions)\n \n             # For Reasoning/Thinking/Knowledge Tools update reasoning_content in RunResponse\n-            for tool_call in model_response.tool_calls:\n-                tool_name = tool_call.get(\"tool_name\", \"\")\n+            for tool_call in model_response.tool_executions:\n+                tool_name = tool_call.tool_name or \"\"\n                 if tool_name.lower() in [\"think\", \"analyze\"]:\n-                    tool_args = tool_call.get(\"tool_args\", {})\n+                    tool_args = tool_call.tool_args or {}\n                     self.update_reasoning_content_from_tool_call(tool_name, tool_args)\n \n         # Update the run_response audio with the model response audio\n@@ -1579,6 +2608,7 @@ class Agent:\n         run_response: RunResponse,\n         run_messages: RunMessages,\n         session_id: str,\n+        user_id: Optional[str] = None,\n         response_format: Optional[Union[Dict, Type[BaseModel]]] = None,\n         stream_intermediate_steps: bool = False,\n     ) -> Iterator[RunResponse]:\n@@ -1777,17 +2807,33 @@ class Agent:\n \n                 yield run_response\n \n+        # Handle tool interruption events\n+        elif model_response_chunk.event in [\n+            ModelResponseEvent.tool_call_confirmation_required.value,\n+            ModelResponseEvent.tool_call_external_execution_required.value,\n+        ]:\n+            # Add tool calls to the run_response\n+            tool_executions_list = model_response_chunk.tool_executions\n+            if tool_executions_list is not None:\n+                # Add tool calls to the agent.run_response\n+                if run_response.tools is None:\n+                    run_response.tools = tool_executions_list\n+                else:\n+                    run_response.tools.extend(tool_executions_list)\n+\n+                # Format tool calls whenever new ones are added during streaming\n+                run_response.formatted_tool_calls = format_tool_calls(run_response.tools)\n         # If the model response is a tool_call_started, add the tool call to the run_response\n         elif (\n             model_response_chunk.event == ModelResponseEvent.tool_call_started.value\n         ):  # Add tool calls to the run_response\n-            new_tool_calls_list = model_response_chunk.tool_calls\n-            if new_tool_calls_list is not None:\n+            tool_executions_list = model_response_chunk.tool_executions\n+            if tool_executions_list is not None:\n                 # Add tool calls to the agent.run_response\n                 if run_response.tools is None:\n-                    run_response.tools = new_tool_calls_list\n+                    run_response.tools = tool_executions_list\n                 else:\n-                    run_response.tools.extend(new_tool_calls_list)\n+                    run_response.tools.extend(tool_executions_list)\n \n                 # Format tool calls whenever new ones are added during streaming\n                 run_response.formatted_tool_calls = format_tool_calls(run_response.tools)\n@@ -1805,34 +2851,32 @@ class Agent:\n         elif model_response_chunk.event == ModelResponseEvent.tool_call_completed.value:\n             reasoning_step: Optional[ReasoningStep] = None\n \n-            new_tool_calls_list = model_response_chunk.tool_calls\n-            if new_tool_calls_list is not None:\n+            tool_executions_list = model_response_chunk.tool_executions\n+            if tool_executions_list is not None:\n                 # Update the existing tool call in the run_response\n                 if run_response.tools:\n                     # Create a mapping of tool_call_id to index\n                     tool_call_index_map = {\n-                        tc[\"tool_call_id\"]: i\n-                        for i, tc in enumerate(run_response.tools)\n-                        if tc.get(\"tool_call_id\") is not None\n+                        tc.tool_call_id: i for i, tc in enumerate(run_response.tools) if tc.tool_call_id is not None\n                     }\n                     # Process tool calls\n-                    for tool_call_dict in new_tool_calls_list:\n-                        tool_call_id = tool_call_dict.get(\"tool_call_id\")\n+                    for tool_call_dict in tool_executions_list:\n+                        tool_call_id = tool_call_dict.tool_call_id or \"\"\n                         index = tool_call_index_map.get(tool_call_id)\n                         if index is not None:\n                             run_response.tools[index] = tool_call_dict\n                 else:\n-                    run_response.tools = new_tool_calls_list\n+                    run_response.tools = tool_executions_list\n \n                 # Only iterate through new tool calls\n-                for tool_call in new_tool_calls_list:\n-                    tool_name = tool_call.get(\"tool_name\", \"\")\n+                for tool_call in tool_executions_list:\n+                    tool_name = tool_call.tool_name or \"\"\n                     if tool_name.lower() in [\"think\", \"analyze\"]:\n-                        tool_args = tool_call.get(\"tool_args\", {})\n+                        tool_args = tool_call.tool_args or {}\n \n                         reasoning_step = self.update_reasoning_content_from_tool_call(tool_name, tool_args)\n \n-                        metrics = tool_call.get(\"metrics\")\n+                        metrics = tool_call.metrics\n                         if metrics is not None and metrics.time is not None:\n                             reasoning_state[\"reasoning_time_taken\"] = reasoning_state[\"reasoning_time_taken\"] + float(\n                                 metrics.time\n@@ -3218,6 +4262,124 @@ class Agent:\n \n         return run_messages\n \n+    def get_continue_run_messages(\n+        self,\n+        session_id: str,\n+        message: Optional[Message] = None,\n+        messages: Optional[List[Message]] = None,\n+    ) -> RunMessages:\n+        \"\"\"This function returns a RunMessages object with the following attributes:\n+            - system_message: The system message for this run\n+            - user_message: The user message for this run\n+            - messages: List of messages to send to the model\n+\n+        It continues from a previous run and completes a tool call that was paused.\n+        \"\"\"\n+\n+        # Initialize the RunMessages object\n+        run_messages = RunMessages()\n+        self.run_response = cast(RunResponse, self.run_response)\n+\n+        # 1. Add system message to run_messages\n+        if messages is None:\n+            messages = []\n+        for m in messages:\n+            if m.role == self.system_message_role:\n+                run_messages.system_message = m\n+                run_messages.messages.append(m)\n+                messages.remove(m)\n+                break\n+\n+        # 2. Add extra messages to run_messages if provided\n+        if self.add_messages is not None:\n+            messages_to_add_to_run_response: List[Message] = []\n+            if run_messages.extra_messages is None:\n+                run_messages.extra_messages = []\n+\n+            for _m in self.add_messages:\n+                if isinstance(_m, Message):\n+                    messages_to_add_to_run_response.append(_m)\n+                    run_messages.messages.append(_m)\n+                    run_messages.extra_messages.append(_m)\n+                elif isinstance(_m, dict):\n+                    try:\n+                        _m_parsed = Message.model_validate(_m)\n+                        messages_to_add_to_run_response.append(_m_parsed)\n+                        run_messages.messages.append(_m_parsed)\n+                        run_messages.extra_messages.append(_m_parsed)\n+                    except Exception as e:\n+                        log_warning(f\"Failed to validate message: {e}\")\n+            # Add the extra messages to the run_response\n+            if len(messages_to_add_to_run_response) > 0:\n+                log_debug(f\"Adding {len(messages_to_add_to_run_response)} extra messages\")\n+                if self.run_response.extra_data is None:\n+                    self.run_response.extra_data = RunResponseExtraData(add_messages=messages_to_add_to_run_response)\n+                else:\n+                    if self.run_response.extra_data.add_messages is None:\n+                        self.run_response.extra_data.add_messages = messages_to_add_to_run_response\n+                    else:\n+                        self.run_response.extra_data.add_messages.extend(messages_to_add_to_run_response)\n+\n+        # 3. Add history to run_messages\n+        if self.add_history_to_messages:\n+            from copy import deepcopy\n+\n+            history: List[Message] = []\n+            if isinstance(self.memory, AgentMemory):\n+                history = self.memory.get_messages_from_last_n_runs(\n+                    last_n=self.num_history_runs, skip_role=self.system_message_role\n+                )\n+            elif isinstance(self.memory, Memory):\n+                history = self.memory.get_messages_from_last_n_runs(\n+                    session_id=session_id, last_n=self.num_history_runs, skip_role=self.system_message_role\n+                )\n+            if len(history) > 0:\n+                # Create a deep copy of the history messages to avoid modifying the original messages\n+                history_copy = [deepcopy(msg) for msg in history]\n+\n+                # Tag each message as coming from history\n+                for _msg in history_copy:\n+                    _msg.from_history = True\n+\n+                log_debug(f\"Adding {len(history_copy)} messages from history\")\n+\n+                run_messages.messages += history_copy\n+\n+        # 4. Add user message\n+        # If message is provided as a dict, try to validate it as a Message\n+        user_message = None\n+        if isinstance(message, Message):\n+            user_message = message\n+        elif isinstance(message, dict):\n+            try:\n+                user_message = Message.model_validate(message)\n+            except Exception as e:\n+                log_warning(f\"Failed to validate message: {e}\")\n+\n+        # Add user message to run_messages\n+        if user_message is not None:\n+            run_messages.user_message = user_message\n+            run_messages.messages.append(user_message)\n+\n+        # 5. Add messages to run_messages if provided\n+        if len(messages or []) > 0:\n+            for _m in messages or []:\n+                if isinstance(_m, Message):\n+                    run_messages.messages.append(_m)\n+                    if run_messages.extra_messages is None:\n+                        run_messages.extra_messages = []\n+                    run_messages.extra_messages.append(_m)\n+                elif isinstance(_m, dict):\n+                    try:\n+                        run_messages.messages.append(Message.model_validate(_m))\n+                        if run_messages.extra_messages is None:\n+                            run_messages.extra_messages = []\n+                        run_messages.extra_messages.append(Message.model_validate(_m))\n+                    except Exception as e:\n+                        log_warning(f\"Failed to validate message: {e}\")\n+\n+        return run_messages\n+\n     def get_session_summary(self, session_id: Optional[str] = None, user_id: Optional[str] = None):\n         \"\"\"Get the session summary for the given session ID and user ID.\"\"\"\n         if self.memory is None:\n@@ -4071,11 +5233,12 @@ class Agent:\n                 log_warning(\"Reasoning error. Reasoning agent is None, continuing regular session...\")\n                 return\n             # Ensure the reasoning agent response model is ReasoningSteps\n-            if reasoning_agent.response_model is not None and not isinstance(reasoning_agent.response_model, type):\n-                if not issubclass(reasoning_agent.response_model, ReasoningSteps):\n-                    log_warning(\n-                        \"Reasoning agent response model should be `ReasoningSteps`, continuing regular session...\"\n-                    )\n+            if (\n+                reasoning_agent.response_model is not None\n+                and not isinstance(reasoning_agent.response_model, type)\n+                and not issubclass(reasoning_agent.response_model, ReasoningSteps)\n+            ):\n+                log_warning(\"Reasoning agent response model should be `ReasoningSteps`, continuing regular session...\")\n                 return\n             # Ensure the reasoning model and agent do not show tool calls\n             reasoning_agent.show_tool_calls = False\n@@ -4281,12 +5444,14 @@ class Agent:\n                 log_warning(\"Reasoning error. Reasoning agent is None, continuing regular session...\")\n                 return\n             # Ensure the reasoning agent response model is ReasoningSteps\n-            if reasoning_agent.response_model is not None and not isinstance(reasoning_agent.response_model, type):\n-                if not issubclass(reasoning_agent.response_model, ReasoningSteps):\n-                    log_warning(\n-                        \"Reasoning agent response model should be `ReasoningSteps`, continuing regular session...\"\n-                    )\n+            if (\n+                reasoning_agent.response_model is not None\n+                and not isinstance(reasoning_agent.response_model, type)\n+                and not issubclass(reasoning_agent.response_model, ReasoningSteps)\n+            ):\n+                log_warning(\"Reasoning agent response model should be `ReasoningSteps`, continuing regular session...\")\n                 return\n+\n             # Ensure the reasoning model and agent do not show tool calls\n             reasoning_agent.show_tool_calls = False\n \n@@ -4911,6 +6076,11 @@ class Agent:\n                     **kwargs,\n                 ):\n                     if isinstance(resp, RunResponse):\n+                        if resp.is_paused:\n+                            response_panel = self._handle_paused_run(resp)\n+                            panels.append(response_panel)\n+                            live_log.update(Group(*panels))\n+                            break\n                         if resp.event == RunEvent.run_response:\n                             if isinstance(resp.content, str):\n                                 _response_content += resp.content\n@@ -4993,8 +6163,8 @@ class Agent:\n                         render = True\n                         # Create bullet points for each tool call\n                         tool_calls_content = Text()\n-                        for tool_call in self.run_response.formatted_tool_calls:\n-                            tool_calls_content.append(f\"\u2022 {tool_call}\\n\")\n+                        for formatted_tool_call in self.run_response.formatted_tool_calls:\n+                            tool_calls_content.append(f\"\u2022 {formatted_tool_call}\\n\")\n \n                         tool_calls_panel = create_panel(\n                             content=tool_calls_content.plain.rstrip(),\n@@ -5094,6 +6264,13 @@ class Agent:\n                 response_timer.stop()\n \n                 reasoning_steps = []\n+\n+                if isinstance(run_response, RunResponse) and run_response.is_paused:\n+                    response_panel = self._handle_paused_run(run_response)\n+                    panels.append(response_panel)\n+                    live_log.update(Group(*panels))\n+                    return\n+\n                 if (\n                     isinstance(run_response, RunResponse)\n                     and run_response.extra_data is not None\n@@ -5143,8 +6320,8 @@ class Agent:\n                 if self.show_tool_calls and isinstance(run_response, RunResponse) and run_response.formatted_tool_calls:\n                     # Create bullet points for each tool call\n                     tool_calls_content = Text()\n-                    for tool_call in run_response.formatted_tool_calls:\n-                        tool_calls_content.append(f\"\u2022 {tool_call}\\n\")\n+                    for formatted_tool_call in run_response.formatted_tool_calls:\n+                        tool_calls_content.append(f\"\u2022 {formatted_tool_call}\\n\")\n \n                     tool_calls_panel = create_panel(\n                         content=tool_calls_content.plain.rstrip(),\n@@ -5313,6 +6490,12 @@ class Agent:\n \n                 async for resp in result:\n                     if isinstance(resp, RunResponse):\n+                        if resp.is_paused:\n+                            response_panel = self._handle_paused_run(resp)\n+                            panels.append(response_panel)\n+                            live_log.update(Group(*panels))\n+                            break\n+\n                         if resp.event == RunEvent.run_response:\n                             if isinstance(resp.content, str):\n                                 _response_content += resp.content\n@@ -5395,8 +6578,8 @@ class Agent:\n                         render = True\n                         # Create bullet points for each tool call\n                         tool_calls_content = Text()\n-                        for tool_call in self.run_response.formatted_tool_calls:\n-                            tool_calls_content.append(f\"\u2022 {tool_call}\\n\")\n+                        for formatted_tool_call in self.run_response.formatted_tool_calls:\n+                            tool_calls_content.append(f\"\u2022 {formatted_tool_call}\\n\")\n \n                         tool_calls_panel = create_panel(\n                             content=tool_calls_content.plain.rstrip(),\n@@ -5495,6 +6678,12 @@ class Agent:\n                 )\n                 response_timer.stop()\n \n+                if isinstance(run_response, RunResponse) and run_response.is_paused:\n+                    response_panel = self._handle_paused_run(run_response)\n+                    panels.append(response_panel)\n+                    live_log.update(Group(*panels))\n+                    return\n+\n                 reasoning_steps = []\n                 if (\n                     isinstance(run_response, RunResponse)\n@@ -5543,8 +6732,8 @@ class Agent:\n \n                 if self.show_tool_calls and isinstance(run_response, RunResponse) and run_response.formatted_tool_calls:\n                     tool_calls_content = Text()\n-                    for tool_call in run_response.formatted_tool_calls:\n-                        tool_calls_content.append(f\"\u2022 {tool_call}\\n\")\n+                    for formatted_tool_call in run_response.formatted_tool_calls:\n+                        tool_calls_content.append(f\"\u2022 {formatted_tool_call}\\n\")\n \n                     tool_calls_panel = create_panel(\n                         content=tool_calls_content.plain.rstrip(),\n@@ -5627,6 +6816,38 @@ class Agent:\n                 panels = [p for p in panels if not isinstance(p, Status)]\n                 live_log.update(Group(*panels))\n \n+    def _handle_paused_run(self, run_response: RunResponse) -> Any:\n+        from rich.text import Text\n+\n+        tool_calls_content = Text(\"Run is paused. \")\n+        if run_response.tools is not None:\n+            if any(tc.requires_confirmation for tc in run_response.tools):\n+                tool_calls_content.append(\"The following tool calls require confirmation:\\n\")\n+            for tool_call in run_response.tools:\n+                if tool_call.requires_confirmation:\n+                    args_str = \"\"\n+                    for arg, value in tool_call.tool_args.items() if tool_call.tool_args else {}:\n+                        args_str += f\"{arg}={value}, \"\n+                    args_str = args_str.rstrip(\", \")\n+                    tool_calls_content.append(f\"\u2022 {tool_call.tool_name}({args_str})\\n\")\n+            if any(tc.external_execution_required for tc in run_response.tools):\n+                tool_calls_content.append(\"The following tool calls require external execution:\\n\")\n+            for tool_call in run_response.tools:\n+                if tool_call.external_execution_required:\n+                    args_str = \"\"\n+                    for arg, value in tool_call.tool_args.items() if tool_call.tool_args else {}:\n+                        args_str += f\"{arg}={value}, \"\n+                    args_str = args_str.rstrip(\", \")\n+                    tool_calls_content.append(f\"\u2022 {tool_call.tool_name}({args_str})\\n\")\n+\n+        # Create panel for response\n+        response_panel = create_panel(\n+            content=tool_calls_content,\n+            title=\"Run Paused\",\n+            border_style=\"blue\",\n+        )\n+        return response_panel\n+\n     def update_reasoning_content_from_tool_call(\n         self, tool_name: str, tool_args: Dict[str, Any]\n     ) -> Optional[ReasoningStep]:\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/agno/api/evals.py",
            "diff": "diff --git a/libs/agno/agno/api/evals.py b/libs/agno/agno/api/evals.py\nnew file mode 100644\nindex 000000000..aa15bc4e7\n--- /dev/null\n+++ b/libs/agno/agno/api/evals.py\n@@ -0,0 +1,33 @@\n+from agno.api.api import api\n+from agno.api.routes import ApiRoutes\n+from agno.api.schemas.evals import EvalRunCreate\n+from agno.cli.settings import agno_cli_settings\n+from agno.utils.log import log_debug\n+\n+\n+def create_eval_run(eval_run: EvalRunCreate) -> None:\n+    \"\"\"Call the API to create an evaluation run.\"\"\"\n+    if not agno_cli_settings.api_enabled:\n+        return\n+\n+    log_debug(\"Calling the API to create an evaluation run\")\n+    with api.AuthenticatedClient() as api_client:\n+        try:\n+            api_client.post(ApiRoutes.EVAL_RUN_CREATE, json={\"eval_run\": eval_run.model_dump(exclude_none=True)})\n+        except Exception as e:\n+            log_debug(f\"Could not create evaluation run: {e}\")\n+    return\n+\n+\n+async def async_create_eval_run(eval_run: EvalRunCreate) -> None:\n+    \"\"\"Call the API to create an evaluation run.\"\"\"\n+    if not agno_cli_settings.api_enabled:\n+        return\n+\n+    log_debug(\"Calling the API to create an evaluation run\")\n+    async with api.AuthenticatedAsyncClient() as api_client:\n+        try:\n+            await api_client.post(ApiRoutes.EVAL_RUN_CREATE, json={\"eval_run\": eval_run.model_dump(exclude_none=True)})\n+        except Exception as e:\n+            log_debug(f\"Could not create evaluation run: {e}\")\n+    return\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/agno/api/routes.py",
            "diff": "diff --git a/libs/agno/agno/api/routes.py b/libs/agno/agno/api/routes.py\nindex 69b6825c5..3428c4fdb 100644\n--- a/libs/agno/agno/api/routes.py\n+++ b/libs/agno/agno/api/routes.py\n@@ -37,3 +37,6 @@ class ApiRoutes:\n     # Playground paths\n     PLAYGROUND_ENDPOINT_CREATE: str = \"/v1/playground/endpoint/create\"\n     PLAYGROUND_APP_DEPLOY: str = \"/v1/playground/app/deploy\"\n+\n+    # Eval paths\n+    EVAL_RUN_CREATE: str = \"/v2/eval-runs\"\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/agno/api/schemas/evals.py",
            "diff": "diff --git a/libs/agno/agno/api/schemas/evals.py b/libs/agno/agno/api/schemas/evals.py\nnew file mode 100644\nindex 000000000..371aaab07\n--- /dev/null\n+++ b/libs/agno/agno/api/schemas/evals.py\n@@ -0,0 +1,25 @@\n+from enum import Enum\n+from typing import Any, Dict, Optional\n+\n+from pydantic import BaseModel\n+\n+\n+class EvalType(str, Enum):\n+    ACCURACY = \"accuracy\"\n+    PERFORMANCE = \"performance\"\n+    RELIABILITY = \"reliability\"\n+\n+\n+class EvalRunCreate(BaseModel):\n+    \"\"\"Data sent to the API to create an evaluation run\"\"\"\n+\n+    agent_id: Optional[str] = None\n+    model_id: Optional[str] = None\n+    model_provider: Optional[str] = None\n+    team_id: Optional[str] = None\n+    name: Optional[str] = None\n+    evaluated_entity_name: Optional[str] = None\n+\n+    run_id: str\n+    eval_type: EvalType\n+    eval_data: Dict[str, Any]\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/agno/document/reader/base.py",
            "diff": "diff --git a/libs/agno/agno/document/reader/base.py b/libs/agno/agno/document/reader/base.py\nindex be2d8a695..62f6410d3 100644\n--- a/libs/agno/agno/document/reader/base.py\n+++ b/libs/agno/agno/document/reader/base.py\n@@ -16,7 +16,10 @@ class Reader:\n     separators: List[str] = field(default_factory=lambda: [\"\\n\", \"\\n\\n\", \"\\r\", \"\\r\\n\", \"\\n\\r\", \"\\t\", \" \", \"  \"])\n     chunking_strategy: Optional[ChunkingStrategy] = None\n \n-    def __init__(self, chunk_size: int = 5000, chunking_strategy: Optional[ChunkingStrategy] = None) -> None:\n+    def __init__(\n+        self, chunk: bool = True, chunk_size: int = 5000, chunking_strategy: Optional[ChunkingStrategy] = None\n+    ) -> None:\n+        self.chunk = chunk\n         self.chunk_size = chunk_size\n         self.chunking_strategy = chunking_strategy\n \n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/agno/document/reader/firecrawl_reader.py",
            "diff": "diff --git a/libs/agno/agno/document/reader/firecrawl_reader.py b/libs/agno/agno/document/reader/firecrawl_reader.py\nindex dbe58c964..a587ba9c3 100644\n--- a/libs/agno/agno/document/reader/firecrawl_reader.py\n+++ b/libs/agno/agno/document/reader/firecrawl_reader.py\n@@ -3,11 +3,12 @@ from dataclasses import dataclass\n from typing import Dict, List, Literal, Optional\n \n from agno.document.base import Document\n+from agno.document.chunking.strategy import ChunkingStrategy\n from agno.document.reader.base import Reader\n from agno.utils.log import log_debug, logger\n \n try:\n-    from firecrawl import FirecrawlApp\n+    from firecrawl import FirecrawlApp  # type: ignore[attr-defined]\n except ImportError:\n     raise ImportError(\"The `firecrawl` package is not installed. Please install it via `pip install firecrawl-py`.\")\n \n@@ -23,10 +24,14 @@ class FirecrawlReader(Reader):\n         api_key: Optional[str] = None,\n         params: Optional[Dict] = None,\n         mode: Literal[\"scrape\", \"crawl\"] = \"scrape\",\n-        *args,\n-        **kwargs,\n+        chunk: bool = True,\n+        chunk_size: int = 5000,\n+        chunking_strategy: Optional[ChunkingStrategy] = None,\n     ) -> None:\n-        super().__init__(*args, **kwargs)\n+        # Initialise base Reader (handles chunk_size / strategy)\n+        super().__init__(chunk=chunk, chunk_size=chunk_size, chunking_strategy=chunking_strategy)\n+\n+        # Firecrawl-specific attributes\n         self.api_key = api_key\n         self.params = params\n         self.mode = mode\n@@ -45,9 +50,15 @@ class FirecrawlReader(Reader):\n         log_debug(f\"Scraping: {url}\")\n \n         app = FirecrawlApp(api_key=self.api_key)\n-        scraped_data = app.scrape_url(url, params=self.params)\n-        # print(scraped_data)\n-        content = scraped_data.get(\"markdown\", \"\")\n+\n+        if self.params:\n+            scraped_data = app.scrape_url(url, **self.params)\n+        else:\n+            scraped_data = app.scrape_url(url)\n+        if isinstance(scraped_data, dict):\n+            content = scraped_data.get(\"markdown\", \"\")\n+        else:\n+            content = getattr(scraped_data, \"markdown\", \"\")\n \n         # Debug logging\n         log_debug(f\"Received content type: {type(content)}\")\n@@ -93,14 +104,23 @@ class FirecrawlReader(Reader):\n         log_debug(f\"Crawling: {url}\")\n \n         app = FirecrawlApp(api_key=self.api_key)\n-        crawl_result = app.crawl_url(url, params=self.params)\n+\n+        if self.params:\n+            crawl_result = app.crawl_url(url, **self.params)\n+        else:\n+            crawl_result = app.crawl_url(url)\n         documents = []\n \n-        # Extract data from crawl results\n-        results_data = crawl_result.get(\"data\", [])\n+        if isinstance(crawl_result, dict):\n+            results_data = crawl_result.get(\"data\", [])\n+        else:\n+            results_data = getattr(crawl_result, \"data\", [])\n         for result in results_data:\n             # Get markdown content, default to empty string if not found\n-            content = result.get(\"markdown\", \"\")\n+            if isinstance(result, dict):\n+                content = result.get(\"markdown\", \"\")\n+            else:\n+                content = getattr(result, \"markdown\", \"\")\n \n             if content:  # Only create document if content exists\n                 if self.chunk:\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/agno/eval/accuracy.py",
            "diff": "diff --git a/libs/agno/agno/eval/accuracy.py b/libs/agno/agno/eval/accuracy.py\nindex a0f33cc8e..bd317de02 100644\n--- a/libs/agno/agno/eval/accuracy.py\n+++ b/libs/agno/agno/eval/accuracy.py\n@@ -1,4 +1,4 @@\n-from dataclasses import dataclass, field\n+from dataclasses import asdict, dataclass, field\n from os import getenv\n from textwrap import dedent\n from typing import TYPE_CHECKING, Callable, List, Optional, Union, cast\n@@ -7,7 +7,8 @@ from uuid import uuid4\n from pydantic import BaseModel, Field\n \n from agno.agent import Agent\n-from agno.eval.utils import store_result_in_file\n+from agno.api.schemas.evals import EvalType\n+from agno.eval.utils import log_eval_run, store_result_in_file\n from agno.exceptions import EvalError\n from agno.models.base import Model\n from agno.utils.log import logger, set_log_level_to_debug, set_log_level_to_info\n@@ -163,6 +164,8 @@ class AccuracyEval:\n     file_path_to_save_results: Optional[str] = None\n     # Enable debug logs\n     debug_mode: bool = getenv(\"AGNO_DEBUG\", \"false\").lower() == \"true\"\n+    # Log the results to the Agno platform. On by default.\n+    monitoring: bool = getenv(\"AGNO_MONITOR\", \"true\").lower() == \"true\"\n \n     def get_evaluator_agent(self) -> Agent:\n         \"\"\"Return the evaluator agent. If not provided, build it based on the evaluator fields and default instructions.\"\"\"\n@@ -357,6 +360,21 @@ Remember: You must only compare the agent_output to the expected_output. The exp\n         if self.print_summary or print_summary:\n             self.result.print_summary(console)\n \n+        # Log results to the Agno platform if requested\n+        if self.monitoring:\n+            log_eval_run(\n+                run_id=self.eval_id,  # type: ignore\n+                run_data=asdict(self.result),\n+                eval_type=EvalType.ACCURACY,\n+                agent_id=self.agent.agent_id if self.agent is not None else None,\n+                model_id=self.agent.model.id if self.agent is not None and self.agent.model is not None else None,\n+                model_provider=self.agent.model.provider\n+                if self.agent is not None and self.agent.model is not None\n+                else None,\n+                name=self.name if self.name is not None else None,\n+                evaluated_entity_name=self.agent.name if self.agent is not None else None,\n+            )\n+\n         logger.debug(f\"*********** Evaluation {self.eval_id} Finished ***********\")\n         return self.result\n \n@@ -418,6 +436,20 @@ Remember: You must only compare the agent_output to the expected_output. The exp\n                     eval_id=self.eval_id,\n                     result=self.result,\n                 )\n+        # Log results to the Agno platform if requested\n+        if self.monitoring:\n+            log_eval_run(\n+                run_id=self.eval_id,  # type: ignore\n+                run_data=asdict(self.result),\n+                eval_type=EvalType.ACCURACY,\n+                agent_id=self.agent.agent_id if self.agent is not None else None,\n+                model_id=self.agent.model.id if self.agent is not None and self.agent.model is not None else None,\n+                model_provider=self.agent.model.provider\n+                if self.agent is not None and self.agent.model is not None\n+                else None,\n+                name=self.name if self.name is not None else None,\n+                evaluated_entity_name=self.agent.name if self.agent is not None else None,\n+            )\n \n-        logger.debug(f\"*********** Evaluation {self.eval_id} Finished ***********\")\n+        logger.debug(f\"*********** Evaluation End: {self.eval_id} ***********\")\n         return self.result\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/agno/eval/performance.py",
            "diff": "diff --git a/libs/agno/agno/eval/performance.py b/libs/agno/agno/eval/performance.py\nindex 3682e62e6..4af84ab2d 100644\n--- a/libs/agno/agno/eval/performance.py\n+++ b/libs/agno/agno/eval/performance.py\n@@ -5,7 +5,8 @@ from os import getenv\n from typing import TYPE_CHECKING, Callable, List, Optional\n from uuid import uuid4\n \n-from agno.eval.utils import store_result_in_file\n+from agno.api.schemas.evals import EvalType\n+from agno.eval.utils import log_eval_run, store_result_in_file\n from agno.utils.log import logger\n from agno.utils.timer import Timer\n \n@@ -177,6 +178,8 @@ class PerformanceEval:\n     file_path_to_save_results: Optional[str] = None\n     # Enable debug logs\n     debug_mode: bool = getenv(\"AGNO_DEBUG\", \"false\").lower() == \"true\"\n+    # Log the results to the Agno platform. On by default.\n+    monitoring: bool = getenv(\"AGNO_MONITOR\", \"true\").lower() == \"true\"\n \n     def _measure_time(self) -> float:\n         \"\"\"Measure execution time for a single run.\"\"\"\n@@ -243,6 +246,7 @@ class PerformanceEval:\n         4. Collect results\n         5. Save results if requested\n         6. Print results as requested\n+        7. Log results to the Agno platform if requested\n         \"\"\"\n         from rich.console import Console\n         from rich.live import Live\n@@ -321,5 +325,34 @@ class PerformanceEval:\n         if self.print_summary or print_summary:\n             self.result.print_summary(console)\n \n+        # 7. Log results to the Agno platform if requested\n+        if self.monitoring:\n+            log_eval_run(\n+                run_id=self.eval_id,  # type: ignore\n+                run_data={\n+                    \"result\": {\n+                        \"avg_run_time\": self.result.avg_run_time,\n+                        \"min_run_time\": self.result.min_run_time,\n+                        \"max_run_time\": self.result.max_run_time,\n+                        \"std_dev_run_time\": self.result.std_dev_run_time,\n+                        \"median_run_time\": self.result.median_run_time,\n+                        \"p95_run_time\": self.result.p95_run_time,\n+                        \"avg_memory_usage\": self.result.avg_memory_usage,\n+                        \"min_memory_usage\": self.result.min_memory_usage,\n+                        \"max_memory_usage\": self.result.max_memory_usage,\n+                        \"std_dev_memory_usage\": self.result.std_dev_memory_usage,\n+                        \"median_memory_usage\": self.result.median_memory_usage,\n+                        \"p95_memory_usage\": self.result.p95_memory_usage,\n+                    },\n+                    \"runs\": [\n+                        {\"runtime\": runtime, \"memory\": memory_usage}\n+                        for runtime, memory_usage in zip(self.result.run_times, self.result.memory_usages)\n+                    ],\n+                },\n+                eval_type=EvalType.PERFORMANCE,\n+                name=self.name if self.name is not None else None,\n+                evaluated_entity_name=self.func.__name__,\n+            )\n+\n         logger.debug(f\"*********** Evaluation End: {self.eval_id} ***********\")\n         return self.result\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/agno/eval/reliability.py",
            "diff": "diff --git a/libs/agno/agno/eval/reliability.py b/libs/agno/agno/eval/reliability.py\nindex d85bcf62f..3d8efb1db 100644\n--- a/libs/agno/agno/eval/reliability.py\n+++ b/libs/agno/agno/eval/reliability.py\n@@ -5,9 +5,11 @@ from uuid import uuid4\n \n if TYPE_CHECKING:\n     from rich.console import Console\n+from dataclasses import asdict\n \n-from agno.eval.utils import store_result_in_file\n-from agno.run.response import RunResponse\n+from agno.agent import RunResponse\n+from agno.api.schemas.evals import EvalType\n+from agno.eval.utils import log_eval_run, store_result_in_file\n from agno.utils.log import logger\n \n \n@@ -56,6 +58,8 @@ class ReliabilityEval:\n     file_path_to_save_results: Optional[str] = None\n     # Enable debug logs\n     debug_mode: bool = getenv(\"AGNO_DEBUG\", \"false\").lower() == \"true\"\n+    # Log the results to the Agno platform. On by default.\n+    monitoring: bool = getenv(\"AGNO_MONITOR\", \"true\").lower() == \"true\"\n \n     def run(self, *, print_results: bool = False) -> Optional[ReliabilityResult]:\n         from rich.console import Console\n@@ -108,5 +112,21 @@ class ReliabilityEval:\n         if self.print_results or print_results:\n             self.result.print_eval(console)\n \n+        # Log results to the Agno platform if requested\n+        if self.monitoring:\n+            log_eval_run(\n+                run_id=self.eval_id,  # type: ignore\n+                run_data=asdict(self.result),\n+                eval_type=EvalType.RELIABILITY,\n+                agent_id=self.agent_response.agent_id if self.agent_response is not None else None,\n+                model_id=self.agent_response.model\n+                if self.agent_response is not None and self.agent_response.model is not None\n+                else None,\n+                model_provider=self.agent_response.model_provider\n+                if self.agent_response is not None and self.agent_response.model_provider is not None\n+                else None,\n+                name=self.name if self.name is not None else None,\n+            )\n+\n         logger.debug(f\"*********** Evaluation End: {self.eval_id} ***********\")\n         return self.result\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/agno/eval/utils.py",
            "diff": "diff --git a/libs/agno/agno/eval/utils.py b/libs/agno/agno/eval/utils.py\nindex 9f1c3e98c..1c632d2f6 100644\n--- a/libs/agno/agno/eval/utils.py\n+++ b/libs/agno/agno/eval/utils.py\n@@ -1,10 +1,10 @@\n-\"\"\"Util logic shared by all eval modules\"\"\"\n-\n from dataclasses import asdict\n from pathlib import Path\n from typing import TYPE_CHECKING, Optional, Union\n \n-from agno.utils.log import logger\n+from agno.api.evals import async_create_eval_run, create_eval_run\n+from agno.api.schemas.evals import EvalRunCreate, EvalType\n+from agno.utils.log import log_debug, logger\n \n if TYPE_CHECKING:\n     from agno.eval.accuracy import AccuracyResult\n@@ -12,6 +12,68 @@ if TYPE_CHECKING:\n     from agno.eval.reliability import ReliabilityResult\n \n \n+def log_eval_run(\n+    run_id: str,\n+    run_data: dict,\n+    eval_type: EvalType,\n+    agent_id: Optional[str] = None,\n+    model_id: Optional[str] = None,\n+    model_provider: Optional[str] = None,\n+    name: Optional[str] = None,\n+    evaluated_entity_name: Optional[str] = None,\n+    team_id: Optional[str] = None,\n+) -> None:\n+    \"\"\"Call the API to create an evaluation run.\"\"\"\n+\n+    try:\n+        create_eval_run(\n+            eval_run=EvalRunCreate(\n+                run_id=run_id,\n+                eval_type=eval_type,\n+                eval_data=run_data,\n+                agent_id=agent_id,\n+                model_id=model_id,\n+                model_provider=model_provider,\n+                name=name,\n+                evaluated_entity_name=evaluated_entity_name,\n+                team_id=team_id,\n+            )\n+        )\n+    except Exception as e:\n+        log_debug(f\"Could not create agent event: {e}\")\n+\n+\n+async def async_log_eval_run(\n+    run_id: str,\n+    run_data: dict,\n+    eval_type: EvalType,\n+    agent_id: Optional[str] = None,\n+    model_id: Optional[str] = None,\n+    model_provider: Optional[str] = None,\n+    name: Optional[str] = None,\n+    evaluated_entity_name: Optional[str] = None,\n+    team_id: Optional[str] = None,\n+) -> None:\n+    \"\"\"Asycn call to the API to create an evaluation run.\"\"\"\n+\n+    try:\n+        await async_create_eval_run(\n+            eval_run=EvalRunCreate(\n+                run_id=run_id,\n+                eval_type=eval_type,\n+                eval_data=run_data,\n+                agent_id=agent_id,\n+                model_id=model_id,\n+                model_provider=model_provider,\n+                team_id=team_id,\n+                name=name,\n+                evaluated_entity_name=evaluated_entity_name,\n+            )\n+        )\n+    except Exception as e:\n+        log_debug(f\"Could not create agent event: {e}\")\n+\n+\n def store_result_in_file(\n     file_path: str,\n     result: Union[\"AccuracyResult\", \"PerformanceResult\", \"ReliabilityResult\"],\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/agno/knowledge/wikipedia.py",
            "diff": "diff --git a/libs/agno/agno/knowledge/wikipedia.py b/libs/agno/agno/knowledge/wikipedia.py\nindex 55c7c3388..0134bed0f 100644\n--- a/libs/agno/agno/knowledge/wikipedia.py\n+++ b/libs/agno/agno/knowledge/wikipedia.py\n@@ -11,6 +11,7 @@ except ImportError:\n \n class WikipediaKnowledgeBase(AgentKnowledge):\n     topics: List[str] = []\n+    auto_suggest: bool = True\n \n     @property\n     def document_lists(self) -> Iterator[List[Document]]:\n@@ -26,6 +27,6 @@ class WikipediaKnowledgeBase(AgentKnowledge):\n                 Document(\n                     name=topic,\n                     meta_data={\"topic\": topic},\n-                    content=wikipedia.summary(topic),\n+                    content=wikipedia.summary(topic, auto_suggest=self.auto_suggest),\n                 )\n             ]\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/agno/models/aws/bedrock.py",
            "diff": "diff --git a/libs/agno/agno/models/aws/bedrock.py b/libs/agno/agno/models/aws/bedrock.py\nindex d233ca468..61c7a6c2c 100644\n--- a/libs/agno/agno/models/aws/bedrock.py\n+++ b/libs/agno/agno/models/aws/bedrock.py\n@@ -6,7 +6,7 @@ from typing import Any, Dict, Iterator, List, Optional, Tuple, Type, Union\n from pydantic import BaseModel\n \n from agno.exceptions import AgnoError, ModelProviderError\n-from agno.models.base import MessageData, Model\n+from agno.models.base import MessageData, Model, _add_usage_metrics_to_assistant_message\n from agno.models.message import Message\n from agno.models.response import ModelResponse\n from agno.utils.log import log_error, log_warning\n@@ -473,7 +473,7 @@ class AwsBedrock(Model):\n                 should_yield = True\n \n             if model_response.response_usage is not None:\n-                self._add_usage_metrics_to_assistant_message(\n+                _add_usage_metrics_to_assistant_message(\n                     assistant_message=assistant_message, response_usage=model_response.response_usage\n                 )\n \n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/agno/models/base.py",
            "diff": "diff --git a/libs/agno/agno/models/base.py b/libs/agno/agno/models/base.py\nindex 4e4ebc613..bc4f01bb5 100644\n--- a/libs/agno/agno/models/base.py\n+++ b/libs/agno/agno/models/base.py\n@@ -11,11 +11,11 @@ from pydantic import BaseModel\n from agno.exceptions import AgentRunException\n from agno.media import AudioResponse, ImageArtifact\n from agno.models.message import Citations, Message, MessageMetrics\n-from agno.models.response import ModelResponse, ModelResponseEvent\n-from agno.tools.function import Function, FunctionCall\n+from agno.models.response import ModelResponse, ModelResponseEvent, ToolExecution\n+from agno.tools.function import Function, FunctionCall, FunctionExecutionResult\n from agno.utils.log import log_debug, log_error, log_warning\n from agno.utils.timer import Timer\n-from agno.utils.tools import get_function_call_for_tool_call\n+from agno.utils.tools import get_function_call_for_tool_call, get_function_call_for_tool_execution\n \n \n @dataclass\n@@ -36,6 +36,165 @@ class MessageData:\n     extra: Optional[Dict[str, Any]] = None\n \n \n+def _log_messages(messages: List[Message]) -> None:\n+    \"\"\"\n+    Log messages for debugging.\n+    \"\"\"\n+    for m in messages:\n+        # Don't log metrics for input messages\n+        m.log(metrics=False)\n+\n+\n+def _add_usage_metrics_to_assistant_message(assistant_message: Message, response_usage: Any) -> None:\n+    \"\"\"\n+    Add usage metrics from the model provider to the assistant message.\n+\n+    Args:\n+        assistant_message: Message to update with metrics\n+        response_usage: Usage data from model provider\n+    \"\"\"\n+\n+    # Standard token metrics\n+    if isinstance(response_usage, dict):\n+        if \"input_tokens\" in response_usage:\n+            assistant_message.metrics.input_tokens = response_usage.get(\"input_tokens\", 0)\n+        if \"output_tokens\" in response_usage:\n+            assistant_message.metrics.output_tokens = response_usage.get(\"output_tokens\", 0)\n+        if \"prompt_tokens\" in response_usage:\n+            assistant_message.metrics.input_tokens = response_usage.get(\"prompt_tokens\", 0)\n+        if \"completion_tokens\" in response_usage:\n+            assistant_message.metrics.output_tokens = response_usage.get(\"completion_tokens\", 0)\n+        if \"total_tokens\" in response_usage:\n+            assistant_message.metrics.total_tokens = response_usage.get(\"total_tokens\", 0)\n+        if \"cached_tokens\" in response_usage:\n+            assistant_message.metrics.cached_tokens = response_usage.get(\"cached_tokens\", 0)\n+        else:\n+            assistant_message.metrics.total_tokens = (\n+                assistant_message.metrics.input_tokens + assistant_message.metrics.output_tokens\n+            )\n+    else:\n+        if hasattr(response_usage, \"input_tokens\") and response_usage.input_tokens:\n+            assistant_message.metrics.input_tokens = response_usage.input_tokens\n+        if hasattr(response_usage, \"output_tokens\") and response_usage.output_tokens:\n+            assistant_message.metrics.output_tokens = response_usage.output_tokens\n+        if hasattr(response_usage, \"prompt_tokens\") and response_usage.prompt_tokens is not None:\n+            assistant_message.metrics.input_tokens = response_usage.prompt_tokens\n+            assistant_message.metrics.prompt_tokens = response_usage.prompt_tokens\n+        if hasattr(response_usage, \"completion_tokens\") and response_usage.completion_tokens is not None:\n+            assistant_message.metrics.output_tokens = response_usage.completion_tokens\n+            assistant_message.metrics.completion_tokens = response_usage.completion_tokens\n+        if hasattr(response_usage, \"total_tokens\") and response_usage.total_tokens is not None:\n+            assistant_message.metrics.total_tokens = response_usage.total_tokens\n+        if hasattr(response_usage, \"cached_tokens\") and response_usage.cached_tokens is not None:\n+            assistant_message.metrics.cached_tokens = response_usage.cached_tokens\n+        else:\n+            assistant_message.metrics.total_tokens = (\n+                assistant_message.metrics.input_tokens + assistant_message.metrics.output_tokens\n+            )\n+\n+    # Additional metrics (e.g., from Groq, Ollama)\n+    if isinstance(response_usage, dict) and \"additional_metrics\" in response_usage:\n+        assistant_message.metrics.additional_metrics = response_usage[\"additional_metrics\"]\n+\n+    # Token details (e.g., from OpenAI)\n+    if hasattr(response_usage, \"prompt_tokens_details\"):\n+        if isinstance(response_usage.prompt_tokens_details, dict):\n+            assistant_message.metrics.prompt_tokens_details = response_usage.prompt_tokens_details\n+            if (\n+                \"audio_tokens\" in response_usage.prompt_tokens_details\n+                and response_usage.prompt_tokens_details[\"audio_tokens\"] is not None\n+            ):\n+                assistant_message.metrics.input_audio_tokens = response_usage.prompt_tokens_details[\"audio_tokens\"]\n+            if (\n+                \"cached_tokens\" in response_usage.prompt_tokens_details\n+                and response_usage.prompt_tokens_details[\"cached_tokens\"] is not None\n+            ):\n+                assistant_message.metrics.cached_tokens = response_usage.prompt_tokens_details[\"cached_tokens\"]\n+        elif hasattr(response_usage.prompt_tokens_details, \"model_dump\"):\n+            assistant_message.metrics.prompt_tokens_details = response_usage.prompt_tokens_details.model_dump(\n+                exclude_none=True\n+            )\n+            if (\n+                hasattr(response_usage.prompt_tokens_details, \"audio_tokens\")\n+                and response_usage.prompt_tokens_details.audio_tokens is not None\n+            ):\n+                assistant_message.metrics.input_audio_tokens = response_usage.prompt_tokens_details.audio_tokens\n+            if (\n+                hasattr(response_usage.prompt_tokens_details, \"cached_tokens\")\n+                and response_usage.prompt_tokens_details.cached_tokens is not None\n+            ):\n+                assistant_message.metrics.cached_tokens = response_usage.prompt_tokens_details.cached_tokens\n+\n+    if hasattr(response_usage, \"completion_tokens_details\"):\n+        if isinstance(response_usage.completion_tokens_details, dict):\n+            assistant_message.metrics.completion_tokens_details = response_usage.completion_tokens_details\n+            if (\n+                \"audio_tokens\" in response_usage.completion_tokens_details\n+                and response_usage.completion_tokens_details[\"audio_tokens\"] is not None\n+            ):\n+                assistant_message.metrics.output_audio_tokens = response_usage.completion_tokens_details[\"audio_tokens\"]\n+            if (\n+                \"reasoning_tokens\" in response_usage.completion_tokens_details\n+                and response_usage.completion_tokens_details[\"reasoning_tokens\"] is not None\n+            ):\n+                assistant_message.metrics.reasoning_tokens = response_usage.completion_tokens_details[\n+                    \"reasoning_tokens\"\n+                ]\n+        elif hasattr(response_usage.completion_tokens_details, \"model_dump\"):\n+            assistant_message.metrics.completion_tokens_details = response_usage.completion_tokens_details.model_dump(\n+                exclude_none=True\n+            )\n+            if (\n+                hasattr(response_usage.completion_tokens_details, \"audio_tokens\")\n+                and response_usage.completion_tokens_details.audio_tokens is not None\n+            ):\n+                assistant_message.metrics.output_audio_tokens = response_usage.completion_tokens_details.audio_tokens\n+            if (\n+                hasattr(response_usage.completion_tokens_details, \"reasoning_tokens\")\n+                and response_usage.completion_tokens_details.reasoning_tokens is not None\n+            ):\n+                assistant_message.metrics.reasoning_tokens = response_usage.completion_tokens_details.reasoning_tokens\n+\n+    assistant_message.metrics.audio_tokens = (\n+        assistant_message.metrics.input_audio_tokens + assistant_message.metrics.output_audio_tokens\n+    )\n+\n+\n+def _handle_agent_exception(a_exc: AgentRunException, additional_messages: Optional[List[Message]] = None) -> None:\n+    \"\"\"Handle AgentRunException and collect additional messages.\"\"\"\n+    if additional_messages is None:\n+        additional_messages = []\n+    if a_exc.user_message is not None:\n+        msg = (\n+            Message(role=\"user\", content=a_exc.user_message)\n+            if isinstance(a_exc.user_message, str)\n+            else a_exc.user_message\n+        )\n+        additional_messages.append(msg)\n+\n+    if a_exc.agent_message is not None:\n+        msg = (\n+            Message(role=\"assistant\", content=a_exc.agent_message)\n+            if isinstance(a_exc.agent_message, str)\n+            else a_exc.agent_message\n+        )\n+        additional_messages.append(msg)\n+\n+    if a_exc.messages:\n+        for m in a_exc.messages:\n+            if isinstance(m, Message):\n+                additional_messages.append(m)\n+            elif isinstance(m, dict):\n+                try:\n+                    additional_messages.append(Message(**m))\n+                except Exception as e:\n+                    log_warning(f\"Failed to convert dict to Message: {e}\")\n+\n+    if a_exc.stop_execution:\n+        for m in additional_messages:\n+            m.stop_after_tool_call = True\n+\n+\n @dataclass\n class Model(ABC):\n     # ID of the model to use.\n@@ -144,7 +303,7 @@ class Model(ABC):\n         log_debug(f\"{self.get_provider()} Response Start\", center=True, symbol=\"-\")\n         log_debug(f\"Model: {self.id}\", center=True, symbol=\"-\")\n \n-        self._log_messages(messages)\n+        _log_messages(messages)\n         model_response = ModelResponse()\n \n         while True:\n@@ -175,10 +334,18 @@ class Model(ABC):\n                     tool_call_limit=tool_call_limit,\n                 ):\n                     if (\n-                        function_call_response.event == ModelResponseEvent.tool_call_completed.value\n-                        and function_call_response.tool_calls is not None\n+                        function_call_response.event\n+                        in [\n+                            ModelResponseEvent.tool_call_completed.value,\n+                            ModelResponseEvent.tool_call_confirmation_required.value,\n+                            ModelResponseEvent.tool_call_external_execution_required.value,\n+                        ]\n+                        and function_call_response.tool_executions is not None\n                     ):\n-                        model_response.tool_calls.extend(function_call_response.tool_calls)\n+                        if model_response.tool_executions is None:\n+                            model_response.tool_executions = []\n+                        model_response.tool_executions.extend(function_call_response.tool_executions)\n+\n                     elif function_call_response.event not in [\n                         ModelResponseEvent.tool_call_started.value,\n                         ModelResponseEvent.tool_call_completed.value,\n@@ -197,6 +364,14 @@ class Model(ABC):\n                 if any(m.stop_after_tool_call for m in function_call_results):\n                     break\n \n+                # If we have any tool calls that require confirmation, break the loop\n+                if any(tc.requires_confirmation for tc in model_response.tool_executions or []):\n+                    break\n+\n+                # If we have any tool calls that require external execution, break the loop\n+                if any(tc.external_execution_required for tc in model_response.tool_executions or []):\n+                    break\n+\n                 # Continue loop to get next response\n                 continue\n \n@@ -221,7 +396,7 @@ class Model(ABC):\n \n         log_debug(f\"{self.get_provider()} Async Response Start\", center=True, symbol=\"-\")\n         log_debug(f\"Model: {self.id}\", center=True, symbol=\"-\")\n-        self._log_messages(messages)\n+        _log_messages(messages)\n         model_response = ModelResponse()\n \n         while True:\n@@ -252,16 +427,23 @@ class Model(ABC):\n                     tool_call_limit=tool_call_limit,\n                 ):\n                     if (\n-                        function_call_response.event == ModelResponseEvent.tool_call_completed.value\n-                        and function_call_response.tool_calls is not None\n+                        function_call_response.event\n+                        in [\n+                            ModelResponseEvent.tool_call_completed.value,\n+                            ModelResponseEvent.tool_call_confirmation_required.value,\n+                            ModelResponseEvent.tool_call_external_execution_required.value,\n+                        ]\n+                        and function_call_response.tool_executions is not None\n                     ):\n-                        model_response.tool_calls.extend(function_call_response.tool_calls)\n+                        if model_response.tool_executions is None:\n+                            model_response.tool_executions = []\n+                        model_response.tool_executions.extend(function_call_response.tool_executions)\n                     elif function_call_response.event not in [\n                         ModelResponseEvent.tool_call_started.value,\n                         ModelResponseEvent.tool_call_completed.value,\n                     ]:\n                         if function_call_response.content:\n-                            model_response.content += function_call_response.content\n+                            model_response.content += function_call_response.content  # type: ignore\n \n                 # Format and add results to messages\n                 self.format_function_call_results(\n@@ -274,6 +456,14 @@ class Model(ABC):\n                 if any(m.stop_after_tool_call for m in function_call_results):\n                     break\n \n+                # If we have any tool calls that require confirmation, break the loop\n+                if any(tc.requires_confirmation for tc in model_response.tool_executions or []):\n+                    break\n+\n+                # If we have any tool calls that require external execution, break the loop\n+                if any(tc.external_execution_required for tc in model_response.tool_executions or []):\n+                    break\n+\n                 # Continue loop to get next response\n                 continue\n \n@@ -472,7 +662,7 @@ class Model(ABC):\n \n         # Add usage metrics if provided\n         if provider_response.response_usage is not None:\n-            self._add_usage_metrics_to_assistant_message(\n+            _add_usage_metrics_to_assistant_message(\n                 assistant_message=assistant_message, response_usage=provider_response.response_usage\n             )\n \n@@ -498,7 +688,7 @@ class Model(ABC):\n         ):\n             model_response_delta = self.parse_provider_response_delta(response_delta)\n             yield from self._populate_stream_data_and_assistant_message(\n-                stream_data=stream_data, assistant_message=assistant_message, model_response=model_response_delta\n+                stream_data=stream_data, assistant_message=assistant_message, model_response_delta=model_response_delta\n             )\n \n     def response_stream(\n@@ -516,7 +706,7 @@ class Model(ABC):\n \n         log_debug(f\"{self.get_provider()} Response Stream Start\", center=True, symbol=\"-\")\n         log_debug(f\"Model: {self.id}\", center=True, symbol=\"-\")\n-        self._log_messages(messages)\n+        _log_messages(messages)\n \n         while True:\n             # Create assistant message and stream data\n@@ -586,6 +776,14 @@ class Model(ABC):\n                 if any(m.stop_after_tool_call for m in function_call_results):\n                     break\n \n+                # If we have any tool calls that require confirmation, break the loop\n+                if any(fc.function.requires_confirmation for fc in function_calls_to_run):\n+                    break\n+\n+                # If we have any tool calls that require external execution, break the loop\n+                if any(fc.function.external_execution for fc in function_calls_to_run):\n+                    break\n+\n                 # Continue loop to get next response\n                 continue\n \n@@ -614,7 +812,7 @@ class Model(ABC):\n         ):  # type: ignore\n             model_response_delta = self.parse_provider_response_delta(response_delta)\n             for model_response in self._populate_stream_data_and_assistant_message(\n-                stream_data=stream_data, assistant_message=assistant_message, model_response=model_response_delta\n+                stream_data=stream_data, assistant_message=assistant_message, model_response_delta=model_response_delta\n             ):\n                 yield model_response\n \n@@ -633,7 +831,7 @@ class Model(ABC):\n \n         log_debug(f\"{self.get_provider()} Async Response Stream Start\", center=True, symbol=\"-\")\n         log_debug(f\"Model: {self.id}\", center=True, symbol=\"-\")\n-        self._log_messages(messages)\n+        _log_messages(messages)\n \n         while True:\n             # Create assistant message and stream data\n@@ -702,6 +900,14 @@ class Model(ABC):\n                 if any(m.stop_after_tool_call for m in function_call_results):\n                     break\n \n+                # If we have any tool calls that require confirmation, break the loop\n+                if any(fc.function.requires_confirmation for fc in function_calls_to_run):\n+                    break\n+\n+                # If we have any tool calls that require external execution, break the loop\n+                if any(fc.function.external_execution for fc in function_calls_to_run):\n+                    break\n+\n                 # Continue loop to get next response\n                 continue\n \n@@ -711,7 +917,7 @@ class Model(ABC):\n         log_debug(f\"{self.get_provider()} Async Response Stream End\", center=True, symbol=\"-\")\n \n     def _populate_stream_data_and_assistant_message(\n-        self, stream_data: MessageData, assistant_message: Message, model_response: ModelResponse\n+        self, stream_data: MessageData, assistant_message: Message, model_response_delta: ModelResponse\n     ) -> Iterator[ModelResponse]:\n         \"\"\"Update the stream data and assistant message with the model response.\"\"\"\n \n@@ -720,75 +926,75 @@ class Model(ABC):\n             assistant_message.metrics.set_time_to_first_token()\n \n         # Add role to assistant message\n-        if model_response.role is not None:\n-            assistant_message.role = model_response.role\n+        if model_response_delta.role is not None:\n+            assistant_message.role = model_response_delta.role\n \n         should_yield = False\n         # Update stream_data content\n-        if model_response.content is not None:\n-            stream_data.response_content += model_response.content\n+        if model_response_delta.content is not None:\n+            stream_data.response_content += model_response_delta.content\n             should_yield = True\n \n-        if model_response.thinking is not None:\n-            stream_data.response_thinking += model_response.thinking\n+        if model_response_delta.thinking is not None:\n+            stream_data.response_thinking += model_response_delta.thinking\n             should_yield = True\n \n-        if model_response.redacted_thinking is not None:\n-            stream_data.response_redacted_thinking += model_response.redacted_thinking\n+        if model_response_delta.redacted_thinking is not None:\n+            stream_data.response_redacted_thinking += model_response_delta.redacted_thinking\n             should_yield = True\n \n-        if model_response.citations is not None:\n-            stream_data.response_citations = model_response.citations\n+        if model_response_delta.citations is not None:\n+            stream_data.response_citations = model_response_delta.citations\n             should_yield = True\n \n-        if model_response.provider_data:\n+        if model_response_delta.provider_data:\n             if stream_data.response_provider_data is None:\n                 stream_data.response_provider_data = {}\n-            stream_data.response_provider_data.update(model_response.provider_data)\n+            stream_data.response_provider_data.update(model_response_delta.provider_data)\n \n         # Update stream_data tool calls\n-        if model_response.tool_calls is not None:\n+        if model_response_delta.tool_calls is not None:\n             if stream_data.response_tool_calls is None:\n                 stream_data.response_tool_calls = []\n-            stream_data.response_tool_calls.extend(model_response.tool_calls)\n+            stream_data.response_tool_calls.extend(model_response_delta.tool_calls)\n             should_yield = True\n \n-        if model_response.audio is not None:\n+        if model_response_delta.audio is not None:\n             if stream_data.response_audio is None:\n                 stream_data.response_audio = AudioResponse(id=str(uuid4()), content=\"\", transcript=\"\")\n \n             # Update the stream data with audio information\n-            if model_response.audio.id is not None:\n-                stream_data.response_audio.id = model_response.audio.id  # type: ignore\n-            if model_response.audio.content is not None:\n-                stream_data.response_audio.content += model_response.audio.content  # type: ignore\n-            if model_response.audio.transcript is not None:\n-                stream_data.response_audio.transcript += model_response.audio.transcript  # type: ignore\n-            if model_response.audio.expires_at is not None:\n-                stream_data.response_audio.expires_at = model_response.audio.expires_at\n-            if model_response.audio.mime_type is not None:\n-                stream_data.response_audio.mime_type = model_response.audio.mime_type\n-            stream_data.response_audio.sample_rate = model_response.audio.sample_rate\n-            stream_data.response_audio.channels = model_response.audio.channels\n+            if model_response_delta.audio.id is not None:\n+                stream_data.response_audio.id = model_response_delta.audio.id  # type: ignore\n+            if model_response_delta.audio.content is not None:\n+                stream_data.response_audio.content += model_response_delta.audio.content  # type: ignore\n+            if model_response_delta.audio.transcript is not None:\n+                stream_data.response_audio.transcript += model_response_delta.audio.transcript  # type: ignore\n+            if model_response_delta.audio.expires_at is not None:\n+                stream_data.response_audio.expires_at = model_response_delta.audio.expires_at\n+            if model_response_delta.audio.mime_type is not None:\n+                stream_data.response_audio.mime_type = model_response_delta.audio.mime_type\n+            stream_data.response_audio.sample_rate = model_response_delta.audio.sample_rate\n+            stream_data.response_audio.channels = model_response_delta.audio.channels\n \n             should_yield = True\n \n-        if model_response.image:\n+        if model_response_delta.image:\n             if stream_data.response_image is None:\n-                stream_data.response_image = model_response.image\n+                stream_data.response_image = model_response_delta.image\n \n-        if model_response.extra is not None:\n+        if model_response_delta.extra is not None:\n             if stream_data.extra is None:\n                 stream_data.extra = {}\n-            stream_data.extra.update(model_response.extra)\n+            stream_data.extra.update(model_response_delta.extra)\n \n-        if model_response.response_usage is not None:\n-            self._add_usage_metrics_to_assistant_message(\n-                assistant_message=assistant_message, response_usage=model_response.response_usage\n+        if model_response_delta.response_usage is not None:\n+            _add_usage_metrics_to_assistant_message(\n+                assistant_message=assistant_message, response_usage=model_response_delta.response_usage\n             )\n \n         if should_yield:\n-            yield model_response\n+            yield model_response_delta\n \n     def parse_tool_calls(self, tool_calls_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n         \"\"\"\n@@ -796,6 +1002,19 @@ class Model(ABC):\n         \"\"\"\n         return tool_calls_data\n \n+    def get_function_call_to_run_from_tool_execution(\n+        self,\n+        tool_execution: ToolExecution,\n+        functions: Optional[Dict[str, Function]] = None,\n+    ) -> FunctionCall:\n+        function_call = get_function_call_for_tool_execution(\n+            tool_execution=tool_execution,\n+            functions=functions,\n+        )\n+        if function_call is None:\n+            raise ValueError(\"Function call not found\")\n+        return function_call\n+\n     def get_function_calls_to_run(\n         self,\n         assistant_message: Message,\n@@ -827,38 +1046,6 @@ class Model(ABC):\n                 function_calls_to_run.append(_function_call)\n         return function_calls_to_run\n \n-    def _handle_agent_exception(self, a_exc: AgentRunException, additional_messages: List[Message]) -> None:\n-        \"\"\"Handle AgentRunException and collect additional messages.\"\"\"\n-        if a_exc.user_message is not None:\n-            msg = (\n-                Message(role=\"user\", content=a_exc.user_message)\n-                if isinstance(a_exc.user_message, str)\n-                else a_exc.user_message\n-            )\n-            additional_messages.append(msg)\n-\n-        if a_exc.agent_message is not None:\n-            msg = (\n-                Message(role=\"assistant\", content=a_exc.agent_message)\n-                if isinstance(a_exc.agent_message, str)\n-                else a_exc.agent_message\n-            )\n-            additional_messages.append(msg)\n-\n-        if a_exc.messages:\n-            for m in a_exc.messages:\n-                if isinstance(m, Message):\n-                    additional_messages.append(m)\n-                elif isinstance(m, dict):\n-                    try:\n-                        additional_messages.append(Message(**m))\n-                    except Exception as e:\n-                        log_warning(f\"Failed to convert dict to Message: {e}\")\n-\n-        if a_exc.stop_execution:\n-            for m in additional_messages:\n-                m.stop_after_tool_call = True\n-\n     def _create_function_call_result(\n         self, fc: FunctionCall, success: bool, output: Optional[Union[List[Any], str]], timer: Timer\n     ) -> Message:\n@@ -874,92 +1061,147 @@ class Model(ABC):\n             metrics=MessageMetrics(time=timer.elapsed),\n         )\n \n+    def run_function_call(\n+        self,\n+        function_call: FunctionCall,\n+        function_call_results: List[Message],\n+        additional_messages: Optional[List[Message]] = None,\n+    ) -> Iterator[ModelResponse]:\n+        # Start function call\n+        function_call_timer = Timer()\n+        function_call_timer.start()\n+        # Yield a tool_call_started event\n+        yield ModelResponse(\n+            content=function_call.get_call_str(),\n+            tool_executions=[\n+                ToolExecution(\n+                    tool_call_id=function_call.call_id,\n+                    tool_name=function_call.function.name,\n+                    tool_args=function_call.arguments,\n+                )\n+            ],\n+            event=ModelResponseEvent.tool_call_started.value,\n+        )\n+\n+        # Run function calls sequentially\n+        function_execution_result: FunctionExecutionResult = FunctionExecutionResult(status=\"failure\")\n+        try:\n+            function_execution_result = function_call.execute()\n+        except AgentRunException as a_exc:\n+            # Update additional messages from function call\n+            _handle_agent_exception(a_exc, additional_messages)\n+            # Set function call success to False if an exception occurred\n+        except Exception as e:\n+            log_error(f\"Error executing function {function_call.function.name}: {e}\")\n+            raise e\n+\n+        function_call_success = function_execution_result.status == \"success\"\n+\n+        # Stop function call timer\n+        function_call_timer.stop()\n+\n+        # Process function call output\n+        function_call_output: str = \"\"\n+\n+        if isinstance(function_call.result, (GeneratorType, collections.abc.Iterator)):\n+            for item in function_call.result:\n+                function_call_output += str(item)\n+                if function_call.function.show_result:\n+                    yield ModelResponse(content=str(item))\n+        else:\n+            function_call_output = str(function_call.result)\n+            if function_call.function.show_result:\n+                yield ModelResponse(content=function_call_output)\n+\n+        # Create and yield function call result\n+        function_call_result = self._create_function_call_result(\n+            function_call, success=function_call_success, output=function_call_output, timer=function_call_timer\n+        )\n+        yield ModelResponse(\n+            content=f\"{function_call.get_call_str()} completed in {function_call_timer.elapsed:.4f}s.\",\n+            tool_executions=[\n+                ToolExecution(\n+                    tool_call_id=function_call_result.tool_call_id,\n+                    tool_name=function_call_result.tool_name,\n+                    tool_args=function_call_result.tool_args,\n+                    tool_call_error=function_call_result.tool_call_error,\n+                    result=str(function_call_result.content),\n+                    stop_after_tool_call=function_call_result.stop_after_tool_call,\n+                    metrics=function_call_result.metrics,\n+                )\n+            ],\n+            event=ModelResponseEvent.tool_call_completed.value,\n+        )\n+\n+        # Add function call to function call results\n+        function_call_results.append(function_call_result)\n+\n     def run_function_calls(\n         self,\n         function_calls: List[FunctionCall],\n         function_call_results: List[Message],\n         tool_call_limit: Optional[int] = None,\n+        additional_messages: Optional[List[Message]] = None,\n     ) -> Iterator[ModelResponse]:\n         if self._function_call_stack is None:\n             self._function_call_stack = []\n \n         # Additional messages from function calls that will be added to the function call results\n-        additional_messages: List[Message] = []\n+        if additional_messages is None:\n+            additional_messages = []\n \n         for fc in function_calls:\n-            # Start function call\n-            function_call_timer = Timer()\n-            function_call_timer.start()\n-            # Yield a tool_call_started event\n-            yield ModelResponse(\n-                content=fc.get_call_str(),\n-                tool_calls=[\n-                    {\n-                        \"role\": self.tool_message_role,\n-                        \"tool_call_id\": fc.call_id,\n-                        \"tool_name\": fc.function.name,\n-                        \"tool_args\": fc.arguments,\n-                    }\n-                ],\n-                event=ModelResponseEvent.tool_call_started.value,\n-            )\n-\n-            # Track if the function call was successful\n-            function_call_success = False\n-            # Run function calls sequentially\n-            try:\n-                function_call_success = fc.execute()\n-            except AgentRunException as a_exc:\n-                # Update additional messages from function call\n-                self._handle_agent_exception(a_exc, additional_messages)\n-                # Set function call success to False if an exception occurred\n-                function_call_success = False\n-            except Exception as e:\n-                log_error(f\"Error executing function {fc.function.name}: {e}\")\n-                raise e\n-\n-            # Stop function call timer\n-            function_call_timer.stop()\n-\n-            # Process function call output\n-            function_call_output: str = \"\"\n+            # The function cannot be executed without user confirmation\n+            if fc.function.requires_confirmation:\n+                yield ModelResponse(\n+                    tool_executions=[\n+                        ToolExecution(\n+                            tool_call_id=fc.call_id,\n+                            tool_name=fc.function.name,\n+                            tool_args=fc.arguments,\n+                            requires_confirmation=True,\n+                        )\n+                    ],\n+                    event=ModelResponseEvent.tool_call_confirmation_required.value,\n+                )\n+                # We don't execute the function call here, we wait for the user to confirm\n+                continue\n \n-            if isinstance(fc.result, (GeneratorType, collections.abc.Iterator)):\n-                for item in fc.result:\n-                    function_call_output += str(item)\n-                    if fc.function.show_result:\n-                        yield ModelResponse(content=str(item))\n-            else:\n-                function_call_output = str(fc.result)\n-                if fc.function.show_result:\n-                    yield ModelResponse(content=function_call_output)\n+            # If the function requires external execution, we yield a message to the user\n+            if fc.function.external_execution:\n+                yield ModelResponse(\n+                    tool_executions=[\n+                        ToolExecution(\n+                            tool_call_id=fc.call_id,\n+                            tool_name=fc.function.name,\n+                            tool_args=fc.arguments,\n+                            external_execution_required=True,\n+                        )\n+                    ],\n+                    event=ModelResponseEvent.tool_call_external_execution_required.value,\n+                )\n+                # We don't execute the function call here, it is executed outside of the agent's control\n+                continue\n \n-            # Create and yield function call result\n-            function_call_result = self._create_function_call_result(\n-                fc, success=function_call_success, output=function_call_output, timer=function_call_timer\n-            )\n-            yield ModelResponse(\n-                content=f\"{fc.get_call_str()} completed in {function_call_timer.elapsed:.4f}s.\",\n-                tool_calls=[function_call_result.to_function_call_dict()],\n-                event=ModelResponseEvent.tool_call_completed.value,\n+            yield from self.run_function_call(\n+                function_call=fc, function_call_results=function_call_results, additional_messages=additional_messages\n             )\n \n-            # Add function call to function call results\n-            function_call_results.append(function_call_result)\n+            # Add function call result to function call results\n             self._function_call_stack.append(fc)\n \n             # Check function call limit\n             if tool_call_limit and len(self._function_call_stack) >= tool_call_limit:\n-                # Deactivate tool calls by setting future tool calls to \"none\"\n                 self._tool_choice = \"none\"\n-                break  # Exit early if we reach the function call limit\n+                break\n \n         # Add any additional messages at the end\n         if additional_messages:\n             function_call_results.extend(additional_messages)\n \n-    async def _arun_function_call(\n-        self, function_call: FunctionCall\n+    async def arun_function_call(\n+        self,\n+        function_call: FunctionCall,\n     ) -> Tuple[Union[bool, AgentRunException], Timer, FunctionCall]:\n         \"\"\"Run a single function call and return its success status, timer, and the FunctionCall object.\"\"\"\n         from inspect import isasyncgenfunction, iscoroutine, iscoroutinefunction\n@@ -974,16 +1216,20 @@ class Model(ABC):\n                 or isasyncgenfunction(function_call.function.entrypoint)\n                 or iscoroutine(function_call.function.entrypoint)\n             ):\n-                success = await function_call.aexecute()\n+                result = await function_call.aexecute()\n+                success = result.status == \"success\"\n+\n             # If any of the hooks are async, we need to run the function call asynchronously\n             elif function_call.function.tool_hooks is not None and any(\n                 iscoroutinefunction(f) for f in function_call.function.tool_hooks\n             ):\n-                success = await function_call.aexecute()\n+                result = await function_call.aexecute()\n+                success = result.status == \"success\"\n             else:\n-                success = await asyncio.to_thread(function_call.execute)\n+                result = await asyncio.to_thread(function_call.execute)\n+                success = result.status == \"success\"\n         except AgentRunException as e:\n-            success = e  # Pass the exception through to be handled by caller\n+            success = e\n         except Exception as e:\n             log_error(f\"Error executing function {function_call.function.name}: {e}\")\n             success = False\n@@ -997,30 +1243,68 @@ class Model(ABC):\n         function_calls: List[FunctionCall],\n         function_call_results: List[Message],\n         tool_call_limit: Optional[int] = None,\n-    ):\n+        additional_messages: Optional[List[Message]] = None,\n+    ) -> AsyncIterator[ModelResponse]:\n         if self._function_call_stack is None:\n             self._function_call_stack = []\n \n         # Additional messages from function calls that will be added to the function call results\n-        additional_messages: List[Message] = []\n+        if additional_messages is None:\n+            additional_messages = []\n \n         # Yield tool_call_started events for all function calls\n         for fc in function_calls:\n+            # Function cannot be executed without user confirmation\n+            if fc.function.requires_confirmation:\n+                yield ModelResponse(\n+                    tool_executions=[\n+                        ToolExecution(\n+                            tool_call_id=fc.call_id,\n+                            tool_name=fc.function.name,\n+                            tool_args=fc.arguments,\n+                            requires_confirmation=True,\n+                        )\n+                    ],\n+                    event=ModelResponseEvent.tool_call_confirmation_required.value,\n+                )\n+                # We don't execute the function call here, we wait for the user to confirm\n+                continue\n+\n+            # If the function requires external execution, we yield a message to the user\n+            if fc.function.external_execution:\n+                yield ModelResponse(\n+                    tool_executions=[\n+                        ToolExecution(\n+                            tool_call_id=fc.call_id,\n+                            tool_name=fc.function.name,\n+                            tool_args=fc.arguments,\n+                            external_execution_required=True,\n+                        )\n+                    ],\n+                    event=ModelResponseEvent.tool_call_external_execution_required.value,\n+                )\n+                # We don't execute the function call here, it is executed outside of the agent's control\n+                continue\n+\n             yield ModelResponse(\n                 content=fc.get_call_str(),\n-                tool_calls=[\n-                    {\n-                        \"role\": self.tool_message_role,\n-                        \"tool_call_id\": fc.call_id,\n-                        \"tool_name\": fc.function.name,\n-                        \"tool_args\": fc.arguments,\n-                    }\n+                tool_executions=[\n+                    ToolExecution(\n+                        tool_call_id=fc.call_id,\n+                        tool_name=fc.function.name,\n+                        tool_args=fc.arguments,\n+                    )\n                 ],\n                 event=ModelResponseEvent.tool_call_started.value,\n             )\n \n-        # Create and run all function calls in parallel\n-        results = await asyncio.gather(*(self._arun_function_call(fc) for fc in function_calls), return_exceptions=True)\n+        # Create and run all function calls in parallel (skip ones that need confirmation)\n+        function_calls_to_run = [\n+            fc for fc in function_calls if not (fc.function.requires_confirmation or fc.function.external_execution)\n+        ]\n+        results = await asyncio.gather(\n+            *(self.arun_function_call(fc) for fc in function_calls_to_run), return_exceptions=True\n+        )\n \n         # Process results\n         for result in results:\n@@ -1036,7 +1320,7 @@ class Model(ABC):\n             if isinstance(function_call_success, AgentRunException):\n                 a_exc = function_call_success\n                 # Update additional messages from function call\n-                self._handle_agent_exception(a_exc, additional_messages)\n+                _handle_agent_exception(a_exc, additional_messages)\n                 # Set function call success to False if an exception occurred\n                 function_call_success = False\n \n@@ -1063,7 +1347,17 @@ class Model(ABC):\n             )\n             yield ModelResponse(\n                 content=f\"{fc.get_call_str()} completed in {function_call_timer.elapsed:.4f}s.\",\n-                tool_calls=[function_call_result.to_function_call_dict()],\n+                tool_executions=[\n+                    ToolExecution(\n+                        tool_call_id=function_call_result.tool_call_id,\n+                        tool_name=function_call_result.tool_name,\n+                        tool_args=function_call_result.tool_args,\n+                        tool_call_error=function_call_result.tool_call_error,\n+                        result=str(function_call_result.content),\n+                        stop_after_tool_call=function_call_result.stop_after_tool_call,\n+                        metrics=function_call_result.metrics,\n+                    )\n+                ],\n                 event=ModelResponseEvent.tool_call_completed.value,\n             )\n \n@@ -1109,134 +1403,6 @@ class Model(ABC):\n         if len(function_call_results) > 0:\n             messages.extend(function_call_results)\n \n-    def _add_usage_metrics_to_assistant_message(self, assistant_message: Message, response_usage: Any) -> None:\n-        \"\"\"\n-        Add usage metrics from the model provider to the assistant message.\n-\n-        Args:\n-            assistant_message: Message to update with metrics\n-            response_usage: Usage data from model provider\n-        \"\"\"\n-\n-        # Standard token metrics\n-        if isinstance(response_usage, dict):\n-            if \"input_tokens\" in response_usage:\n-                assistant_message.metrics.input_tokens = response_usage.get(\"input_tokens\", 0)\n-            if \"output_tokens\" in response_usage:\n-                assistant_message.metrics.output_tokens = response_usage.get(\"output_tokens\", 0)\n-            if \"prompt_tokens\" in response_usage:\n-                assistant_message.metrics.input_tokens = response_usage.get(\"prompt_tokens\", 0)\n-            if \"completion_tokens\" in response_usage:\n-                assistant_message.metrics.output_tokens = response_usage.get(\"completion_tokens\", 0)\n-            if \"total_tokens\" in response_usage:\n-                assistant_message.metrics.total_tokens = response_usage.get(\"total_tokens\", 0)\n-            if \"cached_tokens\" in response_usage:\n-                assistant_message.metrics.cached_tokens = response_usage.get(\"cached_tokens\", 0)\n-            else:\n-                assistant_message.metrics.total_tokens = (\n-                    assistant_message.metrics.input_tokens + assistant_message.metrics.output_tokens\n-                )\n-        else:\n-            if hasattr(response_usage, \"input_tokens\") and response_usage.input_tokens:\n-                assistant_message.metrics.input_tokens = response_usage.input_tokens\n-            if hasattr(response_usage, \"output_tokens\") and response_usage.output_tokens:\n-                assistant_message.metrics.output_tokens = response_usage.output_tokens\n-            if hasattr(response_usage, \"prompt_tokens\") and response_usage.prompt_tokens is not None:\n-                assistant_message.metrics.input_tokens = response_usage.prompt_tokens\n-                assistant_message.metrics.prompt_tokens = response_usage.prompt_tokens\n-            if hasattr(response_usage, \"completion_tokens\") and response_usage.completion_tokens is not None:\n-                assistant_message.metrics.output_tokens = response_usage.completion_tokens\n-                assistant_message.metrics.completion_tokens = response_usage.completion_tokens\n-            if hasattr(response_usage, \"total_tokens\") and response_usage.total_tokens is not None:\n-                assistant_message.metrics.total_tokens = response_usage.total_tokens\n-            if hasattr(response_usage, \"cached_tokens\") and response_usage.cached_tokens is not None:\n-                assistant_message.metrics.cached_tokens = response_usage.cached_tokens\n-            else:\n-                assistant_message.metrics.total_tokens = (\n-                    assistant_message.metrics.input_tokens + assistant_message.metrics.output_tokens\n-                )\n-\n-        # Additional metrics (e.g., from Groq, Ollama)\n-        if isinstance(response_usage, dict) and \"additional_metrics\" in response_usage:\n-            assistant_message.metrics.additional_metrics = response_usage[\"additional_metrics\"]\n-\n-        # Token details (e.g., from OpenAI)\n-        if hasattr(response_usage, \"prompt_tokens_details\"):\n-            if isinstance(response_usage.prompt_tokens_details, dict):\n-                assistant_message.metrics.prompt_tokens_details = response_usage.prompt_tokens_details\n-                if (\n-                    \"audio_tokens\" in response_usage.prompt_tokens_details\n-                    and response_usage.prompt_tokens_details[\"audio_tokens\"] is not None\n-                ):\n-                    assistant_message.metrics.input_audio_tokens = response_usage.prompt_tokens_details[\"audio_tokens\"]\n-                if (\n-                    \"cached_tokens\" in response_usage.prompt_tokens_details\n-                    and response_usage.prompt_tokens_details[\"cached_tokens\"] is not None\n-                ):\n-                    assistant_message.metrics.cached_tokens = response_usage.prompt_tokens_details[\"cached_tokens\"]\n-            elif hasattr(response_usage.prompt_tokens_details, \"model_dump\"):\n-                assistant_message.metrics.prompt_tokens_details = response_usage.prompt_tokens_details.model_dump(\n-                    exclude_none=True\n-                )\n-                if (\n-                    hasattr(response_usage.prompt_tokens_details, \"audio_tokens\")\n-                    and response_usage.prompt_tokens_details.audio_tokens is not None\n-                ):\n-                    assistant_message.metrics.input_audio_tokens = response_usage.prompt_tokens_details.audio_tokens\n-                if (\n-                    hasattr(response_usage.prompt_tokens_details, \"cached_tokens\")\n-                    and response_usage.prompt_tokens_details.cached_tokens is not None\n-                ):\n-                    assistant_message.metrics.cached_tokens = response_usage.prompt_tokens_details.cached_tokens\n-\n-        if hasattr(response_usage, \"completion_tokens_details\"):\n-            if isinstance(response_usage.completion_tokens_details, dict):\n-                assistant_message.metrics.completion_tokens_details = response_usage.completion_tokens_details\n-                if (\n-                    \"audio_tokens\" in response_usage.completion_tokens_details\n-                    and response_usage.completion_tokens_details[\"audio_tokens\"] is not None\n-                ):\n-                    assistant_message.metrics.output_audio_tokens = response_usage.completion_tokens_details[\n-                        \"audio_tokens\"\n-                    ]\n-                if (\n-                    \"reasoning_tokens\" in response_usage.completion_tokens_details\n-                    and response_usage.completion_tokens_details[\"reasoning_tokens\"] is not None\n-                ):\n-                    assistant_message.metrics.reasoning_tokens = response_usage.completion_tokens_details[\n-                        \"reasoning_tokens\"\n-                    ]\n-            elif hasattr(response_usage.completion_tokens_details, \"model_dump\"):\n-                assistant_message.metrics.completion_tokens_details = (\n-                    response_usage.completion_tokens_details.model_dump(exclude_none=True)\n-                )\n-                if (\n-                    hasattr(response_usage.completion_tokens_details, \"audio_tokens\")\n-                    and response_usage.completion_tokens_details.audio_tokens is not None\n-                ):\n-                    assistant_message.metrics.output_audio_tokens = (\n-                        response_usage.completion_tokens_details.audio_tokens\n-                    )\n-                if (\n-                    hasattr(response_usage.completion_tokens_details, \"reasoning_tokens\")\n-                    and response_usage.completion_tokens_details.reasoning_tokens is not None\n-                ):\n-                    assistant_message.metrics.reasoning_tokens = (\n-                        response_usage.completion_tokens_details.reasoning_tokens\n-                    )\n-\n-        assistant_message.metrics.audio_tokens = (\n-            assistant_message.metrics.input_audio_tokens + assistant_message.metrics.output_audio_tokens\n-        )\n-\n-    def _log_messages(self, messages: List[Message]) -> None:\n-        \"\"\"\n-        Log messages for debugging.\n-        \"\"\"\n-        for m in messages:\n-            # Don't log metrics for input messages\n-            m.log(metrics=False)\n-\n     def get_system_message_for_model(self, tools: Optional[List[Any]] = None) -> Optional[str]:\n         return self.system_prompt\n \n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/agno/models/cohere/chat.py",
            "diff": "diff --git a/libs/agno/agno/models/cohere/chat.py b/libs/agno/agno/models/cohere/chat.py\nindex ea7a7f003..f632a04b2 100644\n--- a/libs/agno/agno/models/cohere/chat.py\n+++ b/libs/agno/agno/models/cohere/chat.py\n@@ -5,7 +5,7 @@ from typing import Any, AsyncIterator, Dict, Iterator, List, Optional, Tuple, Ty\n from pydantic import BaseModel\n \n from agno.exceptions import ModelProviderError\n-from agno.models.base import MessageData, Model\n+from agno.models.base import MessageData, Model, _add_usage_metrics_to_assistant_message\n from agno.models.message import Message\n from agno.models.response import ModelResponse\n from agno.utils.log import log_error\n@@ -300,7 +300,7 @@ class Cohere(Model):\n             and response.delta.usage is not None\n             and response.delta.usage.tokens is not None\n         ):\n-            self._add_usage_metrics_to_assistant_message(\n+            _add_usage_metrics_to_assistant_message(\n                 assistant_message=assistant_message,\n                 response_usage={\n                     \"input_tokens\": int(response.delta.usage.tokens.input_tokens) or 0,  # type: ignore\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/agno/models/ollama/tools.py",
            "diff": "diff --git a/libs/agno/agno/models/ollama/tools.py b/libs/agno/agno/models/ollama/tools.py\nindex 6515fb51b..b01c9c053 100644\n--- a/libs/agno/agno/models/ollama/tools.py\n+++ b/libs/agno/agno/models/ollama/tools.py\n@@ -222,7 +222,9 @@ class OllamaTools(Ollama):\n             model_response_delta = self.parse_provider_response_delta(response_delta, tool_call_data)\n             if model_response_delta:\n                 yield from self._populate_stream_data_and_assistant_message(\n-                    stream_data=stream_data, assistant_message=assistant_message, model_response=model_response_delta\n+                    stream_data=stream_data,\n+                    assistant_message=assistant_message,\n+                    model_response_delta=model_response_delta,\n                 )\n \n     async def aprocess_response_stream(\n@@ -245,7 +247,9 @@ class OllamaTools(Ollama):\n             model_response_delta = self.parse_provider_response_delta(response_delta, tool_call_data)\n             if model_response_delta:\n                 for model_response in self._populate_stream_data_and_assistant_message(\n-                    stream_data=stream_data, assistant_message=assistant_message, model_response=model_response_delta\n+                    stream_data=stream_data,\n+                    assistant_message=assistant_message,\n+                    model_response_delta=model_response_delta,\n                 ):\n                     yield model_response\n \n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/agno/models/openai/responses.py",
            "diff": "diff --git a/libs/agno/agno/models/openai/responses.py b/libs/agno/agno/models/openai/responses.py\nindex 7903caeae..245139e82 100644\n--- a/libs/agno/agno/models/openai/responses.py\n+++ b/libs/agno/agno/models/openai/responses.py\n@@ -7,7 +7,7 @@ from pydantic import BaseModel\n \n from agno.exceptions import ModelProviderError\n from agno.media import File\n-from agno.models.base import MessageData, Model\n+from agno.models.base import MessageData, Model, _add_usage_metrics_to_assistant_message\n from agno.models.message import Citations, Message, UrlCitation\n from agno.models.response import ModelResponse\n from agno.utils.log import log_debug, log_error, log_warning\n@@ -782,7 +782,7 @@ class OpenAIResponses(Model):\n             if stream_event.response.usage is not None:\n                 model_response.response_usage = stream_event.response.usage\n \n-            self._add_usage_metrics_to_assistant_message(\n+            _add_usage_metrics_to_assistant_message(\n                 assistant_message=assistant_message,\n                 response_usage=model_response.response_usage,\n             )\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/agno/models/response.py",
            "diff": "diff --git a/libs/agno/agno/models/response.py b/libs/agno/agno/models/response.py\nindex 5db09d315..aefd5371b 100644\n--- a/libs/agno/agno/models/response.py\n+++ b/libs/agno/agno/models/response.py\n@@ -4,17 +4,41 @@ from time import time\n from typing import Any, Dict, List, Optional\n \n from agno.media import AudioResponse, ImageArtifact\n-from agno.models.message import Citations\n+from agno.models.message import Citations, MessageMetrics\n \n \n class ModelResponseEvent(str, Enum):\n     \"\"\"Events that can be sent by the model provider\"\"\"\n \n+    tool_call_confirmation_required = \"ToolCallConfirmationRequired\"\n+    tool_call_external_execution_required = \"ToolCallExternalExecutionRequired\"\n     tool_call_started = \"ToolCallStarted\"\n     tool_call_completed = \"ToolCallCompleted\"\n     assistant_response = \"AssistantResponse\"\n \n \n+@dataclass\n+class ToolExecution:\n+    \"\"\"Execution of a tool\"\"\"\n+\n+    tool_call_id: Optional[str] = None\n+    tool_name: Optional[str] = None\n+    tool_args: Optional[Dict[str, Any]] = None\n+    tool_call_error: Optional[bool] = None\n+    result: Optional[str] = None\n+    metrics: Optional[MessageMetrics] = None\n+\n+    # If True, the agent will stop executing after this tool call.\n+    stop_after_tool_call: bool = False\n+\n+    created_at: int = int(time())\n+\n+    requires_confirmation: Optional[bool] = None\n+    confirmed: Optional[bool] = None\n+\n+    external_execution_required: Optional[bool] = None\n+\n+\n @dataclass\n class ModelResponse:\n     \"\"\"Response from the model provider\"\"\"\n@@ -25,7 +49,13 @@ class ModelResponse:\n     parsed: Optional[Any] = None\n     audio: Optional[AudioResponse] = None\n     image: Optional[ImageArtifact] = None\n+\n+    # Model tool calls\n     tool_calls: List[Dict[str, Any]] = field(default_factory=list)\n+\n+    # Actual tool executions\n+    tool_executions: Optional[List[ToolExecution]] = field(default_factory=list)\n+\n     event: str = ModelResponseEvent.assistant_response.value\n \n     provider_data: Optional[Dict[str, Any]] = None\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/agno/run/response.py",
            "diff": "diff --git a/libs/agno/agno/run/response.py b/libs/agno/agno/run/response.py\nindex fb2217c4d..f4e9f1b30 100644\n--- a/libs/agno/agno/run/response.py\n+++ b/libs/agno/agno/run/response.py\n@@ -7,6 +7,7 @@ from pydantic import BaseModel\n \n from agno.media import AudioArtifact, AudioResponse, ImageArtifact, VideoArtifact\n from agno.models.message import Citations, Message, MessageReferences\n+from agno.models.response import ToolExecution\n from agno.reasoning.step import ReasoningStep\n from agno.utils.log import logger\n \n@@ -19,12 +20,19 @@ class RunEvent(str, Enum):\n     run_completed = \"RunCompleted\"\n     run_error = \"RunError\"\n     run_cancelled = \"RunCancelled\"\n+\n+    run_paused = \"RunPaused\"\n+    run_continued = \"RunContinued\"\n+\n     tool_call_started = \"ToolCallStarted\"\n     tool_call_completed = \"ToolCallCompleted\"\n+\n     reasoning_started = \"ReasoningStarted\"\n     reasoning_step = \"ReasoningStep\"\n     reasoning_completed = \"ReasoningCompleted\"\n+\n     updating_memory = \"UpdatingMemory\"\n+\n     workflow_started = \"WorkflowStarted\"\n     workflow_completed = \"WorkflowCompleted\"\n \n@@ -87,11 +95,12 @@ class RunResponse:\n     messages: Optional[List[Message]] = None\n     metrics: Optional[Dict[str, Any]] = None\n     model: Optional[str] = None\n+    model_provider: Optional[str] = None\n     run_id: Optional[str] = None\n     agent_id: Optional[str] = None\n     session_id: Optional[str] = None\n     workflow_id: Optional[str] = None\n-    tools: Optional[List[Dict[str, Any]]] = None\n+    tools: Optional[List[ToolExecution]] = None\n     formatted_tool_calls: Optional[List[str]] = None\n     images: Optional[List[ImageArtifact]] = None  # Images attached to the response\n     videos: Optional[List[VideoArtifact]] = None  # Videos attached to the response\n@@ -101,6 +110,20 @@ class RunResponse:\n     extra_data: Optional[RunResponseExtraData] = None\n     created_at: int = field(default_factory=lambda: int(time()))\n \n+    @property\n+    def is_paused(self):\n+        if self.event == RunEvent.run_paused:\n+            return True\n+        return False\n+\n+    @property\n+    def tools_requiring_confirmation(self):\n+        return [t for t in self.tools if t.requires_confirmation] if self.tools else []\n+\n+    @property\n+    def tools_awaiting_external_execution(self):\n+        return [t for t in self.tools if t.external_execution_required] if self.tools else []\n+\n     def to_dict(self) -> Dict[str, Any]:\n         _dict = {\n             k: v\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/agno/run/team.py",
            "diff": "diff --git a/libs/agno/agno/run/team.py b/libs/agno/agno/run/team.py\nindex 3735c64d5..a396efb65 100644\n--- a/libs/agno/agno/run/team.py\n+++ b/libs/agno/agno/run/team.py\n@@ -6,6 +6,7 @@ from pydantic import BaseModel\n \n from agno.media import AudioArtifact, AudioResponse, ImageArtifact, VideoArtifact\n from agno.models.message import Citations, Message\n+from agno.models.response import ToolExecution\n from agno.run.response import RunEvent, RunResponse, RunResponseExtraData\n \n \n@@ -21,6 +22,7 @@ class TeamRunResponse:\n     messages: Optional[List[Message]] = None\n     metrics: Optional[Dict[str, Any]] = None\n     model: Optional[str] = None\n+    model_provider: Optional[str] = None\n \n     member_responses: List[Union[\"TeamRunResponse\", RunResponse]] = field(default_factory=list)\n \n@@ -28,7 +30,7 @@ class TeamRunResponse:\n     team_id: Optional[str] = None\n     session_id: Optional[str] = None\n \n-    tools: Optional[List[Dict[str, Any]]] = None\n+    tools: Optional[List[ToolExecution]] = None\n     formatted_tool_calls: Optional[List[str]] = None\n \n     images: Optional[List[ImageArtifact]] = None  # Images from member runs\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/agno/team/team.py",
            "diff": "diff --git a/libs/agno/agno/team/team.py b/libs/agno/agno/team/team.py\nindex 4a5d5368b..356a1e86f 100644\n--- a/libs/agno/agno/team/team.py\n+++ b/libs/agno/agno/team/team.py\n@@ -35,7 +35,7 @@ from agno.memory.team import TeamMemory, TeamRun\n from agno.memory.v2.memory import Memory, SessionSummary\n from agno.models.base import Model\n from agno.models.message import Citations, Message, MessageReferences\n-from agno.models.response import ModelResponse, ModelResponseEvent\n+from agno.models.response import ModelResponse, ModelResponseEvent, ToolExecution\n from agno.reasoning.step import NextAction, ReasoningStep, ReasoningSteps\n from agno.run.messages import RunMessages\n from agno.run.response import RunEvent, RunResponse, RunResponseExtraData\n@@ -746,8 +746,10 @@ class Team:\n             # Run the team\n             try:\n                 self.run_response = TeamRunResponse(run_id=self.run_id, session_id=session_id, team_id=self.team_id)\n+\n                 # Configure the team leader model\n                 self.run_response.model = self.model.id if self.model is not None else None\n+                self.run_response.model_provider = self.model.provider if self.model is not None else None\n \n                 # Prepare run messages\n                 if self.mode == \"route\":\n@@ -1196,8 +1198,10 @@ class Team:\n             # Run the team\n             try:\n                 self.run_response = TeamRunResponse(run_id=self.run_id, session_id=session_id, team_id=self.team_id)\n+\n                 # Configure the team leader model\n                 self.run_response.model = self.model.id if self.model is not None else None\n+                self.run_response.model_provider = self.model.provider if self.model is not None else None\n \n                 # Prepare run messages\n                 if self.mode == \"route\":\n@@ -1452,14 +1456,14 @@ class Team:\n         if model_response.citations is not None:\n             run_response.citations = model_response.citations\n \n-        # Update the run_response tools with the model response tools\n-        if model_response.tool_calls is not None:\n+        # Update the run_response tools with the model response tool_executions\n+        if model_response.tool_executions is not None:\n             if run_response.tools is None:\n-                run_response.tools = model_response.tool_calls\n+                run_response.tools = model_response.tool_executions\n             else:\n-                run_response.tools.extend(model_response.tool_calls)\n+                run_response.tools.extend(model_response.tool_executions)\n \n-        run_response.formatted_tool_calls = format_tool_calls(run_response.tools)\n+        run_response.formatted_tool_calls = format_tool_calls(run_response.tools or [])\n \n         # Update the run_response audio with the model response audio\n         if model_response.audio is not None:\n@@ -1809,16 +1813,16 @@ class Team:\n         # If the model response is a tool_call_started, add the tool call to the run_response\n         elif model_response_chunk.event == ModelResponseEvent.tool_call_started.value:\n             # Add tool calls to the run_response\n-            new_tool_calls_list = model_response_chunk.tool_calls\n-            if new_tool_calls_list is not None:\n+            tool_executions_list = model_response_chunk.tool_executions\n+            if tool_executions_list is not None:\n                 # Add tool calls to the agent.run_response\n                 if run_response.tools is None:\n-                    run_response.tools = new_tool_calls_list\n+                    run_response.tools = tool_executions_list\n                 else:\n-                    run_response.tools.extend(new_tool_calls_list)\n+                    run_response.tools.extend(tool_executions_list)\n \n             # Format tool calls whenever new ones are added during streaming\n-            run_response.formatted_tool_calls = format_tool_calls(run_response.tools)\n+            run_response.formatted_tool_calls = format_tool_calls(run_response.tools or [])\n \n             # Only yield the event if streaming intermediate steps\n             yield self._create_run_response(\n@@ -1831,37 +1835,35 @@ class Team:\n         # If the model response is a tool_call_completed, update the existing tool call in the run_response\n         elif model_response_chunk.event == ModelResponseEvent.tool_call_completed.value:\n             reasoning_step: Optional[ReasoningStep] = None\n-            new_tool_calls_list = model_response_chunk.tool_calls\n-            if new_tool_calls_list is not None:\n+            tool_executions_list = model_response_chunk.tool_executions\n+            if tool_executions_list is not None:\n                 # Update the existing tool call in the run_response\n                 if run_response.tools:\n                     # Create a mapping of tool_call_id to index\n                     tool_call_index_map = {\n-                        tc[\"tool_call_id\"]: i\n-                        for i, tc in enumerate(run_response.tools)\n-                        if tc.get(\"tool_call_id\") is not None\n+                        tc.tool_call_id: i for i, tc in enumerate(run_response.tools) if tc.tool_call_id is not None\n                     }\n                     # Process tool calls\n-                    for tool_call_dict in new_tool_calls_list:\n-                        tool_call_id = tool_call_dict.get(\"tool_call_id\")\n+                    for tool_call_dict in tool_executions_list:\n+                        tool_call_id = tool_call_dict.tool_call_id or \"\"\n                         index = tool_call_index_map.get(tool_call_id)\n                         if index is not None:\n                             run_response.tools[index] = tool_call_dict\n                 else:\n-                    run_response.tools = new_tool_calls_list\n+                    run_response.tools = tool_executions_list\n \n                 # Only iterate through new tool calls\n-                for tool_call in new_tool_calls_list:\n-                    tool_name = tool_call.get(\"tool_name\", \"\")\n+                for tool_call in tool_executions_list:\n+                    tool_name = tool_call.tool_name or \"\"\n                     if tool_name.lower() in [\"think\", \"analyze\"]:\n-                        tool_args = tool_call.get(\"tool_args\", {})\n+                        tool_args = tool_call.tool_args or {}\n \n                         reasoning_step = self.update_reasoning_content_from_tool_call(\n                             run_response, tool_name, tool_args\n                         )\n \n-                        metrics = tool_call.get(\"metrics\")\n-                        if metrics is not None:\n+                        metrics = tool_call.metrics\n+                        if metrics is not None and metrics.time is not None:\n                             reasoning_state[\"reasoning_time_taken\"] = reasoning_state[\"reasoning_time_taken\"] + float(\n                                 metrics.time\n                             )\n@@ -2428,7 +2430,10 @@ class Team:\n                     if self.show_tool_calls and resp.tools:\n                         for tool in resp.tools:\n                             # Generate a unique ID for this tool call\n-                            tool_id = tool.get(\"tool_call_id\", str(hash(str(tool))))\n+                            if tool.tool_call_id:\n+                                tool_id = tool.tool_call_id\n+                            else:\n+                                tool_id = str(hash(str(tool)))\n                             if tool_id not in processed_tool_calls:\n                                 processed_tool_calls.add(tool_id)\n                                 team_tool_calls.append(tool)\n@@ -2448,7 +2453,10 @@ class Team:\n \n                             for tool in member_response.tools:\n                                 # Generate a unique ID for this tool call\n-                                tool_id = tool.get(\"tool_call_id\", str(hash(str(tool))))\n+                                if tool.tool_call_id:\n+                                    tool_id = tool.tool_call_id\n+                                else:\n+                                    tool_id = str(hash(str(tool)))\n                                 if tool_id not in processed_tool_calls:\n                                     processed_tool_calls.add(tool_id)\n                                     member_tool_calls[member_id].append(tool)\n@@ -3204,7 +3212,7 @@ class Team:\n \n         # Track tool calls by member and team\n         member_tool_calls = {}  # type: ignore\n-        team_tool_calls = []\n+        team_tool_calls: List[ToolExecution] = []\n \n         # Track processed tool calls to avoid duplicates\n         processed_tool_calls = set()\n@@ -3274,7 +3282,10 @@ class Team:\n                     if self.show_tool_calls and resp.tools:\n                         for tool in resp.tools:\n                             # Generate a unique ID for this tool call\n-                            tool_id = tool.get(\"tool_call_id\", str(hash(str(tool))))\n+                            if tool.tool_call_id is not None:\n+                                tool_id = tool.tool_call_id\n+                            else:\n+                                tool_id = str(hash(str(tool)))\n                             if tool_id not in processed_tool_calls:\n                                 processed_tool_calls.add(tool_id)\n                                 team_tool_calls.append(tool)\n@@ -3293,7 +3304,10 @@ class Team:\n                                 member_tool_calls[member_id] = []\n \n                             for tool in member_response.tools:\n-                                tool_id = tool.get(\"tool_call_id\", str(hash(str(tool))))\n+                                if tool.tool_call_id is not None:\n+                                    tool_id = tool.tool_call_id\n+                                else:\n+                                    tool_id = str(hash(str(tool)))\n                                 if tool_id not in processed_tool_calls:\n                                     processed_tool_calls.add(tool_id)\n                                     member_tool_calls[member_id].append(tool)\n@@ -4163,7 +4177,7 @@ class Team:\n         content_type: Optional[str] = None,\n         thinking: Optional[str] = None,\n         event: RunEvent = RunEvent.run_response,\n-        tools: Optional[List[Dict[str, Any]]] = None,\n+        tools: Optional[List[ToolExecution]] = None,\n         reasoning_content: Optional[str] = None,\n         audio: Optional[List[AudioArtifact]] = None,\n         images: Optional[List[ImageArtifact]] = None,\n@@ -5073,7 +5087,7 @@ class Team:\n                             member_agent_run_response_chunk.tools is not None\n                             and len(member_agent_run_response_chunk.tools) > 0\n                         ):\n-                            yield \",\".join([tool.get(\"content\", \"\") for tool in member_agent_run_response_chunk.tools])\n+                            yield \",\".join([tool.result for tool in member_agent_run_response_chunk.tools])  # type: ignore\n                 else:\n                     member_agent_run_response = member_agent.run(\n                         member_agent_task, images=images, videos=videos, audio=audio, files=files, stream=False\n@@ -5089,7 +5103,7 @@ class Team:\n                         if len(member_agent_run_response.content.strip()) > 0:\n                             yield f\"Agent {member_agent.name}: {member_agent_run_response.content}\"\n                         elif member_agent_run_response.tools is not None and len(member_agent_run_response.tools) > 0:\n-                            yield f\"Agent {member_agent.name}: {','.join([tool.get('content', '') for tool in member_agent_run_response.tools])}\"\n+                            yield f\"Agent {member_agent.name}: {','.join([tool.result for tool in member_agent_run_response.tools])}\"  # type: ignore\n                     elif issubclass(type(member_agent_run_response.content), BaseModel):\n                         try:\n                             yield f\"Agent {member_agent.name}: {member_agent_run_response.content.model_dump_json(indent=2)}\"  # type: ignore\n@@ -5398,7 +5412,11 @@ class Team:\n                         member_agent_run_response_chunk.tools is not None\n                         and len(member_agent_run_response_chunk.tools) > 0\n                     ):\n-                        yield \",\".join([tool.get(\"content\", \"\") for tool in member_agent_run_response_chunk.tools])\n+                        tool_str = \"\"\n+                        for tool in member_agent_run_response_chunk.tools:\n+                            if tool.result:\n+                                tool_str += f\"{tool.result},\"\n+                        yield tool_str.rstrip(\",\")\n             else:\n                 if not member_agent.knowledge_filters and member_agent.knowledge:\n                     member_agent_run_response = member_agent.run(\n@@ -5427,7 +5445,11 @@ class Team:\n \n                     # If the content is empty but we have tool calls\n                     elif member_agent_run_response.tools is not None and len(member_agent_run_response.tools) > 0:\n-                        yield \",\".join([tool.get(\"content\", \"\") for tool in member_agent_run_response.tools])\n+                        tool_str = \"\"\n+                        for tool in member_agent_run_response.tools:\n+                            if tool.result:\n+                                tool_str += f\"{tool.result},\"\n+                        yield tool_str.rstrip(\",\")\n \n                 elif issubclass(type(member_agent_run_response.content), BaseModel):\n                     try:\n@@ -5576,7 +5598,7 @@ class Team:\n                         member_agent_run_response_chunk.tools is not None\n                         and len(member_agent_run_response_chunk.tools) > 0\n                     ):\n-                        yield \",\".join([tool.get(\"content\", \"\") for tool in member_agent_run_response_chunk.tools])\n+                        yield \",\".join([tool.content for tool in member_agent_run_response_chunk.tools])  # type: ignore\n             else:\n                 if not member_agent.knowledge_filters and member_agent.knowledge:\n                     member_agent_run_response = await member_agent.arun(\n@@ -5604,7 +5626,7 @@ class Team:\n \n                     # If the content is empty but we have tool calls\n                     elif member_agent_run_response.tools is not None and len(member_agent_run_response.tools) > 0:\n-                        yield \",\".join([tool.get(\"content\", \"\") for tool in member_agent_run_response.tools])\n+                        yield \",\".join([tool.content for tool in member_agent_run_response.tools])  # type: ignore\n                 elif issubclass(type(member_agent_run_response.content), BaseModel):\n                     try:\n                         yield member_agent_run_response.content.model_dump_json(indent=2)  # type: ignore\n@@ -5806,7 +5828,12 @@ class Team:\n \n                     # If the content is empty but we have tool calls\n                     elif member_agent_run_response.tools is not None and len(member_agent_run_response.tools) > 0:\n-                        yield \",\".join([tool.get(\"content\", \"\") for tool in member_agent_run_response.tools])\n+                        tool_str = \"\"\n+                        for tool in member_agent_run_response.tools:\n+                            if tool.result:\n+                                tool_str += f\"{tool.result},\"\n+                        yield tool_str.rstrip(\",\")\n+\n                 elif issubclass(type(member_agent_run_response.content), BaseModel):\n                     try:\n                         yield member_agent_run_response.content.model_dump_json(indent=2)  # type: ignore\n@@ -5943,7 +5970,7 @@ class Team:\n \n                     # If the content is empty but we have tool calls\n                     elif member_agent_run_response.tools is not None and len(member_agent_run_response.tools) > 0:\n-                        yield \",\".join([tool.get(\"content\", \"\") for tool in member_agent_run_response.tools])\n+                        yield \",\".join([tool.content for tool in member_agent_run_response.tools])  # type: ignore\n                 elif issubclass(type(member_agent_run_response.content), BaseModel):\n                     try:\n                         yield member_agent_run_response.content.model_dump_json(indent=2)  # type: ignore\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/agno/tools/decorator.py",
            "diff": "diff --git a/libs/agno/agno/tools/decorator.py b/libs/agno/agno/tools/decorator.py\nindex 2dce9e74d..fc5126e05 100644\n--- a/libs/agno/agno/tools/decorator.py\n+++ b/libs/agno/agno/tools/decorator.py\n@@ -24,6 +24,8 @@ def tool(\n     sanitize_arguments: Optional[bool] = None,\n     show_result: Optional[bool] = None,\n     stop_after_tool_call: Optional[bool] = None,\n+    requires_confirmation: Optional[bool] = None,\n+    external_execution: Optional[bool] = None,\n     pre_hook: Optional[Callable] = None,\n     post_hook: Optional[Callable] = None,\n     tool_hooks: Optional[List[Callable]] = None,\n@@ -49,6 +51,8 @@ def tool(*args, **kwargs) -> Union[Function, Callable[[F], Function]]:\n         add_instructions: bool - If True, add instructions to the system message\n         show_result: Optional[bool] - If True, shows the result after function call\n         stop_after_tool_call: Optional[bool] - If True, the agent will stop after the function call.\n+        requires_confirmation: Optional[bool] - If True, the function will require user confirmation before execution\n+        external_execution: Optional[bool] - If True, the function will be executed outside of the agent's context\n         pre_hook: Optional[Callable] - Hook that runs before the function is executed (deprecated, use tool_execution_hook instead).\n         post_hook: Optional[Callable] - Hook that runs after the function is executed (deprecated, use tool_execution_hook instead).\n         tool_hooks: Optional[List[Callable]] - List of hooks that run before and after the function is executed.\n@@ -83,6 +87,8 @@ def tool(*args, **kwargs) -> Union[Function, Callable[[F], Function]]:\n             \"sanitize_arguments\",\n             \"show_result\",\n             \"stop_after_tool_call\",\n+            \"requires_confirmation\",\n+            \"external_execution\",\n             \"pre_hook\",\n             \"post_hook\",\n             \"tool_hooks\",\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/agno/tools/firecrawl.py",
            "diff": "diff --git a/libs/agno/agno/tools/firecrawl.py b/libs/agno/agno/tools/firecrawl.py\nindex 8c940459f..f1f662ebe 100644\n--- a/libs/agno/agno/tools/firecrawl.py\n+++ b/libs/agno/agno/tools/firecrawl.py\n@@ -6,7 +6,7 @@ from agno.tools import Toolkit\n from agno.utils.log import logger\n \n try:\n-    from firecrawl import FirecrawlApp, ScrapeOptions\n+    from firecrawl import FirecrawlApp, ScrapeOptions  # type: ignore[attr-defined]\n except ImportError:\n     raise ImportError(\"`firecrawl-py` not installed. Please install using `pip install firecrawl-py`\")\n \n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/agno/tools/function.py",
            "diff": "diff --git a/libs/agno/agno/tools/function.py b/libs/agno/agno/tools/function.py\nindex 943bcff7f..46f36d20d 100644\n--- a/libs/agno/agno/tools/function.py\n+++ b/libs/agno/agno/tools/function.py\n@@ -1,5 +1,5 @@\n from functools import partial\n-from typing import Any, Callable, Dict, List, Optional, TypeVar, get_type_hints\n+from typing import Any, Callable, Dict, List, Literal, Optional, TypeVar, get_type_hints\n \n from docstring_parser import parse\n from pydantic import BaseModel, Field, validate_call\n@@ -74,6 +74,12 @@ class Function(BaseModel):\n     # A list of hooks to run around tool calls.\n     tool_hooks: Optional[List[Callable]] = None\n \n+    # If True, the function will require confirmation before execution\n+    requires_confirmation: Optional[bool] = None\n+\n+    # If True, the function will be executed outside the agent's control.\n+    external_execution: Optional[bool] = None\n+\n     # Caching configuration\n     cache_results: bool = False\n     cache_dir: Optional[str] = None\n@@ -86,7 +92,10 @@ class Function(BaseModel):\n     _team: Optional[Any] = None\n \n     def to_dict(self) -> Dict[str, Any]:\n-        return self.model_dump(exclude_none=True, include={\"name\", \"description\", \"parameters\", \"strict\"})\n+        return self.model_dump(\n+            exclude_none=True,\n+            include={\"name\", \"description\", \"parameters\", \"strict\", \"requires_confirmation\", \"external_execution\"},\n+        )\n \n     @classmethod\n     def from_callable(cls, c: Callable, strict: bool = False) -> \"Function\":\n@@ -331,6 +340,10 @@ class Function(BaseModel):\n             log_error(f\"Error writing cache: {e}\")\n \n \n+class FunctionExecutionResult(BaseModel):\n+    status: Literal[\"success\", \"failure\"]\n+\n+\n class FunctionCall(BaseModel):\n     \"\"\"Model for Function Calls\"\"\"\n \n@@ -350,7 +363,7 @@ class FunctionCall(BaseModel):\n         \"\"\"Returns a string representation of the function call.\"\"\"\n         import shutil\n \n-        # Get terminal width, default to 80 if can't determine\n+        # Get terminal width, default to 80 if it can't be determined\n         term_width = shutil.get_terminal_size().columns or 80\n         max_arg_len = max(20, (term_width - len(self.function.name) - 4) // 2)\n \n@@ -484,15 +497,14 @@ class FunctionCall(BaseModel):\n         chain = reduce(create_hook_wrapper, hooks, execute_entrypoint)\n         return chain\n \n-    def execute(self) -> bool:\n+    def execute(self) -> FunctionExecutionResult:\n         \"\"\"Runs the function call.\"\"\"\n         from inspect import isgenerator\n \n         if self.function.entrypoint is None:\n-            return False\n+            return FunctionExecutionResult(status=\"failure\")\n \n         log_debug(f\"Running: {self.get_call_str()}\")\n-        function_call_success = False\n \n         # Execute pre-hook if it exists\n         self._handle_pre_hook()\n@@ -508,8 +520,7 @@ class FunctionCall(BaseModel):\n             if cached_result is not None:\n                 log_debug(f\"Cache hit for: {self.get_call_str()}\")\n                 self.result = cached_result\n-                function_call_success = True\n-                return function_call_success\n+                return FunctionExecutionResult(status=\"success\")\n \n         # Execute function\n         try:\n@@ -534,8 +545,6 @@ class FunctionCall(BaseModel):\n                     cache_file = self.function._get_cache_file_path(cache_key)\n                     self.function._save_to_cache(cache_file, self.result)\n \n-            function_call_success = True\n-\n         except AgentRunException as e:\n             log_debug(f\"{e.__class__.__name__}: {e}\")\n             self.error = str(e)\n@@ -544,12 +553,12 @@ class FunctionCall(BaseModel):\n             log_warning(f\"Could not run function {self.get_call_str()}\")\n             log_exception(e)\n             self.error = str(e)\n-            return function_call_success\n+            return FunctionExecutionResult(status=\"failure\")\n \n         # Execute post-hook if it exists\n         self._handle_post_hook()\n \n-        return function_call_success\n+        return FunctionExecutionResult(status=\"success\")\n \n     async def _handle_pre_hook_async(self):\n         \"\"\"Handles the async pre-hook for the function call.\"\"\"\n@@ -667,15 +676,14 @@ class FunctionCall(BaseModel):\n \n         return chain\n \n-    async def aexecute(self) -> bool:\n+    async def aexecute(self) -> FunctionExecutionResult:\n         \"\"\"Runs the function call asynchronously.\"\"\"\n         from inspect import isasyncgen, isasyncgenfunction, iscoroutinefunction, isgenerator\n \n         if self.function.entrypoint is None:\n-            return False\n+            return FunctionExecutionResult(status=\"failure\")\n \n         log_debug(f\"Running: {self.get_call_str()}\")\n-        function_call_success = False\n \n         # Execute pre-hook if it exists\n         if iscoroutinefunction(self.function.pre_hook):\n@@ -695,8 +703,7 @@ class FunctionCall(BaseModel):\n             if cached_result is not None:\n                 log_debug(f\"Cache hit for: {self.get_call_str()}\")\n                 self.result = cached_result\n-                function_call_success = True\n-                return function_call_success\n+                return FunctionExecutionResult(status=\"success\")\n \n         # Execute function\n         try:\n@@ -721,8 +728,6 @@ class FunctionCall(BaseModel):\n                 cache_file = self.function._get_cache_file_path(cache_key)\n                 self.function._save_to_cache(cache_file, self.result)\n \n-            function_call_success = True\n-\n         except AgentRunException as e:\n             log_debug(f\"{e.__class__.__name__}: {e}\")\n             self.error = str(e)\n@@ -731,7 +736,7 @@ class FunctionCall(BaseModel):\n             log_warning(f\"Could not run function {self.get_call_str()}\")\n             log_exception(e)\n             self.error = str(e)\n-            return function_call_success\n+            return FunctionExecutionResult(status=\"failure\")\n \n         # Execute post-hook if it exists\n         if iscoroutinefunction(self.function.post_hook):\n@@ -739,4 +744,4 @@ class FunctionCall(BaseModel):\n         else:\n             self._handle_post_hook()\n \n-        return function_call_success\n+        return FunctionExecutionResult(status=\"success\")\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/agno/tools/models/nebius.py",
            "diff": "diff --git a/libs/agno/agno/tools/models/nebius.py b/libs/agno/agno/tools/models/nebius.py\nindex 864669b2b..bf9c640db 100644\n--- a/libs/agno/agno/tools/models/nebius.py\n+++ b/libs/agno/agno/tools/models/nebius.py\n@@ -52,7 +52,7 @@ class NebiusTools(Toolkit):\n \n     def _get_client(self):\n         if self._nebius_client is None:\n-            self._nebius_client = Nebius(api_key=self.api_key, base_url=self.base_url, id=self.image_model).get_client()\n+            self._nebius_client = Nebius(api_key=self.api_key, base_url=self.base_url, id=self.image_model).get_client()  # type: ignore\n         return self._nebius_client\n \n     def generate_image(\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/agno/tools/todoist.py",
            "diff": "diff --git a/libs/agno/agno/tools/todoist.py b/libs/agno/agno/tools/todoist.py\nindex d0e2952d7..2deb43133 100644\n--- a/libs/agno/agno/tools/todoist.py\n+++ b/libs/agno/agno/tools/todoist.py\n@@ -217,8 +217,8 @@ class TodoistTools(Toolkit):\n     def get_active_tasks(self) -> str:\n         \"\"\"Get all active (not completed) tasks.\"\"\"\n         try:\n-            tasks = self.api.get_tasks()\n-            tasks = list(tasks)[0]\n+            tasks_response = self.api.get_tasks()\n+            tasks = list(tasks_response)[0]\n             tasks_list = []\n             for task in tasks:\n                 task_dict = self._task_to_dict(task)\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/agno/tools/toolkit.py",
            "diff": "diff --git a/libs/agno/agno/tools/toolkit.py b/libs/agno/agno/tools/toolkit.py\nindex 2a05864d0..fced49103 100644\n--- a/libs/agno/agno/tools/toolkit.py\n+++ b/libs/agno/agno/tools/toolkit.py\n@@ -2,7 +2,7 @@ from collections import OrderedDict\n from typing import Any, Callable, Dict, List, Optional\n \n from agno.tools.function import Function\n-from agno.utils.log import log_debug, logger\n+from agno.utils.log import log_debug, log_warning, logger\n \n \n class Toolkit:\n@@ -14,12 +14,14 @@ class Toolkit:\n         add_instructions: bool = False,\n         include_tools: Optional[list[str]] = None,\n         exclude_tools: Optional[list[str]] = None,\n+        requires_confirmation_tools: Optional[list[str]] = None,\n+        external_execution_required_tools: Optional[list[str]] = None,\n+        stop_after_tool_call_tools: Optional[List[str]] = None,\n+        show_result_tools: Optional[List[str]] = None,\n         cache_results: bool = False,\n         cache_ttl: int = 3600,\n         cache_dir: Optional[str] = None,\n         auto_register: bool = True,\n-        stop_after_tool_call_tools: Optional[List[str]] = None,\n-        show_result_tools: Optional[List[str]] = None,\n     ):\n         \"\"\"Initialize a new Toolkit.\n \n@@ -30,6 +32,8 @@ class Toolkit:\n             add_instructions: Whether to add instructions to the toolkit\n             include_tools: List of tool names to include in the toolkit\n             exclude_tools: List of tool names to exclude from the toolkit\n+            requires_confirmation_tools: List of tool names that require user confirmation\n+            external_execution_required_tools: List of tool names that will be executed outside of the agent loop\n             cache_results (bool): Enable in-memory caching of function results.\n             cache_ttl (int): Time-to-live for cached results in seconds.\n             cache_dir (Optional[str]): Directory to store cache files. Defaults to system temp dir.\n@@ -42,8 +46,12 @@ class Toolkit:\n         self.functions: Dict[str, Function] = OrderedDict()\n         self.instructions: Optional[str] = instructions\n         self.add_instructions: bool = add_instructions\n-        self.stop_after_tool_call_tools = stop_after_tool_call_tools or []\n-        self.show_result_tools = show_result_tools or []\n+\n+        self.requires_confirmation_tools: list[str] = requires_confirmation_tools or []\n+        self.external_execution_required_tools: list[str] = external_execution_required_tools or []\n+\n+        self.stop_after_tool_call_tools: list[str] = stop_after_tool_call_tools or []\n+        self.show_result_tools: list[str] = show_result_tools or []\n \n         self._check_tools_filters(\n             available_tools=[tool.__name__ for tool in tools], include_tools=include_tools, exclude_tools=exclude_tools\n@@ -78,6 +86,20 @@ class Toolkit:\n                 if missing_excludes:\n                     raise ValueError(f\"Excluded tool(s) not present in the toolkit: {', '.join(missing_excludes)}\")\n \n+        if self.requires_confirmation_tools:\n+            missing_requires_confirmation = set(self.requires_confirmation_tools) - set(available_tools)\n+            if missing_requires_confirmation:\n+                log_warning(\n+                    f\"Requires confirmation tool(s) not present in the toolkit: {', '.join(missing_requires_confirmation)}\"\n+                )\n+\n+        if self.external_execution_required_tools:\n+            missing_external_execution_required = set(self.external_execution_required_tools) - set(available_tools)\n+            if missing_external_execution_required:\n+                log_warning(\n+                    f\"External execution required tool(s) not present in the toolkit: {', '.join(missing_external_execution_required)}\"\n+                )\n+\n     def _register_tools(self) -> None:\n         \"\"\"Register all tools.\"\"\"\n         for tool in self.tools:\n@@ -108,6 +130,8 @@ class Toolkit:\n                 cache_results=self.cache_results,\n                 cache_dir=self.cache_dir,\n                 cache_ttl=self.cache_ttl,\n+                requires_confirmation=tool_name in self.requires_confirmation_tools,\n+                external_execution=tool_name in self.external_execution_required_tools,\n                 stop_after_tool_call=tool_name in self.stop_after_tool_call_tools,\n                 show_result=tool_name in self.show_result_tools,\n             )\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/agno/tools/yfinance.py",
            "diff": "diff --git a/libs/agno/agno/tools/yfinance.py b/libs/agno/agno/tools/yfinance.py\nindex fdfa95189..d105846d1 100644\n--- a/libs/agno/agno/tools/yfinance.py\n+++ b/libs/agno/agno/tools/yfinance.py\n@@ -39,26 +39,27 @@ class YFinanceTools(Toolkit):\n         enable_all: bool = False,\n         **kwargs,\n     ):\n-        super().__init__(name=\"yfinance_tools\", **kwargs)\n-\n+        tools = []\n         if stock_price or enable_all:\n-            self.register(self.get_current_stock_price)\n+            tools.append(self.get_current_stock_price)\n         if company_info or enable_all:\n-            self.register(self.get_company_info)\n+            tools.append(self.get_company_info)\n         if stock_fundamentals or enable_all:\n-            self.register(self.get_stock_fundamentals)\n+            tools.append(self.get_stock_fundamentals)\n         if income_statements or enable_all:\n-            self.register(self.get_income_statements)\n+            tools.append(self.get_income_statements)\n         if key_financial_ratios or enable_all:\n-            self.register(self.get_key_financial_ratios)\n+            tools.append(self.get_key_financial_ratios)\n         if analyst_recommendations or enable_all:\n-            self.register(self.get_analyst_recommendations)\n+            tools.append(self.get_analyst_recommendations)\n         if company_news or enable_all:\n-            self.register(self.get_company_news)\n+            tools.append(self.get_company_news)\n         if technical_indicators or enable_all:\n-            self.register(self.get_technical_indicators)\n+            tools.append(self.get_technical_indicators)\n         if historical_prices or enable_all:\n-            self.register(self.get_historical_stock_prices)\n+            tools.append(self.get_historical_stock_prices)\n+\n+        super().__init__(name=\"yfinance_tools\", tools=tools, **kwargs)\n \n     def get_current_stock_price(self, symbol: str) -> str:\n         \"\"\"\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/agno/utils/gemini.py",
            "diff": "diff --git a/libs/agno/agno/utils/gemini.py b/libs/agno/agno/utils/gemini.py\nindex ba5daba6b..281fecc49 100644\n--- a/libs/agno/agno/utils/gemini.py\n+++ b/libs/agno/agno/utils/gemini.py\n@@ -133,8 +133,8 @@ def convert_schema(schema_dict: Dict[str, Any]) -> Optional[Schema]:\n             else:\n                 filtered_any_of.append(schema)\n \n-        any_of = filtered_any_of\n-        if len(any_of) == 1:\n+        any_of = filtered_any_of  # type: ignore\n+        if len(any_of) == 1 and any_of[0] is not None:\n             any_of[0].nullable = is_nullable\n             return any_of[0]\n         else:\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/agno/utils/response.py",
            "diff": "diff --git a/libs/agno/agno/utils/response.py b/libs/agno/agno/utils/response.py\nindex 5682d5e05..a77d03c7b 100644\n--- a/libs/agno/agno/utils/response.py\n+++ b/libs/agno/agno/utils/response.py\n@@ -1,7 +1,8 @@\n-from typing import Any, Dict, List, Set, Union\n+from typing import List, Set, Union\n \n from agno.exceptions import RunCancelledException\n from agno.models.message import Message\n+from agno.models.response import ToolExecution\n from agno.reasoning.step import ReasoningStep\n from agno.run.response import RunEvent, RunResponse, RunResponseExtraData\n from agno.run.team import TeamRunResponse\n@@ -53,7 +54,7 @@ def update_run_response_with_reasoning(\n         run_response.extra_data.reasoning_messages.extend(reasoning_agent_messages)\n \n \n-def format_tool_calls(tool_calls: List[Dict[str, Any]]) -> List[str]:\n+def format_tool_calls(tool_calls: List[ToolExecution]) -> List[str]:\n     \"\"\"Format tool calls for display in a readable format.\n \n     Args:\n@@ -64,10 +65,10 @@ def format_tool_calls(tool_calls: List[Dict[str, Any]]) -> List[str]:\n     \"\"\"\n     formatted_tool_calls = []\n     for tool_call in tool_calls:\n-        if \"tool_name\" in tool_call and \"tool_args\" in tool_call:\n-            tool_name = tool_call[\"tool_name\"]\n+        if tool_call.tool_name and tool_call.tool_args:\n+            tool_name = tool_call.tool_name\n             args_str = \"\"\n-            if \"tool_args\" in tool_call and tool_call[\"tool_args\"] is not None:\n-                args_str = \", \".join(f\"{k}={v}\" for k, v in tool_call[\"tool_args\"].items())\n+            if tool_call.tool_args is not None:\n+                args_str = \", \".join(f\"{k}={v}\" for k, v in tool_call.tool_args.items())\n             formatted_tool_calls.append(f\"{tool_name}({args_str})\")\n     return formatted_tool_calls\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/agno/utils/tools.py",
            "diff": "diff --git a/libs/agno/agno/utils/tools.py b/libs/agno/agno/utils/tools.py\nindex a33fe300c..183fc0be3 100644\n--- a/libs/agno/agno/utils/tools.py\n+++ b/libs/agno/agno/utils/tools.py\n@@ -1,5 +1,6 @@\n from typing import Any, Dict, Optional\n \n+from agno.models.response import ToolExecution\n from agno.tools.function import Function, FunctionCall\n from agno.utils.functions import get_function_call\n \n@@ -82,3 +83,20 @@ def remove_function_calls_from_string(\n         end_index = text.find(end_tag) + len(end_tag)\n         text = text[:start_index] + text[end_index:]\n     return text\n+\n+\n+def get_function_call_for_tool_execution(\n+    tool_execution: ToolExecution,\n+    functions: Optional[Dict[str, Function]] = None,\n+) -> Optional[FunctionCall]:\n+    import json\n+\n+    _tool_call_id = tool_execution.tool_call_id\n+    _tool_call_function_name = tool_execution.tool_name or \"\"\n+    _tool_call_function_arguments_str = json.dumps(tool_execution.tool_args)\n+    return get_function_call(\n+        name=_tool_call_function_name,\n+        arguments=_tool_call_function_arguments_str,\n+        call_id=_tool_call_id,\n+        functions=functions,\n+    )\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/agno/vectordb/mongodb/mongodb.py",
            "diff": "diff --git a/libs/agno/agno/vectordb/mongodb/mongodb.py b/libs/agno/agno/vectordb/mongodb/mongodb.py\nindex 5db4e228d..18d407dbd 100644\n--- a/libs/agno/agno/vectordb/mongodb/mongodb.py\n+++ b/libs/agno/agno/vectordb/mongodb/mongodb.py\n@@ -4,9 +4,10 @@ from typing import Any, Dict, List, Optional\n \n from agno.document import Document\n from agno.embedder import Embedder\n-from agno.utils.log import log_debug, log_info, logger\n+from agno.utils.log import log_debug, log_info, log_warning, logger\n from agno.vectordb.base import VectorDb\n from agno.vectordb.distance import Distance\n+from agno.vectordb.search import SearchType\n \n try:\n     from hashlib import md5\n@@ -42,6 +43,10 @@ class MongoDb(VectorDb):\n         client: Optional[MongoClient] = None,\n         search_index_name: Optional[str] = \"vector_index_1\",\n         cosmos_compatibility: Optional[bool] = False,\n+        search_type: SearchType = SearchType.vector,\n+        hybrid_vector_weight: float = 0.5,\n+        hybrid_keyword_weight: float = 0.5,\n+        hybrid_rank_constant: int = 60,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -61,6 +66,10 @@ class MongoDb(VectorDb):\n             client (Optional[MongoClient]): An existing MongoClient instance.\n             search_index_name (str): Name of the search index (default: \"vector_index_1\")\n             cosmos_compatibility (bool): Whether to use Azure Cosmos DB Mongovcore compatibility mode.\n+            search_type: The search type to use when searching for documents.\n+            hybrid_vector_weight (float): Default weight for vector search results in hybrid search.\n+            hybrid_keyword_weight (float): Default weight for keyword search results in hybrid search.\n+            hybrid_rank_constant (int): Default rank constant (k) for Reciprocal Rank Fusion in hybrid search. This constant is added to the rank before taking the reciprocal, helping to smooth scores. A common value is 60.\n             **kwargs: Additional arguments for MongoClient.\n         \"\"\"\n         if not collection_name:\n@@ -71,6 +80,10 @@ class MongoDb(VectorDb):\n         self.database = database\n         self.search_index_name = search_index_name\n         self.cosmos_compatibility = cosmos_compatibility\n+        self.search_type = search_type\n+        self.hybrid_vector_weight = hybrid_vector_weight\n+        self.hybrid_keyword_weight = hybrid_keyword_weight\n+        self.hybrid_rank_constant = hybrid_rank_constant\n \n         if embedder is None:\n             from agno.embedder.openai import OpenAIEmbedder\n@@ -519,6 +532,9 @@ class MongoDb(VectorDb):\n         self, query: str, limit: int = 5, filters: Optional[Dict[str, Any]] = None, min_score: float = 0.0\n     ) -> List[Document]:\n         \"\"\"Search for documents using vector similarity.\"\"\"\n+        if self.search_type == SearchType.hybrid:\n+            return self.hybrid_search(query, limit=limit)\n+\n         query_embedding = self.embedder.get_embedding(query)\n         if query_embedding is None:\n             logger.error(f\"Failed to generate embedding for query: {query}\")\n@@ -653,10 +669,165 @@ class MongoDb(VectorDb):\n             logger.error(f\"Error during keyword search: {e}\")\n             return []\n \n-    def hybrid_search(self, query: str, limit: int = 5) -> List[Document]:\n-        \"\"\"Perform a hybrid search combining vector and keyword-based searches.\"\"\"\n-        log_debug(\"Performing hybrid search is not yet implemented.\")\n-        return []\n+    def hybrid_search(\n+        self,\n+        query: str,\n+        limit: int = 5,\n+    ) -> List[Document]:\n+        \"\"\"\n+        Perform a hybrid search combining vector and keyword-based searches using Reciprocal Rank Fusion.\n+\n+        Weights for vector and keyword search are configured at the instance level (hybrid_vector_weight, hybrid_keyword_weight).\n+        The rank constant k is used in the RRF formula `1 / (rank + k)` to smooth scores.\n+\n+        Reference: https://www.mongodb.com/docs/atlas/atlas-vector-search/tutorials/reciprocal-rank-fusion\n+        \"\"\"\n+\n+        if self.cosmos_compatibility:\n+            log_warning(\"Hybrid search is not implemented for Cosmos DB compatibility mode. Returning empty list.\")\n+            return []\n+\n+        log_debug(f\"Performing hybrid search for query: '{query}' with limit: {limit}\")\n+\n+        query_embedding = self.embedder.get_embedding(query)\n+        if query_embedding is None:\n+            logger.error(f\"Failed to generate embedding for query: {query}\")\n+            return []\n+\n+        collection = self._get_collection()\n+\n+        k = self.hybrid_rank_constant\n+\n+        pipeline = [\n+            # Vector Search Branch\n+            {\n+                \"$vectorSearch\": {\n+                    \"index\": self.search_index_name,\n+                    \"path\": \"embedding\",\n+                    \"queryVector\": query_embedding,\n+                    \"numCandidates\": min(limit * 10, 200),\n+                    \"limit\": limit * 2,\n+                }\n+            },\n+            {\"$group\": {\"_id\": None, \"docs\": {\"$push\": \"$$ROOT\"}}},\n+            {\"$unwind\": {\"path\": \"$docs\", \"includeArrayIndex\": \"rank\"}},\n+            {\n+                \"$addFields\": {\n+                    \"_id\": \"$docs._id\",\n+                    \"name\": \"$docs.name\",\n+                    \"content\": \"$docs.content\",\n+                    \"meta_data\": \"$docs.meta_data\",\n+                    \"vs_score\": {\n+                        \"$divide\": [\n+                            self.hybrid_vector_weight,\n+                            {\"$add\": [\"$rank\", k, 1]},\n+                        ]\n+                    },\n+                    \"fts_score\": 0.0,  # Ensure fts_score exists with a default value\n+                }\n+            },\n+            {\n+                \"$project\": {\n+                    \"_id\": 1,\n+                    \"name\": 1,\n+                    \"content\": 1,\n+                    \"meta_data\": 1,\n+                    \"vs_score\": 1,\n+                    # Now fts_score is included with its value (0.0 here)\n+                    \"fts_score\": 1,\n+                }\n+            },\n+            # Union with Keyword Search Branch\n+            {\n+                \"$unionWith\": {\n+                    \"coll\": self.collection_name,\n+                    \"pipeline\": [\n+                        {\n+                            \"$search\": {\n+                                \"index\": \"default\",\n+                                \"text\": {\"query\": query, \"path\": \"content\"},\n+                            }\n+                        },\n+                        {\"$limit\": limit * 2},\n+                        {\"$group\": {\"_id\": None, \"docs\": {\"$push\": \"$$ROOT\"}}},\n+                        {\"$unwind\": {\"path\": \"$docs\", \"includeArrayIndex\": \"rank\"}},\n+                        {\n+                            \"$addFields\": {\n+                                \"_id\": \"$docs._id\",\n+                                \"name\": \"$docs.name\",\n+                                \"content\": \"$docs.content\",\n+                                \"meta_data\": \"$docs.meta_data\",\n+                                \"vs_score\": 0.0,\n+                                \"fts_score\": {\n+                                    \"$divide\": [\n+                                        self.hybrid_keyword_weight,\n+                                        {\"$add\": [\"$rank\", k, 1]},\n+                                    ]\n+                                },\n+                            }\n+                        },\n+                        {\n+                            \"$project\": {\n+                                \"_id\": 1,\n+                                \"name\": 1,\n+                                \"content\": 1,\n+                                \"meta_data\": 1,\n+                                \"vs_score\": 1,\n+                                \"fts_score\": 1,\n+                            }\n+                        },\n+                    ],\n+                }\n+            },\n+            # Combine and Rank\n+            {\n+                \"$group\": {\n+                    \"_id\": \"$_id\",\n+                    \"name\": {\"$first\": \"$name\"},\n+                    \"content\": {\"$first\": \"$content\"},\n+                    \"meta_data\": {\"$first\": \"$meta_data\"},\n+                    \"vs_score\": {\"$sum\": \"$vs_score\"},\n+                    \"fts_score\": {\"$sum\": \"$fts_score\"},\n+                }\n+            },\n+            {\n+                \"$project\": {\n+                    \"_id\": 1,\n+                    \"name\": 1,\n+                    \"content\": 1,\n+                    \"meta_data\": 1,\n+                    \"score\": {\"$add\": [\"$vs_score\", \"$fts_score\"]},\n+                }\n+            },\n+            {\"$sort\": {\"score\": -1}},\n+            {\"$limit\": limit},\n+        ]\n+\n+        try:\n+            results = list(collection.aggregate(pipeline))\n+            docs = [\n+                Document(\n+                    id=str(doc[\"_id\"]),\n+                    name=doc.get(\"name\"),\n+                    content=doc[\"content\"],\n+                    meta_data={**doc.get(\"meta_data\", {}), \"score\": doc.get(\"score\", 0.0)},\n+                )\n+                for doc in results\n+            ]\n+            log_info(f\"Hybrid search completed. Found {len(docs)} documents.\")\n+            return docs\n+        except errors.OperationFailure as e:\n+            logger.error(\n+                f\"Error during hybrid search, potentially due to missing or misconfigured Atlas Search index for text search: {e}\"\n+            )\n+            logger.error(f\"Details: {e.details}\")\n+            return []\n+        except Exception as e:\n+            logger.error(f\"Error during hybrid search: {e}\")\n+            import traceback\n+\n+            logger.error(f\"Traceback: {traceback.format_exc()}\")\n+            return []\n \n     def drop(self) -> None:\n         \"\"\"Drop the collection and clean up indexes.\"\"\"\n@@ -711,13 +882,15 @@ class MongoDb(VectorDb):\n             try:\n                 collection = self._get_collection()\n                 result = collection.delete_many({})\n-                success = result.deleted_count >= 0  # Consider any deletion (even 0) as success\n+                # Consider any deletion (even 0) as success\n+                success = result.deleted_count >= 0\n                 log_info(f\"Deleted {result.deleted_count} documents from collection.\")\n                 return success\n             except Exception as e:\n                 logger.error(f\"Error deleting documents: {e}\")\n                 return False\n-        return True  # Return True if collection doesn't exist (nothing to delete)\n+        # Return True if collection doesn't exist (nothing to delete)\n+        return True\n \n     def prepare_doc(self, document: Document, filters: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n         \"\"\"Prepare a document for insertion or upsertion into MongoDB.\"\"\"\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/tests/integration/agent/test_external_execution_flows.py",
            "diff": "diff --git a/libs/agno/tests/integration/agent/test_external_execution_flows.py b/libs/agno/tests/integration/agent/test_external_execution_flows.py\nnew file mode 100644\nindex 000000000..7e692a3d9\n--- /dev/null\n+++ b/libs/agno/tests/integration/agent/test_external_execution_flows.py\n@@ -0,0 +1,197 @@\n+import pytest\n+\n+from agno.agent import Agent, RunResponse  # noqa\n+from agno.models.openai import OpenAIChat\n+from agno.tools.decorator import tool\n+\n+\n+def test_tool_call_requires_external_execution():\n+    @tool(external_execution=True)\n+    def send_email(to: str, subject: str, body: str):\n+        pass\n+\n+    agent = Agent(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        tools=[send_email],\n+        show_tool_calls=True,\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"Send an email to john@doe.com with the subject 'Test' and the body 'Hello, how are you?'\")\n+\n+    assert response.is_paused\n+    assert response.tools[0].external_execution_required\n+    assert response.tools[0].tool_name == \"send_email\"\n+    assert response.tools[0].tool_args == {\"to\": \"john@doe.com\", \"subject\": \"Test\", \"body\": \"Hello, how are you?\"}\n+\n+    # Mark the tool as confirmed\n+    response.tools[0].result = \"Email sent to john@doe.com with subject Test and body Hello, how are you?\"\n+\n+    response = agent.continue_run(response)\n+    assert response.is_paused is False\n+\n+\n+def test_tool_call_requires_external_execution_stream():\n+    @tool(external_execution=True)\n+    def send_email(to: str, subject: str, body: str):\n+        pass\n+\n+    agent = Agent(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        tools=[send_email],\n+        show_tool_calls=True,\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    found_external_execution = False\n+    for response in agent.run(\n+        \"Send an email to john@doe.com with the subject 'Test' and the body 'Hello, how are you?'\", stream=True\n+    ):\n+        if response.is_paused:\n+            assert response.tools[0].external_execution_required\n+            assert response.tools[0].tool_name == \"send_email\"\n+            assert response.tools[0].tool_args == {\n+                \"to\": \"john@doe.com\",\n+                \"subject\": \"Test\",\n+                \"body\": \"Hello, how are you?\",\n+            }\n+\n+            # Mark the tool as confirmed\n+            response.tools[0].result = \"Email sent to john@doe.com with subject Test and body Hello, how are you?\"\n+            found_external_execution = True\n+    assert found_external_execution, \"No tools were found to require external execution\"\n+\n+    found_external_execution = False\n+    for response in agent.continue_run(response, stream=True):\n+        if response.is_paused:\n+            found_external_execution = True\n+    assert found_external_execution is False, \"Some tools still require external execution\"\n+\n+\n+@pytest.mark.asyncio\n+async def test_tool_call_requires_external_execution_async():\n+    @tool(external_execution=True)\n+    async def send_email(to: str, subject: str, body: str):\n+        pass\n+\n+    agent = Agent(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        tools=[send_email],\n+        show_tool_calls=True,\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = await agent.arun(\"What is the weather in Tokyo?\")\n+\n+    assert response.is_paused\n+    assert response.tools[0].external_execution_required\n+    assert response.tools[0].tool_name == \"send_email\"\n+    assert response.tools[0].tool_args == {\"to\": \"john@doe.com\", \"subject\": \"Test\", \"body\": \"Hello, how are you?\"}\n+\n+    # Mark the tool as confirmed\n+    response.tools[0].result = \"Email sent to john@doe.com with subject Test and body Hello, how are you?\"\n+\n+    response = await agent.acontinue_run(response)\n+    assert response.is_paused is False\n+\n+\n+def test_tool_call_requires_external_execution_error():\n+    @tool(external_execution=True)\n+    def send_email(to: str, subject: str, body: str):\n+        pass\n+\n+    agent = Agent(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        tools=[send_email],\n+        show_tool_calls=True,\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"Send an email to john@doe.com with the subject 'Test' and the body 'Hello, how are you?'\")\n+\n+    # Check that we cannot continue without confirmation\n+    with pytest.raises(ValueError):\n+        response = agent.continue_run(response)\n+\n+\n+@pytest.mark.asyncio\n+async def test_tool_call_requires_external_execution_stream_async():\n+    @tool(external_execution=True)\n+    async def send_email(to: str, subject: str, body: str):\n+        pass\n+\n+    agent = Agent(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        tools=[send_email],\n+        show_tool_calls=True,\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    found_external_execution = False\n+    async for response in await agent.arun(\n+        \"Send an email to john@doe.com with the subject 'Test' and the body 'Hello, how are you?'\", stream=True\n+    ):\n+        if response.is_paused:\n+            assert response.tools[0].external_execution_required\n+            assert response.tools[0].tool_name == \"send_email\"\n+            assert response.tools[0].tool_args == {\n+                \"to\": \"john@doe.com\",\n+                \"subject\": \"Test\",\n+                \"body\": \"Hello, how are you?\",\n+            }\n+\n+            # Mark the tool as confirmed\n+            response.tools[0].result = \"Email sent to john@doe.com with subject Test and body Hello, how are you?\"\n+            found_external_execution = True\n+    assert found_external_execution, \"No tools were found to require external execution\"\n+\n+    found_external_execution = False\n+    async for response in await agent.acontinue_run(response, stream=True):\n+        if response.is_paused:\n+            found_external_execution = True\n+    assert found_external_execution is False, \"Some tools still require external execution\"\n+\n+\n+def test_tool_call_multiple_requires_external_execution():\n+    @tool(external_execution=True)\n+    def get_the_weather(city: str):\n+        pass\n+\n+    def get_activities(city: str):\n+        pass\n+\n+    agent = Agent(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        tools=[get_the_weather, get_activities],\n+        show_tool_calls=True,\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"What is the weather in Tokyo and what are the activities?\")\n+\n+    assert response.is_paused\n+    tool_found = False\n+    for _t in response.tools:\n+        if _t.external_execution_required:\n+            tool_found = True\n+            assert _t.tool_name == \"get_the_weather\"\n+            assert _t.tool_args == {\"city\": \"Tokyo\"}\n+            _t.result = \"It is currently 70 degrees and cloudy in Tokyo\"\n+\n+    assert tool_found, \"No tool was found to require external execution\"\n+\n+    response = agent.continue_run(response)\n+    assert response.is_paused is False\n+    assert response.content\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/tests/integration/agent/test_user_confirmation_flows.py",
            "diff": "diff --git a/libs/agno/tests/integration/agent/test_user_confirmation_flows.py b/libs/agno/tests/integration/agent/test_user_confirmation_flows.py\nnew file mode 100644\nindex 000000000..861059ba7\n--- /dev/null\n+++ b/libs/agno/tests/integration/agent/test_user_confirmation_flows.py\n@@ -0,0 +1,187 @@\n+import pytest\n+\n+from agno.agent import Agent, RunResponse  # noqa\n+from agno.models.openai import OpenAIChat\n+from agno.tools.decorator import tool\n+\n+\n+def test_tool_call_requires_confirmation():\n+    @tool(requires_confirmation=True)\n+    def get_the_weather(city: str):\n+        return f\"It is currently 70 degrees and cloudy in {city}\"\n+\n+    agent = Agent(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        tools=[get_the_weather],\n+        show_tool_calls=True,\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"What is the weather in Tokyo?\")\n+\n+    assert response.is_paused\n+    assert response.tools[0].requires_confirmation\n+    assert response.tools[0].tool_name == \"get_the_weather\"\n+    assert response.tools[0].tool_args == {\"city\": \"Tokyo\"}\n+\n+    # Mark the tool as confirmed\n+    response.tools[0].confirmed = True\n+\n+    response = agent.continue_run(response)\n+    assert response.is_paused is False\n+    assert response.tools[0].result == \"It is currently 70 degrees and cloudy in Tokyo\"\n+\n+\n+def test_tool_call_requires_confirmation_stream():\n+    @tool(requires_confirmation=True)\n+    def get_the_weather(city: str):\n+        return f\"It is currently 70 degrees and cloudy in {city}\"\n+\n+    agent = Agent(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        tools=[get_the_weather],\n+        show_tool_calls=True,\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    found_confirmation = False\n+    for response in agent.run(\"What is the weather in Tokyo?\", stream=True):\n+        if response.is_paused:\n+            assert response.tools[0].requires_confirmation\n+            assert response.tools[0].tool_name == \"get_the_weather\"\n+            assert response.tools[0].tool_args == {\"city\": \"Tokyo\"}\n+\n+            # Mark the tool as confirmed\n+            response.tools[0].confirmed = True\n+            found_confirmation = True\n+    assert found_confirmation, \"No tools were found to require confirmation\"\n+\n+    found_confirmation = False\n+    for response in agent.continue_run(response, stream=True):\n+        if response.is_paused:\n+            found_confirmation = True\n+    assert found_confirmation is False, \"Some tools still require confirmation\"\n+\n+\n+@pytest.mark.asyncio\n+async def test_tool_call_requires_confirmation_async():\n+    @tool(requires_confirmation=True)\n+    async def get_the_weather(city: str):\n+        return f\"It is currently 70 degrees and cloudy in {city}\"\n+\n+    agent = Agent(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        tools=[get_the_weather],\n+        show_tool_calls=True,\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = await agent.arun(\"What is the weather in Tokyo?\")\n+\n+    assert response.is_paused\n+    assert response.tools[0].requires_confirmation\n+    assert response.tools[0].tool_name == \"get_the_weather\"\n+    assert response.tools[0].tool_args == {\"city\": \"Tokyo\"}\n+\n+    # Mark the tool as confirmed\n+    response.tools[0].confirmed = True\n+\n+    response = await agent.acontinue_run(response)\n+    assert response.is_paused is False\n+    assert response.tools[0].result == \"It is currently 70 degrees and cloudy in Tokyo\"\n+\n+\n+def test_tool_call_requires_confirmation_error():\n+    @tool(requires_confirmation=True)\n+    def get_the_weather(city: str):\n+        return f\"It is currently 70 degrees and cloudy in {city}\"\n+\n+    agent = Agent(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        tools=[get_the_weather],\n+        show_tool_calls=True,\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"What is the weather in Tokyo?\")\n+\n+    # Check that we cannot continue without confirmation\n+    with pytest.raises(ValueError):\n+        response = agent.continue_run(response)\n+\n+\n+@pytest.mark.asyncio\n+async def test_tool_call_requires_confirmation_stream_async():\n+    @tool(requires_confirmation=True)\n+    async def get_the_weather(city: str):\n+        return f\"It is currently 70 degrees and cloudy in {city}\"\n+\n+    agent = Agent(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        tools=[get_the_weather],\n+        show_tool_calls=True,\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    found_confirmation = False\n+    async for response in await agent.arun(\"What is the weather in Tokyo?\", stream=True):\n+        if response.is_paused:\n+            assert response.tools[0].requires_confirmation\n+            assert response.tools[0].tool_name == \"get_the_weather\"\n+            assert response.tools[0].tool_args == {\"city\": \"Tokyo\"}\n+\n+            # Mark the tool as confirmed\n+            response.tools[0].confirmed = True\n+            found_confirmation = True\n+    assert found_confirmation, \"No tools were found to require confirmation\"\n+\n+    found_confirmation = False\n+    async for response in await agent.acontinue_run(response, stream=True):\n+        if response.is_paused:\n+            found_confirmation = True\n+    assert found_confirmation is False, \"Some tools still require confirmation\"\n+\n+\n+def test_tool_call_multiple_requires_confirmation():\n+    @tool(requires_confirmation=True)\n+    def get_the_weather(city: str):\n+        return f\"It is currently 70 degrees and cloudy in {city}\"\n+\n+    def get_activities(city: str):\n+        return f\"The following activities are available in {city}: \\n - Shopping \\n - Eating \\n - Drinking\"\n+\n+    agent = Agent(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        tools=[get_the_weather, get_activities],\n+        show_tool_calls=True,\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"What is the weather in Tokyo and what are the activities?\")\n+\n+    assert response.is_paused\n+    tool_found = False\n+    for _t in response.tools:\n+        if _t.requires_confirmation:\n+            tool_found = True\n+            assert _t.tool_name == \"get_the_weather\"\n+            assert _t.tool_args == {\"city\": \"Tokyo\"}\n+            _t.confirmed = True\n+\n+    assert tool_found, \"No tool was found to require confirmation\"\n+\n+    response = agent.continue_run(response)\n+    assert response.is_paused is False\n+    assert response.content\n"
        },
        {
            "commit": "55c8c01643e65d53cf29664e5d5dc6d7307272d6",
            "file_path": "libs/agno/tests/unit/reader/test_firecrawl_reader.py",
            "diff": "diff --git a/libs/agno/tests/unit/reader/test_firecrawl_reader.py b/libs/agno/tests/unit/reader/test_firecrawl_reader.py\nindex 63ff22ef3..8a9d48471 100644\n--- a/libs/agno/tests/unit/reader/test_firecrawl_reader.py\n+++ b/libs/agno/tests/unit/reader/test_firecrawl_reader.py\n@@ -57,7 +57,7 @@ def test_scrape_basic(mock_scrape_response):\n \n         # Verify FirecrawlApp was called correctly\n         MockFirecrawlApp.assert_called_once_with(api_key=None)\n-        mock_app.scrape_url.assert_called_once_with(\"https://example.com\", params=None)\n+        mock_app.scrape_url.assert_called_once_with(\"https://example.com\")\n \n \n def test_scrape_with_api_key_and_params():\n@@ -75,7 +75,7 @@ def test_scrape_with_api_key_and_params():\n \n         # Verify FirecrawlApp was called with correct parameters\n         MockFirecrawlApp.assert_called_once_with(api_key=api_key)\n-        mock_app.scrape_url.assert_called_once_with(\"https://example.com\", params=params)\n+        mock_app.scrape_url.assert_called_once_with(\"https://example.com\", **params)\n \n \n def test_scrape_empty_response():\n@@ -167,7 +167,7 @@ def test_crawl_basic(mock_crawl_response):\n \n         # Verify FirecrawlApp was called correctly\n         MockFirecrawlApp.assert_called_once_with(api_key=None)\n-        mock_app.crawl_url.assert_called_once_with(\"https://example.com\", params=None)\n+        mock_app.crawl_url.assert_called_once_with(\"https://example.com\")\n \n \n def test_crawl_empty_response():\n"
        }
    ]
}