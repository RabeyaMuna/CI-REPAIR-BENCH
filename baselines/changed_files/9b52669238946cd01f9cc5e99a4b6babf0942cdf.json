{
    "sha_fail": "9b52669238946cd01f9cc5e99a4b6babf0942cdf",
    "changed_files": [
        {
            "commit": "9b52669238946cd01f9cc5e99a4b6babf0942cdf",
            "file_path": "libs/agno/tests/integration/models/cerebras/test_basic.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/cerebras/test_basic.py b/libs/agno/tests/integration/models/cerebras/test_basic.py\nnew file mode 100644\nindex 000000000..fd1491aeb\n--- /dev/null\n+++ b/libs/agno/tests/integration/models/cerebras/test_basic.py\n@@ -0,0 +1,138 @@\n+import pytest\n+from pydantic import BaseModel, Field\n+\n+from agno.agent import Agent, RunResponse\n+from agno.models.cerebras import Cerebras\n+from agno.storage.sqlite import SqliteStorage\n+\n+\n+def _assert_metrics(response: RunResponse):\n+    input_tokens = response.metrics.get(\"input_tokens\", [])\n+    output_tokens = response.metrics.get(\"output_tokens\", [])\n+    total_tokens = response.metrics.get(\"total_tokens\", [])\n+\n+    assert sum(input_tokens) > 0\n+    assert sum(output_tokens) > 0\n+    assert sum(total_tokens) > 0\n+    assert sum(total_tokens) == sum(input_tokens) + sum(output_tokens)\n+\n+\n+def test_basic():\n+    agent = Agent(model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"), markdown=True, telemetry=False, monitoring=False)\n+\n+    response: RunResponse = agent.run(\"Share a 2 sentence horror story\")\n+\n+    assert response.content is not None\n+    assert len(response.messages) == 3\n+    assert [m.role for m in response.messages] == [\"system\", \"user\", \"assistant\"]\n+\n+    _assert_metrics(response)\n+\n+\n+def test_basic_stream():\n+    agent = Agent(model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"), markdown=True, telemetry=False, monitoring=False)\n+\n+    response_stream = agent.run(\"Share a 2 sentence horror story\", stream=True)\n+\n+    # Verify it's an iterator\n+    assert hasattr(response_stream, \"__iter__\")\n+\n+    responses = list(response_stream)\n+    assert len(responses) > 0\n+    for response in responses:\n+        assert isinstance(response, RunResponse)\n+        assert response.content is not None\n+\n+    _assert_metrics(agent.run_response)\n+\n+\n+@pytest.mark.asyncio\n+async def test_async_basic():\n+    agent = Agent(model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"), markdown=True, telemetry=False, monitoring=False)\n+\n+    response = await agent.arun(\"Share a 2 sentence horror story\")\n+\n+    assert response.content is not None\n+    assert len(response.messages) == 3\n+    assert [m.role for m in response.messages] == [\"system\", \"user\", \"assistant\"]\n+    _assert_metrics(response)\n+\n+\n+@pytest.mark.asyncio\n+async def test_async_basic_stream():\n+    agent = Agent(model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"), markdown=True, telemetry=False, monitoring=False)\n+\n+    response_stream = await agent.arun(\"Share a 2 sentence horror story\", stream=True)\n+\n+    async for response in response_stream:\n+        assert isinstance(response, RunResponse)\n+        assert response.content is not None\n+\n+    _assert_metrics(agent.run_response)\n+\n+\n+def test_with_memory():\n+    agent = Agent(\n+        model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"),\n+        add_history_to_messages=True,\n+        num_history_runs=5,\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    # First interaction\n+    response1 = agent.run(\"My name is John Smith\")\n+    assert response1.content is not None\n+\n+    # Second interaction should remember the name\n+    response2 = agent.run(\"What's my name?\")\n+    assert \"John Smith\" in response2.content\n+\n+    # Verify memories were created\n+    messages = agent.get_messages_for_session()\n+    assert len(messages) == 5\n+    assert [m.role for m in messages] == [\"system\", \"user\", \"assistant\", \"user\", \"assistant\"]\n+\n+    # Test metrics structure and types\n+    _assert_metrics(response2)\n+\n+\n+def test_structured_output():\n+    class MovieScript(BaseModel):\n+        title: str = Field(..., description=\"Movie title\")\n+        genre: str = Field(..., description=\"Movie genre\")\n+        plot: str = Field(..., description=\"Brief plot summary\")\n+\n+    agent = Agent(\n+        model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"),\n+        response_model=MovieScript,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"Create a movie about time travel\")\n+\n+    # Verify structured output\n+    assert isinstance(response.content, MovieScript)\n+    assert response.content.title is not None\n+    assert response.content.genre is not None\n+    assert response.content.plot is not None\n+\n+\n+def test_history():\n+    agent = Agent(\n+        model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"),\n+        storage=SqliteStorage(table_name=\"agent_sessions\", db_file=\"tmp/agent_storage.db\"),\n+        add_history_to_messages=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+    agent.run(\"Hello\")\n+    assert len(agent.run_response.messages) == 2\n+    agent.run(\"Hello 2\")\n+    assert len(agent.run_response.messages) == 4\n+    agent.run(\"Hello 3\")\n+    assert len(agent.run_response.messages) == 6\n+    agent.run(\"Hello 4\")\n+    assert len(agent.run_response.messages) == 8\n"
        },
        {
            "commit": "9b52669238946cd01f9cc5e99a4b6babf0942cdf",
            "file_path": "libs/agno/tests/integration/models/cerebras/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/cerebras/test_tool_use.py b/libs/agno/tests/integration/models/cerebras/test_tool_use.py\nnew file mode 100644\nindex 000000000..1aa1675cb\n--- /dev/null\n+++ b/libs/agno/tests/integration/models/cerebras/test_tool_use.py\n@@ -0,0 +1,113 @@\n+from typing import Optional\n+\n+import pytest\n+\n+from agno.agent import Agent, RunResponse  # noqa\n+from agno.models.cerebras import Cerebras\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.tools.googlesearch import GoogleSearchTools\n+\n+\n+def test_tool_use():\n+    agent = Agent(\n+        model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"),\n+        tools=[GoogleSearchTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"What's happening in France?\")\n+\n+    # Verify tool usage\n+    assert any(msg.tool_calls for msg in response.messages)\n+    assert response.content is not None\n+    assert \"France\" in response.content\n+\n+\n+def test_tool_use_stream():\n+    agent = Agent(\n+        model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"),\n+        tools=[GoogleSearchTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response_stream = agent.run(\"What's happening in France?\", stream=True)\n+\n+    responses = []\n+    tool_call_seen = False\n+\n+    for chunk in response_stream:\n+        assert isinstance(chunk, RunResponse)\n+        responses.append(chunk)\n+        if chunk.tools:\n+            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+                tool_call_seen = True\n+\n+    assert len(responses) > 0\n+    assert tool_call_seen, \"No tool calls observed in stream\"\n+    assert any(\"France\" in r.content for r in responses if r.content)\n+\n+\n+@pytest.mark.asyncio\n+async def test_async_tool_use():\n+    agent = Agent(\n+        model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"),\n+        tools=[GoogleSearchTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = await agent.arun(\"What's happening in France?\")\n+\n+    # Verify tool usage\n+    assert any(msg.tool_calls for msg in response.messages if msg.role == \"assistant\")\n+    assert response.content is not None\n+    assert \"France\" in response.content\n+\n+\n+@pytest.mark.asyncio\n+async def test_async_tool_use_stream():\n+    agent = Agent(\n+        model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"),\n+        tools=[GoogleSearchTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response_stream = await agent.arun(\"What's happening in France?\", stream=True)\n+\n+    responses = []\n+    tool_call_seen = False\n+\n+    async for chunk in response_stream:\n+        assert isinstance(chunk, RunResponse)\n+        responses.append(chunk)\n+        if chunk.tools:\n+            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+                tool_call_seen = True\n+\n+    assert len(responses) > 0\n+    assert tool_call_seen, \"No tool calls observed in stream\"\n+    assert any(\"France\" in r.content for r in responses if r.content)\n+\n+\n+def test_tool_use_with_content():\n+    agent = Agent(\n+        model=Cerebras(id=\"llama-4-scout-17b-16e-instruct\"),\n+        tools=[GoogleSearchTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"What's happening in France? Summarize the key events.\")\n+\n+    # Verify tool usage\n+    assert any(msg.tool_calls for msg in response.messages)\n+    assert response.content is not None\n+    assert \"France\" in response.content\n"
        },
        {
            "commit": "9b52669238946cd01f9cc5e99a4b6babf0942cdf",
            "file_path": "libs/agno/tests/integration/models/cerebras_openai/test_basic.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/cerebras_openai/test_basic.py b/libs/agno/tests/integration/models/cerebras_openai/test_basic.py\nnew file mode 100644\nindex 000000000..96939ceda\n--- /dev/null\n+++ b/libs/agno/tests/integration/models/cerebras_openai/test_basic.py\n@@ -0,0 +1,128 @@\n+import pytest\n+from pydantic import BaseModel, Field\n+\n+from agno.agent import Agent, RunResponse\n+from agno.models.cerebras import CerebrasOpenAI\n+from agno.storage.sqlite import SqliteStorage\n+\n+\n+def _assert_metrics(response: RunResponse):\n+    input_tokens = response.metrics.get(\"input_tokens\", [])\n+    output_tokens = response.metrics.get(\"output_tokens\", [])\n+    total_tokens = response.metrics.get(\"total_tokens\", [])\n+\n+    assert sum(input_tokens) > 0\n+    assert sum(output_tokens) > 0\n+    assert sum(total_tokens) > 0\n+    assert sum(total_tokens) == sum(input_tokens) + sum(output_tokens)\n+\n+\n+def test_basic():\n+    agent = Agent(\n+        model=CerebrasOpenAI(id=\"llama-4-scout-17b-16e-instruct\"), markdown=True, telemetry=False, monitoring=False\n+    )\n+\n+    response: RunResponse = agent.run(\"Share a 2 sentence horror story\")\n+\n+    assert response.content is not None\n+    assert len(response.messages) == 3\n+    assert [m.role for m in response.messages] == [\"system\", \"user\", \"assistant\"]\n+\n+    _assert_metrics(response)\n+\n+\n+def test_basic_stream():\n+    agent = Agent(\n+        model=CerebrasOpenAI(id=\"llama-4-scout-17b-16e-instruct\"), markdown=True, telemetry=False, monitoring=False\n+    )\n+\n+    response_stream = agent.run(\"Share a 2 sentence horror story\", stream=True)\n+\n+    # Verify it's an iterator\n+    assert hasattr(response_stream, \"__iter__\")\n+\n+    responses = list(response_stream)\n+    assert len(responses) > 0\n+    for response in responses:\n+        assert isinstance(response, RunResponse)\n+        assert response.content is not None\n+\n+    _assert_metrics(agent.run_response)\n+\n+\n+@pytest.mark.asyncio\n+async def test_async_basic():\n+    agent = Agent(\n+        model=CerebrasOpenAI(id=\"llama-4-scout-17b-16e-instruct\"), markdown=True, telemetry=False, monitoring=False\n+    )\n+\n+    response = await agent.arun(\"Share a 2 sentence horror story\")\n+\n+    assert response.content is not None\n+    assert len(response.messages) == 3\n+    assert [m.role for m in response.messages] == [\"system\", \"user\", \"assistant\"]\n+    _assert_metrics(response)\n+\n+\n+@pytest.mark.asyncio\n+async def test_async_basic_stream():\n+    agent = Agent(\n+        model=CerebrasOpenAI(id=\"llama-4-scout-17b-16e-instruct\"), markdown=True, telemetry=False, monitoring=False\n+    )\n+\n+    response_stream = await agent.arun(\"Share a 2 sentence horror story\", stream=True)\n+\n+    async for response in response_stream:\n+        assert isinstance(response, RunResponse)\n+        assert response.content is not None\n+\n+    _assert_metrics(agent.run_response)\n+\n+\n+def test_with_memory():\n+    agent = Agent(\n+        model=CerebrasOpenAI(id=\"llama-4-scout-17b-16e-instruct\"),\n+        add_history_to_messages=True,\n+        num_history_runs=5,\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    # First interaction\n+    response1 = agent.run(\"My name is John Smith\")\n+    assert response1.content is not None\n+\n+    # Second interaction should remember the name\n+    response2 = agent.run(\"What's my name?\")\n+    assert \"John Smith\" in response2.content\n+\n+    # Verify memories were created\n+    messages = agent.get_messages_for_session()\n+    assert len(messages) == 5\n+    assert [m.role for m in messages] == [\"system\", \"user\", \"assistant\", \"user\", \"assistant\"]\n+\n+    # Test metrics structure and types\n+    _assert_metrics(response2)\n+\n+\n+def test_structured_output():\n+    class MovieScript(BaseModel):\n+        title: str = Field(..., description=\"Movie title\")\n+        genre: str = Field(..., description=\"Movie genre\")\n+        plot: str = Field(..., description=\"Brief plot summary\")\n+\n+    agent = Agent(\n+        model=CerebrasOpenAI(id=\"llama-4-scout-17b-16e-instruct\"),\n+        response_model=MovieScript,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"Create a movie about time travel\")\n+\n+    # Verify structured output\n+    assert isinstance(response.content, MovieScript)\n+    assert response.content.title is not None\n+    assert response.content.genre is not None\n+    assert response.content.plot is not None\n"
        },
        {
            "commit": "9b52669238946cd01f9cc5e99a4b6babf0942cdf",
            "file_path": "libs/agno/tests/integration/models/cerebras_openai/test_tool_use.py",
            "diff": "diff --git a/libs/agno/tests/integration/models/cerebras_openai/test_tool_use.py b/libs/agno/tests/integration/models/cerebras_openai/test_tool_use.py\nnew file mode 100644\nindex 000000000..53ab63dde\n--- /dev/null\n+++ b/libs/agno/tests/integration/models/cerebras_openai/test_tool_use.py\n@@ -0,0 +1,111 @@\n+import pytest\n+\n+from agno.agent import Agent, RunResponse  # noqa\n+from agno.models.cerebras import CerebrasOpenAI\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.tools.googlesearch import GoogleSearchTools\n+\n+\n+def test_tool_use():\n+    agent = Agent(\n+        model=CerebrasOpenAI(id=\"llama-4-scout-17b-16e-instruct\"),\n+        tools=[GoogleSearchTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"What's happening in France?\")\n+\n+    # Verify tool usage\n+    assert any(msg.tool_calls for msg in response.messages)\n+    assert response.content is not None\n+    assert \"France\" in response.content\n+\n+\n+def test_tool_use_stream():\n+    agent = Agent(\n+        model=CerebrasOpenAI(id=\"llama-4-scout-17b-16e-instruct\"),\n+        tools=[GoogleSearchTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response_stream = agent.run(\"What's happening in France?\", stream=True)\n+\n+    responses = []\n+    tool_call_seen = False\n+\n+    for chunk in response_stream:\n+        assert isinstance(chunk, RunResponse)\n+        responses.append(chunk)\n+        if chunk.tools:\n+            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+                tool_call_seen = True\n+\n+    assert len(responses) > 0\n+    assert tool_call_seen, \"No tool calls observed in stream\"\n+    assert any(\"France\" in r.content for r in responses if r.content)\n+\n+\n+@pytest.mark.asyncio\n+async def test_async_tool_use():\n+    agent = Agent(\n+        model=CerebrasOpenAI(id=\"llama-4-scout-17b-16e-instruct\"),\n+        tools=[GoogleSearchTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = await agent.arun(\"What's happening in France?\")\n+\n+    # Verify tool usage\n+    assert any(msg.tool_calls for msg in response.messages if msg.role == \"assistant\")\n+    assert response.content is not None\n+    assert \"France\" in response.content\n+\n+\n+@pytest.mark.asyncio\n+async def test_async_tool_use_stream():\n+    agent = Agent(\n+        model=CerebrasOpenAI(id=\"llama-4-scout-17b-16e-instruct\"),\n+        tools=[GoogleSearchTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response_stream = await agent.arun(\"What's happening in France?\", stream=True)\n+\n+    responses = []\n+    tool_call_seen = False\n+\n+    async for chunk in response_stream:\n+        assert isinstance(chunk, RunResponse)\n+        responses.append(chunk)\n+        if chunk.tools:\n+            if any(tc.get(\"tool_name\") for tc in chunk.tools):\n+                tool_call_seen = True\n+\n+    assert len(responses) > 0\n+    assert tool_call_seen, \"No tool calls observed in stream\"\n+    assert any(\"France\" in r.content for r in responses if r.content)\n+\n+\n+def test_tool_use_with_content():\n+    agent = Agent(\n+        model=CerebrasOpenAI(id=\"llama-4-scout-17b-16e-instruct\"),\n+        tools=[GoogleSearchTools(cache_results=True)],\n+        show_tool_calls=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response = agent.run(\"What's happening in France? Summarize the key events.\")\n+\n+    # Verify tool usage\n+    assert any(msg.tool_calls for msg in response.messages)\n+    assert response.content is not None\n+    assert \"France\" in response.content\n"
        }
    ]
}