{
    "sha_fail": "7751beb21807fa7f206079b8f69bf887ec16a199",
    "changed_files": [
        {
            "commit": "7751beb21807fa7f206079b8f69bf887ec16a199",
            "file_path": "dask/dataframe/_compat.py",
            "diff": "diff --git a/dask/dataframe/_compat.py b/dask/dataframe/_compat.py\nindex e6fa2c21..749fb559 100644\n--- a/dask/dataframe/_compat.py\n+++ b/dask/dataframe/_compat.py\n@@ -26,6 +26,7 @@ PANDAS_GE_300 = PANDAS_VERSION.major >= 3\n \n PYARROW_VERSION = Version(pa.__version__)\n PYARROW_GE_1500 = PYARROW_VERSION.release >= (15, 0, 0)\n+PYARROW_GE_2101 = PYARROW_VERSION.release >= (21, 0, 1)\n \n import pandas.testing as tm\n \n"
        },
        {
            "commit": "7751beb21807fa7f206079b8f69bf887ec16a199",
            "file_path": "dask/dataframe/backends.py",
            "diff": "diff --git a/dask/dataframe/backends.py b/dask/dataframe/backends.py\nindex be9c797b..71fa9cc8 100644\n--- a/dask/dataframe/backends.py\n+++ b/dask/dataframe/backends.py\n@@ -5,6 +5,7 @@ from collections.abc import Iterable\n \n import numpy as np\n import pandas as pd\n+import pyarrow as pa\n from pandas.api.types import is_scalar, union_categoricals\n \n from dask.array import Array\n@@ -189,6 +190,19 @@ def _(x):\n @make_meta_dispatch.register((pd.Series, pd.DataFrame))\n def _(x, index=None):\n     out = x.iloc[:0].copy(deep=True)\n+\n+    # https://github.com/pandas-dev/pandas/issues/61930\n+    # pandas shallow copies arrow-backed extension arrays.\n+    # Use pyarrow.compute.take to get a new array that doesn't\n+    # share any memory with the original array.\n+\n+    for k, v in out.items():\n+        if isinstance(v.array, pd.arrays.ArrowExtensionArray):\n+            values = pa.chunked_array([v.array]).combine_chunks()\n+            out[k] = v._constructor(\n+                pd.array(values, dtype=v.array.dtype), index=v.index, name=v.name\n+            )\n+\n     # index isn't copied by default in pandas, even if deep=true\n     out.index = out.index.copy(deep=True)\n     return out\n@@ -565,6 +579,38 @@ def group_split_pandas(df, c, k, ignore_index=False):\n     return ShuffleGroupResult(zip(range(k), parts))\n \n \n+def _union_categoricals_wrapper(\n+    dfs: list[pd.CategoricalIndex] | list[pd.Series], **kwargs\n+) -> pd.Categorical:\n+    \"\"\"\n+    A wrapper around pandas' union_categoricals that handles some dtype issues.\n+\n+    union_categoricals requires that the dtype of each array's categories match.\n+    So you can't union ``Categorical(['a', 'b'])`` and ``Categorical([1, 2])``\n+    since the dtype (str vs. int) doesn't match.\n+\n+    *Somewhere* in Dask, we're possibly creating an empty ``Categorical``\n+    with a dtype of ``object``. In pandas 2.x, we could union that with string\n+    categories since they both used object dtype. But pandas 3.x uses string\n+    dtype for categories.\n+\n+    This wrapper handles that by creating a new ``Categorical`` with the\n+    correct dtype.\n+    \"\"\"\n+    categories_dtypes = {cat.dtype.categories.dtype.name for cat in dfs}\n+    if \"object\" in categories_dtypes and \"str\" in categories_dtypes:\n+        dfs = [\n+            (\n+                type(cat)(pd.Categorical(pd.Index([], dtype=\"str\")), name=cat.name)\n+                if cat.dtype.categories.dtype.name == \"object\" and len(cat) == 0\n+                else cat\n+            )\n+            for cat in dfs\n+        ]\n+\n+    return union_categoricals(dfs, **kwargs)\n+\n+\n @concat_dispatch.register((pd.DataFrame, pd.Series, pd.Index))\n def concat_pandas(\n     dfs,\n@@ -587,7 +633,8 @@ def concat_pandas(\n                 if not isinstance(dfs[i], pd.CategoricalIndex):\n                     dfs[i] = dfs[i].astype(\"category\")\n             return pd.CategoricalIndex(\n-                union_categoricals(dfs, ignore_order=ignore_order), name=dfs[0].name\n+                _union_categoricals_wrapper(dfs, ignore_order=ignore_order),\n+                name=dfs[0].name,\n             )\n         elif isinstance(dfs[0], pd.MultiIndex):\n             first, rest = dfs[0], dfs[1:]\n@@ -682,7 +729,7 @@ def concat_pandas(\n                             codes, sample.cat.categories, sample.cat.ordered\n                         )\n                         parts.append(data)\n-                out[col] = union_categoricals(parts, ignore_order=ignore_order)\n+                out[col] = _union_categoricals_wrapper(parts, ignore_order=ignore_order)\n                 # Pandas resets index type on assignment if frame is empty\n                 # https://github.com/pandas-dev/pandas/issues/17101\n                 if not len(temp_ind):\n"
        },
        {
            "commit": "7751beb21807fa7f206079b8f69bf887ec16a199",
            "file_path": "dask/dataframe/dask_expr/io/tests/test_from_pandas.py",
            "diff": "diff --git a/dask/dataframe/dask_expr/io/tests/test_from_pandas.py b/dask/dataframe/dask_expr/io/tests/test_from_pandas.py\nindex dab466b6..534d721c 100644\n--- a/dask/dataframe/dask_expr/io/tests/test_from_pandas.py\n+++ b/dask/dataframe/dask_expr/io/tests/test_from_pandas.py\n@@ -5,6 +5,7 @@ import copy\n import pytest\n \n import dask\n+from dask.dataframe._compat import PANDAS_GE_300\n from dask.dataframe.dask_expr import from_pandas, repartition\n from dask.dataframe.dask_expr.tests._util import _backend_library\n from dask.dataframe.utils import assert_eq, pyarrow_strings_enabled\n@@ -126,12 +127,18 @@ def test_from_pandas_string_option():\n     assert df.compute().index.dtype == dtype\n     assert_eq(df, pdf)\n \n+    if PANDAS_GE_300:\n+        dtype = \"string\"\n+    else:\n+        dtype = \"object\"\n+\n     with dask.config.set({\"dataframe.convert-string\": False}):\n         df = from_pandas(pdf, npartitions=2)\n-        assert df.dtypes[\"y\"] == \"object\"\n-        assert df.index.dtype == \"object\"\n-        assert df.compute().dtypes[\"y\"] == \"object\"\n-        assert df.compute().index.dtype == \"object\"\n+\n+        assert df.dtypes[\"y\"] == dtype\n+        assert df.index.dtype == dtype\n+        assert df.compute().dtypes[\"y\"] == dtype\n+        assert df.compute().index.dtype == dtype\n         assert_eq(df, pdf)\n \n \n"
        },
        {
            "commit": "7751beb21807fa7f206079b8f69bf887ec16a199",
            "file_path": "dask/dataframe/dask_expr/tests/test_collection.py",
            "diff": "diff --git a/dask/dataframe/dask_expr/tests/test_collection.py b/dask/dataframe/dask_expr/tests/test_collection.py\nindex 84530207..5386a101 100644\n--- a/dask/dataframe/dask_expr/tests/test_collection.py\n+++ b/dask/dataframe/dask_expr/tests/test_collection.py\n@@ -15,7 +15,7 @@ import dask\n import dask.array as da\n from dask._compatibility import WINDOWS\n from dask.array.numpy_compat import NUMPY_GE_200\n-from dask.dataframe._compat import PANDAS_GE_210, PANDAS_GE_220\n+from dask.dataframe._compat import PANDAS_GE_210, PANDAS_GE_220, PANDAS_GE_300\n from dask.dataframe.dask_expr import (\n     DataFrame,\n     Series,\n@@ -872,6 +872,7 @@ def test_to_datetime():\n         to_datetime(1490195805)\n \n \n+@pytest.mark.filterwarnings(\"ignore::UserWarning\")\n def test_to_numeric(pdf, df):\n     import dask.array as da\n \n@@ -1635,10 +1636,14 @@ def test_values():\n \n     df = from_pandas(pdf, 2)\n \n-    if pyarrow_strings_enabled():\n+    if pyarrow_strings_enabled() or PANDAS_GE_300:\n         with pytest.warns(UserWarning, match=\"extension dtypes\"):\n             assert_eq(df.values, pdf.values)\n-            assert_eq(df.x.values, pdf.x.values)\n+            values = df.x.values\n+            if not PANDAS_GE_300:\n+                # https://github.com/dask/dask/issues/2713\n+                # dask lacks an extension array type.\n+                assert_eq(values, pdf.x.values)\n     else:\n         assert_eq(df.values, pdf.values)\n         assert_eq(df.x.values, pdf.x.values)\n"
        },
        {
            "commit": "7751beb21807fa7f206079b8f69bf887ec16a199",
            "file_path": "dask/dataframe/dask_expr/tests/test_shuffle.py",
            "diff": "diff --git a/dask/dataframe/dask_expr/tests/test_shuffle.py b/dask/dataframe/dask_expr/tests/test_shuffle.py\nindex 7da6e1b1..92874001 100644\n--- a/dask/dataframe/dask_expr/tests/test_shuffle.py\n+++ b/dask/dataframe/dask_expr/tests/test_shuffle.py\n@@ -584,10 +584,11 @@ def test_index_nulls(null_value):\n     )\n     ddf = from_pandas(df, npartitions=2)\n     with pytest.raises(NotImplementedError, match=\"presence of nulls\"):\n-        with pytest.warns(UserWarning):\n-            ddf.set_index(\n-                ddf[\"non_numeric\"].map({\"foo\": \"foo\", \"bar\": null_value})\n-            ).compute()\n+        ddf.set_index(\n+            ddf[\"non_numeric\"].map(\n+                {\"foo\": \"foo\", \"bar\": null_value}, meta=ddf[\"non_numeric\"]._meta\n+            )\n+        ).compute()\n \n \n @pytest.mark.parametrize(\"freq\", [\"16h\", \"-16h\"])\n"
        },
        {
            "commit": "7751beb21807fa7f206079b8f69bf887ec16a199",
            "file_path": "dask/dataframe/io/parquet/arrow.py",
            "diff": "diff --git a/dask/dataframe/io/parquet/arrow.py b/dask/dataframe/io/parquet/arrow.py\nindex d2a16baf..ec24d08d 100644\n--- a/dask/dataframe/io/parquet/arrow.py\n+++ b/dask/dataframe/io/parquet/arrow.py\n@@ -721,7 +721,7 @@ class ArrowDatasetEngine(Engine):\n                     \"Appended columns not the same.\\n\"\n                     \"Previous: {} | New: {}\".format(names, list(df.columns))\n                 )\n-            elif (pd.Series(dtypes).loc[names] != df[names].dtypes).any():\n+            elif pd.Series(dtypes).loc[names].tolist() != df[names].dtypes.tolist():\n                 # TODO Coerce values for compatible but different dtypes\n                 raise ValueError(\n                     \"Appended dtypes differ.\\n{}\".format(\n"
        },
        {
            "commit": "7751beb21807fa7f206079b8f69bf887ec16a199",
            "file_path": "dask/dataframe/io/tests/test_parquet.py",
            "diff": "diff --git a/dask/dataframe/io/tests/test_parquet.py b/dask/dataframe/io/tests/test_parquet.py\nindex e2481da1..1b057b79 100644\n--- a/dask/dataframe/io/tests/test_parquet.py\n+++ b/dask/dataframe/io/tests/test_parquet.py\n@@ -17,7 +17,7 @@ from packaging.version import Version\n import dask\n import dask.dataframe as dd\n import dask.multiprocessing\n-from dask.dataframe._compat import PANDAS_GE_202, PANDAS_GE_300\n+from dask.dataframe._compat import PANDAS_GE_202, PANDAS_GE_300, PYARROW_GE_2101\n from dask.dataframe.io.parquet.core import get_engine\n from dask.dataframe.utils import assert_eq, pyarrow_strings_enabled\n from dask.utils import natural_sort_key\n@@ -1272,7 +1272,11 @@ def test_pyarrow_schema_mismatch_error(tmpdir):\n     msg = str(rec.value)\n     assert \"Failed to convert partition to expected pyarrow schema\" in msg\n     assert \"y: double\" in str(rec.value)\n-    assert \"y: string\" in str(rec.value)\n+\n+    if PANDAS_GE_300:\n+        assert \"y: large_string\" in str(rec.value)\n+    else:\n+        assert \"y: string\" in str(rec.value)\n \n \n @PYARROW_MARK\n@@ -2664,9 +2668,22 @@ def test_arrow_to_pandas(tmpdir, engine):\n     assert got.A.dtype == got.compute().A.dtype\n \n \n+PYARROW_LARGE_STRING_XFAIL = pytest.mark.xfail(\n+    condition=PANDAS_GE_300 and not PYARROW_GE_2101,\n+    reason=\"https://github.com/apache/arrow/issues/47177\",\n+    strict=True,\n+)\n+\n+\n @pytest.mark.parametrize(\n     \"write_cols\",\n-    [[\"part\", \"col\"], [\"part\", \"kind\", \"col\"]],\n+    [\n+        [\"part\", \"col\"],\n+        pytest.param(\n+            [\"part\", \"kind\", \"col\"],\n+            marks=PYARROW_LARGE_STRING_XFAIL,\n+        ),\n+    ],\n )\n def test_partitioned_column_overlap(tmpdir, engine, write_cols):\n     tmpdir.mkdir(\"part=a\")\n@@ -2691,15 +2708,21 @@ def test_partitioned_column_overlap(tmpdir, engine, write_cols):\n         assert_eq(result, expect, check_index=False)\n     else:\n         # For now, partial overlap between partition columns and\n-        # real columns is not allowed for pyarrow\n-        with pytest.raises(ValueError):\n+        # real columns is not allowed for\n+        with pytest.raises((ValueError, pa.ArrowTypeError)):\n             dd.read_parquet(path, engine=engine)\n \n \n @PYARROW_MARK\n @pytest.mark.parametrize(\n     \"write_cols\",\n-    [[\"col\"], [\"part\", \"col\"]],\n+    [\n+        [\"col\"],\n+        pytest.param(\n+            [\"part\", \"col\"],\n+            marks=PYARROW_LARGE_STRING_XFAIL,\n+        ),\n+    ],\n )\n def test_partitioned_no_pandas_metadata(tmpdir, engine, write_cols):\n     # See: https://github.com/dask/dask/issues/8087\n@@ -2734,6 +2757,7 @@ def test_partitioned_no_pandas_metadata(tmpdir, engine, write_cols):\n \n \n @PYARROW_MARK\n+@PYARROW_LARGE_STRING_XFAIL\n def test_pyarrow_directory_partitioning(tmpdir):\n     # Manually construct directory-partitioned dataset\n     path1 = tmpdir.mkdir(\"a\")\n"
        },
        {
            "commit": "7751beb21807fa7f206079b8f69bf887ec16a199",
            "file_path": "dask/dataframe/io/tests/test_sql.py",
            "diff": "diff --git a/dask/dataframe/io/tests/test_sql.py b/dask/dataframe/io/tests/test_sql.py\nindex a68ec58e..07207a71 100644\n--- a/dask/dataframe/io/tests/test_sql.py\n+++ b/dask/dataframe/io/tests/test_sql.py\n@@ -511,12 +511,17 @@ def test_to_sql(npartitions, parallel):\n         result = read_sql_table(\"test\", uri, \"age\")\n         assert_eq(df_by_age, result)\n \n+    if PANDAS_GE_300:\n+        string_dtype = \"str\"\n+    else:\n+        string_dtype = \"object\"\n+\n     # Index column can't have \"object\" dtype if no partitions are provided\n     with tmp_db_uri() as uri:\n         ddf.set_index(\"name\").to_sql(\"test\", uri)\n         with pytest.raises(\n             TypeError,\n-            match='Provided index column is of type \"object\".  If divisions is not provided the index column type must be numeric or datetime.',  # noqa: E501\n+            match=f'Provided index column is of type \"{string_dtype}\".  If divisions is not provided the index column type must be numeric or datetime.',  # noqa: E501\n         ):\n             read_sql_table(\"test\", uri, \"name\")\n \n"
        },
        {
            "commit": "7751beb21807fa7f206079b8f69bf887ec16a199",
            "file_path": "dask/dataframe/tests/test_dataframe.py",
            "diff": "diff --git a/dask/dataframe/tests/test_dataframe.py b/dask/dataframe/tests/test_dataframe.py\nindex 4503f43a..25b427d9 100644\n--- a/dask/dataframe/tests/test_dataframe.py\n+++ b/dask/dataframe/tests/test_dataframe.py\n@@ -1452,7 +1452,8 @@ def test_dataframe_quantile(method, expected, numeric_only):\n         numeric_only_kwarg = {\"numeric_only\": numeric_only}\n \n     if numeric_only is False or numeric_only is None:\n-        with pytest.raises(TypeError):\n+        # TypeError for pandas<3, ArrowNotImplementedError for pandas>=3\n+        with pytest.raises((TypeError, ArrowNotImplementedError)):\n             df.quantile(**numeric_only_kwarg)\n         with pytest.raises(\n             (TypeError, ArrowNotImplementedError, ValueError),\n@@ -3566,10 +3567,11 @@ def test_index_nulls(null_value):\n     # an object column with only some nulls fails\n     ddf = dd.from_pandas(df, npartitions=2)\n     with pytest.raises(NotImplementedError, match=\"presence of nulls\"):\n-        with pytest.warns(UserWarning, match=\"meta\"):\n-            ddf.set_index(\n-                ddf[\"non_numeric\"].map({\"foo\": \"foo\", \"bar\": null_value})\n-            ).compute()\n+        ddf.set_index(\n+            ddf[\"non_numeric\"].map(\n+                {\"foo\": \"foo\", \"bar\": null_value}, meta=ddf[\"non_numeric\"]._meta\n+            )\n+        ).compute()\n \n \n def test_set_index_with_index():\n@@ -4054,7 +4056,11 @@ def test_values():\n         ctx = pytest.warns(UserWarning, match=\"object dtype\")\n     with ctx:\n         result = ddf.x.values\n-    assert_eq(df.x.values, result)\n+\n+    if not PANDAS_GE_300:\n+        # Dask currently lacks an extension type.\n+        # https://github.com/dask/dask/issues/5001\n+        assert_eq(df.x.values, result)\n     assert_eq(df.y.values, ddf.y.values)\n     assert_eq(df.index.values, ddf.index.values)\n \n"
        },
        {
            "commit": "7751beb21807fa7f206079b8f69bf887ec16a199",
            "file_path": "dask/dataframe/tests/test_ufunc.py",
            "diff": "diff --git a/dask/dataframe/tests/test_ufunc.py b/dask/dataframe/tests/test_ufunc.py\nindex e89da037..6478032d 100644\n--- a/dask/dataframe/tests/test_ufunc.py\n+++ b/dask/dataframe/tests/test_ufunc.py\n@@ -117,7 +117,7 @@ def test_ufunc(pandas_input, ufunc):\n         assert_eq(dafunc(dask_input), npfunc(pandas_input))\n \n     # Index\n-    if pandas_input.index.dtype in [object, str]:\n+    if pandas_input.index.dtype.name in [\"object\", \"str\"]:\n         return\n     if ufunc in (\"logical_not\", \"signbit\", \"isnan\", \"isinf\", \"isfinite\"):\n         return\n"
        },
        {
            "commit": "7751beb21807fa7f206079b8f69bf887ec16a199",
            "file_path": "dask/dataframe/tests/test_utils_dataframe.py",
            "diff": "diff --git a/dask/dataframe/tests/test_utils_dataframe.py b/dask/dataframe/tests/test_utils_dataframe.py\nindex eee517f6..527575c6 100644\n--- a/dask/dataframe/tests/test_utils_dataframe.py\n+++ b/dask/dataframe/tests/test_utils_dataframe.py\n@@ -1,6 +1,7 @@\n from __future__ import annotations\n \n import re\n+import textwrap\n import warnings\n from collections.abc import Iterable\n \n@@ -44,12 +45,19 @@ def test_make_meta():\n     assert isinstance(meta.index, type(df.index))\n     # - ensure no references to original data arrays are kept\n     for col in \"abc\":\n-        meta_pointer = meta[col].values.__array_interface__[\"data\"][0]\n-        df_pointer = df[col].values.__array_interface__[\"data\"][0]\n+        if PANDAS_GE_300 and col == \"b\":\n+            # backed by an arrow array\n+            meta_pointer = meta[col].array._pa_array.chunks[0].buffers()[1].address\n+            df_pointer = df[col].array._pa_array.chunks[0].buffers()[1].address\n+        else:\n+            # backed by a numpy array\n+            meta_pointer = meta[col].values.__array_interface__[\"data\"][0]\n+            df_pointer = df[col].values.__array_interface__[\"data\"][0]\n         assert meta_pointer != df_pointer\n-    meta_pointer = meta.index.values.__array_interface__[\"data\"][0]\n-    df_pointer = df.index.values.__array_interface__[\"data\"][0]\n-    assert meta_pointer != df_pointer\n+\n+        meta_index_pointer = meta.index.values.__array_interface__[\"data\"][0]\n+        df_index_pointer = df.index.values.__array_interface__[\"data\"][0]\n+        assert meta_index_pointer != df_index_pointer\n \n     # Pandas series\n     meta = make_meta(df.a)\n@@ -78,14 +86,22 @@ def test_make_meta():\n     assert make_meta(ddf) is ddf._meta\n \n     # Dict\n-    meta = make_meta({\"a\": \"i8\", \"b\": \"O\", \"c\": \"f8\"})\n+    if PANDAS_GE_300:\n+        meta = make_meta({\"a\": \"i8\", \"b\": \"str\", \"c\": \"f8\"})\n+    else:\n+        meta = make_meta({\"a\": \"i8\", \"b\": \"O\", \"c\": \"f8\"})\n+\n     assert isinstance(meta, pd.DataFrame)\n     assert len(meta) == 0\n     assert (meta.dtypes == df.dtypes).all()\n     assert isinstance(meta.index, pd.RangeIndex)\n \n     # List\n-    meta = make_meta([(\"a\", \"i8\"), (\"c\", \"f8\"), (\"b\", \"O\")])\n+    if PANDAS_GE_300:\n+        meta = make_meta([(\"a\", \"i8\"), (\"c\", \"f8\"), (\"b\", \"str\")])\n+    else:\n+        meta = make_meta([(\"a\", \"i8\"), (\"c\", \"f8\"), (\"b\", \"O\")])\n+\n     assert (meta.columns == [\"a\", \"c\", \"b\"]).all()\n     assert len(meta) == 0\n     assert (meta.dtypes == df.dtypes[meta.dtypes.index]).all()\n@@ -103,7 +119,10 @@ def test_make_meta():\n         \"\"\"Custom class iterator returning pandas types.\"\"\"\n \n         def __init__(self, max=0):\n-            self.types = [(\"a\", \"i8\"), (\"c\", \"f8\"), (\"b\", \"O\")]\n+            if PANDAS_GE_300:\n+                self.types = [(\"a\", \"i8\"), (\"c\", \"f8\"), (\"b\", \"str\")]\n+            else:\n+                self.types = [(\"a\", \"i8\"), (\"c\", \"f8\"), (\"b\", \"O\")]\n \n         def __iter__(self):\n             self.n = 0\n@@ -201,8 +220,13 @@ def test_meta_nonempty():\n     df3 = meta_nonempty(df2)\n     assert (df3.dtypes == df2.dtypes).all()\n     assert df3[\"A\"][0] == \"Alice\"\n-    assert df3[\"B\"][0] == \"foo\"\n-    assert df3[\"C\"][0] == \"foo\"\n+\n+    if PANDAS_GE_300:\n+        assert df3[\"B\"][0] == \"a\"\n+        assert df3[\"C\"][0] == \"a\"\n+    else:\n+        assert df3[\"B\"][0] == \"foo\"\n+        assert df3[\"C\"][0] == \"foo\"\n     assert df3[\"D\"][0] == np.float32(1)\n     assert df3[\"D\"][0].dtype == \"f4\"\n     assert df3[\"E\"][0] == np.int32(1)\n@@ -210,7 +234,10 @@ def test_meta_nonempty():\n     assert df3[\"F\"][0] == pd.Timestamp(\"1970-01-01 00:00:00\")\n     assert df3[\"G\"][0] == pd.Timestamp(\"1970-01-01 00:00:00\", tz=\"America/New_York\")\n     assert df3[\"H\"][0] == pd.Timedelta(\"1\")\n-    assert df3[\"I\"][0] == \"foo\"\n+    if PANDAS_GE_300:\n+        assert type(df3[\"I\"][0]) is object\n+    else:\n+        assert df3[\"I\"][0] == \"foo\"\n     assert df3[\"J\"][0] == UNKNOWN_CATEGORIES\n     assert len(df3[\"K\"].cat.categories) == 0\n \n@@ -223,8 +250,13 @@ def test_meta_duplicated():\n     df = pd.DataFrame(columns=[\"A\", \"A\", \"B\"])\n     res = meta_nonempty(df)\n \n+    if PANDAS_GE_300:\n+        o = dd.utils._object\n+    else:\n+        o = \"foo\"\n+\n     exp = pd.DataFrame(\n-        [[\"foo\", \"foo\", \"foo\"], [\"foo\", \"foo\", \"foo\"]],\n+        [[o, o, o], [o, o, o]],\n         index=meta_nonempty(df.index),\n         columns=[\"A\", \"A\", \"B\"],\n     )\n@@ -418,35 +450,46 @@ def test_check_meta():\n         check_meta(df2, meta2, funcname=\"from_delayed\")\n     frame = \"pandas.core.frame.DataFrame\" if not PANDAS_GE_300 else \"pandas.DataFrame\"\n \n-    exp = (\n-        \"Metadata mismatch found in `from_delayed`.\\n\"\n-        \"\\n\"\n-        f\"Partition type: `{frame}`\\n\"\n-        \"+--------+----------+----------+\\n\"\n-        \"| Column | Found    | Expected |\\n\"\n-        \"+--------+----------+----------+\\n\"\n-        \"| 'a'    | object   | category |\\n\"\n-        \"| 'c'    | -        | float64  |\\n\"\n-        \"| 'e'    | category | -        |\\n\"\n-        \"+--------+----------+----------+\"\n+    if PANDAS_GE_300:\n+        string_type = \"str   \"  # space for alignment\n+    else:\n+        string_type = \"object\"\n+\n+    exp = textwrap.dedent(\n+        f\"\"\"\\\n+        Metadata mismatch found in `from_delayed`.\n+\n+        Partition type: `{frame}`\n+        +--------+----------+----------+\n+        | Column | Found    | Expected |\n+        +--------+----------+----------+\n+        | 'a'    | {string_type}   | category |\n+        | 'c'    | -        | float64  |\n+        | 'e'    | category | -        |\n+        +--------+----------+----------+\"\"\"\n     )\n+\n     assert str(err.value) == exp\n \n     # pandas dtype metadata error\n     with pytest.raises(ValueError) as err:\n         check_meta(df.a, pd.Series([], dtype=\"string\"), numeric_equal=False)\n-    assert str(err.value) == (\n-        \"Metadata mismatch found.\\n\"\n-        \"\\n\"\n-        f\"Partition type: `{series}`\\n\"\n-        \"+----------+--------+\\n\"\n-        \"|          | dtype  |\\n\"\n-        \"+----------+--------+\\n\"\n-        \"| Found    | object |\\n\"\n-        \"| Expected | string |\\n\"\n-        \"+----------+--------+\"\n+\n+    expected = textwrap.dedent(\n+        f\"\"\"\\\n+        Metadata mismatch found.\n+\n+        Partition type: `{series}`\n+        +----------+--------+\n+        |          | dtype  |\n+        +----------+--------+\n+        | Found    | {string_type} |\n+        | Expected | string |\n+        +----------+--------+\"\"\"\n     )\n \n+    assert str(err.value) == expected\n+\n \n def test_check_matching_columns_raises_appropriate_errors():\n     df = pd.DataFrame(columns=[\"a\", \"b\", \"c\"])\n"
        },
        {
            "commit": "7751beb21807fa7f206079b8f69bf887ec16a199",
            "file_path": "dask/dataframe/utils.py",
            "diff": "diff --git a/dask/dataframe/utils.py b/dask/dataframe/utils.py\nindex bbf2f60e..81fdbcbc 100644\n--- a/dask/dataframe/utils.py\n+++ b/dask/dataframe/utils.py\n@@ -8,7 +8,7 @@ import traceback\n from collections.abc import Callable, Iterable, Mapping, Sequence\n from contextlib import contextmanager\n from numbers import Number\n-from typing import TypeVar, overload\n+from typing import Any, TypeVar, overload\n \n import numpy as np\n import pandas as pd\n@@ -17,7 +17,7 @@ from pandas.api.types import is_dtype_equal\n import dask\n from dask.base import is_dask_collection\n from dask.core import get_deps\n-from dask.dataframe._compat import tm  # noqa: F401\n+from dask.dataframe._compat import PANDAS_GE_300, tm  # noqa: F401\n from dask.dataframe.dispatch import (  # noqa : F401\n     is_categorical_dtype_dispatch,\n     make_meta,\n@@ -267,7 +267,7 @@ def _empty_series(name, dtype, index=None):\n     return pd.Series([], dtype=dtype, name=name, index=index)\n \n \n-_simple_fake_mapping = {\n+_simple_fake_mapping: dict[str, Any] = {\n     \"b\": np.bool_(True),\n     \"V\": np.void(b\" \"),\n     \"M\": np.datetime64(\"1970-01-01\"),\n@@ -275,9 +275,15 @@ _simple_fake_mapping = {\n     \"S\": np.str_(\"foo\"),\n     \"a\": np.str_(\"foo\"),\n     \"U\": np.str_(\"foo\"),\n-    \"O\": \"foo\",\n }\n \n+_object = object()\n+\n+if PANDAS_GE_300:\n+    _simple_fake_mapping[\"O\"] = _object\n+else:\n+    _simple_fake_mapping[\"O\"] = \"foo\"\n+\n \n def _scalar_from_dtype(dtype):\n     if dtype.kind in (\"i\", \"f\", \"u\"):\n"
        },
        {
            "commit": "7751beb21807fa7f206079b8f69bf887ec16a199",
            "file_path": "dask/tests/test_base.py",
            "diff": "diff --git a/dask/tests/test_base.py b/dask/tests/test_base.py\nindex 6c4c33d2..fcfe46dd 100644\n--- a/dask/tests/test_base.py\n+++ b/dask/tests/test_base.py\n@@ -478,7 +478,9 @@ def test_compute_dataframe_valid_unicode_in_bytes():\n @pytest.mark.skipif(\"not dd\")\n def test_compute_dataframe_invalid_unicode():\n     # see https://github.com/dask/dask/issues/2713\n-    df = pd.DataFrame(data=np.random.random((3, 1)), columns=[\"\\ud83d\"])\n+    df = pd.DataFrame(\n+        data=np.random.random((3, 1)), columns=pd.Index([\"\\ud83d\"], dtype=\"object\")\n+    )\n     dd.from_pandas(df, npartitions=4)\n \n \n"
        },
        {
            "commit": "7751beb21807fa7f206079b8f69bf887ec16a199",
            "file_path": "dask/tests/test_tokenize.py",
            "diff": "diff --git a/dask/tests/test_tokenize.py b/dask/tests/test_tokenize.py\nindex 406e14af..2855704a 100644\n--- a/dask/tests/test_tokenize.py\n+++ b/dask/tests/test_tokenize.py\n@@ -570,17 +570,34 @@ def test_tokenize_pandas():\n     a = pd.DataFrame({\"x\": [1, 2, 3], \"y\": [\"a\", \"b\", \"a\"]})\n     b = pd.DataFrame({\"x\": [1, 2, 3], \"y\": [\"a\", \"b\", \"a\"]})\n     a[\"z\"] = a.y.astype(\"category\")\n+\n+    # tokenize is currently sensitive to fragmentation of the pyarrow Array\n+    # backing the columns. Create a new one to work around this.\n+    a.columns = pd.Index([\"x\", \"y\", \"z\"])\n     assert check_tokenize(a) != check_tokenize(b)\n     b[\"z\"] = a.y.astype(\"category\")\n+\n+    b.columns = pd.Index([\"x\", \"y\", \"z\"])\n     assert check_tokenize(a) == check_tokenize(b)\n \n \n+@pytest.mark.skipif(\"not pd\")\n+def test_tokenize_pandas_fragmented_index():\n+    s = pd.concat([pd.Series(1, index=[\"a\", \"b\"]), pd.Series(2, index=[\"c\", \"d\"])])\n+    check_tokenize(s)\n+\n+\n @pytest.mark.skipif(\"not pd\")\n def test_tokenize_pandas_invalid_unicode():\n     # see https://github.com/dask/dask/issues/2713\n     df = pd.DataFrame(\n-        {\"x\\ud83d\": [1, 2, 3], \"y\\ud83d\": [\"4\", \"asd\\ud83d\", None]}, index=[1, 2, 3]\n+        {\n+            \"x\": [1, 2, 3],\n+            \"y\": pd.Series([\"4\", \"asd\\ud83d\", None], dtype=\"object\"),\n+        },\n+        index=[1, 2, 3],\n     )\n+    df.columns = pd.Index([\"x\\ud83d\", \"y\\ud83d\"], dtype=\"object\")\n     check_tokenize(df)\n \n \n"
        },
        {
            "commit": "7751beb21807fa7f206079b8f69bf887ec16a199",
            "file_path": "dask/tokenize.py",
            "diff": "diff --git a/dask/tokenize.py b/dask/tokenize.py\nindex f2bf86a8..dcb3c1d7 100644\n--- a/dask/tokenize.py\n+++ b/dask/tokenize.py\n@@ -278,6 +278,8 @@ def _normalize_dataclass(obj):\n def register_pandas():\n     import pandas as pd\n \n+    from dask.dataframe._compat import PANDAS_GE_210\n+\n     @normalize_token.register(pd.RangeIndex)\n     def normalize_range_index(x):\n         return type(x), x.start, x.stop, x.step, x.dtype, x.name\n@@ -285,6 +287,20 @@ def register_pandas():\n     @normalize_token.register(pd.Index)\n     def normalize_index(ind):\n         values = ind.array\n+\n+        if isinstance(values, pd.arrays.ArrowExtensionArray):\n+            import pyarrow as pa\n+\n+            # these are sensitive to fragmentation of the backing Arrow array.\n+            # Because common operations like DataFrame.getitem and DataFrame.setitem\n+            # result in fragmented Arrow arrays, we'll consolidate them here.\n+\n+            if PANDAS_GE_210:\n+                # avoid combining chunks by using chunked_array\n+                values = pa.chunked_array([values._pa_array]).combine_chunks()\n+            else:\n+                values = pa.array(values)\n+\n         return type(ind), ind.name, normalize_token(values)\n \n     @normalize_token.register(pd.MultiIndex)\n@@ -394,15 +410,26 @@ def register_pyarrow():\n         )\n \n     @normalize_token.register(pa.Array)\n-    def normalize_chunked_array(arr):\n+    def normalize_array(arr):\n+        buffers = arr.buffers()\n+        # pyarrow does something clever when (de)serializing an array that has\n+        # an empty validity map: The buffers for the deserialized array will\n+        # have `None` instead of the empty validity map.\n+        #\n+        # We'll replicate that behavior here to ensure we get consistent\n+        # tokenization.\n+        buffers = arr.buffers()\n+        if len(buffers) and buffers[0] is not None and arr.null_count == 0:\n+            buffers[0] = None\n+\n         return (\n             \"pa.Array\",\n             normalize_token(arr.type),\n-            normalize_token(arr.buffers()),\n+            normalize_token(buffers),\n         )\n \n     @normalize_token.register(pa.Buffer)\n-    def normalize_chunked_array(buf):\n+    def normalize_buffer(buf):\n         return (\"pa.Buffer\", hash_buffer_hex(buf))\n \n \n"
        }
    ]
}