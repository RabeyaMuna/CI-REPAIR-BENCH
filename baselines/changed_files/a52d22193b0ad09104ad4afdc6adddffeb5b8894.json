{
    "sha_fail": "a52d22193b0ad09104ad4afdc6adddffeb5b8894",
    "changed_files": [
        {
            "commit": "a52d22193b0ad09104ad4afdc6adddffeb5b8894",
            "file_path": "cookbook/evals/performance/team_response_with_memory_simple.py",
            "diff": "diff --git a/cookbook/evals/performance/team_response_with_memory_simple.py b/cookbook/evals/performance/team_response_with_memory_simple.py\nindex 02450ba17..a9c48a295 100644\n--- a/cookbook/evals/performance/team_response_with_memory_simple.py\n+++ b/cookbook/evals/performance/team_response_with_memory_simple.py\n@@ -9,7 +9,6 @@ from agno.models.openai import OpenAIChat\n from agno.storage.postgres import PostgresStorage\n from agno.team.team import Team\n \n-\n cities = [\n     \"New York\",\n     \"Los Angeles\",\n@@ -43,6 +42,7 @@ memory = Memory(db=memory_db)\n def get_weather(city: str) -> str:\n     return f\"The weather in {city} is sunny.\"\n \n+\n weather_agent = Agent(\n     agent_id=\"weather_agent\",\n     model=OpenAIChat(id=\"gpt-4o-mini\"),\n"
        },
        {
            "commit": "a52d22193b0ad09104ad4afdc6adddffeb5b8894",
            "file_path": "cookbook/models/openai/chat/async_basic_stream.py",
            "diff": "diff --git a/cookbook/models/openai/chat/async_basic_stream.py b/cookbook/models/openai/chat/async_basic_stream.py\nindex f243cd2a3..b280a761a 100644\n--- a/cookbook/models/openai/chat/async_basic_stream.py\n+++ b/cookbook/models/openai/chat/async_basic_stream.py\n@@ -7,11 +7,11 @@ from agno.models.openai import OpenAIChat\n agent = Agent(model=OpenAIChat(id=\"gpt-4o\"), markdown=True)\n \n # Get the response in a variable\n-run_response: Iterator[RunResponseEvent] = agent.run(\n-    \"Share a 2 sentence horror story\", stream=True\n-)\n-for chunk in run_response:\n-    print(chunk.content, end=\"\")\n+# run_response: Iterator[RunResponseEvent] = agent.run(\n+#     \"Share a 2 sentence horror story\", stream=True\n+# )\n+# for chunk in run_response:\n+#     print(chunk.content, end=\"\")\n \n # # Print the response in the terminal\n-# asyncio.run(agent.aprint_response(\"Share a 2 sentence horror story\", stream=True))\n+asyncio.run(agent.aprint_response(\"Share a 2 sentence horror story\", stream=True))\n"
        },
        {
            "commit": "a52d22193b0ad09104ad4afdc6adddffeb5b8894",
            "file_path": "libs/agno/agno/agent/agent.py",
            "diff": "diff --git a/libs/agno/agno/agent/agent.py b/libs/agno/agno/agent/agent.py\nindex b33422746..8bbb130b4 100644\n--- a/libs/agno/agno/agent/agent.py\n+++ b/libs/agno/agno/agent/agent.py\n@@ -6848,12 +6848,11 @@ class Agent:\n                         ):\n                             reasoning_steps = resp.extra_data.reasoning_steps\n \n-                    response_content_stream: Union[str, Markdown] = _response_content\n-\n+                    response_content_stream: str = _response_content\n                     # Escape special tags before markdown conversion\n                     if self.markdown:\n                         escaped_content = escape_markdown_tags(_response_content, tags_to_include_in_markdown)\n-                        response_content_stream = Markdown(escaped_content)\n+                        response_content_batch = Markdown(escaped_content)\n                     panels = [status]\n \n                     if message and show_message:\n@@ -6931,26 +6930,32 @@ class Agent:\n                             border_style=\"yellow\",\n                         )\n                         panels.append(tool_calls_panel)\n+                        live_log.update(Group(*panels))\n \n-                response_panel = None\n-                # Check if we have any response content to display\n-                response_content = (\n-                    response_content_stream\n-                    if response_content_stream\n-                    and isinstance(response_content_stream, str)\n-                    and len(response_content_stream) > 0\n-                    else response_content_batch\n-                )\n-                if response_content:\n-                    render = True\n-                    response_panel = create_panel(\n-                        content=response_content,\n-                        title=f\"Response ({response_timer.elapsed:.1f}s)\",\n-                        border_style=\"blue\",\n+                    response_panel = None\n+                    # Check if we have any response content to display\n+                    response_content = (\n+                        response_content_stream\n+                        if response_content_stream and len(response_content_stream) > 0 and not self.markdown\n+                        else response_content_batch\n                     )\n-                    panels.append(response_panel)\n-                    if render:\n-                        live_log.update(Group(*panels))\n+\n+                    if isinstance(response_content, Markdown):\n+                        if response_content.markup is not None and response_content.markup.strip():\n+                            response_content = response_content.markup\n+                        else:\n+                            response_content = None\n+\n+                    if response_content:\n+                        render = True\n+                        response_panel = create_panel(\n+                            content=response_content,\n+                            title=f\"Response ({response_timer.elapsed:.1f}s)\",\n+                            border_style=\"blue\",\n+                        )\n+                        panels.append(response_panel)\n+                        if render:\n+                            live_log.update(Group(*panels))\n \n                     if (\n                         isinstance(resp, tuple(get_args(RunResponseEvent)))\n@@ -7293,11 +7298,11 @@ class Agent:\n                         ):\n                             reasoning_steps = resp.extra_data.reasoning_steps\n \n-                    response_content_stream: Union[str, Markdown] = _response_content\n+                    response_content_stream: str = _response_content\n                     # Escape special tags before markdown conversion\n                     if self.markdown:\n                         escaped_content = escape_markdown_tags(_response_content, tags_to_include_in_markdown)\n-                        response_content_stream = Markdown(escaped_content)\n+                        response_content_batch = Markdown(escaped_content)\n \n                     panels = [status]\n \n@@ -7382,11 +7387,15 @@ class Agent:\n                     # Check if we have any response content to display\n                     response_content = (\n                         response_content_stream\n-                        if response_content_stream\n-                        and isinstance(response_content_stream, str)\n-                        and len(response_content_stream) > 0\n+                        if response_content_stream and len(response_content_stream) > 0\n                         else response_content_batch\n                     )\n+                    if isinstance(response_content, Markdown):\n+                        if response_content.markup is not None and response_content.markup.strip():\n+                            response_content = response_content.markup\n+                        else:\n+                            response_content = None\n+\n                     if response_content:\n                         render = True\n                         response_panel = create_panel(\n"
        }
    ]
}