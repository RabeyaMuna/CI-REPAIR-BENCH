{
    "sha_fail": "daad35b9c60770a655eaaea4565bf81f63ae1b38",
    "changed_files": [
        {
            "commit": "daad35b9c60770a655eaaea4565bf81f63ae1b38",
            "file_path": "prompts/code_prompts.py",
            "diff": "diff --git a/prompts/code_prompts.py b/prompts/code_prompts.py\nindex 18c710c..c507632 100644\n--- a/prompts/code_prompts.py\n+++ b/prompts/code_prompts.py\n@@ -213,18 +213,30 @@ Output Format:\n # Code Analysis Prompts\n PAPER_ALGORITHM_ANALYSIS_PROMPT = \"\"\"You are extracting COMPLETE implementation details from a research paper. Your goal is to capture EVERY algorithm, formula, and technical detail needed for perfect reproduction.\n \n-# CRITICAL INSTRUCTION\n-Read the ENTIRE paper, especially ALL method/algorithm sections. Extract EVERY piece of information that would be needed to write code.\n+# INTELLIGENT DOCUMENT READING STRATEGY\n+\n+## IMPORTANT: Use Segmented Reading for Algorithm Extraction\n+To avoid token limits and efficiently extract algorithm details, use the intelligent segmentation system:\n+\n+1. **Primary Algorithm Extraction** - Use read_document_segments tool with:\n+   - query_type: \"algorithm_extraction\"\n+   - keywords: [\"algorithm\", \"method\", \"procedure\", \"formula\", \"equation\", \"implementation\"]\n+   - max_segments: 3\n+   - max_total_chars: 6000\n+\n+2. **Supplementary Details** - Make additional calls if needed with:\n+   - keywords: [\"hyperparameter\", \"training\", \"optimization\", \"loss\", \"objective\"]\n+   - keywords: [\"experiment\", \"setup\", \"configuration\", \"parameter\"]\n+\n+3. **This approach ensures** you get the most algorithm-relevant content without missing critical details\n \n # DETAILED EXTRACTION PROTOCOL\n \n-## 1. COMPLETE PAPER SCAN\n-Read these sections IN FULL:\n-- Abstract (for overview)\n-- ALL Method/Algorithm sections (usually 3-5)\n-- Implementation Details section (if exists)\n-- Experiments section (for hyperparameters)\n-- Appendix (for additional details)\n+## 1. INTELLIGENT ALGORITHM SCAN\n+Use the segmented reading approach to focus on algorithm sections:\n+- Method/Algorithm sections (captured automatically by segmentation)\n+- Implementation Details (targeted retrieval)\n+- Hyperparameters and training details (focused extraction)\n \n ## 2. ALGORITHM DEEP EXTRACTION\n For EVERY algorithm/method/procedure mentioned:\n@@ -370,10 +382,25 @@ PAPER_CONCEPT_ANALYSIS_PROMPT = \"\"\"You are doing a COMPREHENSIVE analysis of a r\n # OBJECTIVE\n Map out the ENTIRE paper structure and identify ALL components that need implementation for successful reproduction.\n \n+# INTELLIGENT DOCUMENT READING STRATEGY\n+\n+## IMPORTANT: Use Segmented Reading for Optimal Performance\n+Instead of reading the entire document at once (which may hit token limits), use the intelligent segmentation system:\n+\n+1. **Use read_document_segments tool** with these parameters:\n+   - query_type: \"concept_analysis\"\n+   - keywords: [\"introduction\", \"overview\", \"architecture\", \"system\", \"framework\", \"concept\", \"method\"]\n+   - max_segments: 3\n+   - max_total_chars: 6000\n+\n+2. **This will automatically find and retrieve** the most relevant sections for concept analysis without token overflow\n+\n+3. **If you need additional sections**, make follow-up calls with different keywords like [\"experiment\", \"evaluation\", \"results\"] or [\"conclusion\", \"discussion\"]\n+\n # COMPREHENSIVE ANALYSIS PROTOCOL\n \n-## 1. FULL PAPER STRUCTURAL ANALYSIS\n-Read the ENTIRE paper and create a complete map:\n+## 1. INTELLIGENT PAPER STRUCTURAL ANALYSIS\n+Use the segmented reading approach to create a complete map:\n \n ```yaml\n paper_structure_map:\n@@ -532,6 +559,21 @@ You receive two exhaustive analyses:\n 1. **Comprehensive Paper Analysis**: Complete paper structure, components, and requirements\n 2. **Complete Algorithm Extraction**: All algorithms, formulas, pseudocode, and technical details\n \n+Plus you can use segmented reading to access any specific paper sections needed for planning.\n+\n+# INTELLIGENT DOCUMENT ACCESS\n+\n+## IMPORTANT: Use Segmented Reading for Detailed Planning\n+When you need additional details beyond the provided analyses, use the intelligent segmentation system:\n+\n+1. **Use read_document_segments tool** with these parameters:\n+   - query_type: \"code_planning\"\n+   - keywords: Specific to what you need, e.g., [\"implementation\", \"code\", \"experiment\", \"setup\", \"configuration\"]\n+   - max_segments: 3\n+   - max_total_chars: 8000\n+\n+2. **This approach ensures** you access the most planning-relevant content without token limits\n+\n # OBJECTIVE\n Create an implementation plan so detailed that a developer can reproduce the ENTIRE paper without reading it.\n \n@@ -1205,3 +1247,475 @@ project_plan:\n - Include requirements.txt or equivalent\n - Keep it minimal but complete (max 15 files)\n - Use tree format: \u251c\u2500\u2500 \u2500 \u2502 symbols for visual hierarchy\"\"\"\n+\n+# =============================================================================\n+# TRADITIONAL PROMPTS (Non-segmented versions for smaller documents)\n+# =============================================================================\n+\n+# Traditional Algorithm Analysis Prompt (No Segmentation)\n+PAPER_ALGORITHM_ANALYSIS_PROMPT_TRADITIONAL = \"\"\"You are extracting COMPLETE implementation details from a research paper. Your goal is to capture EVERY algorithm, formula, and technical detail needed for perfect reproduction.\n+\n+# DOCUMENT READING STRATEGY\n+\n+## TRADITIONAL APPROACH: Full Document Reading\n+Read the complete document to ensure comprehensive coverage of all algorithmic details:\n+\n+1. **Locate and read the markdown (.md) file** in the paper directory\n+2. **Analyze the entire document** to capture all algorithms, methods, and formulas\n+3. **Extract complete implementation details** without missing any components\n+\n+# DETAILED EXTRACTION PROTOCOL\n+\n+## 1. COMPREHENSIVE ALGORITHM SCAN\n+Read through the entire document systematically:\n+- Method/Algorithm sections\n+- Implementation Details\n+- Hyperparameters and training details\n+- Mathematical formulations\n+\n+## 2. ALGORITHM DEEP EXTRACTION\n+For EVERY algorithm/method/procedure mentioned:\n+\n+### Algorithm Structure\n+```yaml\n+algorithm_name: \"[Exact name from paper]\"\n+section: \"[e.g., Section 3.2]\"\n+algorithm_box: \"[e.g., Algorithm 1 on page 4]\"\n+\n+pseudocode: |\n+  [COPY THE EXACT PSEUDOCODE FROM PAPER]\n+  Input: ...\n+  Output: ...\n+  1. Initialize ...\n+  2. For each ...\n+     2.1 Calculate ...\n+  [Keep exact formatting and numbering]\n+\n+mathematical_formulation:\n+  - equation: \"[Copy formula EXACTLY, e.g., L = L_task + \u03bb*L_explain]\"\n+    equation_number: \"[e.g., Eq. 3]\"\n+    where:\n+      L_task: \"task loss\"\n+      L_explain: \"explanation loss\"\n+      \u03bb: \"weighting parameter (default: 0.5)\"\n+\n+step_by_step_breakdown:\n+  1. \"[Detailed explanation of what step 1 does]\"\n+  2. \"[What step 2 computes and why]\"\n+\n+implementation_details:\n+  - \"Uses softmax temperature \u03c4 = 0.1\"\n+  - \"Gradient clipping at norm 1.0\"\n+  - \"Initialize weights with Xavier uniform\"\n+```\n+\n+## 3. COMPONENT EXTRACTION\n+For EVERY component/module mentioned:\n+\n+### Component Details\n+```yaml\n+component_name: \"[e.g., Mask Network, Critic Network]\"\n+purpose: \"[What this component does in the system]\"\n+architecture:\n+  input: \"[shape and meaning]\"\n+  layers:\n+    - \"[Conv2d(3, 64, kernel=3, stride=1)]\"\n+    - \"[ReLU activation]\"\n+    - \"[BatchNorm2d(64)]\"\n+  output: \"[shape and meaning]\"\n+\n+special_features:\n+  - \"[Any unique aspects]\"\n+  - \"[Special initialization]\"\n+```\n+\n+## 4. TRAINING PROCEDURE\n+Extract the COMPLETE training process:\n+\n+```yaml\n+training_loop:\n+  outer_iterations: \"[number or condition]\"\n+  inner_iterations: \"[number or condition]\"\n+\n+  steps:\n+    1. \"Sample batch of size B from buffer\"\n+    2. \"Compute importance weights using...\"\n+    3. \"Update policy with loss...\"\n+\n+  loss_functions:\n+    - name: \"policy_loss\"\n+      formula: \"[exact formula]\"\n+      components: \"[what each term means]\"\n+\n+  optimization:\n+    optimizer: \"Adam\"\n+    learning_rate: \"3e-4\"\n+    lr_schedule: \"linear decay to 0\"\n+    gradient_norm: \"clip at 0.5\"\n+```\n+\n+## 5. HYPERPARAMETERS HUNT\n+Search EVERYWHERE (text, tables, captions) for:\n+\n+```yaml\n+hyperparameters:\n+  # Training\n+  batch_size: 64\n+  buffer_size: 1e6\n+  discount_gamma: 0.99\n+\n+  # Architecture\n+  hidden_units: [256, 256]\n+  activation: \"ReLU\"\n+\n+  # Algorithm-specific\n+  explanation_weight: 0.5\n+  exploration_bonus_scale: 0.1\n+  reset_probability: 0.3\n+\n+  # Found in:\n+  location_references:\n+    - \"batch_size: Table 1\"\n+    - \"hidden_units: Section 4.1\"\n+```\n+\n+# OUTPUT FORMAT\n+```yaml\n+complete_algorithm_extraction:\n+  paper_structure:\n+    method_sections: \"[3, 3.1, 3.2, 3.3, 4]\"\n+    algorithm_count: \"[total number found]\"\n+\n+  main_algorithm:\n+    [COMPLETE DETAILS AS ABOVE]\n+\n+  supporting_algorithms:\n+    - [EACH SUPPORTING ALGORITHM WITH FULL DETAILS]\n+\n+  components:\n+    - [EVERY COMPONENT WITH ARCHITECTURE]\n+\n+  training_details:\n+    [COMPLETE TRAINING PROCEDURE]\n+\n+  all_hyperparameters:\n+    [EVERY PARAMETER WITH VALUE AND SOURCE]\n+\n+  implementation_notes:\n+    - \"[Any implementation hint from paper]\"\n+    - \"[Tricks mentioned in text]\"\n+\n+  missing_but_critical:\n+    - \"[What's not specified but essential]\"\n+    - \"[With suggested defaults]\"\n+```\n+\n+BE EXHAUSTIVE. A developer should be able to implement the ENTIRE paper using only your extraction.\"\"\"\n+\n+# Traditional Concept Analysis Prompt (No Segmentation)\n+PAPER_CONCEPT_ANALYSIS_PROMPT_TRADITIONAL = \"\"\"You are doing a COMPREHENSIVE analysis of a research paper to understand its complete structure, contributions, and implementation requirements.\n+\n+# OBJECTIVE\n+Map out the ENTIRE paper structure and identify ALL components that need implementation for successful reproduction.\n+\n+# DOCUMENT READING STRATEGY\n+\n+## TRADITIONAL APPROACH: Complete Document Analysis\n+Read the entire document systematically to ensure comprehensive understanding:\n+\n+1. **Locate and read the markdown (.md) file** in the paper directory\n+2. **Analyze the complete document structure** from introduction to conclusion\n+3. **Extract all conceptual frameworks** and implementation requirements\n+\n+# COMPREHENSIVE ANALYSIS PROTOCOL\n+\n+## 1. COMPLETE PAPER STRUCTURAL ANALYSIS\n+Create a full map of the document:\n+\n+```yaml\n+paper_structure_map:\n+  title: \"[Full paper title]\"\n+\n+  sections:\n+    1_introduction:\n+      main_claims: \"[What the paper claims to achieve]\"\n+      problem_definition: \"[Exact problem being solved]\"\n+\n+    2_related_work:\n+      key_comparisons: \"[Methods this work builds upon or competes with]\"\n+\n+    3_method:  # May have multiple subsections\n+      subsections:\n+        3.1: \"[Title and main content]\"\n+        3.2: \"[Title and main content]\"\n+      algorithms_presented: \"[List all algorithms by name]\"\n+\n+    4_experiments:\n+      environments: \"[All test environments/datasets]\"\n+      baselines: \"[All comparison methods]\"\n+      metrics: \"[All evaluation metrics used]\"\n+\n+    5_results:\n+      main_findings: \"[Key results that prove the method works]\"\n+      tables_figures: \"[Important result tables/figures to reproduce]\"\n+```\n+\n+## 2. METHOD DECOMPOSITION\n+For the main method/approach:\n+\n+```yaml\n+method_decomposition:\n+  method_name: \"[Full name and acronym]\"\n+\n+  core_components:  # Break down into implementable pieces\n+    component_1:\n+      name: \"[e.g., State Importance Estimator]\"\n+      purpose: \"[Why this component exists]\"\n+      paper_section: \"[Where it's described]\"\n+\n+    component_2:\n+      name: \"[e.g., Policy Refinement Module]\"\n+      purpose: \"[Its role in the system]\"\n+      paper_section: \"[Where it's described]\"\n+\n+  component_interactions:\n+    - \"[How component 1 feeds into component 2]\"\n+    - \"[Data flow between components]\"\n+\n+  theoretical_foundation:\n+    key_insight: \"[The main theoretical insight]\"\n+    why_it_works: \"[Intuitive explanation]\"\n+```\n+\n+## 3. IMPLEMENTATION REQUIREMENTS MAPPING\n+Map paper content to code requirements:\n+\n+```yaml\n+implementation_map:\n+  algorithms_to_implement:\n+    - algorithm: \"[Name from paper]\"\n+      section: \"[Where defined]\"\n+      complexity: \"[Simple/Medium/Complex]\"\n+      dependencies: \"[What it needs to work]\"\n+\n+  models_to_build:\n+    - model: \"[Neural network or other model]\"\n+      architecture_location: \"[Section describing it]\"\n+      purpose: \"[What this model does]\"\n+\n+  data_processing:\n+    - pipeline: \"[Data preprocessing needed]\"\n+      requirements: \"[What the data should look like]\"\n+\n+  evaluation_suite:\n+    - metric: \"[Metric name]\"\n+      formula_location: \"[Where it's defined]\"\n+      purpose: \"[What it measures]\"\n+```\n+\n+## 4. EXPERIMENT REPRODUCTION PLAN\n+Identify ALL experiments needed:\n+\n+```yaml\n+experiments_analysis:\n+  main_results:\n+    - experiment: \"[Name/description]\"\n+      proves: \"[What claim this validates]\"\n+      requires: \"[Components needed to run this]\"\n+      expected_outcome: \"[Specific numbers/trends]\"\n+\n+  ablation_studies:\n+    - study: \"[What is being ablated]\"\n+      purpose: \"[What this demonstrates]\"\n+\n+  baseline_comparisons:\n+    - baseline: \"[Method name]\"\n+      implementation_required: \"[Yes/No/Partial]\"\n+      source: \"[Where to find implementation]\"\n+```\n+\n+## 5. CRITICAL SUCCESS FACTORS\n+What defines successful reproduction:\n+\n+```yaml\n+success_criteria:\n+  must_achieve:\n+    - \"[Primary result that must be reproduced]\"\n+    - \"[Core behavior that must be demonstrated]\"\n+\n+  should_achieve:\n+    - \"[Secondary results that validate the method]\"\n+\n+  validation_evidence:\n+    - \"[Specific figure/table to reproduce]\"\n+    - \"[Qualitative behavior to demonstrate]\"\n+```\n+\n+# OUTPUT FORMAT\n+```yaml\n+comprehensive_paper_analysis:\n+  executive_summary:\n+    paper_title: \"[Full title]\"\n+    core_contribution: \"[One sentence summary]\"\n+    implementation_complexity: \"[Low/Medium/High]\"\n+    estimated_components: \"[Number of major components to build]\"\n+\n+  complete_structure_map:\n+    [FULL SECTION BREAKDOWN AS ABOVE]\n+\n+  method_architecture:\n+    [DETAILED COMPONENT BREAKDOWN]\n+\n+  implementation_requirements:\n+    [ALL ALGORITHMS, MODELS, DATA, METRICS]\n+\n+  reproduction_roadmap:\n+    phase_1: \"[What to implement first]\"\n+    phase_2: \"[What to build next]\"\n+    phase_3: \"[Final components and validation]\"\n+\n+  validation_checklist:\n+    - \"[ ] [Specific result to achieve]\"\n+    - \"[ ] [Behavior to demonstrate]\"\n+    - \"[ ] [Metric to match]\"\n+```\n+\n+BE THOROUGH. Miss nothing. The output should be a complete blueprint for reproduction.\"\"\"\n+\n+# Traditional Code Planning Prompt (No Segmentation)\n+CODE_PLANNING_PROMPT_TRADITIONAL = \"\"\"You are creating a DETAILED, COMPLETE reproduction plan by integrating comprehensive analysis results.\n+\n+# INPUT\n+You receive two exhaustive analyses:\n+1. **Comprehensive Paper Analysis**: Complete paper structure, components, and requirements\n+2. **Complete Algorithm Extraction**: All algorithms, formulas, pseudocode, and technical details\n+\n+Plus you can access the complete paper document by reading the markdown file directly.\n+\n+# TRADITIONAL DOCUMENT ACCESS\n+\n+## Direct Paper Reading\n+For any additional details needed beyond the provided analyses:\n+\n+1. **Read the complete markdown (.md) file** in the paper directory\n+2. **Access any section directly** without token limitations for smaller documents\n+3. **Cross-reference information** across the entire document as needed\n+\n+# OBJECTIVE\n+Create an implementation plan so detailed that a developer can reproduce the ENTIRE paper without reading it.\n+\n+# DETAILED SYNTHESIS PROCESS\n+\n+## 1. MERGE ALL INFORMATION\n+Combine EVERYTHING from both analyses:\n+- Every algorithm with its pseudocode\n+- Every component with its architecture\n+- Every hyperparameter with its value\n+- Every experiment with expected results\n+\n+## 2. MAP CONTENT TO IMPLEMENTATION\n+\n+For each component you identify, specify how it will be implemented:\n+\n+```\n+# DESIGN YOUR MAPPING: Connect paper content to code organization\n+[For each algorithm/component/method in the paper]:\n+  - What it does and where it's described in the paper\n+  - How you'll organize the code (files, classes, functions - your choice)\n+  - What specific formulas, algorithms, or procedures need implementation\n+  - Dependencies and relationships with other components\n+  - Implementation approach that makes sense for this specific paper\n+```\n+\n+## 3. EXTRACT ALL TECHNICAL DETAILS\n+\n+Identify every technical detail that needs implementation:\n+\n+```\n+# COMPREHENSIVE TECHNICAL EXTRACTION:\n+[Gather all implementation-relevant details from the paper]:\n+  - All algorithms with complete pseudocode and mathematical formulations\n+  - All parameters, hyperparameters, and configuration values\n+  - All architectural details (if applicable to your paper type)\n+  - All experimental procedures and evaluation methods\n+  - Any implementation hints, tricks, or special considerations mentioned\n+```\n+\n+# COMPREHENSIVE OUTPUT FORMAT\n+\n+```yaml\n+complete_reproduction_plan:\n+  paper_info:\n+    title: \"[Full paper title]\"\n+    core_contribution: \"[Main innovation being reproduced]\"\n+\n+  # SECTION 1: File Structure Design\n+\n+  # DESIGN YOUR OWN STRUCTURE: Create a file organization that best serves this specific paper\n+  # - Analyze what the paper contains (algorithms, models, experiments, systems, etc.)\n+  # - Organize files and directories in the most logical way for implementation\n+  # - Create meaningful names and groupings based on paper content\n+  # - Keep it clean, intuitive, and focused on what actually needs to be implemented\n+\n+  file_structure: |\n+    [Design and specify your own project structure here]\n+    [Organize based on what this paper actually contains and needs]\n+    [Create directories and files that make sense for this specific implementation]\n+    [No templates - create what works best for this paper]\n+\n+  # SECTION 2: Implementation Components\n+\n+  # IDENTIFY AND SPECIFY: What needs to be implemented based on this paper\n+  # - List all algorithms, models, systems, or components mentioned\n+  # - Map each to implementation details and file locations\n+  # - Include formulas, pseudocode, and technical specifications\n+  # - Organize in whatever way makes sense for this paper\n+\n+  implementation_components: |\n+    [List and specify all components that need implementation]\n+    [For each component: purpose, location, algorithms, formulas, technical details]\n+    [Organize and structure this based on the paper's actual content]\n+\n+  # SECTION 3: Validation & Evaluation\n+\n+  # DESIGN VALIDATION: How to verify the implementation works correctly\n+  # - Define what experiments, tests, or proofs are needed\n+  # - Specify expected results from the paper (figures, tables, theorems)\n+  # - Design validation approach appropriate for this paper's domain\n+  # - Include setup requirements and success criteria\n+\n+  validation_approach: |\n+    [Design validation strategy appropriate for this paper]\n+    [Specify experiments, tests, or mathematical verification needed]\n+    [Define expected results and success criteria]\n+    [Include any special setup or evaluation requirements]\n+\n+  # SECTION 4: Environment & Dependencies\n+\n+  # SPECIFY REQUIREMENTS: What's needed to run this implementation\n+  # - Programming language and version requirements\n+  # - External libraries and exact versions (if specified in paper)\n+  # - Hardware requirements (GPU, memory, etc.)\n+  # - Any special setup or installation steps\n+\n+  environment_setup: |\n+    [List all dependencies and environment requirements for this specific paper]\n+    [Include versions where specified, reasonable defaults where not]\n+    [Note any special hardware or software requirements]\n+\n+  # SECTION 5: Implementation Strategy\n+\n+  # PLAN YOUR APPROACH: How to implement this paper step by step\n+  # - Break down implementation into logical phases\n+  # - Identify dependencies between components\n+  # - Plan verification and testing at each stage\n+  # - Handle missing details with reasonable defaults\n+\n+  implementation_strategy: |\n+    [Design your implementation approach for this specific paper]\n+    [Break into phases that make sense for this paper's components]\n+    [Plan testing and verification throughout the process]\n+    [Address any missing details or ambiguities in the paper]\n+```\n+\n+BE EXHAUSTIVE. Every algorithm, every formula, every parameter, every file should be specified in complete detail.\"\"\"\n"
        },
        {
            "commit": "daad35b9c60770a655eaaea4565bf81f63ae1b38",
            "file_path": "tools/code_implementation_server.py",
            "diff": "diff --git a/tools/code_implementation_server.py b/tools/code_implementation_server.py\nindex 311970a..6a3a12f 100644\n--- a/tools/code_implementation_server.py\n+++ b/tools/code_implementation_server.py\n@@ -571,7 +571,7 @@ def _normalize_file_path(file_path: str) -> str:\n     normalized = normalized.replace(\"\\\\\", \"/\")\n \n     # Remove common prefixes to make matching more flexible\n-    common_prefixes = [\"rice/\", \"src/\", \"./rice/\", \"./src/\", \"./\"]\n+    common_prefixes = [\"src/\", \"./src/\", \"./\", \"core/\", \"lib/\", \"main/\"]\n     for prefix in common_prefixes:\n         if normalized.startswith(prefix):\n             normalized = normalized[len(prefix) :]\n@@ -621,7 +621,7 @@ def _paths_match(\n \n def _remove_common_prefixes(file_path: str) -> str:\n     \"\"\"Remove common prefixes from file path\"\"\"\n-    prefixes_to_remove = [\"rice/\", \"src/\", \"core/\", \"./\"]\n+    prefixes_to_remove = [\"src/\", \"core/\", \"./\", \"lib/\", \"main/\"]\n     path = file_path\n \n     for prefix in prefixes_to_remove:\n"
        },
        {
            "commit": "daad35b9c60770a655eaaea4565bf81f63ae1b38",
            "file_path": "tools/code_indexer.py",
            "diff": "diff --git a/tools/code_indexer.py b/tools/code_indexer.py\nindex a19dfb6..aadd8ae 100644\n--- a/tools/code_indexer.py\n+++ b/tools/code_indexer.py\n@@ -24,46 +24,8 @@ from dataclasses import dataclass, asdict\n from typing import List, Dict, Any\n \n # MCP Agent imports for LLM\n-from mcp_agent.workflows.llm.augmented_llm_anthropic import AnthropicAugmentedLLM\n-from mcp_agent.workflows.llm.augmented_llm_openai import OpenAIAugmentedLLM\n import yaml\n-\n-\n-def get_preferred_llm_class(config_path: str = \"mcp_agent.secrets.yaml\"):\n-    \"\"\"\n-    Automatically select the LLM class based on API key availability in configuration.\n-\n-    Reads from YAML config file and returns AnthropicAugmentedLLM if anthropic.api_key\n-    is available, otherwise returns OpenAIAugmentedLLM.\n-\n-    Args:\n-        config_path: Path to the YAML configuration file\n-\n-    Returns:\n-        class: The preferred LLM class\n-    \"\"\"\n-    try:\n-        # Try to read the configuration file\n-        if os.path.exists(config_path):\n-            with open(config_path, \"r\", encoding=\"utf-8\") as f:\n-                config = yaml.safe_load(f)\n-\n-            # Check for anthropic API key in config\n-            anthropic_config = config.get(\"anthropic\", {})\n-            anthropic_key = anthropic_config.get(\"api_key\", \"\")\n-\n-            if anthropic_key and anthropic_key.strip() and not anthropic_key == \"\":\n-                return AnthropicAugmentedLLM\n-            else:\n-                return OpenAIAugmentedLLM\n-        else:\n-            print(f\"\ud83e\udd16 Config file {config_path} not found, using OpenAIAugmentedLLM\")\n-            return OpenAIAugmentedLLM\n-\n-    except Exception as e:\n-        print(f\"\ud83e\udd16 Error reading config file {config_path}: {e}\")\n-        print(\"\ud83e\udd16 Falling back to OpenAIAugmentedLLM\")\n-        return OpenAIAugmentedLLM\n+from utils.llm_utils import get_preferred_llm_class\n \n \n def get_default_models(config_path: str = \"mcp_agent.config.yaml\"):\n"
        },
        {
            "commit": "daad35b9c60770a655eaaea4565bf81f63ae1b38",
            "file_path": "tools/code_reference_indexer.py",
            "diff": "diff --git a/tools/code_reference_indexer.py b/tools/code_reference_indexer.py\nindex c24650e..c4abc01 100644\n--- a/tools/code_reference_indexer.py\n+++ b/tools/code_reference_indexer.py\n@@ -201,15 +201,24 @@ def find_direct_relationships_in_cache(\n     \"\"\"Find direct relationships with target file from provided cache\"\"\"\n     relationships = []\n \n-    # Normalize target file path (remove rice/ prefix if exists)\n-    normalized_target = target_file.replace(\"rice/\", \"\").strip(\"/\")\n+    # Normalize target file path (remove common prefixes if exists)\n+    common_prefixes = [\"src/\", \"core/\", \"lib/\", \"main/\", \"./\"]\n+    normalized_target = target_file.strip(\"/\")\n+    for prefix in common_prefixes:\n+        if normalized_target.startswith(prefix):\n+            normalized_target = normalized_target[len(prefix) :]\n+            break\n \n     # Collect relationship information from all index files\n     for repo_name, index_data in index_cache.items():\n         repo_relationships = extract_relationships(index_data)\n         for rel in repo_relationships:\n             # Normalize target file path in relationship\n-            normalized_rel_target = rel.target_file_path.replace(\"rice/\", \"\").strip(\"/\")\n+            normalized_rel_target = rel.target_file_path.strip(\"/\")\n+            for prefix in common_prefixes:\n+                if normalized_rel_target.startswith(prefix):\n+                    normalized_rel_target = normalized_rel_target[len(prefix) :]\n+                    break\n \n             # Check target file path matching (support multiple matching methods)\n             if (\n"
        },
        {
            "commit": "daad35b9c60770a655eaaea4565bf81f63ae1b38",
            "file_path": "tools/document_segmentation_server.py",
            "diff": "diff --git a/tools/document_segmentation_server.py b/tools/document_segmentation_server.py\nnew file mode 100644\nindex 0000000..3ce17bf\n--- /dev/null\n+++ b/tools/document_segmentation_server.py\n@@ -0,0 +1,1612 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+Document Segmentation MCP Server\n+\n+This MCP server provides intelligent document segmentation and retrieval functions for handling\n+large research papers and technical documents that exceed LLM token limits.\n+\n+==== CORE FUNCTIONALITY ====\n+1. Analyze document structure and type using semantic content analysis\n+2. Create intelligent segments based on content semantics, not just structure\n+3. Provide query-aware segment retrieval with relevance scoring\n+4. Support both structured (papers with headers) and unstructured documents\n+5. Configurable segmentation strategies based on document complexity\n+\n+==== MCP TOOLS PROVIDED ====\n+\n+\ud83d\udcc4 analyze_and_segment_document(paper_dir: str, force_refresh: bool = False)\n+   Purpose: Analyzes document structure and creates intelligent segments\n+   - Detects document type (research paper, technical doc, algorithm-focused, etc.)\n+   - Selects optimal segmentation strategy based on content analysis\n+   - Creates semantic segments preserving algorithm and concept integrity\n+   - Stores segmentation index for efficient retrieval\n+   - Returns: JSON with segmentation status, strategy used, and segment count\n+\n+\ud83d\udcd6 read_document_segments(paper_dir: str, query_type: str, keywords: List[str] = None, \n+                         max_segments: int = 3, max_total_chars: int = None)\n+   Purpose: Intelligently retrieves relevant document segments based on query context\n+   - query_type: \"concept_analysis\", \"algorithm_extraction\", or \"code_planning\"\n+   - Uses semantic relevance scoring to rank segments\n+   - Applies query-specific filtering and keyword matching\n+   - Dynamically calculates optimal character limits based on content complexity\n+   - Returns: JSON with selected segments optimized for the specific query type\n+\n+\ud83d\udccb get_document_overview(paper_dir: str)\n+   Purpose: Provides high-level overview of document structure and available segments\n+   - Shows document type and segmentation strategy used\n+   - Lists all segments with titles, content types, and relevance scores\n+   - Displays segment statistics (character counts, keyword summaries)\n+   - Returns: JSON with complete document analysis metadata\n+\n+==== SEGMENTATION STRATEGIES ====\n+- semantic_research_focused: For academic papers with complex algorithmic content\n+- algorithm_preserve_integrity: Maintains algorithm blocks and formula chains intact\n+- concept_implementation_hybrid: Merges related concepts with implementation details\n+- semantic_chunking_enhanced: Advanced boundary detection for long documents\n+- content_aware_segmentation: Adaptive chunking based on content density\n+\n+==== INTELLIGENT FEATURES ====\n+- Semantic boundary detection (not just structural)\n+- Algorithm block identification and preservation\n+- Formula chain recognition and grouping\n+- Concept-implementation relationship mapping\n+- Multi-level relevance scoring (content type, importance, keyword matching)\n+- Backward compatibility with existing document indexes\n+- Configurable via mcp_agent.config.yaml (enabled/disabled, size thresholds)\n+\n+Usage:\n+python tools/document_segmentation_server.py\n+\"\"\"\n+\n+import os\n+import re\n+import json\n+import sys\n+import io\n+from pathlib import Path\n+from typing import Dict, Any, List, Optional, Tuple\n+import hashlib\n+import logging\n+from datetime import datetime\n+from dataclasses import dataclass, asdict\n+\n+# Set standard output encoding to UTF-8\n+if sys.stdout.encoding != \"utf-8\":\n+    try:\n+        if hasattr(sys.stdout, \"reconfigure\"):\n+            sys.stdout.reconfigure(encoding=\"utf-8\")\n+            sys.stderr.reconfigure(encoding=\"utf-8\")\n+        else:\n+            sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding=\"utf-8\")\n+            sys.stderr = io.TextIOWrapper(sys.stderr.detach(), encoding=\"utf-8\")\n+    except Exception as e:\n+        print(f\"Warning: Could not set UTF-8 encoding: {e}\")\n+\n+# Import MCP related modules\n+from mcp.server.fastmcp import FastMCP\n+\n+# Setup logging\n+logging.basicConfig(level=logging.INFO)\n+logger = logging.getLogger(__name__)\n+\n+# Create FastMCP server instance\n+mcp = FastMCP(\"document-segmentation-server\")\n+\n+@dataclass\n+class DocumentSegment:\n+    \"\"\"Represents a document segment with metadata\"\"\"\n+    id: str\n+    title: str\n+    content: str\n+    content_type: str  # \"introduction\", \"methodology\", \"algorithm\", \"results\", etc.\n+    keywords: List[str]\n+    char_start: int\n+    char_end: int\n+    char_count: int\n+    relevance_scores: Dict[str, float]  # Scores for different query types\n+    section_path: str  # e.g., \"3.2.1\" for nested sections\n+\n+@dataclass \n+class DocumentIndex:\n+    \"\"\"Document index containing all segments and metadata\"\"\"\n+    document_path: str\n+    document_type: str  # \"academic_paper\", \"technical_doc\", \"code_doc\", \"general\"\n+    segmentation_strategy: str\n+    total_segments: int\n+    total_chars: int\n+    segments: List[DocumentSegment]\n+    created_at: str\n+    \n+class DocumentAnalyzer:\n+    \"\"\"Enhanced document analyzer using semantic content analysis instead of mechanical structure detection\"\"\"\n+    \n+    # More precise semantic indicators, weighted by importance\n+    ALGORITHM_INDICATORS = {\n+        \"high\": [\"algorithm\", \"procedure\", \"method\", \"approach\", \"technique\", \"framework\"],\n+        \"medium\": [\"step\", \"process\", \"implementation\", \"computation\", \"calculation\"],\n+        \"low\": [\"example\", \"illustration\", \"demonstration\"]\n+    }\n+    \n+    TECHNICAL_CONCEPT_INDICATORS = {\n+        \"high\": [\"formula\", \"equation\", \"theorem\", \"lemma\", \"proof\", \"definition\"],\n+        \"medium\": [\"parameter\", \"variable\", \"function\", \"model\", \"architecture\"],\n+        \"low\": [\"notation\", \"symbol\", \"term\"]\n+    }\n+    \n+    IMPLEMENTATION_INDICATORS = {\n+        \"high\": [\"code\", \"implementation\", \"programming\", \"software\", \"system\"],\n+        \"medium\": [\"design\", \"structure\", \"module\", \"component\", \"interface\"],\n+        \"low\": [\"tool\", \"library\", \"package\"]\n+    }\n+    \n+    # Semantic features of document types (not just based on titles)\n+    RESEARCH_PAPER_PATTERNS = [\n+        r\"(?i)\\babstract\\b.*?\\n.*?(introduction|motivation|background)\",\n+        r\"(?i)(methodology|method).*?(experiment|evaluation|result)\",\n+        r\"(?i)(conclusion|future work|limitation).*?(reference|bibliography)\",\n+        r\"(?i)(related work|literature review|prior art)\"\n+    ]\n+    \n+    TECHNICAL_DOC_PATTERNS = [\n+        r\"(?i)(getting started|installation|setup).*?(usage|example)\",\n+        r\"(?i)(api|interface|specification).*?(parameter|endpoint)\",\n+        r\"(?i)(tutorial|guide|walkthrough).*?(step|instruction)\",\n+        r\"(?i)(troubleshooting|faq|common issues)\"\n+    ]\n+    \n+    def analyze_document_type(self, content: str) -> Tuple[str, float]:\n+        \"\"\"\n+        Enhanced document type analysis based on semantic content patterns\n+        \n+        Returns:\n+            Tuple[str, float]: (document_type, confidence_score)\n+        \"\"\"\n+        content_lower = content.lower()\n+        \n+        # Calculate weighted semantic indicator scores\n+        algorithm_score = self._calculate_weighted_score(content_lower, self.ALGORITHM_INDICATORS)\n+        concept_score = self._calculate_weighted_score(content_lower, self.TECHNICAL_CONCEPT_INDICATORS)\n+        implementation_score = self._calculate_weighted_score(content_lower, self.IMPLEMENTATION_INDICATORS)\n+        \n+        # Detect semantic patterns of document types\n+        research_pattern_score = self._detect_pattern_score(content, self.RESEARCH_PAPER_PATTERNS)\n+        technical_pattern_score = self._detect_pattern_score(content, self.TECHNICAL_DOC_PATTERNS)\n+        \n+        # Comprehensive evaluation of document type\n+        total_research_score = algorithm_score + concept_score + research_pattern_score * 2\n+        total_technical_score = implementation_score + technical_pattern_score * 2\n+        \n+        # Determine document type based on content density and pattern matching\n+        if research_pattern_score > 0.5 and total_research_score > 3.0:\n+            return \"research_paper\", min(0.95, 0.6 + research_pattern_score * 0.35)\n+        elif algorithm_score > 2.0 and concept_score > 1.5:\n+            return \"algorithm_focused\", 0.85\n+        elif total_technical_score > 2.5:\n+            return \"technical_doc\", 0.8\n+        elif implementation_score > 1.5:\n+            return \"implementation_guide\", 0.75\n+        else:\n+            return \"general_document\", 0.5\n+    \n+    def _calculate_weighted_score(self, content: str, indicators: Dict[str, List[str]]) -> float:\n+        \"\"\"Calculate weighted semantic indicator scores\"\"\"\n+        score = 0.0\n+        for weight_level, terms in indicators.items():\n+            weight = {\"high\": 3.0, \"medium\": 2.0, \"low\": 1.0}[weight_level]\n+            for term in terms:\n+                if term in content:\n+                    score += weight * (content.count(term) * 0.5 + 1)  # Consider term frequency\n+        return score\n+    \n+    def _detect_pattern_score(self, content: str, patterns: List[str]) -> float:\n+        \"\"\"Detect semantic pattern matching scores\"\"\"\n+        matches = 0\n+        for pattern in patterns:\n+            if re.search(pattern, content, re.DOTALL):\n+                matches += 1\n+        return matches / len(patterns)\n+    \n+    def detect_segmentation_strategy(self, content: str, doc_type: str) -> str:\n+        \"\"\"\n+        Intelligently determine the best segmentation strategy based on content semantics rather than mechanical structure\n+        \"\"\"\n+        # Analyze content characteristics\n+        algorithm_density = self._calculate_algorithm_density(content)\n+        concept_complexity = self._calculate_concept_complexity(content)\n+        implementation_detail_level = self._calculate_implementation_detail_level(content)\n+        \n+        # Select strategy based on document type and content characteristics\n+        if doc_type == \"research_paper\" and algorithm_density > 0.3:\n+            return \"semantic_research_focused\"\n+        elif doc_type == \"algorithm_focused\" or algorithm_density > 0.5:\n+            return \"algorithm_preserve_integrity\"\n+        elif concept_complexity > 0.4 and implementation_detail_level > 0.3:\n+            return \"concept_implementation_hybrid\"\n+        elif len(content) > 15000:  # Long documents\n+            return \"semantic_chunking_enhanced\"\n+        else:\n+            return \"content_aware_segmentation\"\n+    \n+    def _calculate_algorithm_density(self, content: str) -> float:\n+        \"\"\"Calculate algorithm content density\"\"\"\n+        total_chars = len(content)\n+        algorithm_chars = 0\n+        \n+        # Identify algorithm blocks\n+        algorithm_patterns = [\n+            r'(?i)(algorithm\\s+\\d+|procedure\\s+\\d+)',\n+            r'(?i)(step\\s+\\d+|phase\\s+\\d+)',\n+            r'(?i)(input:|output:|return:|initialize:)',\n+            r'(?i)(for\\s+each|while|if.*then|else)',\n+            r'(?i)(function|method|procedure).*\\('\n+        ]\n+        \n+        for pattern in algorithm_patterns:\n+            matches = re.finditer(pattern, content)\n+            for match in matches:\n+                # Estimate algorithm block size (expand forward and backward from match point)\n+                start = max(0, match.start() - 200)\n+                end = min(len(content), match.end() + 800)\n+                algorithm_chars += end - start\n+        \n+        return min(1.0, algorithm_chars / total_chars)\n+    \n+    def _calculate_concept_complexity(self, content: str) -> float:\n+        \"\"\"Calculate concept complexity\"\"\"\n+        concept_indicators = self.TECHNICAL_CONCEPT_INDICATORS\n+        complexity_score = 0.0\n+        \n+        for level, terms in concept_indicators.items():\n+            weight = {\"high\": 3.0, \"medium\": 2.0, \"low\": 1.0}[level]\n+            for term in terms:\n+                complexity_score += content.lower().count(term) * weight\n+        \n+        # Normalize to 0-1 range\n+        return min(1.0, complexity_score / 100)\n+    \n+    def _calculate_implementation_detail_level(self, content: str) -> float:\n+        \"\"\"Calculate implementation detail level\"\"\"\n+        implementation_patterns = [\n+            r'(?i)(code|implementation|programming)',\n+            r'(?i)(class|function|method|variable)',\n+            r'(?i)(import|include|library)',\n+            r'(?i)(parameter|argument|return)',\n+            r'(?i)(example|demo|tutorial)'\n+        ]\n+        \n+        detail_score = 0\n+        for pattern in implementation_patterns:\n+            detail_score += len(re.findall(pattern, content))\n+        \n+        return min(1.0, detail_score / 50)\n+\n+class DocumentSegmenter:\n+    \"\"\"Creates intelligent segments from documents\"\"\"\n+    \n+    def __init__(self):\n+        self.analyzer = DocumentAnalyzer()\n+    \n+    def segment_document(self, content: str, strategy: str) -> List[DocumentSegment]:\n+        \"\"\"\n+        Perform intelligent segmentation using the specified strategy\n+        \"\"\"\n+        if strategy == \"semantic_research_focused\":\n+            return self._segment_research_paper_semantically(content)\n+        elif strategy == \"algorithm_preserve_integrity\":\n+            return self._segment_preserve_algorithm_integrity(content)\n+        elif strategy == \"concept_implementation_hybrid\":\n+            return self._segment_concept_implementation_hybrid(content)\n+        elif strategy == \"semantic_chunking_enhanced\":\n+            return self._segment_by_enhanced_semantic_chunks(content)\n+        elif strategy == \"content_aware_segmentation\":\n+            return self._segment_content_aware(content)\n+        else:\n+            # Compatibility with legacy strategies\n+            return self._segment_by_enhanced_semantic_chunks(content)\n+    \n+    def _segment_by_headers(self, content: str) -> List[DocumentSegment]:\n+        \"\"\"Segment document based on markdown headers\"\"\"\n+        segments = []\n+        lines = content.split('\\n')\n+        current_segment = []\n+        current_header = None\n+        current_level = 0\n+        char_pos = 0\n+        \n+        for line in lines:\n+            line_with_newline = line + '\\n'\n+            \n+            # Check if line is a header\n+            header_match = re.match(r'^(#{1,6})\\s+(.+)$', line)\n+            \n+            if header_match:\n+                # Save previous segment if exists\n+                if current_segment and current_header:\n+                    segment_content = '\\n'.join(current_segment).strip()\n+                    if segment_content:\n+                        # Analyze content type and importance\n+                        content_type = self._classify_content_type(current_header, segment_content)\n+                        importance_score = 0.8 if content_type in ['algorithm', 'formula'] else 0.7\n+                        \n+                        segment = self._create_enhanced_segment(\n+                            segment_content, current_header, \n+                            char_pos - len(segment_content.encode('utf-8')), char_pos,\n+                            importance_score, content_type\n+                        )\n+                        segments.append(segment)\n+                \n+                # Start new segment\n+                current_level = len(header_match.group(1))\n+                current_header = header_match.group(2).strip()\n+                current_segment = [line]\n+            else:\n+                if current_segment is not None:\n+                    current_segment.append(line)\n+            \n+            char_pos += len(line_with_newline.encode('utf-8'))\n+        \n+        # Add final segment\n+        if current_segment and current_header:\n+            segment_content = '\\n'.join(current_segment).strip()\n+            if segment_content:\n+                # Analyze content type and importance\n+                content_type = self._classify_content_type(current_header, segment_content)\n+                importance_score = 0.8 if content_type in ['algorithm', 'formula'] else 0.7\n+                \n+                segment = self._create_enhanced_segment(\n+                    segment_content, current_header, \n+                    char_pos - len(segment_content.encode('utf-8')), char_pos,\n+                    importance_score, content_type\n+                )\n+                segments.append(segment)\n+        \n+        return segments\n+    \n+    def _segment_preserve_algorithm_integrity(self, content: str) -> List[DocumentSegment]:\n+        \"\"\"Smart segmentation strategy that preserves algorithm integrity\"\"\"\n+        segments = []\n+        \n+        # 1. Identify algorithm blocks and related descriptions\n+        algorithm_blocks = self._identify_algorithm_blocks(content)\n+        \n+        # 2. Identify concept definition groups\n+        concept_groups = self._identify_concept_groups(content)\n+        \n+        # 3. Identify formula derivation chains\n+        formula_chains = self._identify_formula_chains(content)\n+        \n+        # 4. Merge related content blocks to ensure integrity\n+        content_blocks = self._merge_related_content_blocks(\n+            algorithm_blocks, concept_groups, formula_chains, content\n+        )\n+        \n+        # 5. Convert to DocumentSegment\n+        for i, block in enumerate(content_blocks):\n+            segment = self._create_enhanced_segment(\n+                block['content'], \n+                block['title'], \n+                block['start_pos'], \n+                block['end_pos'],\n+                block['importance_score'],\n+                block['content_type']\n+            )\n+            segments.append(segment)\n+        \n+        return segments\n+    \n+    def _segment_research_paper_semantically(self, content: str) -> List[DocumentSegment]:\n+        \"\"\"Semantic segmentation specifically for research papers\"\"\"\n+        segments = []\n+        \n+        # Identify semantic structure of research papers\n+        paper_sections = self._identify_research_paper_sections(content)\n+        \n+        for section in paper_sections:\n+            # Ensure each section contains sufficient context\n+            enhanced_content = self._enhance_section_with_context(section, content)\n+            \n+            segment = self._create_enhanced_segment(\n+                enhanced_content['content'],\n+                enhanced_content['title'],\n+                enhanced_content['start_pos'],\n+                enhanced_content['end_pos'],\n+                enhanced_content['importance_score'],\n+                enhanced_content['content_type']\n+            )\n+            segments.append(segment)\n+        \n+        return segments\n+    \n+    def _segment_concept_implementation_hybrid(self, content: str) -> List[DocumentSegment]:\n+        \"\"\"Intelligent segmentation combining concepts and implementation\"\"\"\n+        segments = []\n+        \n+        # Identify concept-implementation correspondence\n+        concept_impl_pairs = self._identify_concept_implementation_pairs(content)\n+        \n+        for pair in concept_impl_pairs:\n+            # Merge related concepts and implementations into one segment\n+            merged_content = self._merge_concept_with_implementation(pair, content)\n+            \n+            segment = self._create_enhanced_segment(\n+                merged_content['content'],\n+                merged_content['title'],\n+                merged_content['start_pos'],\n+                merged_content['end_pos'],\n+                merged_content['importance_score'],\n+                merged_content['content_type']\n+            )\n+            segments.append(segment)\n+        \n+        return segments\n+    \n+    def _segment_by_enhanced_semantic_chunks(self, content: str) -> List[DocumentSegment]:\n+        \"\"\"Enhanced semantic chunk segmentation\"\"\"\n+        segments = []\n+        \n+        # Use improved semantic boundary detection\n+        semantic_boundaries = self._detect_semantic_boundaries(content)\n+        \n+        current_start = 0\n+        for i, boundary in enumerate(semantic_boundaries):\n+            chunk_content = content[current_start:boundary['position']]\n+            \n+            if len(chunk_content.strip()) > 200:  # Minimum content threshold\n+                segment = self._create_enhanced_segment(\n+                    chunk_content,\n+                    boundary['suggested_title'],\n+                    current_start,\n+                    boundary['position'],\n+                    boundary['importance_score'],\n+                    boundary['content_type']\n+                )\n+                segments.append(segment)\n+            \n+            current_start = boundary['position']\n+        \n+        # Handle the final segment\n+        if current_start < len(content):\n+            final_content = content[current_start:]\n+            if len(final_content.strip()) > 200:\n+                segment = self._create_enhanced_segment(\n+                    final_content,\n+                    \"Final Section\",\n+                    current_start,\n+                    len(content),\n+                    0.7,\n+                    \"general\"\n+                )\n+                segments.append(segment)\n+        \n+        return segments\n+    \n+    def _segment_content_aware(self, content: str) -> List[DocumentSegment]:\n+        \"\"\"Content-aware intelligent segmentation\"\"\"\n+        segments = []\n+        \n+        # Adaptive segmentation size\n+        optimal_chunk_size = self._calculate_optimal_chunk_size(content)\n+        \n+        # Segment based on content density\n+        content_chunks = self._create_content_aware_chunks(content, optimal_chunk_size)\n+        \n+        for chunk in content_chunks:\n+            segment = self._create_enhanced_segment(\n+                chunk['content'],\n+                chunk['title'],\n+                chunk['start_pos'],\n+                chunk['end_pos'],\n+                chunk['importance_score'],\n+                chunk['content_type']\n+            )\n+            segments.append(segment)\n+        \n+        return segments\n+    \n+    def _segment_academic_paper(self, content: str) -> List[DocumentSegment]:\n+        \"\"\"Segment academic paper using semantic understanding\"\"\"\n+        # First try header-based segmentation\n+        headers = re.findall(r'^(#{1,6})\\s+(.+)$', content, re.MULTILINE)\n+        if len(headers) >= 2:\n+            return self._segment_by_headers(content)\n+        \n+        # Fallback to semantic detection of academic sections\n+        sections = self._detect_academic_sections(content)\n+        segments = []\n+        \n+        for section in sections:\n+            # Determine importance based on section type\n+            section_type = section.get('type', 'general')\n+            content_type = section_type if section_type in ['algorithm', 'formula', 'introduction', 'conclusion'] else 'general'\n+            importance_score = {\n+                'algorithm': 0.95,\n+                'formula': 0.9,\n+                'introduction': 0.85,\n+                'conclusion': 0.8\n+            }.get(content_type, 0.7)\n+            \n+            segment = self._create_enhanced_segment(\n+                section['content'], \n+                section['title'], \n+                section['start_pos'], \n+                section['end_pos'],\n+                importance_score,\n+                content_type\n+            )\n+            segments.append(segment)\n+        \n+        return segments\n+    \n+    def _detect_academic_sections(self, content: str) -> List[Dict]:\n+        \"\"\"Detect academic paper sections even without clear headers\"\"\"\n+        sections = []\n+        \n+        # Common academic section patterns\n+        section_patterns = [\n+            (r'(?i)(abstract|\u6458\u8981)', 'introduction'),\n+            (r'(?i)(introduction|\u5f15\u8a00|\u7b80\u4ecb)', 'introduction'),\n+            (r'(?i)(related work|\u76f8\u5173\u5de5\u4f5c|\u80cc\u666f)', 'background'),\n+            (r'(?i)(method|methodology|approach|\u65b9\u6cd5)', 'methodology'),\n+            (r'(?i)(algorithm|\u7b97\u6cd5)', 'algorithm'),\n+            (r'(?i)(experiment|\u5b9e\u9a8c|evaluation|\u8bc4\u4f30)', 'experiment'),\n+            (r'(?i)(result|\u7ed3\u679c|finding)', 'results'),\n+            (r'(?i)(conclusion|\u7ed3\u8bba|\u603b\u7ed3)', 'conclusion'),\n+            (r'(?i)(reference|\u53c2\u8003\u6587\u732e|bibliography)', 'references')\n+        ]\n+        \n+        current_pos = 0\n+        for i, (pattern, section_type) in enumerate(section_patterns):\n+            match = re.search(pattern, content[current_pos:], re.IGNORECASE)\n+            if match:\n+                start_pos = current_pos + match.start()\n+                \n+                # Find end position (next section or end of document)\n+                next_pos = len(content)\n+                for next_pattern, _ in section_patterns[i+1:]:\n+                    next_match = re.search(next_pattern, content[start_pos+100:], re.IGNORECASE)\n+                    if next_match:\n+                        next_pos = start_pos + 100 + next_match.start()\n+                        break\n+                \n+                section_content = content[start_pos:next_pos].strip()\n+                if len(section_content) > 50:  # Minimum content length\n+                    # Calculate importance score and content type\n+                    importance_score = self._calculate_paragraph_importance(section_content, section_type)\n+                    content_type = self._classify_content_type(match.group(1), section_content)\n+                    \n+                    sections.append({\n+                        'title': match.group(1),\n+                        'content': section_content,\n+                        'start_pos': start_pos,\n+                        'end_pos': next_pos,\n+                        'type': section_type,\n+                        'importance_score': importance_score,\n+                        'content_type': content_type\n+                    })\n+                \n+                current_pos = next_pos\n+        \n+        return sections\n+    \n+    def _segment_by_semantic_chunks(self, content: str) -> List[DocumentSegment]:\n+        \"\"\"Segment long documents into semantic chunks\"\"\"\n+        # Split into paragraphs first\n+        paragraphs = [p.strip() for p in content.split('\\n\\n') if p.strip()]\n+        \n+        segments = []\n+        current_chunk = []\n+        current_chunk_size = 0\n+        chunk_size_limit = 3000  # characters\n+        overlap_size = 200\n+        \n+        char_pos = 0\n+        \n+        for para in paragraphs:\n+            para_size = len(para)\n+            \n+            # If adding this paragraph exceeds limit, create a segment\n+            if current_chunk_size + para_size > chunk_size_limit and current_chunk:\n+                chunk_content = '\\n\\n'.join(current_chunk)\n+                # Analyze semantic chunk content type\n+                content_type = self._classify_paragraph_type(chunk_content)\n+                importance_score = self._calculate_paragraph_importance(chunk_content, content_type)\n+                \n+                segment = self._create_enhanced_segment(\n+                    chunk_content, \n+                    f\"Section {len(segments) + 1}\",\n+                    char_pos - len(chunk_content.encode('utf-8')),\n+                    char_pos,\n+                    importance_score,\n+                    content_type\n+                )\n+                segments.append(segment)\n+                \n+                # Keep last part for overlap\n+                overlap_content = chunk_content[-overlap_size:] if len(chunk_content) > overlap_size else \"\"\n+                current_chunk = [overlap_content, para] if overlap_content else [para]\n+                current_chunk_size = len(overlap_content) + para_size\n+            else:\n+                current_chunk.append(para)\n+                current_chunk_size += para_size\n+            \n+            char_pos += para_size + 2  # +2 for \\n\\n\n+        \n+        # Add final chunk\n+        if current_chunk:\n+            chunk_content = '\\n\\n'.join(current_chunk)\n+            # Analyze final chunk content type\n+            content_type = self._classify_paragraph_type(chunk_content)\n+            importance_score = self._calculate_paragraph_importance(chunk_content, content_type)\n+            \n+            segment = self._create_enhanced_segment(\n+                chunk_content,\n+                f\"Section {len(segments) + 1}\",\n+                char_pos - len(chunk_content.encode('utf-8')),\n+                char_pos,\n+                importance_score,\n+                content_type\n+            )\n+            segments.append(segment)\n+        \n+        return segments\n+    \n+    def _segment_by_paragraphs(self, content: str) -> List[DocumentSegment]:\n+        \"\"\"Simple paragraph-based segmentation for short documents\"\"\"\n+        paragraphs = [p.strip() for p in content.split('\\n\\n') if p.strip()]\n+        segments = []\n+        char_pos = 0\n+        \n+        for i, para in enumerate(paragraphs):\n+            if len(para) > 100:  # Only include substantial paragraphs\n+                # Analyze paragraph type and importance\n+                content_type = self._classify_paragraph_type(para)\n+                importance_score = self._calculate_paragraph_importance(para, content_type)\n+                \n+                segment = self._create_enhanced_segment(\n+                    para,\n+                    f\"Paragraph {i + 1}\",\n+                    char_pos,\n+                    char_pos + len(para.encode('utf-8')),\n+                    importance_score,\n+                    content_type\n+                )\n+                segments.append(segment)\n+            char_pos += len(para.encode('utf-8')) + 2\n+        \n+        return segments\n+    \n+    # =============== Enhanced intelligent segmentation helper methods ===============\n+    \n+    def _identify_algorithm_blocks(self, content: str) -> List[Dict]:\n+        \"\"\"Identify algorithm blocks and related descriptions\"\"\"\n+        algorithm_blocks = []\n+        \n+        # Algorithm block identification patterns\n+        algorithm_patterns = [\n+            r'(?i)(algorithm\\s+\\d+|procedure\\s+\\d+|method\\s+\\d+).*?(?=algorithm\\s+\\d+|procedure\\s+\\d+|method\\s+\\d+|$)',\n+            r'(?i)(input:|output:|returns?:|require:|ensure:).*?(?=\\n\\s*\\n|\\n\\s*(?:input:|output:|returns?:|require:|ensure:)|$)',\n+            r'(?i)(for\\s+each|while|if.*then|repeat.*until).*?(?=\\n\\s*\\n|$)',\n+            r'(?i)(step\\s+\\d+|phase\\s+\\d+).*?(?=step\\s+\\d+|phase\\s+\\d+|\\n\\s*\\n|$)'\n+        ]\n+        \n+        for pattern in algorithm_patterns:\n+            matches = re.finditer(pattern, content, re.DOTALL)\n+            for match in matches:\n+                # Expand context to include complete descriptions\n+                start = max(0, match.start() - 300)\n+                end = min(len(content), match.end() + 500)\n+                \n+                # Find natural boundaries\n+                while start > 0 and content[start] not in '\\n.!?':\n+                    start -= 1\n+                while end < len(content) and content[end] not in '\\n.!?':\n+                    end += 1\n+                \n+                algorithm_blocks.append({\n+                    'start_pos': start,\n+                    'end_pos': end,\n+                    'content': content[start:end].strip(),\n+                    'title': self._extract_algorithm_title(content[match.start():match.end()]),\n+                    'importance_score': 0.95,  # High importance for algorithm blocks\n+                    'content_type': 'algorithm'\n+                })\n+        \n+        return algorithm_blocks\n+    \n+    def _identify_concept_groups(self, content: str) -> List[Dict]:\n+        \"\"\"Identify concept definition groups\"\"\"\n+        concept_groups = []\n+        \n+        # Concept definition patterns\n+        concept_patterns = [\n+            r'(?i)(definition|define|let|denote|given).*?(?=\\n\\s*\\n|definition|define|let|denote|$)',\n+            r'(?i)(theorem|lemma|proposition|corollary).*?(?=\\n\\s*\\n|theorem|lemma|proposition|corollary|$)',\n+            r'(?i)(notation|symbol|parameter).*?(?=\\n\\s*\\n|notation|symbol|parameter|$)'\n+        ]\n+        \n+        for pattern in concept_patterns:\n+            matches = re.finditer(pattern, content, re.DOTALL)\n+            for match in matches:\n+                # Expand context\n+                start = max(0, match.start() - 200)\n+                end = min(len(content), match.end() + 300)\n+                \n+                concept_groups.append({\n+                    'start_pos': start,\n+                    'end_pos': end,\n+                    'content': content[start:end].strip(),\n+                    'title': self._extract_concept_title(content[match.start():match.end()]),\n+                    'importance_score': 0.85,\n+                    'content_type': 'concept'\n+                })\n+        \n+        return concept_groups\n+    \n+    def _identify_formula_chains(self, content: str) -> List[Dict]:\n+        \"\"\"Identify formula derivation chains\"\"\"\n+        formula_chains = []\n+        \n+        # Formula patterns\n+        formula_patterns = [\n+            r'\\$\\$.*?\\$\\$',  # Block-level mathematical formulas\n+            r'\\$[^$]+\\$',    # Inline mathematical formulas\n+            r'(?i)(equation|formula).*?(?=\\n\\s*\\n|equation|formula|$)',\n+            r'(?i)(where|such that|given that).*?(?=\\n\\s*\\n|where|such that|given that|$)'\n+        ]\n+        \n+        # Find dense formula regions\n+        formula_positions = []\n+        for pattern in formula_patterns:\n+            matches = re.finditer(pattern, content, re.DOTALL)\n+            for match in matches:\n+                formula_positions.append((match.start(), match.end()))\n+        \n+        # Merge nearby formulas into formula chains\n+        formula_positions.sort()\n+        if formula_positions:\n+            current_chain_start = formula_positions[0][0]\n+            current_chain_end = formula_positions[0][1]\n+            \n+            for start, end in formula_positions[1:]:\n+                if start - current_chain_end < 500:  # Merge formulas within 500 characters\n+                    current_chain_end = end\n+                else:\n+                    # Save current chain\n+                    formula_chains.append({\n+                        'start_pos': max(0, current_chain_start - 200),\n+                        'end_pos': min(len(content), current_chain_end + 200),\n+                        'content': content[max(0, current_chain_start - 200):min(len(content), current_chain_end + 200)].strip(),\n+                        'title': 'Mathematical Formulation',\n+                        'importance_score': 0.9,\n+                        'content_type': 'formula'\n+                    })\n+                    current_chain_start = start\n+                    current_chain_end = end\n+            \n+            # Add the last chain\n+            formula_chains.append({\n+                'start_pos': max(0, current_chain_start - 200),\n+                'end_pos': min(len(content), current_chain_end + 200),\n+                'content': content[max(0, current_chain_start - 200):min(len(content), current_chain_end + 200)].strip(),\n+                'title': 'Mathematical Formulation',\n+                'importance_score': 0.9,\n+                'content_type': 'formula'\n+            })\n+        \n+        return formula_chains\n+    \n+    def _merge_related_content_blocks(self, algorithm_blocks: List[Dict], concept_groups: List[Dict], \n+                                    formula_chains: List[Dict], content: str) -> List[Dict]:\n+        \"\"\"Merge related content blocks to ensure integrity\"\"\"\n+        all_blocks = algorithm_blocks + concept_groups + formula_chains\n+        all_blocks.sort(key=lambda x: x['start_pos'])\n+        \n+        merged_blocks = []\n+        i = 0\n+        \n+        while i < len(all_blocks):\n+            current_block = all_blocks[i]\n+            \n+            # Check if can merge with the next block\n+            while i + 1 < len(all_blocks):\n+                next_block = all_blocks[i + 1]\n+                \n+                # If blocks are close or content related, merge them\n+                if (next_block['start_pos'] - current_block['end_pos'] < 300 or\n+                    self._are_blocks_related(current_block, next_block)):\n+                    \n+                    # Merge blocks\n+                    merged_content = content[current_block['start_pos']:next_block['end_pos']]\n+                    current_block = {\n+                        'start_pos': current_block['start_pos'],\n+                        'end_pos': next_block['end_pos'],\n+                        'content': merged_content.strip(),\n+                        'title': f\"{current_block['title']} & {next_block['title']}\",\n+                        'importance_score': max(current_block['importance_score'], next_block['importance_score']),\n+                        'content_type': 'merged'\n+                    }\n+                    i += 1\n+                else:\n+                    break\n+            \n+            merged_blocks.append(current_block)\n+            i += 1\n+        \n+        return merged_blocks\n+    \n+    def _are_blocks_related(self, block1: Dict, block2: Dict) -> bool:\n+        \"\"\"Determine if two content blocks are related\"\"\"\n+        # Check content type associations\n+        related_types = [\n+            ('algorithm', 'formula'),\n+            ('concept', 'algorithm'),\n+            ('formula', 'concept')\n+        ]\n+        \n+        for type1, type2 in related_types:\n+            if ((block1['content_type'] == type1 and block2['content_type'] == type2) or\n+                (block1['content_type'] == type2 and block2['content_type'] == type1)):\n+                return True\n+        \n+        return False\n+    \n+    def _extract_algorithm_title(self, text: str) -> str:\n+        \"\"\"Extract title from algorithm text\"\"\"\n+        lines = text.split('\\n')[:3]  # First 3 lines\n+        for line in lines:\n+            line = line.strip()\n+            if line and len(line) < 100:  # Reasonable title length\n+                # Clean title\n+                title = re.sub(r'[^\\w\\s-]', '', line)\n+                if title:\n+                    return title[:50]  # Limit title length\n+        return \"Algorithm Block\"\n+    \n+    def _extract_concept_title(self, text: str) -> str:\n+        \"\"\"Extract title from concept text\"\"\"\n+        lines = text.split('\\n')[:2]\n+        for line in lines:\n+            line = line.strip()\n+            if line and len(line) < 80:\n+                title = re.sub(r'[^\\w\\s-]', '', line)\n+                if title:\n+                    return title[:50]\n+        return \"Concept Definition\"\n+    \n+    def _create_enhanced_segment(self, content: str, title: str, start_pos: int, end_pos: int,\n+                               importance_score: float, content_type: str) -> DocumentSegment:\n+        \"\"\"Create enhanced document segment\"\"\"\n+        # Generate unique ID\n+        segment_id = hashlib.md5(f\"{title}_{start_pos}_{end_pos}_{importance_score}\".encode()).hexdigest()[:8]\n+        \n+        # Extract keywords\n+        keywords = self._extract_enhanced_keywords(content, content_type)\n+        \n+        # Calculate enhanced relevance scores\n+        relevance_scores = self._calculate_enhanced_relevance_scores(content, content_type, importance_score)\n+        \n+        return DocumentSegment(\n+            id=segment_id,\n+            title=title,\n+            content=content,\n+            content_type=content_type,\n+            keywords=keywords,\n+            char_start=start_pos,\n+            char_end=end_pos,\n+            char_count=len(content),\n+            relevance_scores=relevance_scores,\n+            section_path=title\n+        )\n+    \n+    def _extract_enhanced_keywords(self, content: str, content_type: str) -> List[str]:\n+        \"\"\"Extract enhanced keywords based on content type\"\"\"\n+        words = re.findall(r'\\b[a-zA-Z]{3,}\\b', content.lower())\n+        \n+        # Adjust stopwords based on content type\n+        if content_type == 'algorithm':\n+            algorithm_stopwords = {'step', 'then', 'else', 'end', 'begin', 'start', 'stop'}\n+            words = [w for w in words if w not in algorithm_stopwords]\n+        elif content_type == 'formula':\n+            formula_keywords = ['equation', 'formula', 'where', 'given', 'such', 'that']\n+            words.extend(formula_keywords)\n+        \n+        # General stopwords\n+        general_stopwords = {'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'her', 'was', 'one', 'our', 'had', 'but', 'have', 'this', 'that', 'with', 'from', 'they', 'she', 'been', 'were', 'said', 'each', 'which', 'their'}\n+        \n+        keywords = [w for w in set(words) if w not in general_stopwords and len(w) > 3]\n+        return keywords[:25]  # Increase keyword count\n+    \n+    def _calculate_enhanced_relevance_scores(self, content: str, content_type: str, importance_score: float) -> Dict[str, float]:\n+        \"\"\"Calculate enhanced relevance scores\"\"\"\n+        content_lower = content.lower()\n+        \n+        base_scores = {\n+            'concept_analysis': 0.5,\n+            'algorithm_extraction': 0.5,\n+            'code_planning': 0.5\n+        }\n+        \n+        # Adjust base scores based on content type and importance\n+        if content_type == 'algorithm':\n+            base_scores['algorithm_extraction'] = importance_score\n+            base_scores['code_planning'] = importance_score * 0.9\n+            base_scores['concept_analysis'] = importance_score * 0.7\n+        elif content_type == 'concept':\n+            base_scores['concept_analysis'] = importance_score\n+            base_scores['algorithm_extraction'] = importance_score * 0.8\n+            base_scores['code_planning'] = importance_score * 0.6\n+        elif content_type == 'formula':\n+            base_scores['algorithm_extraction'] = importance_score\n+            base_scores['concept_analysis'] = importance_score * 0.8\n+            base_scores['code_planning'] = importance_score * 0.9\n+        elif content_type == 'merged':\n+            # Merged content is usually important\n+            base_scores = {k: importance_score * 0.95 for k in base_scores}\n+        \n+        # Additional bonus based on content density\n+        algorithm_indicators = ['algorithm', 'method', 'procedure', 'step', 'process']\n+        concept_indicators = ['definition', 'concept', 'framework', 'approach']\n+        implementation_indicators = ['implementation', 'code', 'function', 'design']\n+        \n+        for query_type, indicators in [\n+            ('algorithm_extraction', algorithm_indicators),\n+            ('concept_analysis', concept_indicators),\n+            ('code_planning', implementation_indicators)\n+        ]:\n+            density_bonus = sum(1 for indicator in indicators if indicator in content_lower) * 0.1\n+            base_scores[query_type] = min(1.0, base_scores[query_type] + density_bonus)\n+        \n+        return base_scores\n+    \n+    # Placeholder methods - can be further implemented later\n+    def _identify_research_paper_sections(self, content: str) -> List[Dict]:\n+        \"\"\"Identify research paper sections - simplified implementation\"\"\"\n+        # Temporarily use improved semantic detection\n+        return self._detect_academic_sections(content)\n+    \n+    def _enhance_section_with_context(self, section: Dict, content: str) -> Dict:\n+        \"\"\"Add context to sections - simplified implementation\"\"\"\n+        return section\n+    \n+    def _identify_concept_implementation_pairs(self, content: str) -> List[Dict]:\n+        \"\"\"Identify concept-implementation pairs - simplified implementation\"\"\"\n+        return []\n+    \n+    def _merge_concept_with_implementation(self, pair: Dict, content: str) -> Dict:\n+        \"\"\"Merge concepts with implementation - simplified implementation\"\"\"\n+        return pair\n+    \n+    def _detect_semantic_boundaries(self, content: str) -> List[Dict]:\n+        \"\"\"Detect semantic boundaries - based on paragraphs and logical separators\"\"\"\n+        boundaries = []\n+        \n+        # Split paragraphs by double line breaks\n+        paragraphs = content.split('\\n\\n')\n+        current_pos = 0\n+        \n+        for i, para in enumerate(paragraphs):\n+            if len(para.strip()) > 100:  # Valid paragraph\n+                # Analyze paragraph type\n+                content_type = self._classify_paragraph_type(para)\n+                importance_score = self._calculate_paragraph_importance(para, content_type)\n+                \n+                boundaries.append({\n+                    'position': current_pos + len(para),\n+                    'suggested_title': self._extract_paragraph_title(para, i+1),\n+                    'importance_score': importance_score,\n+                    'content_type': content_type\n+                })\n+            \n+            current_pos += len(para) + 2  # +2 for \\n\\n\n+        \n+        return boundaries\n+    \n+    def _classify_paragraph_type(self, paragraph: str) -> str:\n+        \"\"\"Classify paragraph type\"\"\"\n+        para_lower = paragraph.lower()\n+        \n+        if 'algorithm' in para_lower or 'procedure' in para_lower:\n+            return 'algorithm'\n+        elif 'formula' in para_lower or '$$' in paragraph:\n+            return 'formula'\n+        elif any(word in para_lower for word in ['introduction', 'overview', 'abstract']):\n+            return 'introduction'\n+        elif any(word in para_lower for word in ['conclusion', 'summary', 'result']):\n+            return 'conclusion'\n+        else:\n+            return 'general'\n+    \n+    def _calculate_paragraph_importance(self, paragraph: str, content_type: str) -> float:\n+        \"\"\"Calculate paragraph importance\"\"\"\n+        if content_type == 'algorithm':\n+            return 0.95\n+        elif content_type == 'formula':\n+            return 0.9\n+        elif content_type == 'introduction':\n+            return 0.85\n+        elif content_type == 'conclusion':\n+            return 0.8\n+        else:\n+            return 0.7\n+    \n+    def _extract_paragraph_title(self, paragraph: str, index: int) -> str:\n+        \"\"\"Extract paragraph title\"\"\"\n+        lines = paragraph.split('\\n')\n+        for line in lines[:2]:\n+            if line.startswith('#'):\n+                return line.strip('# ')\n+            elif len(line) < 80 and line.strip():\n+                return line.strip()\n+        return f\"Section {index}\"\n+    \n+    def _calculate_optimal_chunk_size(self, content: str) -> int:\n+        \"\"\"Calculate optimal chunk size\"\"\"\n+        # Dynamically adjust based on content complexity\n+        complexity = self.analyzer._calculate_concept_complexity(content)\n+        if complexity > 0.7:\n+            return 4000  # Complex content needs larger chunks\n+        elif complexity > 0.4:\n+            return 3000\n+        else:\n+            return 2000\n+    \n+    def _create_content_aware_chunks(self, content: str, chunk_size: int) -> List[Dict]:\n+        \"\"\"Create content-aware chunks - simplified implementation\"\"\"\n+        chunks = []\n+        paragraphs = [p.strip() for p in content.split('\\n\\n') if p.strip()]\n+        \n+        current_chunk = []\n+        current_size = 0\n+        start_pos = 0\n+        \n+        for para in paragraphs:\n+            para_size = len(para)\n+            \n+            if current_size + para_size > chunk_size and current_chunk:\n+                chunk_content = '\\n\\n'.join(current_chunk)\n+                chunks.append({\n+                    'content': chunk_content,\n+                    'title': f\"Section {len(chunks) + 1}\",\n+                    'start_pos': start_pos,\n+                    'end_pos': start_pos + len(chunk_content),\n+                    'importance_score': 0.7,\n+                    'content_type': 'general'\n+                })\n+                \n+                current_chunk = [para]\n+                current_size = para_size\n+                start_pos += len(chunk_content) + 2\n+            else:\n+                current_chunk.append(para)\n+                current_size += para_size\n+        \n+        # Add the last chunk\n+        if current_chunk:\n+            chunk_content = '\\n\\n'.join(current_chunk)\n+            chunks.append({\n+                'content': chunk_content,\n+                'title': f\"Section {len(chunks) + 1}\",\n+                'start_pos': start_pos,\n+                'end_pos': start_pos + len(chunk_content),\n+                'importance_score': 0.7,\n+                'content_type': 'general'\n+            })\n+        \n+        return chunks\n+    \n+    def _create_segment(self, content: str, title: str, start_pos: int, end_pos: int) -> DocumentSegment:\n+        \"\"\"Create a DocumentSegment with metadata\"\"\"\n+        # Generate unique ID\n+        segment_id = hashlib.md5(f\"{title}_{start_pos}_{end_pos}\".encode()).hexdigest()[:8]\n+        \n+        # Extract keywords from content\n+        keywords = self._extract_keywords(content)\n+        \n+        # Determine content type\n+        content_type = self._classify_content_type(title, content)\n+        \n+        # Calculate relevance scores for different query types\n+        relevance_scores = self._calculate_relevance_scores(content, content_type)\n+        \n+        return DocumentSegment(\n+            id=segment_id,\n+            title=title,\n+            content=content,\n+            content_type=content_type,\n+            keywords=keywords,\n+            char_start=start_pos,\n+            char_end=end_pos,\n+            char_count=len(content),\n+            relevance_scores=relevance_scores,\n+            section_path=title  # Simplified for now\n+        )\n+    \n+    def _extract_keywords(self, content: str) -> List[str]:\n+        \"\"\"Extract relevant keywords from content\"\"\"\n+        # Simple keyword extraction - could be enhanced with NLP\n+        words = re.findall(r'\\b[a-zA-Z]{3,}\\b', content.lower())\n+        \n+        # Remove common words\n+        stopwords = {'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'her', 'was', 'one', 'our', 'had', 'but', 'have', 'this', 'that', 'with', 'from', 'they', 'she', 'been', 'were', 'said', 'each', 'which', 'their'}\n+        \n+        keywords = [w for w in set(words) if w not in stopwords and len(w) > 3]\n+        return keywords[:20]  # Top 20 keywords\n+    \n+    def _classify_content_type(self, title: str, content: str) -> str:\n+        \"\"\"Classify the type of content based on title and content\"\"\"\n+        title_lower = title.lower()\n+        content_lower = content.lower()\n+        \n+        if any(word in title_lower for word in ['introduction', 'abstract', 'overview']):\n+            return 'introduction'\n+        elif any(word in title_lower for word in ['method', 'approach', 'algorithm']):\n+            return 'methodology'\n+        elif any(word in title_lower for word in ['experiment', 'evaluation', 'result']):\n+            return 'experiment'\n+        elif any(word in title_lower for word in ['conclusion', 'discussion', 'summary']):\n+            return 'conclusion'\n+        elif any(word in title_lower for word in ['reference', 'bibliography']):\n+            return 'references'\n+        elif 'algorithm' in content_lower or 'procedure' in content_lower:\n+            return 'algorithm'\n+        else:\n+            return 'general'\n+    \n+    def _calculate_relevance_scores(self, content: str, content_type: str) -> Dict[str, float]:\n+        \"\"\"Calculate relevance scores for different query types\"\"\"\n+        content_lower = content.lower()\n+        \n+        scores = {\n+            'concept_analysis': 0.5,\n+            'algorithm_extraction': 0.5,\n+            'code_planning': 0.5\n+        }\n+        \n+        # Concept analysis relevance\n+        concept_indicators = ['introduction', 'overview', 'architecture', 'system', 'framework', 'concept', 'approach']\n+        concept_score = sum(1 for indicator in concept_indicators if indicator in content_lower) / len(concept_indicators)\n+        scores['concept_analysis'] = min(1.0, concept_score + (0.8 if content_type == 'introduction' else 0))\n+        \n+        # Algorithm extraction relevance\n+        algorithm_indicators = ['algorithm', 'method', 'procedure', 'formula', 'equation', 'step', 'process']\n+        algorithm_score = sum(1 for indicator in algorithm_indicators if indicator in content_lower) / len(algorithm_indicators)\n+        scores['algorithm_extraction'] = min(1.0, algorithm_score + (0.9 if content_type == 'methodology' else 0))\n+        \n+        # Code planning relevance\n+        code_indicators = ['implementation', 'code', 'function', 'class', 'module', 'structure', 'design']\n+        code_score = sum(1 for indicator in code_indicators if indicator in content_lower) / len(code_indicators)\n+        scores['code_planning'] = min(1.0, code_score + (0.7 if content_type in ['methodology', 'algorithm'] else 0))\n+        \n+        return scores\n+\n+# Global variables\n+DOCUMENT_INDEXES: Dict[str, DocumentIndex] = {}\n+segmenter = DocumentSegmenter()\n+\n+def get_segments_dir(paper_dir: str) -> str:\n+    \"\"\"Get the segments directory path\"\"\"\n+    return os.path.join(paper_dir, \"document_segments\")\n+\n+def ensure_segments_dir_exists(segments_dir: str):\n+    \"\"\"Ensure segments directory exists\"\"\"\n+    os.makedirs(segments_dir, exist_ok=True)\n+\n+@mcp.tool()\n+async def analyze_and_segment_document(paper_dir: str, force_refresh: bool = False) -> str:\n+    \"\"\"\n+    Analyze document structure and create intelligent segments\n+    \n+    Args:\n+        paper_dir: Path to the paper directory\n+        force_refresh: Whether to force re-analysis even if segments exist\n+        \n+    Returns:\n+        JSON string with segmentation results\n+    \"\"\"\n+    try:\n+        # Find markdown file in paper directory\n+        md_files = [f for f in os.listdir(paper_dir) if f.endswith('.md')]\n+        if not md_files:\n+            return json.dumps({\n+                \"status\": \"error\",\n+                \"message\": f\"No markdown file found in {paper_dir}\"\n+            }, ensure_ascii=False, indent=2)\n+        \n+        md_file_path = os.path.join(paper_dir, md_files[0])\n+        segments_dir = get_segments_dir(paper_dir)\n+        index_file_path = os.path.join(segments_dir, \"document_index.json\")\n+        \n+        # Check if analysis already exists and is recent\n+        if not force_refresh and os.path.exists(index_file_path):\n+            try:\n+                with open(index_file_path, 'r', encoding='utf-8') as f:\n+                    existing_index = json.load(f)\n+                    \n+                    # Compatibility handling: ensure segments data structure is correct\n+                    if 'segments' in existing_index:\n+                        segments_data = []\n+                        for seg_data in existing_index['segments']:\n+                            # Ensure all required fields exist\n+                            segment_dict = dict(seg_data)\n+                            \n+                            if 'content_type' not in segment_dict:\n+                                segment_dict['content_type'] = 'general'\n+                            if 'keywords' not in segment_dict:\n+                                segment_dict['keywords'] = []\n+                            if 'relevance_scores' not in segment_dict:\n+                                segment_dict['relevance_scores'] = {\n+                                    'concept_analysis': 0.5,\n+                                    'algorithm_extraction': 0.5,\n+                                    'code_planning': 0.5\n+                                }\n+                            if 'section_path' not in segment_dict:\n+                                segment_dict['section_path'] = segment_dict.get('title', 'Unknown')\n+                            \n+                            segments_data.append(DocumentSegment(**segment_dict))\n+                        \n+                        existing_index['segments'] = segments_data\n+                    \n+                    DOCUMENT_INDEXES[paper_dir] = DocumentIndex(**existing_index)\n+                return json.dumps({\n+                    \"status\": \"success\",\n+                    \"message\": \"Using existing document analysis\",\n+                    \"segments_dir\": segments_dir,\n+                    \"total_segments\": existing_index[\"total_segments\"]\n+                }, ensure_ascii=False, indent=2)\n+            \n+            except Exception as e:\n+                logger.error(f\"Failed to load existing index: {e}\")\n+                logger.info(\"Will perform fresh analysis instead\")\n+                # Remove corrupted index file and continue with new analysis\n+                try:\n+                    os.remove(index_file_path)\n+                except:\n+                    pass\n+        \n+        # Read document content\n+        with open(md_file_path, 'r', encoding='utf-8') as f:\n+            content = f.read()\n+        \n+        # Analyze document\n+        analyzer = DocumentAnalyzer()\n+        doc_type, confidence = analyzer.analyze_document_type(content)\n+        strategy = analyzer.detect_segmentation_strategy(content, doc_type)\n+        \n+        # Create segments\n+        segments = segmenter.segment_document(content, strategy)\n+        \n+        # Create document index\n+        document_index = DocumentIndex(\n+            document_path=md_file_path,\n+            document_type=doc_type,\n+            segmentation_strategy=strategy,\n+            total_segments=len(segments),\n+            total_chars=len(content),\n+            segments=segments,\n+            created_at=datetime.now().isoformat()\n+        )\n+        \n+        # Save segments\n+        ensure_segments_dir_exists(segments_dir)\n+        \n+        # Save document index\n+        with open(index_file_path, 'w', encoding='utf-8') as f:\n+            json.dump(asdict(document_index), f, ensure_ascii=False, indent=2, default=str)\n+        \n+        # Save individual segment files for fallback\n+        for segment in segments:\n+            segment_file_path = os.path.join(segments_dir, f\"segment_{segment.id}.md\")\n+            with open(segment_file_path, 'w', encoding='utf-8') as f:\n+                f.write(f\"# {segment.title}\\n\\n\")\n+                f.write(f\"**Content Type:** {segment.content_type}\\n\")\n+                f.write(f\"**Keywords:** {', '.join(segment.keywords[:10])}\\n\\n\")\n+                f.write(segment.content)\n+        \n+        # Store in memory\n+        DOCUMENT_INDEXES[paper_dir] = document_index\n+        \n+        logger.info(f\"Document segmentation completed: {len(segments)} segments created\")\n+        \n+        return json.dumps({\n+            \"status\": \"success\",\n+            \"message\": f\"Document analysis completed with {strategy} strategy\",\n+            \"document_type\": doc_type,\n+            \"segmentation_strategy\": strategy,\n+            \"segments_dir\": segments_dir,\n+            \"total_segments\": len(segments),\n+            \"total_chars\": len(content)\n+        }, ensure_ascii=False, indent=2)\n+        \n+    except Exception as e:\n+        logger.error(f\"Error in analyze_and_segment_document: {e}\")\n+        return json.dumps({\n+            \"status\": \"error\",\n+            \"message\": f\"Failed to analyze document: {str(e)}\"\n+        }, ensure_ascii=False, indent=2)\n+\n+@mcp.tool()\n+async def read_document_segments(\n+    paper_dir: str,\n+    query_type: str,\n+    keywords: List[str] = None,\n+    max_segments: int = 3,\n+    max_total_chars: int = None\n+) -> str:\n+    \"\"\"\n+    Intelligently retrieve relevant document segments based on query type\n+    \n+    Args:\n+        paper_dir: Path to the paper directory\n+        query_type: Type of query - \"concept_analysis\", \"algorithm_extraction\", or \"code_planning\"\n+        keywords: Optional list of keywords to search for\n+        max_segments: Maximum number of segments to return\n+        max_total_chars: Maximum total characters to return\n+        \n+    Returns:\n+        JSON string with selected segments\n+    \"\"\"\n+    try:\n+        # Ensure document is analyzed\n+        if paper_dir not in DOCUMENT_INDEXES:\n+            segments_dir = get_segments_dir(paper_dir)\n+            index_file_path = os.path.join(segments_dir, \"document_index.json\")\n+            \n+            if os.path.exists(index_file_path):\n+                with open(index_file_path, 'r', encoding='utf-8') as f:\n+                    index_data = json.load(f)\n+                    # Convert dict back to DocumentIndex with backward compatibility\n+                    segments_data = []\n+                    for seg_data in index_data.get('segments', []):\n+                        # Ensure all required fields exist, provide default values\n+                        segment_dict = dict(seg_data)\n+                        \n+                        # Compatibility handling: add missing fields\n+                        if 'content_type' not in segment_dict:\n+                            segment_dict['content_type'] = 'general'\n+                        if 'keywords' not in segment_dict:\n+                            segment_dict['keywords'] = []\n+                        if 'relevance_scores' not in segment_dict:\n+                            segment_dict['relevance_scores'] = {\n+                                'concept_analysis': 0.5,\n+                                'algorithm_extraction': 0.5,\n+                                'code_planning': 0.5\n+                            }\n+                        if 'section_path' not in segment_dict:\n+                            segment_dict['section_path'] = segment_dict.get('title', 'Unknown')\n+                        \n+                        segment = DocumentSegment(**segment_dict)\n+                        segments_data.append(segment)\n+                    \n+                    index_data['segments'] = segments_data\n+                    DOCUMENT_INDEXES[paper_dir] = DocumentIndex(**index_data)\n+            else:\n+                # Auto-analyze if not found\n+                await analyze_and_segment_document(paper_dir)\n+        \n+        document_index = DOCUMENT_INDEXES[paper_dir]\n+        \n+        # Dynamically calculate character limit\n+        if max_total_chars is None:\n+            max_total_chars = _calculate_adaptive_char_limit(document_index, query_type)\n+        \n+        # Score and rank segments with enhanced algorithm\n+        scored_segments = []\n+        for segment in document_index.segments:\n+            # Base relevance score (already enhanced in new system)\n+            relevance_score = segment.relevance_scores.get(query_type, 0.5)\n+            \n+            # Enhanced keyword matching with position weighting\n+            if keywords:\n+                keyword_score = _calculate_enhanced_keyword_score(segment, keywords)\n+                relevance_score += keyword_score\n+            \n+            # Content completeness bonus\n+            completeness_bonus = _calculate_completeness_bonus(segment, document_index)\n+            relevance_score += completeness_bonus\n+            \n+            scored_segments.append((segment, relevance_score))\n+        \n+        # Sort by enhanced relevance score\n+        scored_segments.sort(key=lambda x: x[1], reverse=True)\n+        \n+        # Intelligent segment selection with integrity preservation\n+        selected_segments = _select_segments_with_integrity(\n+            scored_segments, max_segments, max_total_chars, query_type\n+        )\n+        \n+        total_chars = sum(seg[\"char_count\"] for seg in selected_segments)\n+        \n+        logger.info(f\"Selected {len(selected_segments)} segments for {query_type} query\")\n+        \n+        return json.dumps({\n+            \"status\": \"success\",\n+            \"query_type\": query_type,\n+            \"keywords\": keywords or [],\n+            \"total_segments_available\": len(document_index.segments),\n+            \"segments_selected\": len(selected_segments),\n+            \"total_chars\": total_chars,\n+            \"max_chars_used\": max_total_chars,\n+            \"segments\": selected_segments\n+        }, ensure_ascii=False, indent=2)\n+        \n+    except Exception as e:\n+        logger.error(f\"Error in read_document_segments: {e}\")\n+        return json.dumps({\n+            \"status\": \"error\",\n+            \"message\": f\"Failed to read document segments: {str(e)}\"\n+        }, ensure_ascii=False, indent=2)\n+\n+@mcp.tool()\n+async def get_document_overview(paper_dir: str) -> str:\n+    \"\"\"\n+    Get overview of document structure and available segments\n+    \n+    Args:\n+        paper_dir: Path to the paper directory\n+        \n+    Returns:\n+        JSON string with document overview\n+    \"\"\"\n+    try:\n+        # Ensure document is analyzed\n+        if paper_dir not in DOCUMENT_INDEXES:\n+            await analyze_and_segment_document(paper_dir)\n+        \n+        document_index = DOCUMENT_INDEXES[paper_dir]\n+        \n+        # Create overview\n+        segment_summaries = []\n+        for segment in document_index.segments:\n+            segment_summaries.append({\n+                \"id\": segment.id,\n+                \"title\": segment.title,\n+                \"content_type\": segment.content_type,\n+                \"char_count\": segment.char_count,\n+                \"keywords\": segment.keywords[:5],  # Top 5 keywords\n+                \"relevance_scores\": segment.relevance_scores\n+            })\n+        \n+        return json.dumps({\n+            \"status\": \"success\",\n+            \"document_path\": document_index.document_path,\n+            \"document_type\": document_index.document_type,\n+            \"segmentation_strategy\": document_index.segmentation_strategy,\n+            \"total_segments\": document_index.total_segments,\n+            \"total_chars\": document_index.total_chars,\n+            \"created_at\": document_index.created_at,\n+            \"segments_overview\": segment_summaries\n+        }, ensure_ascii=False, indent=2)\n+        \n+    except Exception as e:\n+        logger.error(f\"Error in get_document_overview: {e}\")\n+        return json.dumps({\n+            \"status\": \"error\",\n+            \"message\": f\"Failed to get document overview: {str(e)}\"\n+        }, ensure_ascii=False, indent=2)\n+\n+# =============== Enhanced retrieval system helper methods ===============\n+\n+def _calculate_adaptive_char_limit(document_index: DocumentIndex, query_type: str) -> int:\n+    \"\"\"Dynamically calculate character limit based on document complexity and query type\"\"\"\n+    base_limit = 6000\n+    \n+    # Adjust based on document type\n+    if document_index.document_type == \"research_paper\":\n+        base_limit = 10000\n+    elif document_index.document_type == \"algorithm_focused\":\n+        base_limit = 12000\n+    elif document_index.segmentation_strategy == \"algorithm_preserve_integrity\":\n+        base_limit = 15000\n+    \n+    # Adjust based on query type\n+    query_multipliers = {\n+        \"algorithm_extraction\": 1.5,  # Algorithms need more context\n+        \"concept_analysis\": 1.2,\n+        \"code_planning\": 1.3\n+    }\n+    \n+    multiplier = query_multipliers.get(query_type, 1.0)\n+    return int(base_limit * multiplier)\n+\n+def _calculate_enhanced_keyword_score(segment: DocumentSegment, keywords: List[str]) -> float:\n+    \"\"\"Calculate enhanced keyword matching score\"\"\"\n+    score = 0.0\n+    content_lower = segment.content.lower()\n+    title_lower = segment.title.lower()\n+    \n+    for keyword in keywords:\n+        keyword_lower = keyword.lower()\n+        \n+        # Title matching has higher weight\n+        if keyword_lower in title_lower:\n+            score += 0.3\n+        \n+        # Content matching\n+        content_matches = content_lower.count(keyword_lower)\n+        if content_matches > 0:\n+            # Consider term frequency and position\n+            frequency_score = min(0.2, content_matches * 0.05)\n+            \n+            # Check if in important position (first 25% of content)\n+            early_content = content_lower[:len(content_lower)//4]\n+            if keyword_lower in early_content:\n+                frequency_score += 0.1\n+            \n+            score += frequency_score\n+    \n+    return min(0.6, score)  # Limit maximum bonus\n+\n+def _calculate_completeness_bonus(segment: DocumentSegment, document_index: DocumentIndex) -> float:\n+    \"\"\"Calculate content completeness bonus\"\"\"\n+    bonus = 0.0\n+    \n+    # Completeness bonus for algorithm and formula content\n+    if segment.content_type in ['algorithm', 'formula', 'merged']:\n+        bonus += 0.2\n+    \n+    # Long paragraphs usually contain more complete information\n+    if segment.char_count > 2000:\n+        bonus += 0.1\n+    elif segment.char_count > 4000:\n+        bonus += 0.15\n+    \n+    # High importance paragraph bonus\n+    if segment.relevance_scores.get('algorithm_extraction', 0) > 0.8:\n+        bonus += 0.1\n+    \n+    return min(0.3, bonus)\n+\n+def _select_segments_with_integrity(scored_segments: List[Tuple], max_segments: int, \n+                                  max_total_chars: int, query_type: str) -> List[Dict]:\n+    \"\"\"Intelligently select segments while maintaining content integrity\"\"\"\n+    selected_segments = []\n+    total_chars = 0\n+    \n+    # First select the highest scoring segments\n+    for segment, score in scored_segments:\n+        if len(selected_segments) >= max_segments:\n+            break\n+            \n+        if total_chars + segment.char_count <= max_total_chars:\n+            selected_segments.append({\n+                \"id\": segment.id,\n+                \"title\": segment.title,\n+                \"content\": segment.content,\n+                \"content_type\": segment.content_type,\n+                \"relevance_score\": score,\n+                \"char_count\": segment.char_count\n+            })\n+            total_chars += segment.char_count\n+        elif len(selected_segments) == 0:\n+            # If the first segment exceeds the limit, truncate but preserve it\n+            truncated_content = segment.content[:max_total_chars - 200] + \"\\n\\n[Content truncated for length...]\"\n+            selected_segments.append({\n+                \"id\": segment.id,\n+                \"title\": segment.title,\n+                \"content\": truncated_content,\n+                \"content_type\": segment.content_type,\n+                \"relevance_score\": score,\n+                \"char_count\": len(truncated_content)\n+            })\n+            break\n+    \n+    # If there's remaining space, try to add relevant small segments\n+    remaining_chars = max_total_chars - total_chars\n+    if remaining_chars > 500 and len(selected_segments) < max_segments:\n+        for segment, score in scored_segments[len(selected_segments):]:\n+            if segment.char_count <= remaining_chars and len(selected_segments) < max_segments:\n+                selected_segments.append({\n+                    \"id\": segment.id,\n+                    \"title\": segment.title,\n+                    \"content\": segment.content,\n+                    \"content_type\": segment.content_type,\n+                    \"relevance_score\": score,\n+                    \"char_count\": segment.char_count\n+                })\n+                remaining_chars -= segment.char_count\n+    \n+    return selected_segments\n+\n+if __name__ == \"__main__\":\n+    # Run the MCP server\n+    mcp.run()\n\\ No newline at end of file\n"
        },
        {
            "commit": "daad35b9c60770a655eaaea4565bf81f63ae1b38",
            "file_path": "utils/llm_utils.py",
            "diff": "diff --git a/utils/llm_utils.py b/utils/llm_utils.py\nnew file mode 100644\nindex 0000000..05d3d28\n--- /dev/null\n+++ b/utils/llm_utils.py\n@@ -0,0 +1,210 @@\n+\"\"\"\n+LLM utility functions for DeepCode project.\n+\n+This module provides common LLM-related utilities to avoid circular imports\n+and reduce code duplication across the project.\n+\"\"\"\n+\n+import os\n+import yaml\n+from typing import Any, Type, Dict, Tuple\n+\n+# Import LLM classes\n+from mcp_agent.workflows.llm.augmented_llm_anthropic import AnthropicAugmentedLLM\n+from mcp_agent.workflows.llm.augmented_llm_openai import OpenAIAugmentedLLM\n+\n+\n+def get_preferred_llm_class(config_path: str = \"mcp_agent.secrets.yaml\") -> Type[Any]:\n+    \"\"\"\n+    Automatically select the LLM class based on API key availability in configuration.\n+\n+    Reads from YAML config file and returns AnthropicAugmentedLLM if anthropic.api_key\n+    is available, otherwise returns OpenAIAugmentedLLM.\n+\n+    Args:\n+        config_path: Path to the YAML configuration file\n+\n+    Returns:\n+        class: The preferred LLM class\n+    \"\"\"\n+    try:\n+        # Try to read the configuration file\n+        if os.path.exists(config_path):\n+            with open(config_path, \"r\", encoding=\"utf-8\") as f:\n+                config = yaml.safe_load(f)\n+\n+            # Check for anthropic API key in config\n+            anthropic_config = config.get(\"anthropic\", {})\n+            anthropic_key = anthropic_config.get(\"api_key\", \"\")\n+\n+            if anthropic_key and anthropic_key.strip() and not anthropic_key == \"\":\n+                # print(\"\ud83e\udd16 Using AnthropicAugmentedLLM (Anthropic API key found in config)\")\n+                return AnthropicAugmentedLLM\n+            else:\n+                # print(\"\ud83e\udd16 Using OpenAIAugmentedLLM (Anthropic API key not configured)\")\n+                return OpenAIAugmentedLLM\n+        else:\n+            print(f\"\ud83e\udd16 Config file {config_path} not found, using OpenAIAugmentedLLM\")\n+            return OpenAIAugmentedLLM\n+\n+    except Exception as e:\n+        print(f\"\ud83e\udd16 Error reading config file {config_path}: {e}\")\n+        print(\"\ud83e\udd16 Falling back to OpenAIAugmentedLLM\")\n+        return OpenAIAugmentedLLM\n+\n+\n+def get_default_models(config_path: str = \"mcp_agent.config.yaml\") -> dict:\n+    \"\"\"\n+    Get default models configuration from config file.\n+    \n+    Args:\n+        config_path: Path to the main configuration file\n+        \n+    Returns:\n+        dict: Default models configuration\n+    \"\"\"\n+    try:\n+        if os.path.exists(config_path):\n+            with open(config_path, \"r\", encoding=\"utf-8\") as f:\n+                config = yaml.safe_load(f)\n+            \n+            # Extract model configurations\n+            openai_config = config.get(\"openai\", {})\n+            anthropic_config = config.get(\"anthropic\", {})\n+            \n+            return {\n+                \"anthropic\": anthropic_config.get(\"default_model\", \"claude-sonnet-4-20250514\"),\n+                \"openai\": openai_config.get(\"default_model\", \"o3-mini\")\n+            }\n+        else:\n+            print(f\"\ud83e\udd16 Config file {config_path} not found, using default models\")\n+            return {\"anthropic\": \"claude-sonnet-4-20250514\", \"openai\": \"o3-mini\"}\n+\n+    except Exception as e:\n+        print(f\"\ud83e\udd16 Error reading config file {config_path}: {e}\")\n+        print(\"\ud83e\udd16 Falling back to default models\")\n+        return {\"anthropic\": \"claude-sonnet-4-20250514\", \"openai\": \"o3-mini\"}\n+\n+\n+def get_document_segmentation_config(config_path: str = \"mcp_agent.config.yaml\") -> Dict[str, Any]:\n+    \"\"\"\n+    Get document segmentation configuration from config file.\n+    \n+    Args:\n+        config_path: Path to the main configuration file\n+        \n+    Returns:\n+        Dict containing segmentation configuration with default values\n+    \"\"\"\n+    try:\n+        if os.path.exists(config_path):\n+            with open(config_path, \"r\", encoding=\"utf-8\") as f:\n+                config = yaml.safe_load(f)\n+            \n+            # Get document segmentation config with defaults\n+            seg_config = config.get(\"document_segmentation\", {})\n+            return {\n+                \"enabled\": seg_config.get(\"enabled\", True),\n+                \"size_threshold_chars\": seg_config.get(\"size_threshold_chars\", 50000)\n+            }\n+        else:\n+            print(f\"\ud83d\udcc4 Config file {config_path} not found, using default segmentation settings\")\n+            return {\"enabled\": True, \"size_threshold_chars\": 50000}\n+            \n+    except Exception as e:\n+        print(f\"\ud83d\udcc4 Error reading segmentation config from {config_path}: {e}\")\n+        print(\"\ud83d\udcc4 Using default segmentation settings\")\n+        return {\"enabled\": True, \"size_threshold_chars\": 50000}\n+\n+\n+def should_use_document_segmentation(document_content: str, config_path: str = \"mcp_agent.config.yaml\") -> Tuple[bool, str]:\n+    \"\"\"\n+    Determine whether to use document segmentation based on configuration and document size.\n+    \n+    Args:\n+        document_content: The content of the document to analyze\n+        config_path: Path to the configuration file\n+        \n+    Returns:\n+        Tuple of (should_segment, reason) where:\n+        - should_segment: Boolean indicating whether to use segmentation\n+        - reason: String explaining the decision\n+    \"\"\"\n+    seg_config = get_document_segmentation_config(config_path)\n+    \n+    if not seg_config[\"enabled\"]:\n+        return False, \"Document segmentation disabled in configuration\"\n+    \n+    doc_size = len(document_content)\n+    threshold = seg_config[\"size_threshold_chars\"]\n+    \n+    if doc_size > threshold:\n+        return True, f\"Document size ({doc_size:,} chars) exceeds threshold ({threshold:,} chars)\"\n+    else:\n+        return False, f\"Document size ({doc_size:,} chars) below threshold ({threshold:,} chars)\"\n+\n+\n+def get_adaptive_agent_config(use_segmentation: bool, search_server_names: list = None) -> Dict[str, list]:\n+    \"\"\"\n+    Get adaptive agent configuration based on whether to use document segmentation.\n+    \n+    Args:\n+        use_segmentation: Whether to include document-segmentation server\n+        search_server_names: Base search server names (from get_search_server_names)\n+        \n+    Returns:\n+        Dict containing server configurations for different agents\n+    \"\"\"\n+    if search_server_names is None:\n+        search_server_names = []\n+    \n+    # Base configuration\n+    config = {\n+        \"concept_analysis\": [],\n+        \"algorithm_analysis\": search_server_names.copy(),\n+        \"code_planner\": search_server_names.copy()\n+    }\n+    \n+    # Add document-segmentation server if needed\n+    if use_segmentation:\n+        config[\"concept_analysis\"] = [\"document-segmentation\"]\n+        if \"document-segmentation\" not in config[\"algorithm_analysis\"]:\n+            config[\"algorithm_analysis\"].append(\"document-segmentation\")\n+        if \"document-segmentation\" not in config[\"code_planner\"]:\n+            config[\"code_planner\"].append(\"document-segmentation\")\n+    \n+    return config\n+\n+\n+def get_adaptive_prompts(use_segmentation: bool) -> Dict[str, str]:\n+    \"\"\"\n+    Get appropriate prompt versions based on segmentation usage.\n+    \n+    Args:\n+        use_segmentation: Whether to use segmented reading prompts\n+        \n+    Returns:\n+        Dict containing prompt configurations\n+    \"\"\"\n+    # Import here to avoid circular imports\n+    from prompts.code_prompts import (\n+        PAPER_CONCEPT_ANALYSIS_PROMPT,\n+        PAPER_ALGORITHM_ANALYSIS_PROMPT, \n+        CODE_PLANNING_PROMPT,\n+        PAPER_CONCEPT_ANALYSIS_PROMPT_TRADITIONAL,\n+        PAPER_ALGORITHM_ANALYSIS_PROMPT_TRADITIONAL,\n+        CODE_PLANNING_PROMPT_TRADITIONAL\n+    )\n+    \n+    if use_segmentation:\n+        return {\n+            \"concept_analysis\": PAPER_CONCEPT_ANALYSIS_PROMPT,\n+            \"algorithm_analysis\": PAPER_ALGORITHM_ANALYSIS_PROMPT,\n+            \"code_planning\": CODE_PLANNING_PROMPT\n+        }\n+    else:\n+        return {\n+            \"concept_analysis\": PAPER_CONCEPT_ANALYSIS_PROMPT_TRADITIONAL,\n+            \"algorithm_analysis\": PAPER_ALGORITHM_ANALYSIS_PROMPT_TRADITIONAL,\n+            \"code_planning\": CODE_PLANNING_PROMPT_TRADITIONAL\n+        }\n\\ No newline at end of file\n"
        },
        {
            "commit": "daad35b9c60770a655eaaea4565bf81f63ae1b38",
            "file_path": "workflows/agent_orchestration_engine.py",
            "diff": "diff --git a/workflows/agent_orchestration_engine.py b/workflows/agent_orchestration_engine.py\nindex db4fc5c..0292611 100644\n--- a/workflows/agent_orchestration_engine.py\n+++ b/workflows/agent_orchestration_engine.py\n@@ -31,12 +31,10 @@ import json\n import os\n import re\n import yaml\n-from typing import Callable, Dict, List, Optional, Tuple\n+from typing import Any, Callable, Dict, List, Optional, Tuple\n \n # MCP Agent imports\n from mcp_agent.agents.agent import Agent\n-from mcp_agent.workflows.llm.augmented_llm_anthropic import AnthropicAugmentedLLM\n-from mcp_agent.workflows.llm.augmented_llm_openai import OpenAIAugmentedLLM\n from mcp_agent.workflows.llm.augmented_llm import RequestParams\n from mcp_agent.workflows.parallel.parallel_llm import ParallelLLM\n \n@@ -45,9 +43,6 @@ from prompts.code_prompts import (\n     PAPER_INPUT_ANALYZER_PROMPT,\n     PAPER_DOWNLOADER_PROMPT,\n     PAPER_REFERENCE_ANALYZER_PROMPT,\n-    PAPER_ALGORITHM_ANALYSIS_PROMPT,\n-    PAPER_CONCEPT_ANALYSIS_PROMPT,\n-    CODE_PLANNING_PROMPT,\n     CHAT_AGENT_PLANNING_PROMPT,\n )\n from utils.file_processor import FileProcessor\n@@ -55,50 +50,18 @@ from workflows.code_implementation_workflow import CodeImplementationWorkflow\n from workflows.code_implementation_workflow_index import (\n     CodeImplementationWorkflowWithIndex,\n )\n+from utils.llm_utils import (\n+    get_preferred_llm_class,\n+    should_use_document_segmentation,\n+    get_adaptive_agent_config,\n+    get_adaptive_prompts,\n+)\n+from workflows.agents.document_segmentation_agent import prepare_document_segments\n \n # Environment configuration\n os.environ[\"PYTHONDONTWRITEBYTECODE\"] = \"1\"  # Prevent .pyc file generation\n \n \n-def get_preferred_llm_class(config_path: str = \"mcp_agent.secrets.yaml\"):\n-    \"\"\"\n-    Automatically select the LLM class based on API key availability in configuration.\n-\n-    Reads from YAML config file and returns AnthropicAugmentedLLM if anthropic.api_key\n-    is available, otherwise returns OpenAIAugmentedLLM.\n-\n-    Args:\n-        config_path: Path to the YAML configuration file\n-\n-    Returns:\n-        class: The preferred LLM class\n-    \"\"\"\n-    try:\n-        # Try to read the configuration file\n-        if os.path.exists(config_path):\n-            with open(config_path, \"r\", encoding=\"utf-8\") as f:\n-                config = yaml.safe_load(f)\n-\n-            # Check for anthropic API key in config\n-            anthropic_config = config.get(\"anthropic\", {})\n-            anthropic_key = anthropic_config.get(\"api_key\", \"\")\n-\n-            if anthropic_key and anthropic_key.strip() and not anthropic_key == \"\":\n-                # print(\"\ud83e\udd16 Using AnthropicAugmentedLLM (Anthropic API key found in config)\")\n-                return AnthropicAugmentedLLM\n-            else:\n-                # print(\"\ud83e\udd16 Using OpenAIAugmentedLLM (Anthropic API key not configured)\")\n-                return OpenAIAugmentedLLM\n-        else:\n-            print(f\"\ud83e\udd16 Config file {config_path} not found, using OpenAIAugmentedLLM\")\n-            return OpenAIAugmentedLLM\n-\n-    except Exception as e:\n-        print(f\"\ud83e\udd16 Error reading config file {config_path}: {e}\")\n-        print(\"\ud83e\udd16 Falling back to OpenAIAugmentedLLM\")\n-        return OpenAIAugmentedLLM\n-\n-\n def get_default_search_server(config_path: str = \"mcp_agent.config.yaml\"):\n     \"\"\"\n     Get the default search server from configuration.\n@@ -368,11 +331,13 @@ async def run_resource_processor(analysis_result: str, logger) -> str:\n         )\n \n \n-async def run_code_analyzer(paper_dir: str, logger) -> str:\n+async def run_code_analyzer(\n+    paper_dir: str, logger, use_segmentation: bool = True\n+) -> str:\n     \"\"\"\n-    Run the code analysis workflow using multiple agents for comprehensive code planning.\n+    Run the adaptive code analysis workflow using multiple agents for comprehensive code planning.\n \n-    This function orchestrates three specialized agents:\n+    This function orchestrates three specialized agents with adaptive configuration:\n     - ConceptAnalysisAgent: Analyzes system architecture and conceptual framework\n     - AlgorithmAnalysisAgent: Extracts algorithms, formulas, and technical details\n     - CodePlannerAgent: Integrates outputs into a comprehensive implementation plan\n@@ -380,24 +345,35 @@ async def run_code_analyzer(paper_dir: str, logger) -> str:\n     Args:\n         paper_dir: Directory path containing the research paper and related resources\n         logger: Logger instance for logging information\n+        use_segmentation: Whether to use document segmentation capabilities\n \n     Returns:\n         str: Comprehensive analysis result from the coordinated agents\n     \"\"\"\n+    # Get adaptive configuration based on segmentation usage\n+    search_server_names = get_search_server_names()\n+    agent_config = get_adaptive_agent_config(use_segmentation, search_server_names)\n+    prompts = get_adaptive_prompts(use_segmentation)\n+\n+    print(\n+        f\"\ud83d\udcca Code analysis mode: {'Segmented' if use_segmentation else 'Traditional'}\"\n+    )\n+    print(f\"   Agent configurations: {agent_config}\")\n+\n     concept_analysis_agent = Agent(\n         name=\"ConceptAnalysisAgent\",\n-        instruction=PAPER_CONCEPT_ANALYSIS_PROMPT,\n-        server_names=[\"filesystem\"],\n+        instruction=prompts[\"concept_analysis\"],\n+        server_names=agent_config[\"concept_analysis\"],\n     )\n     algorithm_analysis_agent = Agent(\n         name=\"AlgorithmAnalysisAgent\",\n-        instruction=PAPER_ALGORITHM_ANALYSIS_PROMPT,\n-        server_names=get_search_server_names(additional_servers=[\"filesystem\"]),\n+        instruction=prompts[\"algorithm_analysis\"],\n+        server_names=agent_config[\"algorithm_analysis\"],\n     )\n     code_planner_agent = Agent(\n         name=\"CodePlannerAgent\",\n-        instruction=CODE_PLANNING_PROMPT,\n-        server_names=get_search_server_names(),\n+        instruction=prompts[\"code_planning\"],\n+        server_names=agent_config[\"code_planner\"],\n     )\n \n     code_aggregator_agent = ParallelLLM(\n@@ -640,6 +616,140 @@ async def orchestrate_reference_intelligence_agent(\n     return reference_result\n \n \n+async def orchestrate_document_preprocessing_agent(\n+    dir_info: Dict[str, str], logger\n+) -> Dict[str, Any]:\n+    \"\"\"\n+    Orchestrate adaptive document preprocessing with intelligent segmentation control.\n+\n+    This agent autonomously determines whether to use document segmentation based on\n+    configuration settings and document size, then applies the appropriate processing strategy.\n+\n+    Args:\n+        dir_info: Workspace infrastructure metadata\n+        logger: Logger instance for preprocessing tracking\n+\n+    Returns:\n+        dict: Document preprocessing result with segmentation metadata\n+    \"\"\"\n+\n+    try:\n+        print(\"\ud83d\udd0d Starting adaptive document preprocessing...\")\n+        print(f\"   Paper directory: {dir_info['paper_dir']}\")\n+\n+        # Step 1: Check if any markdown files exist\n+        md_files = []\n+        try:\n+            md_files = [\n+                f for f in os.listdir(dir_info[\"paper_dir\"]) if f.endswith(\".md\")\n+            ]\n+        except Exception as e:\n+            print(f\"\u26a0\ufe0f Error reading paper directory: {e}\")\n+\n+        if not md_files:\n+            print(\"\u2139\ufe0f No markdown files found - skipping document preprocessing\")\n+            dir_info[\"segments_ready\"] = False\n+            dir_info[\"use_segmentation\"] = False\n+            return {\n+                \"status\": \"skipped\",\n+                \"reason\": \"no_markdown_files\",\n+                \"paper_dir\": dir_info[\"paper_dir\"],\n+                \"segments_ready\": False,\n+                \"use_segmentation\": False,\n+            }\n+\n+        # Step 2: Read document content to determine size\n+        md_path = os.path.join(dir_info[\"paper_dir\"], md_files[0])\n+        try:\n+            with open(md_path, \"r\", encoding=\"utf-8\") as f:\n+                document_content = f.read()\n+        except Exception as e:\n+            print(f\"\u26a0\ufe0f Error reading document content: {e}\")\n+            dir_info[\"segments_ready\"] = False\n+            dir_info[\"use_segmentation\"] = False\n+            return {\n+                \"status\": \"error\",\n+                \"error_message\": f\"Failed to read document: {str(e)}\",\n+                \"paper_dir\": dir_info[\"paper_dir\"],\n+                \"segments_ready\": False,\n+                \"use_segmentation\": False,\n+            }\n+\n+        # Step 3: Determine if segmentation should be used\n+        should_segment, reason = should_use_document_segmentation(document_content)\n+        print(f\"\ud83d\udcca Segmentation decision: {should_segment}\")\n+        print(f\"   Reason: {reason}\")\n+\n+        # Store decision in dir_info for downstream agents\n+        dir_info[\"use_segmentation\"] = should_segment\n+\n+        if should_segment:\n+            print(\"\ud83d\udd27 Using intelligent document segmentation workflow...\")\n+\n+            # Prepare document segments using the segmentation agent\n+            segmentation_result = await prepare_document_segments(\n+                paper_dir=dir_info[\"paper_dir\"], logger=logger\n+            )\n+\n+            if segmentation_result[\"status\"] == \"success\":\n+                print(\"\u2705 Document segmentation completed successfully!\")\n+                print(f\"   Segments directory: {segmentation_result['segments_dir']}\")\n+                print(\"   \ud83e\udde0 Intelligent segments ready for planning agents\")\n+\n+                # Add segment information to dir_info for downstream agents\n+                dir_info[\"segments_dir\"] = segmentation_result[\"segments_dir\"]\n+                dir_info[\"segments_ready\"] = True\n+\n+                return segmentation_result\n+\n+            else:\n+                print(\n+                    f\"\u26a0\ufe0f Document segmentation failed: {segmentation_result.get('error_message', 'Unknown error')}\"\n+                )\n+                print(\"   Falling back to traditional full-document processing...\")\n+                dir_info[\"segments_ready\"] = False\n+                dir_info[\"use_segmentation\"] = False\n+\n+                return {\n+                    \"status\": \"fallback_to_traditional\",\n+                    \"original_error\": segmentation_result.get(\n+                        \"error_message\", \"Unknown error\"\n+                    ),\n+                    \"paper_dir\": dir_info[\"paper_dir\"],\n+                    \"segments_ready\": False,\n+                    \"use_segmentation\": False,\n+                    \"fallback_reason\": \"segmentation_failed\",\n+                }\n+        else:\n+            print(\"\ud83d\udcd6 Using traditional full-document reading workflow...\")\n+            dir_info[\"segments_ready\"] = False\n+\n+            return {\n+                \"status\": \"traditional\",\n+                \"reason\": reason,\n+                \"paper_dir\": dir_info[\"paper_dir\"],\n+                \"segments_ready\": False,\n+                \"use_segmentation\": False,\n+                \"document_size\": len(document_content),\n+            }\n+\n+    except Exception as e:\n+        print(f\"\u274c Error during document preprocessing: {e}\")\n+        print(\"   Continuing with traditional full-document processing...\")\n+\n+        # Ensure fallback settings\n+        dir_info[\"segments_ready\"] = False\n+        dir_info[\"use_segmentation\"] = False\n+\n+        return {\n+            \"status\": \"error\",\n+            \"paper_dir\": dir_info[\"paper_dir\"],\n+            \"segments_ready\": False,\n+            \"use_segmentation\": False,\n+            \"error_message\": str(e),\n+        }\n+\n+\n async def orchestrate_code_planning_agent(\n     dir_info: Dict[str, str], logger, progress_callback: Optional[Callable] = None\n ):\n@@ -661,7 +771,13 @@ async def orchestrate_code_planning_agent(\n \n     # Check if initial plan already exists\n     if not os.path.exists(initial_plan_path):\n-        initial_plan_result = await run_code_analyzer(dir_info[\"paper_dir\"], logger)\n+        # Use segmentation setting from preprocessing phase\n+        use_segmentation = dir_info.get(\"use_segmentation\", True)\n+        print(f\"\ud83d\udcca Planning mode: {'Segmented' if use_segmentation else 'Traditional'}\")\n+\n+        initial_plan_result = await run_code_analyzer(\n+            dir_info[\"paper_dir\"], logger, use_segmentation=use_segmentation\n+        )\n         with open(initial_plan_path, \"w\", encoding=\"utf-8\") as f:\n             f.write(initial_plan_result)\n         print(f\"Initial plan saved to {initial_plan_path}\")\n@@ -1137,6 +1253,32 @@ async def execute_multi_agent_research_pipeline(\n         )\n         await asyncio.sleep(30)\n \n+        # Phase 3.5: Document Segmentation and Preprocessing\n+\n+        segmentation_result = await orchestrate_document_preprocessing_agent(\n+            dir_info, logger\n+        )\n+\n+        # Handle segmentation result\n+        if segmentation_result[\"status\"] == \"success\":\n+            print(\"\u2705 Document preprocessing completed successfully!\")\n+            print(\n+                f\"   \ud83d\udcca Using segmentation: {dir_info.get('use_segmentation', False)}\"\n+            )\n+            if dir_info.get(\"segments_ready\", False):\n+                print(\n+                    f\"   \ud83d\udcc1 Segments directory: {segmentation_result.get('segments_dir', 'N/A')}\"\n+                )\n+        elif segmentation_result[\"status\"] == \"fallback_to_traditional\":\n+            print(\"\u26a0\ufe0f Document segmentation failed, using traditional processing\")\n+            print(\n+                f\"   Original error: {segmentation_result.get('original_error', 'Unknown')}\"\n+            )\n+        else:\n+            print(\n+                f\"\u26a0\ufe0f Document preprocessing encountered issues: {segmentation_result.get('error_message', 'Unknown')}\"\n+            )\n+\n         # Phase 4: Code Planning Orchestration\n         await orchestrate_code_planning_agent(dir_info, logger, progress_callback)\n \n"
        },
        {
            "commit": "daad35b9c60770a655eaaea4565bf81f63ae1b38",
            "file_path": "workflows/agents/code_implementation_agent.py",
            "diff": "diff --git a/workflows/agents/code_implementation_agent.py b/workflows/agents/code_implementation_agent.py\nindex 2104682..7504c06 100644\n--- a/workflows/agents/code_implementation_agent.py\n+++ b/workflows/agents/code_implementation_agent.py\n@@ -949,8 +949,8 @@ class CodeImplementationAgent:\n         # Simulate that at least one file has been implemented (to trigger optimization)\n         self.files_implemented_count = 1\n \n-        # Test with a file that should have a summary\n-        test_file = \"rice/config.py\"\n+        # Test with a generic config file that should have a summary\n+        test_file = \"config.py\"\n \n         print(f\"\ud83d\udcc1 Testing automatic optimization for: {test_file}\")\n         print(f\"\ud83d\udcca Files implemented count: {self.files_implemented_count}\")\n@@ -1002,13 +1002,13 @@ class CodeImplementationAgent:\n         print(\"\ud83d\udd04 AUTOMATIC READ_FILE OPTIMIZATION TEST COMPLETE\")\n         print(\"=\" * 80)\n \n-    async def test_summary_optimization(self, test_file_path: str = \"rice/config.py\"):\n+    async def test_summary_optimization(self, test_file_path: str = \"config.py\"):\n         \"\"\"\n         Test the summary optimization functionality with a specific file\n         \u6d4b\u8bd5\u7279\u5b9a\u6587\u4ef6\u7684\u603b\u7ed3\u4f18\u5316\u529f\u80fd\n \n         Args:\n-            test_file_path: File path to test (default: rice/config.py which should be in summary)\n+            test_file_path: File path to test (default: config.py which should be in summary)\n         \"\"\"\n         if not self.mcp_agent:\n             return False\n"
        },
        {
            "commit": "daad35b9c60770a655eaaea4565bf81f63ae1b38",
            "file_path": "workflows/agents/document_segmentation_agent.py",
            "diff": "diff --git a/workflows/agents/document_segmentation_agent.py b/workflows/agents/document_segmentation_agent.py\nnew file mode 100644\nindex 0000000..a9e4164\n--- /dev/null\n+++ b/workflows/agents/document_segmentation_agent.py\n@@ -0,0 +1,361 @@\n+\"\"\"\n+Document Segmentation Agent\n+\n+A lightweight agent that coordinates with the document segmentation MCP server\n+to analyze document structure and prepare segments for other agents.\n+\"\"\"\n+\n+import asyncio\n+import os\n+import logging\n+from typing import Dict, Any, Optional\n+\n+from mcp_agent.agents.agent import Agent\n+from utils.llm_utils import get_preferred_llm_class\n+\n+\n+class DocumentSegmentationAgent:\n+    \"\"\"\n+    Intelligent document segmentation agent with semantic analysis capabilities.\n+    \n+    This enhanced agent provides:\n+    1. **Semantic Document Classification**: Content-based document type identification\n+    2. **Adaptive Segmentation Strategy**: Algorithm integrity and semantic coherence preservation\n+    3. **Planning Agent Optimization**: Segment preparation specifically optimized for downstream agents\n+    4. **Quality Intelligence Validation**: Advanced metrics for completeness and technical accuracy\n+    5. **Algorithm Completeness Protection**: Ensures critical algorithms and formulas remain intact\n+    \n+    Key improvements over traditional segmentation:\n+    - Semantic content analysis vs mechanical structure splitting\n+    - Dynamic character limits based on content complexity\n+    - Enhanced relevance scoring for planning agents\n+    - Algorithm and formula integrity preservation\n+    - Content type-aware segmentation strategies\n+    \"\"\"\n+    \n+    def __init__(self, logger: Optional[logging.Logger] = None):\n+        self.logger = logger or self._create_default_logger()\n+        self.mcp_agent = None\n+    \n+    def _create_default_logger(self) -> logging.Logger:\n+        \"\"\"Create default logger if none provided\"\"\"\n+        logger = logging.getLogger(f\"{__name__}.DocumentSegmentationAgent\")\n+        logger.setLevel(logging.INFO)\n+        return logger\n+    \n+    async def __aenter__(self):\n+        \"\"\"Async context manager entry\"\"\"\n+        await self.initialize()\n+        return self\n+    \n+    async def __aexit__(self, exc_type, exc_val, exc_tb):\n+        \"\"\"Async context manager exit\"\"\"\n+        await self.cleanup()\n+    \n+    async def initialize(self):\n+        \"\"\"Initialize the MCP agent connection\"\"\"\n+        try:\n+            self.mcp_agent = Agent(\n+                name=\"DocumentSegmentationCoordinator\",\n+                instruction=\"\"\"You are an intelligent document segmentation coordinator that leverages advanced semantic analysis for optimal document processing.\n+\n+Your enhanced capabilities include:\n+1. **Semantic Content Analysis**: Coordinate intelligent document type classification based on content semantics rather than structural patterns\n+2. **Algorithm Integrity Protection**: Ensure algorithm blocks, formulas, and related content maintain logical coherence \n+3. **Adaptive Segmentation Strategy**: Select optimal segmentation approaches (semantic_research_focused, algorithm_preserve_integrity, concept_implementation_hybrid, etc.)\n+4. **Quality Intelligence Validation**: Assess segmentation quality using enhanced metrics for completeness, relevance, and technical accuracy\n+5. **Planning Agent Optimization**: Ensure segments are specifically optimized for ConceptAnalysisAgent, AlgorithmAnalysisAgent, and CodePlannerAgent needs\n+\n+**Key Principles**:\n+- Prioritize content semantics over mechanical structure\n+- Preserve algorithm and formula completeness\n+- Optimize for downstream agent token efficiency\n+- Ensure technical content integrity\n+- Provide actionable quality assessments\n+\n+Use the enhanced document-segmentation tools to deliver superior segmentation results that significantly improve planning agent performance.\"\"\",\n+                server_names=[\"document-segmentation\", \"filesystem\"]\n+            )\n+            \n+            # Initialize the agent context\n+            await self.mcp_agent.__aenter__()\n+            \n+            # Attach LLM\n+            self.llm = await self.mcp_agent.attach_llm(get_preferred_llm_class())\n+            \n+            self.logger.info(\"DocumentSegmentationAgent initialized successfully\")\n+            \n+        except Exception as e:\n+            self.logger.error(f\"Failed to initialize DocumentSegmentationAgent: {e}\")\n+            raise\n+    \n+    async def cleanup(self):\n+        \"\"\"Cleanup resources\"\"\"\n+        if self.mcp_agent:\n+            try:\n+                await self.mcp_agent.__aexit__(None, None, None)\n+            except Exception as e:\n+                self.logger.warning(f\"Error during cleanup: {e}\")\n+    \n+    async def analyze_and_prepare_document(\n+        self, \n+        paper_dir: str, \n+        force_refresh: bool = False\n+    ) -> Dict[str, Any]:\n+        \"\"\"\n+        Perform intelligent semantic analysis and create optimized document segments.\n+        \n+        This method coordinates with the enhanced document segmentation server to:\n+        - Classify document type using semantic content analysis\n+        - Select optimal segmentation strategy (semantic_research_focused, algorithm_preserve_integrity, etc.)\n+        - Preserve algorithm and formula integrity\n+        - Optimize segments for downstream planning agents\n+        \n+        Args:\n+            paper_dir: Path to the paper directory\n+            force_refresh: Whether to force re-analysis with latest algorithms\n+            \n+        Returns:\n+            Dict containing enhanced analysis results and intelligent segment information\n+        \"\"\"\n+        try:\n+            self.logger.info(f\"Starting document analysis for: {paper_dir}\")\n+            \n+            # Check if markdown file exists\n+            md_files = [f for f in os.listdir(paper_dir) if f.endswith('.md')]\n+            if not md_files:\n+                raise ValueError(f\"No markdown file found in {paper_dir}\")\n+            \n+            # Use the enhanced document segmentation tool\n+            message = f\"\"\"Please perform intelligent semantic analysis and segmentation for the document in directory: {paper_dir}\n+\n+Use the analyze_and_segment_document tool with these parameters:\n+- paper_dir: {paper_dir}\n+- force_refresh: {force_refresh}\n+\n+**Focus on these enhanced objectives**:\n+1. **Semantic Document Classification**: Identify document type using content semantics (research_paper, algorithm_focused, technical_doc, etc.)\n+2. **Intelligent Segmentation Strategy**: Select the optimal strategy based on content analysis:\n+   - `semantic_research_focused` for research papers with high algorithm density\n+   - `algorithm_preserve_integrity` for algorithm-heavy documents\n+   - `concept_implementation_hybrid` for mixed concept/implementation content\n+3. **Algorithm Completeness**: Ensure algorithm blocks, formulas, and related descriptions remain logically connected\n+4. **Planning Agent Optimization**: Create segments that maximize effectiveness for ConceptAnalysisAgent, AlgorithmAnalysisAgent, and CodePlannerAgent\n+\n+After segmentation, get a document overview and provide:\n+- Quality assessment of semantic segmentation approach\n+- Algorithm/formula integrity verification\n+- Recommendations for planning agent optimization\n+- Technical content completeness evaluation\"\"\"\n+            \n+            result = await self.llm.generate_str(message=message)\n+            \n+            self.logger.info(\"Document analysis completed successfully\")\n+            \n+            # Parse the result and return structured information\n+            return {\n+                \"status\": \"success\",\n+                \"paper_dir\": paper_dir,\n+                \"analysis_result\": result,\n+                \"segments_available\": True\n+            }\n+            \n+        except Exception as e:\n+            self.logger.error(f\"Error in document analysis: {e}\")\n+            return {\n+                \"status\": \"error\",\n+                \"paper_dir\": paper_dir,\n+                \"error_message\": str(e),\n+                \"segments_available\": False\n+            }\n+    \n+    async def get_document_overview(self, paper_dir: str) -> Dict[str, Any]:\n+        \"\"\"\n+        Get overview of document structure and segments.\n+        \n+        Args:\n+            paper_dir: Path to the paper directory\n+            \n+        Returns:\n+            Dict containing document overview information\n+        \"\"\"\n+        try:\n+            message = f\"\"\"Please provide an intelligent overview of the enhanced document segmentation for: {paper_dir}\n+\n+Use the get_document_overview tool to retrieve:\n+- **Semantic Document Classification**: Document type and confidence score\n+- **Adaptive Segmentation Strategy**: Strategy used and reasoning\n+- **Segment Intelligence**: Total segments with enhanced metadata\n+- **Content Type Distribution**: Breakdown by algorithm, concept, formula, implementation content\n+- **Quality Intelligence Assessment**: Completeness, coherence, and planning agent optimization\n+\n+Provide a comprehensive analysis focusing on:\n+1. Semantic vs structural segmentation quality\n+2. Algorithm and formula integrity preservation\n+3. Segment relevance for downstream planning agents\n+4. Technical content distribution and completeness\"\"\"\n+            \n+            result = await self.llm.generate_str(message=message)\n+            \n+            return {\n+                \"status\": \"success\",\n+                \"paper_dir\": paper_dir,\n+                \"overview_result\": result\n+            }\n+            \n+        except Exception as e:\n+            self.logger.error(f\"Error getting document overview: {e}\")\n+            return {\n+                \"status\": \"error\",\n+                \"paper_dir\": paper_dir,\n+                \"error_message\": str(e)\n+            }\n+    \n+    async def validate_segmentation_quality(self, paper_dir: str) -> Dict[str, Any]:\n+        \"\"\"\n+        Validate the quality of document segmentation.\n+        \n+        Args:\n+            paper_dir: Path to the paper directory\n+            \n+        Returns:\n+            Dict containing validation results\n+        \"\"\"\n+        try:\n+            # Get overview first\n+            overview_result = await self.get_document_overview(paper_dir)\n+            \n+            if overview_result[\"status\"] != \"success\":\n+                return overview_result\n+            \n+            # Analyze enhanced segmentation quality\n+            message = f\"\"\"Based on the intelligent document overview for {paper_dir}, please evaluate the enhanced segmentation quality using advanced criteria.\n+\n+**Enhanced Quality Assessment Factors**:\n+1. **Semantic Coherence**: Do segments maintain logical content boundaries vs mechanical structural splits?\n+2. **Algorithm Integrity**: Are algorithm blocks, formulas, and related explanations kept together?\n+3. **Content Type Optimization**: Are different content types (algorithm, concept, formula, implementation) properly identified and scored?\n+4. **Planning Agent Effectiveness**: Will ConceptAnalysisAgent, AlgorithmAnalysisAgent, and CodePlannerAgent receive optimal information?\n+5. **Dynamic Sizing**: Are segments adaptively sized based on content complexity rather than fixed limits?\n+6. **Technical Completeness**: Are critical technical details preserved without fragmentation?\n+\n+**Provide specific recommendations for**:\n+- Semantic segmentation improvements\n+- Algorithm/formula integrity enhancements  \n+- Planning agent optimization opportunities\n+- Content distribution balance adjustments\"\"\"\n+            \n+            validation_result = await self.llm.generate_str(message=message)\n+            \n+            return {\n+                \"status\": \"success\",\n+                \"paper_dir\": paper_dir,\n+                \"validation_result\": validation_result,\n+                \"overview_data\": overview_result\n+            }\n+            \n+        except Exception as e:\n+            self.logger.error(f\"Error validating segmentation quality: {e}\")\n+            return {\n+                \"status\": \"error\",\n+                \"paper_dir\": paper_dir,\n+                \"error_message\": str(e)\n+            }\n+\n+\n+async def run_document_segmentation_analysis(\n+    paper_dir: str, \n+    logger: Optional[logging.Logger] = None,\n+    force_refresh: bool = False\n+) -> Dict[str, Any]:\n+    \"\"\"\n+    Convenience function to run document segmentation analysis.\n+    \n+    Args:\n+        paper_dir: Path to the paper directory\n+        logger: Optional logger instance\n+        force_refresh: Whether to force re-analysis\n+        \n+    Returns:\n+        Dict containing analysis results\n+    \"\"\"\n+    async with DocumentSegmentationAgent(logger=logger) as agent:\n+        # Analyze and prepare document\n+        analysis_result = await agent.analyze_and_prepare_document(\n+            paper_dir, force_refresh=force_refresh\n+        )\n+        \n+        if analysis_result[\"status\"] == \"success\":\n+            # Validate segmentation quality\n+            validation_result = await agent.validate_segmentation_quality(paper_dir)\n+            analysis_result[\"validation\"] = validation_result\n+        \n+        return analysis_result\n+\n+\n+# Utility function for integration with existing workflow\n+async def prepare_document_segments(\n+    paper_dir: str,\n+    logger: Optional[logging.Logger] = None\n+) -> Dict[str, Any]:\n+    \"\"\"\n+    Prepare intelligent document segments optimized for planning agents.\n+    \n+    This enhanced function leverages semantic analysis to create segments that:\n+    - Preserve algorithm and formula integrity\n+    - Optimize for ConceptAnalysisAgent, AlgorithmAnalysisAgent, and CodePlannerAgent\n+    - Use adaptive character limits based on content complexity\n+    - Maintain technical content completeness\n+    \n+    Called from the orchestration engine (Phase 3.5) to prepare documents \n+    before the planning phase with superior segmentation quality.\n+    \n+    Args:\n+        paper_dir: Path to the paper directory containing markdown file\n+        logger: Optional logger instance for tracking\n+        \n+    Returns:\n+        Dict containing enhanced preparation results and intelligent metadata\n+    \"\"\"\n+    try:\n+        logger = logger or logging.getLogger(__name__)\n+        logger.info(f\"Preparing document segments for: {paper_dir}\")\n+        \n+        # Run analysis\n+        result = await run_document_segmentation_analysis(\n+            paper_dir=paper_dir,\n+            logger=logger,\n+            force_refresh=False  # Use cached analysis if available\n+        )\n+        \n+        if result[\"status\"] == \"success\":\n+            logger.info(\"Document segments prepared successfully\")\n+            \n+            # Create metadata for downstream agents\n+            segments_dir = os.path.join(paper_dir, \"document_segments\")\n+            \n+            return {\n+                \"status\": \"success\",\n+                \"paper_dir\": paper_dir,\n+                \"segments_dir\": segments_dir,\n+                \"segments_ready\": True,\n+                \"analysis_summary\": result.get(\"analysis_result\", \"\"),\n+                \"validation_summary\": result.get(\"validation\", {}).get(\"validation_result\", \"\")\n+            }\n+        else:\n+            logger.error(f\"Document segmentation failed: {result.get('error_message', 'Unknown error')}\")\n+            return {\n+                \"status\": \"error\",\n+                \"paper_dir\": paper_dir,\n+                \"segments_ready\": False,\n+                \"error_message\": result.get(\"error_message\", \"Document segmentation failed\")\n+            }\n+            \n+    except Exception as e:\n+        logger.error(f\"Error preparing document segments: {e}\")\n+        return {\n+            \"status\": \"error\",\n+            \"paper_dir\": paper_dir,\n+            \"segments_ready\": False,\n+            \"error_message\": str(e)\n+        }\n\\ No newline at end of file\n"
        },
        {
            "commit": "daad35b9c60770a655eaaea4565bf81f63ae1b38",
            "file_path": "workflows/code_implementation_workflow.py",
            "diff": "diff --git a/workflows/code_implementation_workflow.py b/workflows/code_implementation_workflow.py\nindex e00c690..86fc04c 100644\n--- a/workflows/code_implementation_workflow.py\n+++ b/workflows/code_implementation_workflow.py\n@@ -22,8 +22,6 @@ from typing import Dict, Any, Optional, List\n \n # MCP Agent imports\n from mcp_agent.agents.agent import Agent\n-from mcp_agent.workflows.llm.augmented_llm_anthropic import AnthropicAugmentedLLM\n-from mcp_agent.workflows.llm.augmented_llm_openai import OpenAIAugmentedLLM\n \n # Local imports\n sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n@@ -34,78 +32,10 @@ from prompts.code_prompts import (\n from workflows.agents import CodeImplementationAgent\n from workflows.agents.memory_agent_concise import ConciseMemoryAgent\n from config.mcp_tool_definitions import get_mcp_tools\n+from utils.llm_utils import get_preferred_llm_class, get_default_models\n # DialogueLogger removed - no longer needed\n \n \n-def get_preferred_llm_class(config_path: str = \"mcp_agent.secrets.yaml\"):\n-    \"\"\"\n-    Automatically select the LLM class based on API key availability in configuration.\n-\n-    Reads from YAML config file and returns AnthropicAugmentedLLM if anthropic.api_key\n-    is available, otherwise returns OpenAIAugmentedLLM.\n-\n-    Args:\n-        config_path: Path to the YAML configuration file\n-\n-    Returns:\n-        class: The preferred LLM class\n-    \"\"\"\n-    try:\n-        # Try to read the configuration file\n-        if os.path.exists(config_path):\n-            with open(config_path, \"r\", encoding=\"utf-8\") as f:\n-                config = yaml.safe_load(f)\n-\n-            # Check for anthropic API key in config\n-            anthropic_config = config.get(\"anthropic\", {})\n-            anthropic_key = anthropic_config.get(\"api_key\", \"\")\n-\n-            if anthropic_key and anthropic_key.strip() and not anthropic_key == \"\":\n-                # print(\"\ud83e\udd16 Using AnthropicAugmentedLLM (Anthropic API key found in config)\")\n-                return AnthropicAugmentedLLM\n-            else:\n-                # print(\"\ud83e\udd16 Using OpenAIAugmentedLLM (Anthropic API key not configured)\")\n-                return OpenAIAugmentedLLM\n-        else:\n-            print(f\"\ud83e\udd16 Config file {config_path} not found, using OpenAIAugmentedLLM\")\n-            return OpenAIAugmentedLLM\n-\n-    except Exception as e:\n-        print(f\"\ud83e\udd16 Error reading config file {config_path}: {e}\")\n-        print(\"\ud83e\udd16 Falling back to OpenAIAugmentedLLM\")\n-        return OpenAIAugmentedLLM\n-\n-\n-def get_default_models(config_path: str = \"mcp_agent.config.yaml\"):\n-    \"\"\"\n-    Get default models from configuration file.\n-\n-    Args:\n-        config_path: Path to the configuration file\n-\n-    Returns:\n-        dict: Dictionary with 'anthropic' and 'openai' default models\n-    \"\"\"\n-    try:\n-        if os.path.exists(config_path):\n-            with open(config_path, \"r\", encoding=\"utf-8\") as f:\n-                config = yaml.safe_load(f)\n-\n-            anthropic_model = config.get(\"anthropic\", {}).get(\n-                \"default_model\", \"claude-sonnet-4-20250514\"\n-            )\n-            openai_model = config.get(\"openai\", {}).get(\"default_model\", \"o3-mini\")\n-\n-            return {\"anthropic\": anthropic_model, \"openai\": openai_model}\n-        else:\n-            print(f\"Config file {config_path} not found, using default models\")\n-            return {\"anthropic\": \"claude-sonnet-4-20250514\", \"openai\": \"o3-mini\"}\n-\n-    except Exception as e:\n-        print(f\"Error reading config file {config_path}: {e}\")\n-        return {\"anthropic\": \"claude-sonnet-4-20250514\", \"openai\": \"o3-mini\"}\n-\n-\n class CodeImplementationWorkflow:\n     \"\"\"\n     Paper Code Implementation Workflow Manager\n"
        },
        {
            "commit": "daad35b9c60770a655eaaea4565bf81f63ae1b38",
            "file_path": "workflows/code_implementation_workflow_index.py",
            "diff": "diff --git a/workflows/code_implementation_workflow_index.py b/workflows/code_implementation_workflow_index.py\nindex 349a97f..a12024b 100644\n--- a/workflows/code_implementation_workflow_index.py\n+++ b/workflows/code_implementation_workflow_index.py\n@@ -22,8 +22,6 @@ from typing import Dict, Any, Optional, List\n \n # MCP Agent imports\n from mcp_agent.agents.agent import Agent\n-from mcp_agent.workflows.llm.augmented_llm_anthropic import AnthropicAugmentedLLM\n-from mcp_agent.workflows.llm.augmented_llm_openai import OpenAIAugmentedLLM\n \n # Local imports\n sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n@@ -34,78 +32,10 @@ from prompts.code_prompts import (\n from workflows.agents import CodeImplementationAgent\n from workflows.agents.memory_agent_concise import ConciseMemoryAgent\n from config.mcp_tool_definitions_index import get_mcp_tools\n+from utils.llm_utils import get_preferred_llm_class, get_default_models\n # DialogueLogger removed - no longer needed\n \n \n-def get_preferred_llm_class(config_path: str = \"mcp_agent.secrets.yaml\"):\n-    \"\"\"\n-    Automatically select the LLM class based on API key availability in configuration.\n-\n-    Reads from YAML config file and returns AnthropicAugmentedLLM if anthropic.api_key\n-    is available, otherwise returns OpenAIAugmentedLLM.\n-\n-    Args:\n-        config_path: Path to the YAML configuration file\n-\n-    Returns:\n-        class: The preferred LLM class\n-    \"\"\"\n-    try:\n-        # Try to read the configuration file\n-        if os.path.exists(config_path):\n-            with open(config_path, \"r\", encoding=\"utf-8\") as f:\n-                config = yaml.safe_load(f)\n-\n-            # Check for anthropic API key in config\n-            anthropic_config = config.get(\"anthropic\", {})\n-            anthropic_key = anthropic_config.get(\"api_key\", \"\")\n-\n-            if anthropic_key and anthropic_key.strip() and not anthropic_key == \"\":\n-                # print(\"\ud83e\udd16 Using AnthropicAugmentedLLM (Anthropic API key found in config)\")\n-                return AnthropicAugmentedLLM\n-            else:\n-                # print(\"\ud83e\udd16 Using OpenAIAugmentedLLM (Anthropic API key not configured)\")\n-                return OpenAIAugmentedLLM\n-        else:\n-            print(f\"\ud83e\udd16 Config file {config_path} not found, using OpenAIAugmentedLLM\")\n-            return OpenAIAugmentedLLM\n-\n-    except Exception as e:\n-        print(f\"\ud83e\udd16 Error reading config file {config_path}: {e}\")\n-        print(\"\ud83e\udd16 Falling back to OpenAIAugmentedLLM\")\n-        return OpenAIAugmentedLLM\n-\n-\n-def get_default_models(config_path: str = \"mcp_agent.config.yaml\"):\n-    \"\"\"\n-    Get default models from configuration file.\n-\n-    Args:\n-        config_path: Path to the configuration file\n-\n-    Returns:\n-        dict: Dictionary with 'anthropic' and 'openai' default models\n-    \"\"\"\n-    try:\n-        if os.path.exists(config_path):\n-            with open(config_path, \"r\", encoding=\"utf-8\") as f:\n-                config = yaml.safe_load(f)\n-\n-            anthropic_model = config.get(\"anthropic\", {}).get(\n-                \"default_model\", \"claude-sonnet-4-20250514\"\n-            )\n-            openai_model = config.get(\"openai\", {}).get(\"default_model\", \"o3-mini\")\n-\n-            return {\"anthropic\": anthropic_model, \"openai\": openai_model}\n-        else:\n-            print(f\"Config file {config_path} not found, using default models\")\n-            return {\"anthropic\": \"claude-sonnet-4-20250514\", \"openai\": \"o3-mini\"}\n-\n-    except Exception as e:\n-        print(f\"Error reading config file {config_path}: {e}\")\n-        return {\"anthropic\": \"claude-sonnet-4-20250514\", \"openai\": \"o3-mini\"}\n-\n-\n class CodeImplementationWorkflowWithIndex:\n     \"\"\"\n     Paper Code Implementation Workflow Manager with Code Reference Indexer\n"
        },
        {
            "commit": "daad35b9c60770a655eaaea4565bf81f63ae1b38",
            "file_path": "workflows/codebase_index_workflow.py",
            "diff": "diff --git a/workflows/codebase_index_workflow.py b/workflows/codebase_index_workflow.py\nindex 920d137..b42a7ff 100644\n--- a/workflows/codebase_index_workflow.py\n+++ b/workflows/codebase_index_workflow.py\n@@ -97,9 +97,9 @@ class CodebaseIndexWorkflow:\n \n         # Fallback: look for any code block containing project structure\n         code_block_patterns = [\n-            r\"```[^\\n]*\\n(rice_framework/.*?(?:\u251c\u2500\u2500|\u2514\u2500\u2500).*?)\\n```\",\n             r\"```[^\\n]*\\n(project/.*?(?:\u251c\u2500\u2500|\u2514\u2500\u2500).*?)\\n```\",\n             r\"```[^\\n]*\\n(src/.*?(?:\u251c\u2500\u2500|\u2514\u2500\u2500).*?)\\n```\",\n+            r\"```[^\\n]*\\n(core/.*?(?:\u251c\u2500\u2500|\u2514\u2500\u2500).*?)\\n```\",\n             r\"```[^\\n]*\\n(.*?(?:\u251c\u2500\u2500|\u2514\u2500\u2500).*?(?:\\.py|\\.txt|\\.md|\\.yaml).*?)\\n```\",\n         ]\n \n@@ -146,12 +146,15 @@ class CodebaseIndexWorkflow:\n             # Create tree structure\n             structure_lines = []\n \n-            # Determine root directory name\n-            root_name = (\n-                \"rice_framework\"\n-                if any(\"rice\" in f for f in file_mentions)\n-                else \"project\"\n-            )\n+            # Determine root directory name from common patterns\n+            if any(\"src/\" in f for f in file_mentions):\n+                root_name = \"src\"\n+            elif any(\"core/\" in f for f in file_mentions):\n+                root_name = \"core\"\n+            elif any(\"lib/\" in f for f in file_mentions):\n+                root_name = \"lib\"\n+            else:\n+                root_name = \"project\"\n             structure_lines.append(f\"{root_name}/\")\n \n             # Add directories and files\n"
        }
    ]
}