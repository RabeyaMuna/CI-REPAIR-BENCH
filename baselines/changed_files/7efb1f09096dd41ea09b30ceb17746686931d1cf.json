{
    "sha_fail": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
    "changed_files": [
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/agent_concepts/other/debug_level.py",
            "diff": "diff --git a/cookbook/agent_concepts/other/debug_level.py b/cookbook/agent_concepts/other/debug_level.py\nnew file mode 100644\nindex 000000000..cf80118d6\n--- /dev/null\n+++ b/cookbook/agent_concepts/other/debug_level.py\n@@ -0,0 +1,34 @@\n+\"\"\"\n+This example shows how to set the debug level of an agent.\n+\n+The debug level is a number between 1 and 2.\n+\n+1: Basic debug information\n+2: Detailed debug information\n+\n+The default debug level is 1.\n+\"\"\"\n+\n+from agno.agent.agent import Agent\n+from agno.models.anthropic.claude import Claude\n+from agno.tools.yfinance import YFinanceTools\n+\n+# Basic debug information\n+agent = Agent(\n+    model=Claude(id=\"claude-3-5-sonnet-20240620\"),\n+    tools=[YFinanceTools()],\n+    debug_mode=True,\n+    debug_level=1,\n+)\n+\n+agent.print_response(\"What is the current price of Tesla?\")\n+\n+# Verbose debug information\n+agent = Agent(\n+    model=Claude(id=\"claude-3-5-sonnet-20240620\"),\n+    tools=[YFinanceTools()],\n+    debug_mode=True,\n+    debug_level=2,\n+)\n+\n+agent.print_response(\"What is the current price of Apple?\")\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/agent_concepts/other/parse_model_stream.py",
            "diff": "diff --git a/cookbook/agent_concepts/other/parse_model_stream.py b/cookbook/agent_concepts/other/parse_model_stream.py\nnew file mode 100644\nindex 000000000..fbccad6bb\n--- /dev/null\n+++ b/cookbook/agent_concepts/other/parse_model_stream.py\n@@ -0,0 +1,85 @@\n+import random\n+from typing import Iterator, List\n+\n+from agno.agent import Agent, RunResponseEvent\n+from agno.models.anthropic import Claude\n+from agno.models.openai import OpenAIChat\n+from pydantic import BaseModel, Field\n+from rich.pretty import pprint  # noqa\n+\n+\n+class NationalParkAdventure(BaseModel):\n+    park_name: str = Field(..., description=\"Name of the national park\")\n+    best_season: str = Field(\n+        ...,\n+        description=\"Optimal time of year to visit this park (e.g., 'Late spring to early fall')\",\n+    )\n+    signature_attractions: List[str] = Field(\n+        ...,\n+        description=\"Must-see landmarks, viewpoints, or natural features in the park\",\n+    )\n+    recommended_trails: List[str] = Field(\n+        ...,\n+        description=\"Top hiking trails with difficulty levels (e.g., 'Angel's Landing - Strenuous')\",\n+    )\n+    wildlife_encounters: List[str] = Field(\n+        ..., description=\"Animals visitors are likely to spot, with viewing tips\"\n+    )\n+    photography_spots: List[str] = Field(\n+        ...,\n+        description=\"Best locations for capturing stunning photos, including sunrise/sunset spots\",\n+    )\n+    camping_options: List[str] = Field(\n+        ..., description=\"Available camping areas, from primitive to RV-friendly sites\"\n+    )\n+    safety_warnings: List[str] = Field(\n+        ..., description=\"Important safety considerations specific to this park\"\n+    )\n+    hidden_gems: List[str] = Field(\n+        ..., description=\"Lesser-known spots or experiences that most visitors miss\"\n+    )\n+    difficulty_rating: int = Field(\n+        ...,\n+        ge=1,\n+        le=5,\n+        description=\"Overall park difficulty for average visitor (1=easy, 5=very challenging)\",\n+    )\n+    estimated_days: int = Field(\n+        ...,\n+        ge=1,\n+        le=14,\n+        description=\"Recommended number of days to properly explore the park\",\n+    )\n+    special_permits_needed: List[str] = Field(\n+        default=[],\n+        description=\"Any special permits or reservations required for certain activities\",\n+    )\n+\n+\n+agent = Agent(\n+    parser_model=Claude(id=\"claude-sonnet-4-20250514\"),\n+    description=\"You help people plan amazing national park adventures and provide detailed park guides.\",\n+    response_model=NationalParkAdventure,\n+    model=OpenAIChat(id=\"gpt-4o-mini\"),\n+)\n+\n+# Get the response in a variable\n+national_parks = [\n+    \"Yellowstone National Park\",\n+    \"Yosemite National Park\",\n+    \"Grand Canyon National Park\",\n+    \"Zion National Park\",\n+    \"Grand Teton National Park\",\n+    \"Rocky Mountain National Park\",\n+    \"Acadia National Park\",\n+    \"Mount Rainier National Park\",\n+    \"Great Smoky Mountains National Park\",\n+    \"Rocky National Park\",\n+]\n+\n+# Get the response in a variable\n+run_events: Iterator[RunResponseEvent] = agent.run(\n+    national_parks[random.randint(0, len(national_parks) - 1)], stream=True\n+)\n+for event in run_events:\n+    pprint(event)\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/evals/performance/team_response_with_memory_multi_user.py",
            "diff": "diff --git a/cookbook/evals/performance/team_response_with_memory_multi_user.py b/cookbook/evals/performance/team_response_with_memory_multi_user.py\nnew file mode 100644\nindex 000000000..46d6b4121\n--- /dev/null\n+++ b/cookbook/evals/performance/team_response_with_memory_multi_user.py\n@@ -0,0 +1,142 @@\n+import asyncio\n+import random\n+\n+from agno.agent import Agent\n+from agno.eval.performance import PerformanceEval\n+from agno.memory.v2.db.postgres import PostgresMemoryDb\n+from agno.memory.v2.memory import Memory\n+from agno.models.openai import OpenAIChat\n+from agno.storage.postgres import PostgresStorage\n+from agno.team.team import Team\n+\n+users = [\n+    \"abel@example.com\",\n+    \"ben@example.com\",\n+    \"charlie@example.com\",\n+    \"dave@example.com\",\n+    \"edward@example.com\",\n+]\n+\n+cities = [\n+    \"New York\",\n+    \"Los Angeles\",\n+    \"Chicago\",\n+    \"Houston\",\n+    \"Miami\",\n+    \"San Francisco\",\n+    \"Seattle\",\n+    \"Boston\",\n+    \"Washington D.C.\",\n+    \"Atlanta\",\n+    \"Denver\",\n+    \"Las Vegas\",\n+]\n+\n+\n+db_url = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n+\n+agent_storage = PostgresStorage(\n+    table_name=\"agent_sessions\", db_url=db_url, auto_upgrade_schema=True\n+)\n+\n+team_storage = PostgresStorage(\n+    table_name=\"team_sessions\", db_url=db_url, auto_upgrade_schema=True\n+)\n+\n+memory_db = PostgresMemoryDb(table_name=\"memory\", db_url=db_url)\n+memory = Memory(db=memory_db)\n+\n+\n+def get_weather(city: str) -> str:\n+    return f\"The weather in {city} is sunny.\"\n+\n+\n+def get_activities(city: str) -> str:\n+    activities = [\n+        \"hiking\",\n+        \"biking\",\n+        \"swimming\",\n+        \"kayaking\",\n+        \"museum visits\",\n+        \"shopping\",\n+        \"sightseeing\",\n+        \"cafe hopping\",\n+        \"theater\",\n+        \"picnicking\",\n+    ]\n+    selected_activities = random.sample(activities, k=3)\n+    return f\"The activities in {city} are {', '.join(selected_activities)}.\"\n+\n+\n+agent_1 = Agent(\n+    agent_id=\"agent_1\",\n+    model=OpenAIChat(id=\"gpt-4o-mini\"),\n+    description=\"You are a helpful assistant that can answer questions about the weather.\",\n+    instructions=\"Be concise, reply with one sentence.\",\n+    tools=[get_weather],\n+    memory=memory,\n+    storage=agent_storage,\n+    add_history_to_messages=True,\n+)\n+\n+agent_2 = Agent(\n+    agent_id=\"agent_2\",\n+    model=OpenAIChat(id=\"gpt-4o-mini\"),\n+    description=\"You are a helpful assistant that can answer questions about activities in a city.\",\n+    instructions=\"Be concise, reply with one sentence.\",\n+    tools=[get_activities],\n+    memory=memory,\n+    storage=agent_storage,\n+    add_history_to_messages=True,\n+)\n+\n+team = Team(\n+    members=[agent_1, agent_2],\n+    model=OpenAIChat(id=\"gpt-4o-mini\"),\n+    instructions=\"Be concise, reply with one sentence.\",\n+    memory=memory,\n+    storage=team_storage,\n+    markdown=True,\n+    enable_user_memories=True,\n+    add_history_to_messages=True,\n+)\n+\n+\n+async def run_team():\n+    async def run_team_for_user(user: str):\n+        random_city = random.choice(cities)\n+        await team.arun(\n+            message=f\"I love {random_city}! What activities and weather can I expect in {random_city}?\",\n+            user_id=user,\n+            session_id=f\"session_{user}\",\n+        )\n+\n+    tasks = []\n+\n+    # Run all 5 users concurrently\n+    for user in users:\n+        tasks.append(run_team_for_user(user))\n+\n+    await asyncio.gather(*tasks)\n+\n+    print(\"Team memory runs:\", len(team.memory.runs))\n+    print(\"Team memory memories:\", len(team.memory.memories))\n+\n+    return \"Successfully ran team\"\n+\n+\n+team_response_with_memory_impact = PerformanceEval(\n+    name=\"Team Memory Impact\",\n+    func=run_team,\n+    num_iterations=5,\n+    warmup_runs=0,\n+    measure_runtime=False,\n+    debug_mode=True,\n+)\n+\n+if __name__ == \"__main__\":\n+    asyncio.run(\n+        team_response_with_memory_impact.arun(\n+            print_results=True, print_summary=True, with_growth_tracking=True\n+        )\n+    )\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/evals/performance/team_response_with_memory_simple.py",
            "diff": "diff --git a/cookbook/evals/performance/team_response_with_memory_simple.py b/cookbook/evals/performance/team_response_with_memory_simple.py\nnew file mode 100644\nindex 000000000..a9c48a295\n--- /dev/null\n+++ b/cookbook/evals/performance/team_response_with_memory_simple.py\n@@ -0,0 +1,95 @@\n+import asyncio\n+import random\n+\n+from agno.agent import Agent\n+from agno.eval.performance import PerformanceEval\n+from agno.memory.v2.db.postgres import PostgresMemoryDb\n+from agno.memory.v2.memory import Memory\n+from agno.models.openai import OpenAIChat\n+from agno.storage.postgres import PostgresStorage\n+from agno.team.team import Team\n+\n+cities = [\n+    \"New York\",\n+    \"Los Angeles\",\n+    \"Chicago\",\n+    \"Houston\",\n+    \"Miami\",\n+    \"San Francisco\",\n+    \"Seattle\",\n+    \"Boston\",\n+    \"Washington D.C.\",\n+    \"Atlanta\",\n+    \"Denver\",\n+    \"Las Vegas\",\n+]\n+\n+\n+db_url = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n+\n+agent_storage = PostgresStorage(\n+    table_name=\"agent_sessions\", db_url=db_url, auto_upgrade_schema=True\n+)\n+\n+team_storage = PostgresStorage(\n+    table_name=\"team_sessions\", db_url=db_url, auto_upgrade_schema=True\n+)\n+\n+memory_db = PostgresMemoryDb(table_name=\"memory\", db_url=db_url)\n+memory = Memory(db=memory_db)\n+\n+\n+def get_weather(city: str) -> str:\n+    return f\"The weather in {city} is sunny.\"\n+\n+\n+weather_agent = Agent(\n+    agent_id=\"weather_agent\",\n+    model=OpenAIChat(id=\"gpt-4o-mini\"),\n+    role=\"Weather Agent\",\n+    description=\"You are a helpful assistant that can answer questions about the weather.\",\n+    instructions=\"Be concise, reply with one sentence.\",\n+    tools=[get_weather],\n+    memory=memory,\n+    storage=agent_storage,\n+    add_history_to_messages=True,\n+)\n+\n+team = Team(\n+    members=[weather_agent],\n+    model=OpenAIChat(id=\"gpt-4o-mini\"),\n+    instructions=\"Be concise, reply with one sentence.\",\n+    memory=memory,\n+    storage=team_storage,\n+    markdown=True,\n+    enable_user_memories=True,\n+    add_history_to_messages=True,\n+)\n+\n+\n+async def run_team():\n+    random_city = random.choice(cities)\n+    await team.arun(\n+        message=f\"I love {random_city}! What weather can I expect in {random_city}?\",\n+        stream=True,\n+        stream_intermediate_steps=True,\n+    )\n+\n+    return \"Successfully ran team\"\n+\n+\n+team_response_with_memory_impact = PerformanceEval(\n+    name=\"Team Memory Impact\",\n+    func=run_team,\n+    num_iterations=5,\n+    warmup_runs=0,\n+    measure_runtime=False,\n+    debug_mode=True,\n+)\n+\n+if __name__ == \"__main__\":\n+    asyncio.run(\n+        team_response_with_memory_impact.arun(\n+            print_results=True, print_summary=True, with_growth_tracking=True\n+        )\n+    )\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/examples/streamlit_apps/agentic_rag/utils.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/agentic_rag/utils.py b/cookbook/examples/streamlit_apps/agentic_rag/utils.py\nindex 25a7b1b74..05afea35a 100644\n--- a/cookbook/examples/streamlit_apps/agentic_rag/utils.py\n+++ b/cookbook/examples/streamlit_apps/agentic_rag/utils.py\n@@ -94,7 +94,7 @@ def display_tool_calls(tool_calls_container, tools: List[ToolExecution]):\n                 if _metrics:\n                     st.markdown(\"**Metrics:**\")\n                     st.json(\n-                        _metrics if isinstance(_metrics, dict) else _metrics._to_dict()\n+                        _metrics if isinstance(_metrics, dict) else _metrics.to_dict()\n                     )\n \n \n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/examples/streamlit_apps/parallel_world_builder/utils.py",
            "diff": "diff --git a/cookbook/examples/streamlit_apps/parallel_world_builder/utils.py b/cookbook/examples/streamlit_apps/parallel_world_builder/utils.py\nindex 14c3c0043..76d227232 100644\n--- a/cookbook/examples/streamlit_apps/parallel_world_builder/utils.py\n+++ b/cookbook/examples/streamlit_apps/parallel_world_builder/utils.py\n@@ -98,5 +98,5 @@ def display_tool_calls(tool_calls_container, tools: List[ToolExecution]):\n                 if _metrics:\n                     st.markdown(\"**Metrics:**\")\n                     st.json(\n-                        _metrics if isinstance(_metrics, dict) else _metrics._to_dict()\n+                        _metrics if isinstance(_metrics, dict) else _metrics.to_dict()\n                     )\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/models/anthropic/structured_output_stream.py",
            "diff": "diff --git a/cookbook/models/anthropic/structured_output_stream.py b/cookbook/models/anthropic/structured_output_stream.py\nnew file mode 100644\nindex 000000000..ed36d652d\n--- /dev/null\n+++ b/cookbook/models/anthropic/structured_output_stream.py\n@@ -0,0 +1,40 @@\n+from typing import Dict, List\n+\n+from agno.agent import Agent\n+from agno.models.anthropic import Claude\n+from pydantic import BaseModel, Field\n+\n+\n+class MovieScript(BaseModel):\n+    setting: str = Field(\n+        ..., description=\"Provide a nice setting for a blockbuster movie.\"\n+    )\n+    ending: str = Field(\n+        ...,\n+        description=\"Ending of the movie. If not available, provide a happy ending.\",\n+    )\n+    genre: str = Field(\n+        ...,\n+        description=\"Genre of the movie. If not available, select action, thriller or romantic comedy.\",\n+    )\n+    name: str = Field(..., description=\"Give a name to this movie\")\n+    characters: List[str] = Field(..., description=\"Name of characters for this movie.\")\n+    storyline: str = Field(\n+        ..., description=\"3 sentence storyline for the movie. Make it exciting!\"\n+    )\n+    rating: Dict[str, int] = Field(\n+        ...,\n+        description=\"Your own rating of the movie. 1-10. Return a dictionary with the keys 'story' and 'acting'.\",\n+    )\n+\n+\n+# Agent that uses structured outputs\n+structured_output_agent = Agent(\n+    model=Claude(id=\"claude-sonnet-4-20250514\"),\n+    description=\"You write movie scripts.\",\n+    response_model=MovieScript,\n+)\n+\n+structured_output_agent.print_response(\n+    \"New York\", stream=True, stream_intermediate_steps=True\n+)\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/models/google/gemini/agent_with_thinking_budget.py",
            "diff": "diff --git a/cookbook/models/google/gemini/agent_with_thinking_budget.py b/cookbook/models/google/gemini/agent_with_thinking_budget.py\nnew file mode 100644\nindex 000000000..9764894d6\n--- /dev/null\n+++ b/cookbook/models/google/gemini/agent_with_thinking_budget.py\n@@ -0,0 +1,25 @@\n+\"\"\"\n+An example of how to use the thinking budget parameter with the Gemini model.\n+This requires `google-genai > 1.10.0`\n+\n+- Turn off thinking use thinking_budget=0\n+- Turn on dynamic thinking use thinking_budget=-1\n+- To use a specific thinking token budget (e.g. 1280) use thinking_budget=1280\n+- Use include_thoughts=True to get the thought summaries in the response.\n+\"\"\"\n+\n+from agno.agent import Agent\n+from agno.models.google import Gemini\n+\n+task = (\n+    \"Three missionaries and three cannibals need to cross a river. \"\n+    \"They have a boat that can carry up to two people at a time. \"\n+    \"If, at any time, the cannibals outnumber the missionaries on either side of the river, the cannibals will eat the missionaries. \"\n+    \"How can all six people get across the river safely? Provide a step-by-step solution and show the solutions as an ascii diagram\"\n+)\n+\n+agent = Agent(\n+    model=Gemini(id=\"gemini-2.5-pro\", thinking_budget=1280, include_thoughts=True),\n+    markdown=True,\n+)\n+agent.print_response(task, stream=True)\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/models/google/gemini/structured_output_stream.py",
            "diff": "diff --git a/cookbook/models/google/gemini/structured_output_stream.py b/cookbook/models/google/gemini/structured_output_stream.py\nnew file mode 100644\nindex 000000000..468b365f2\n--- /dev/null\n+++ b/cookbook/models/google/gemini/structured_output_stream.py\n@@ -0,0 +1,40 @@\n+from typing import Dict, List\n+\n+from agno.agent import Agent\n+from agno.models.google import Gemini\n+from pydantic import BaseModel, Field\n+\n+\n+class MovieScript(BaseModel):\n+    setting: str = Field(\n+        ..., description=\"Provide a nice setting for a blockbuster movie.\"\n+    )\n+    ending: str = Field(\n+        ...,\n+        description=\"Ending of the movie. If not available, provide a happy ending.\",\n+    )\n+    genre: str = Field(\n+        ...,\n+        description=\"Genre of the movie. If not available, select action, thriller or romantic comedy.\",\n+    )\n+    name: str = Field(..., description=\"Give a name to this movie\")\n+    characters: List[str] = Field(..., description=\"Name of characters for this movie.\")\n+    storyline: str = Field(\n+        ..., description=\"3 sentence storyline for the movie. Make it exciting!\"\n+    )\n+    rating: Dict[str, int] = Field(\n+        ...,\n+        description=\"Your own rating of the movie. 1-10. Return a dictionary with the keys 'story' and 'acting'.\",\n+    )\n+\n+\n+# Agent that uses structured outputs\n+structured_output_agent = Agent(\n+    model=Gemini(id=\"gemini-2.0-flash-001\"),\n+    description=\"You write movie scripts.\",\n+    response_model=MovieScript,\n+)\n+\n+structured_output_agent.print_response(\n+    \"New York\", stream=True, stream_intermediate_steps=True\n+)\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/models/openai/chat/async_basic_stream.py",
            "diff": "diff --git a/cookbook/models/openai/chat/async_basic_stream.py b/cookbook/models/openai/chat/async_basic_stream.py\nindex f243cd2a3..b280a761a 100644\n--- a/cookbook/models/openai/chat/async_basic_stream.py\n+++ b/cookbook/models/openai/chat/async_basic_stream.py\n@@ -7,11 +7,11 @@ from agno.models.openai import OpenAIChat\n agent = Agent(model=OpenAIChat(id=\"gpt-4o\"), markdown=True)\n \n # Get the response in a variable\n-run_response: Iterator[RunResponseEvent] = agent.run(\n-    \"Share a 2 sentence horror story\", stream=True\n-)\n-for chunk in run_response:\n-    print(chunk.content, end=\"\")\n+# run_response: Iterator[RunResponseEvent] = agent.run(\n+#     \"Share a 2 sentence horror story\", stream=True\n+# )\n+# for chunk in run_response:\n+#     print(chunk.content, end=\"\")\n \n # # Print the response in the terminal\n-# asyncio.run(agent.aprint_response(\"Share a 2 sentence horror story\", stream=True))\n+asyncio.run(agent.aprint_response(\"Share a 2 sentence horror story\", stream=True))\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/models/xai/live_search_agent.py",
            "diff": "diff --git a/cookbook/models/xai/live_search_agent.py b/cookbook/models/xai/live_search_agent.py\nnew file mode 100644\nindex 000000000..3184505f3\n--- /dev/null\n+++ b/cookbook/models/xai/live_search_agent.py\n@@ -0,0 +1,15 @@\n+from agno.agent import Agent\n+from agno.models.xai.xai import xAI\n+\n+agent = Agent(\n+    model=xAI(\n+        id=\"grok-3\",\n+        search_parameters={\n+            \"mode\": \"on\",\n+            \"max_search_results\": 20,\n+            \"return_citations\": True,\n+        },\n+    ),\n+    markdown=True,\n+)\n+agent.print_response(\"Provide me a digest of world news in the last 24 hours.\")\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/models/xai/live_search_agent_stream.py",
            "diff": "diff --git a/cookbook/models/xai/live_search_agent_stream.py b/cookbook/models/xai/live_search_agent_stream.py\nnew file mode 100644\nindex 000000000..7251c2665\n--- /dev/null\n+++ b/cookbook/models/xai/live_search_agent_stream.py\n@@ -0,0 +1,17 @@\n+from agno.agent import Agent\n+from agno.models.xai.xai import xAI\n+\n+agent = Agent(\n+    model=xAI(\n+        id=\"grok-3\",\n+        search_parameters={\n+            \"mode\": \"on\",\n+            \"max_search_results\": 20,\n+            \"return_citations\": True,\n+        },\n+    ),\n+    markdown=True,\n+)\n+agent.print_response(\n+    \"Provide me a digest of world news in the last 24 hours.\", stream=True\n+)\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/observability/agent_ops.py",
            "diff": "diff --git a/cookbook/observability/agent_ops.py b/cookbook/observability/agent_ops.py\nnew file mode 100644\nindex 000000000..06841bd7f\n--- /dev/null\n+++ b/cookbook/observability/agent_ops.py\n@@ -0,0 +1,25 @@\n+\"\"\"\n+This example shows how to use agentops to log model calls.\n+\n+Steps to get started with agentops:\n+1. Install agentops: pip install agentops\n+2. Obtain an API key from https://app.agentops.ai/\n+3. Export environment variables like AGENTOPS_API_KEY and OPENAI_API_KEY.\n+4. Run the script.\n+\n+You can view the logs in the AgentOps dashboard: https://app.agentops.ai/\n+\"\"\"\n+\n+import agentops\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+\n+# Initialize AgentOps\n+agentops.init()\n+\n+# Create and run an agent\n+agent = Agent(model=OpenAIChat(id=\"gpt-4o\"))\n+response = agent.run(\"Share a 2 sentence horror story\")\n+\n+# Print the response\n+print(response.content)\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/scripts/run_mysql.sh",
            "diff": "diff --git a/cookbook/scripts/run_mysql.sh b/cookbook/scripts/run_mysql.sh\nindex 987fc664c..756bc6e45 100755\n--- a/cookbook/scripts/run_mysql.sh\n+++ b/cookbook/scripts/run_mysql.sh\n@@ -1,10 +1,8 @@\n docker run -d \\\n-  -e MYSQL_ROOT_PASSWORD=agno \\\n-  -e MYSQL_DATABASE=agno \\\n-  -e MYSQL_USER=agno \\\n-  -e MYSQL_PASSWORD=agno \\\n-  -p 3306:3306 \\\n-  -v mysql_data:/var/lib/mysql \\\n-  -v $(pwd)/cookbook/mysql-init:/docker-entrypoint-initdb.d \\\n   --name mysql \\\n-  mysql:8.0\n+  -e MYSQL_ROOT_PASSWORD=ai \\\n+  -e MYSQL_DATABASE=ai \\\n+  -e MYSQL_USER=ai \\\n+  -e MYSQL_PASSWORD=ai \\\n+  -p 3306:3306 \\\n+  -d mysql:8\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/storage/mysql_storage/__init__.py",
            "diff": "diff --git a/cookbook/storage/mysql_storage/__init__.py b/cookbook/storage/mysql_storage/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/storage/mysql_storage/mysql_storage_for_agent.py",
            "diff": "diff --git a/cookbook/storage/mysql_storage/mysql_storage_for_agent.py b/cookbook/storage/mysql_storage/mysql_storage_for_agent.py\nnew file mode 100644\nindex 000000000..967c08b6a\n--- /dev/null\n+++ b/cookbook/storage/mysql_storage/mysql_storage_for_agent.py\n@@ -0,0 +1,19 @@\n+\"\"\"Run `pip install duckduckgo-search sqlalchemy openai` to install dependencies.\"\"\"\n+\n+from agno.agent import Agent\n+from agno.storage.mysql import MySQLStorage\n+\n+db_url = \"mysql+pymysql://ai:ai@localhost:3306/ai\"\n+\n+agent = Agent(\n+    storage=MySQLStorage(\n+        table_name=\"agent_sessions\", db_url=db_url, auto_upgrade_schema=True\n+    ),\n+    add_history_to_messages=True,\n+)\n+agent.print_response(\"How many people live in Canada?\")\n+agent.print_response(\"What is their national anthem called?\")\n+\n+print(\n+    f\"Session IDs created in DB: {agent.storage.get_all_session_ids(entity_id=agent.agent_id)}\"\n+)\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/storage/mysql_storage/mysql_storage_for_team.py",
            "diff": "diff --git a/cookbook/storage/mysql_storage/mysql_storage_for_team.py b/cookbook/storage/mysql_storage/mysql_storage_for_team.py\nnew file mode 100644\nindex 000000000..ab3d25646\n--- /dev/null\n+++ b/cookbook/storage/mysql_storage/mysql_storage_for_team.py\n@@ -0,0 +1,67 @@\n+\"\"\"\n+1. Run: `pip install openai duckduckgo-search newspaper4k lxml_html_clean agno` to install the dependencies\n+2. Run: `python cookbook/storage/mongodb_storage/mongodb_storage_for_team.py` to run the agent\n+\"\"\"\n+\n+from typing import List\n+\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+from agno.storage.mysql import MySQLStorage\n+from agno.team import Team\n+from agno.tools.duckduckgo import DuckDuckGoTools\n+from agno.tools.hackernews import HackerNewsTools\n+from pydantic import BaseModel\n+\n+# MySQL connection settings\n+db_url = \"mysql+pymysql://ai:ai@localhost:3306/ai\"\n+\n+\n+class Article(BaseModel):\n+    title: str\n+    summary: str\n+    reference_links: List[str]\n+\n+\n+hn_researcher = Agent(\n+    name=\"HackerNews Researcher\",\n+    model=OpenAIChat(\"gpt-4o\"),\n+    role=\"Gets top stories from hackernews.\",\n+    tools=[HackerNewsTools()],\n+)\n+\n+web_searcher = Agent(\n+    name=\"Web Searcher\",\n+    model=OpenAIChat(\"gpt-4o\"),\n+    role=\"Searches the web for information on a topic\",\n+    tools=[DuckDuckGoTools()],\n+    add_datetime_to_instructions=True,\n+)\n+\n+\n+hn_team = Team(\n+    name=\"HackerNews Team\",\n+    mode=\"coordinate\",\n+    model=OpenAIChat(\"gpt-4o\"),\n+    members=[hn_researcher, web_searcher],\n+    storage=MySQLStorage(\n+        table_name=\"team_sessions\", db_url=db_url, auto_upgrade_schema=True\n+    ),\n+    instructions=[\n+        \"First, search hackernews for what the user is asking about.\",\n+        \"Then, ask the web searcher to search for each story to get more information.\",\n+        \"Finally, provide a thoughtful and engaging summary.\",\n+    ],\n+    response_model=Article,\n+    show_tool_calls=True,\n+    markdown=True,\n+    show_members_responses=True,\n+    add_member_tools_to_system_message=False,\n+)\n+\n+hn_team.print_response(\"Write an article about the top 2 stories on hackernews\")\n+\n+\n+print(\n+    f\"Session IDs created in DB: {hn_team.storage.get_all_session_ids(entity_id=hn_team.team_id)}\"\n+)\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/storage/mysql_storage/mysql_storage_for_workflow.py",
            "diff": "diff --git a/cookbook/storage/mysql_storage/mysql_storage_for_workflow.py b/cookbook/storage/mysql_storage/mysql_storage_for_workflow.py\nnew file mode 100644\nindex 000000000..49afdd277\n--- /dev/null\n+++ b/cookbook/storage/mysql_storage/mysql_storage_for_workflow.py\n@@ -0,0 +1,94 @@\n+import json\n+from typing import Iterator\n+\n+import httpx\n+from agno.agent import Agent\n+from agno.run.response import RunResponse\n+from agno.storage.mysql import MySQLStorage\n+from agno.tools.newspaper4k import Newspaper4kTools\n+from agno.utils.log import logger\n+from agno.utils.pprint import pprint_run_response\n+from agno.workflow import Workflow\n+\n+db_url = \"mysql+pymysql://ai:ai@localhost:3306/ai\"\n+\n+\n+class HackerNewsReporter(Workflow):\n+    description: str = (\n+        \"Get the top stories from Hacker News and write a report on them.\"\n+    )\n+\n+    hn_agent: Agent = Agent(\n+        description=\"Get the top stories from hackernews. \"\n+        \"Share all possible information, including url, score, title and summary if available.\",\n+        show_tool_calls=True,\n+    )\n+\n+    writer: Agent = Agent(\n+        tools=[Newspaper4kTools()],\n+        description=\"Write an engaging report on the top stories from hackernews.\",\n+        instructions=[\n+            \"You will be provided with top stories and their links.\",\n+            \"Carefully read each article and think about the contents\",\n+            \"Then generate a final New York Times worthy article\",\n+            \"Break the article into sections and provide key takeaways at the end.\",\n+            \"Make sure the title is catchy and engaging.\",\n+            \"Share score, title, url and summary of every article.\",\n+            \"Give the section relevant titles and provide details/facts/processes in each section.\"\n+            \"Ignore articles that you cannot read or understand.\",\n+            \"REMEMBER: you are writing for the New York Times, so the quality of the article is important.\",\n+        ],\n+    )\n+\n+    def get_top_hackernews_stories(self, num_stories: int = 10) -> str:\n+        \"\"\"Use this function to get top stories from Hacker News.\n+\n+        Args:\n+            num_stories (int): Number of stories to return. Defaults to 10.\n+\n+        Returns:\n+            str: JSON string of top stories.\n+        \"\"\"\n+\n+        # Fetch top story IDs\n+        response = httpx.get(\"https://hacker-news.firebaseio.com/v0/topstories.json\")\n+        story_ids = response.json()\n+\n+        # Fetch story details\n+        stories = []\n+        for story_id in story_ids[:num_stories]:\n+            story_response = httpx.get(\n+                f\"https://hacker-news.firebaseio.com/v0/item/{story_id}.json\"\n+            )\n+            story = story_response.json()\n+            story[\"username\"] = story[\"by\"]\n+            stories.append(story)\n+        return json.dumps(stories)\n+\n+    def run(self, num_stories: int = 5) -> Iterator[RunResponse]:\n+        # Set the tools for hn_agent here to avoid circular reference\n+        self.hn_agent.tools = [self.get_top_hackernews_stories]\n+\n+        logger.info(f\"Getting top {num_stories} stories from HackerNews.\")\n+        top_stories: RunResponse = self.hn_agent.run(num_stories=num_stories)\n+        if top_stories is None or not top_stories.content:\n+            yield RunResponse(\n+                run_id=self.run_id, content=\"Sorry, could not get the top stories.\"\n+            )\n+            return\n+\n+        logger.info(\"Reading each story and writing a report.\")\n+        yield from self.writer.run(top_stories.content, stream=True)\n+\n+\n+if __name__ == \"__main__\":\n+    # Run workflow\n+    storage = MySQLStorage(\n+        table_name=\"workflow_sessions\", db_url=db_url, auto_upgrade_schema=True\n+    )\n+    storage.drop()\n+    report: Iterator[RunResponse] = HackerNewsReporter(storage=storage).run(\n+        num_stories=5\n+    )\n+    # Print the report\n+    pprint_run_response(report, markdown=True, show_time=True)\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/teams/team_with_parser_model.py",
            "diff": "diff --git a/cookbook/teams/team_with_parser_model.py b/cookbook/teams/team_with_parser_model.py\nnew file mode 100644\nindex 000000000..9f945572a\n--- /dev/null\n+++ b/cookbook/teams/team_with_parser_model.py\n@@ -0,0 +1,101 @@\n+import random\n+from typing import Iterator, List  # noqa\n+\n+from agno.agent import Agent, RunResponse, RunResponseEvent  # noqa\n+from agno.models.anthropic import Claude\n+from agno.models.openai import OpenAIChat\n+from agno.team import Team\n+from pydantic import BaseModel, Field\n+from rich.pretty import pprint\n+\n+\n+class NationalParkAdventure(BaseModel):\n+    park_name: str = Field(..., description=\"Name of the national park\")\n+    best_season: str = Field(\n+        ...,\n+        description=\"Optimal time of year to visit this park (e.g., 'Late spring to early fall')\",\n+    )\n+    signature_attractions: List[str] = Field(\n+        ...,\n+        description=\"Must-see landmarks, viewpoints, or natural features in the park\",\n+    )\n+    recommended_trails: List[str] = Field(\n+        ...,\n+        description=\"Top hiking trails with difficulty levels (e.g., 'Angel's Landing - Strenuous')\",\n+    )\n+    wildlife_encounters: List[str] = Field(\n+        ..., description=\"Animals visitors are likely to spot, with viewing tips\"\n+    )\n+    photography_spots: List[str] = Field(\n+        ...,\n+        description=\"Best locations for capturing stunning photos, including sunrise/sunset spots\",\n+    )\n+    camping_options: List[str] = Field(\n+        ..., description=\"Available camping areas, from primitive to RV-friendly sites\"\n+    )\n+    safety_warnings: List[str] = Field(\n+        ..., description=\"Important safety considerations specific to this park\"\n+    )\n+    hidden_gems: List[str] = Field(\n+        ..., description=\"Lesser-known spots or experiences that most visitors miss\"\n+    )\n+    difficulty_rating: int = Field(\n+        ...,\n+        ge=1,\n+        le=5,\n+        description=\"Overall park difficulty for average visitor (1=easy, 5=very challenging)\",\n+    )\n+    estimated_days: int = Field(\n+        ...,\n+        ge=1,\n+        le=14,\n+        description=\"Recommended number of days to properly explore the park\",\n+    )\n+    special_permits_needed: List[str] = Field(\n+        default=[],\n+        description=\"Any special permits or reservations required for certain activities\",\n+    )\n+\n+\n+itinerary_planner = Agent(\n+    name=\"Itinerary Planner\",\n+    model=Claude(id=\"claude-sonnet-4-20250514\"),\n+    description=\"You help people plan amazing national park adventures and provide detailed park guides.\",\n+)\n+\n+weather_expert = Agent(\n+    name=\"Weather Expert\",\n+    model=Claude(id=\"claude-sonnet-4-20250514\"),\n+    description=\"You are a weather expert and can provide detailed weather information for a given location.\",\n+)\n+\n+national_park_expert = Team(\n+    model=OpenAIChat(id=\"gpt-4.1\"),\n+    members=[itinerary_planner, weather_expert],\n+    response_model=NationalParkAdventure,\n+    parser_model=OpenAIChat(id=\"gpt-4o\"),\n+)\n+\n+# Get the response in a variable\n+national_parks = [\n+    \"Yellowstone National Park\",\n+    \"Yosemite National Park\",\n+    \"Grand Canyon National Park\",\n+    \"Zion National Park\",\n+    \"Grand Teton National Park\",\n+    \"Rocky Mountain National Park\",\n+    \"Acadia National Park\",\n+    \"Mount Rainier National Park\",\n+    \"Great Smoky Mountains National Park\",\n+    \"Rocky National Park\",\n+]\n+# Get the response in a variable\n+run: RunResponse = national_park_expert.run(\n+    f\"What is the best season to visit {national_parks[random.randint(0, len(national_parks) - 1)]}? Please provide a detailed one week itinerary for a visit to the park.\"\n+)\n+pprint(run.content)\n+\n+# Stream the response\n+# run_events: Iterator[RunResponseEvent] = national_park_expert.run(f\"What is the best season to visit {national_parks[random.randint(0, len(national_parks) - 1)]}? Please provide a detailed one week itinerary for a visit to the park.\", stream=True)\n+# for event in run_events:\n+#     pprint(event)\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/tools/mcp/mem0.py",
            "diff": "diff --git a/cookbook/tools/mcp/mem0.py b/cookbook/tools/mcp/mem0.py\nindex c346419e0..5f21b837b 100644\n--- a/cookbook/tools/mcp/mem0.py\n+++ b/cookbook/tools/mcp/mem0.py\n@@ -4,7 +4,7 @@\n This example demonstrates how to use Agno's MCP integration together with Mem0, to build a personalized code reviewer.\n \n - Run your Mem0 MCP server. Full instructions: https://github.com/mem0ai/mem0-mcp\n-- Run: `pip install agno mcp-sdk` to install the dependencies\n+- Run: `pip install agno mcp` to install the dependencies\n \"\"\"\n \n import asyncio\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/tools/mcp/oxylabs.py",
            "diff": "diff --git a/cookbook/tools/mcp/oxylabs.py b/cookbook/tools/mcp/oxylabs.py\nnew file mode 100644\nindex 000000000..079cd2132\n--- /dev/null\n+++ b/cookbook/tools/mcp/oxylabs.py\n@@ -0,0 +1,31 @@\n+import asyncio\n+import os\n+\n+from agno.agent import Agent\n+from agno.models.google import Gemini\n+from agno.tools.mcp import MCPTools\n+\n+\n+async def run_agent_prompt():\n+    async with MCPTools(\n+        command=\"uvx oxylabs-mcp\",\n+        env={\n+            \"OXYLABS_USERNAME\": os.getenv(\"OXYLABS_USERNAME\"),\n+            \"OXYLABS_PASSWORD\": os.getenv(\"OXYLABS_PASSWORD\"),\n+        },\n+    ) as server:\n+        agent = Agent(\n+            model=Gemini(api_key=os.getenv(\"GEMINI_API_KEY\")),\n+            tools=[server],\n+            instructions=[\"Use MCP tools to fulfill the requests\"],\n+            markdown=True,\n+        )\n+        await agent.aprint_response(\n+            \"Go to oxylabs.io, look for career page, \"\n+            \"go to it and return all job titles in markdown format. \"\n+            \"Don't invent URLs, start from one provided.\"\n+        )\n+\n+\n+if __name__ == \"__main__\":\n+    asyncio.run(run_agent_prompt())\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/tools/mcp/pipedream_auth.py",
            "diff": "diff --git a/cookbook/tools/mcp/pipedream_auth.py b/cookbook/tools/mcp/pipedream_auth.py\nindex c45be63c8..7fbb2bf5e 100644\n--- a/cookbook/tools/mcp/pipedream_auth.py\n+++ b/cookbook/tools/mcp/pipedream_auth.py\n@@ -11,7 +11,7 @@ This is useful if your app is interfacing with the MCP servers in behalf of your\n     - MCP_ACCESS_TOKEN: The access token you previously got\n     - PIPEDREAM_PROJECT_ID: The project id of the Pipedream project you want to use\n     - PIPEDREAM_ENVIRONMENT: The environment of the Pipedream project you want to use\n-3. Install dependencies: pip install agno mcp-sdk\n+3. Install dependencies: pip install agno mcp\n \"\"\"\n \n import asyncio\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/tools/mcp/pipedream_google_calendar.py",
            "diff": "diff --git a/cookbook/tools/mcp/pipedream_google_calendar.py b/cookbook/tools/mcp/pipedream_google_calendar.py\nindex bf059effb..50e570319 100644\n--- a/cookbook/tools/mcp/pipedream_google_calendar.py\n+++ b/cookbook/tools/mcp/pipedream_google_calendar.py\n@@ -3,10 +3,10 @@\n \n This example shows how to use Pipedream MCP servers (in this case the Google Calendar one) with Agno Agents.\n \n-1. Connect your Pipedream and Google Calendar accounts: https://mcp.pipedream.com/app/google-calendar\n-2. Get your Pipedream MCP server url: https://mcp.pipedream.com/app/google-calendar\n+1. Connect your Pipedream and Google Calendar accounts: https://mcp.pipedream.com/app/google_calendar\n+2. Get your Pipedream MCP server url: https://mcp.pipedream.com/app/google_calendar\n 3. Set the MCP_SERVER_URL environment variable to the MCP server url you got above\n-4. Install dependencies: pip install agno mcp-sdk\n+4. Install dependencies: pip install agno mcp\n \"\"\"\n \n import asyncio\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/tools/mcp/pipedream_linkedin.py",
            "diff": "diff --git a/cookbook/tools/mcp/pipedream_linkedin.py b/cookbook/tools/mcp/pipedream_linkedin.py\nindex 6f260d3aa..d9a6f71ec 100644\n--- a/cookbook/tools/mcp/pipedream_linkedin.py\n+++ b/cookbook/tools/mcp/pipedream_linkedin.py\n@@ -6,7 +6,7 @@ This example shows how to use Pipedream MCP servers (in this case the LinkedIn o\n 1. Connect your Pipedream and LinkedIn accounts: https://mcp.pipedream.com/app/linkedin\n 2. Get your Pipedream MCP server url: https://mcp.pipedream.com/app/linkedin\n 3. Set the MCP_SERVER_URL environment variable to the MCP server url you got above\n-4. Install dependencies: pip install agno mcp-sdk\n+4. Install dependencies: pip install agno mcp\n \"\"\"\n \n import asyncio\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/tools/mcp/pipedream_slack.py",
            "diff": "diff --git a/cookbook/tools/mcp/pipedream_slack.py b/cookbook/tools/mcp/pipedream_slack.py\nindex 13992b44f..6a58518da 100644\n--- a/cookbook/tools/mcp/pipedream_slack.py\n+++ b/cookbook/tools/mcp/pipedream_slack.py\n@@ -6,7 +6,7 @@ This example shows how to use Pipedream MCP servers (in this case the Slack one)\n 1. Connect your Pipedream and Slack accounts: https://mcp.pipedream.com/app/slack\n 2. Get your Pipedream MCP server url: https://mcp.pipedream.com/app/slack\n 3. Set the MCP_SERVER_URL environment variable to the MCP server url you got above\n-4. Install dependencies: pip install agno mcp-sdk\n+4. Install dependencies: pip install agno mcp\n \n \"\"\"\n \n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/tools/mcp/stripe.py",
            "diff": "diff --git a/cookbook/tools/mcp/stripe.py b/cookbook/tools/mcp/stripe.py\nindex b14191368..11a8af6be 100644\n--- a/cookbook/tools/mcp/stripe.py\n+++ b/cookbook/tools/mcp/stripe.py\n@@ -6,7 +6,7 @@ This example demonstrates how to create an Agno agent that interacts with the St\n Setup:\n 2. Install Python dependencies:\n    ```bash\n-   pip install agno mcp-sdk\n+   pip install agno mcp\n    ```\n 3. Set Environment Variable: export STRIPE_SECRET_KEY=***.\n \n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/tools/mcp/supabase.py",
            "diff": "diff --git a/cookbook/tools/mcp/supabase.py b/cookbook/tools/mcp/supabase.py\nindex ac0ee01f9..1813b8087 100644\n--- a/cookbook/tools/mcp/supabase.py\n+++ b/cookbook/tools/mcp/supabase.py\n@@ -6,7 +6,7 @@ Setup:\n 1. Install Python dependencies:\n \n ```bash\n-pip install agno mcp-sdk\n+pip install agno mcp\n ```\n \n 2. Create a Supabase Access Token: https://supabase.com/dashboard/account/tokens and set it as the SUPABASE_ACCESS_TOKEN environment variable.\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/tools/oxylabs_tools.py",
            "diff": "diff --git a/cookbook/tools/oxylabs_tools.py b/cookbook/tools/oxylabs_tools.py\nnew file mode 100644\nindex 000000000..fe59298ff\n--- /dev/null\n+++ b/cookbook/tools/oxylabs_tools.py\n@@ -0,0 +1,26 @@\n+from agno.agent import Agent\n+from agno.tools.oxylabs import OxylabsTools\n+\n+agent = Agent(\n+    tools=[OxylabsTools()],\n+    markdown=True,\n+    show_tool_calls=True,\n+)\n+\n+# Example 1: Google Search\n+agent.print_response(\n+    \"Let's search for 'latest iPhone reviews' and provide a summary of the top 3 results. \",\n+)\n+\n+# Example 2: Amazon Product Search\n+# agent.print_response(\n+#     \"Let's search for an Amazon product with ASIN 'B07FZ8S74R' (Echo Dot). \",\n+# )\n+\n+# Example 3: Multi-Domain Amazon Search\n+# agent.print_response(\n+#     \"Use search_amazon_products to search for 'gaming keyboards' on both:\\n\"\n+#     \"1. Amazon US (domain='com')\\n\"\n+#     \"2. Amazon UK (domain='co.uk')\\n\"\n+#     \"Compare the top 3 results from each region including pricing and availability.\"\n+# )\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/tools/scrapegraph_tools.py",
            "diff": "diff --git a/cookbook/tools/scrapegraph_tools.py b/cookbook/tools/scrapegraph_tools.py\nindex a07aa558e..5a1ddfbbb 100644\n--- a/cookbook/tools/scrapegraph_tools.py\n+++ b/cookbook/tools/scrapegraph_tools.py\n@@ -25,3 +25,23 @@ agent_md = Agent(tools=[scrapegraph_md], show_tool_calls=True, markdown=True)\n agent_md.print_response(\n     \"Fetch and convert https://www.wired.com/category/science/ to markdown format\"\n )\n+\n+# Example 3: Enable searchscraper\n+scrapegraph_search = ScrapeGraphTools(searchscraper=True)\n+\n+agent_search = Agent(tools=[scrapegraph_search], show_tool_calls=True, markdown=True)\n+\n+# Use searchscraper\n+agent_search.print_response(\n+    \"Use searchscraper to find the CEO of company X and their contact details from https://example.com\"\n+)\n+\n+# Example 4: Enable crawl\n+scrapegraph_crawl = ScrapeGraphTools(crawl=True)\n+\n+agent_crawl = Agent(tools=[scrapegraph_crawl], show_tool_calls=True, markdown=True)\n+\n+# Use crawl (schema must be provided as a dict in the tool call)\n+agent_crawl.print_response(\n+    \"Use crawl to extract what the company does and get text content from privacy and terms from https://scrapegraphai.com/ with a suitable schema.\"\n+)\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/tools/serper_tools.py",
            "diff": "diff --git a/cookbook/tools/serper_tools.py b/cookbook/tools/serper_tools.py\nindex 272510a25..26cf2762d 100644\n--- a/cookbook/tools/serper_tools.py\n+++ b/cookbook/tools/serper_tools.py\n@@ -1,9 +1,33 @@\n \"\"\"\n-This is a simple example of how to use the SerperTools class. You can obtain an API key from https://serper.dev/\n+This is a example of an agent using the Serper Toolkit.\n+\n+You can obtain an API key from https://serper.dev/\n+\n+ - Set your API key as an environment variable: export SERPER_API_KEY=\"your_api_key_here\"\n+ - or pass api_key to the SerperTools class\n \"\"\"\n \n from agno.agent import Agent\n from agno.tools.serper import SerperTools\n \n-agent = Agent(tools=[SerperTools(location=\"us\")], show_tool_calls=True)\n-agent.print_response(\"Whats happening in the USA?\", markdown=True)\n+agent = Agent(\n+    tools=[SerperTools()],\n+    show_tool_calls=True,\n+)\n+\n+agent.print_response(\n+    \"Search for the latest news about artificial intelligence developments\",\n+    markdown=True,\n+)\n+\n+# Example 2: Google Scholar Search\n+# agent.print_response(\n+#     \"Find 2 recent academic papers about large language model safety and alignment\",\n+#     markdown=True,\n+# )\n+\n+# Example 3: Web Scraping\n+# agent.print_response(\n+#     \"Scrape and summarize the main content from this OpenAI blog post: https://openai.com/index/gpt-4/\",\n+#     markdown=True\n+# )\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/tools/valyu_tools.py",
            "diff": "diff --git a/cookbook/tools/valyu_tools.py b/cookbook/tools/valyu_tools.py\nnew file mode 100644\nindex 000000000..98fbeb618\n--- /dev/null\n+++ b/cookbook/tools/valyu_tools.py\n@@ -0,0 +1,41 @@\n+\"\"\"\n+This cookbook demonstrates how to use the Valyu Toolkit for academic and web search.\n+\n+Prerequisites:\n+- Install: pip install valyu\n+- Get API key: https://platform.valyu.network\n+- Set environment variable: export VALYU_API_KEY with your api key or pass the api key while initializing the toolkit\n+\"\"\"\n+\n+from agno.agent import Agent\n+from agno.tools.valyu import ValyuTools\n+\n+agent = Agent(\n+    tools=[ValyuTools()],\n+    show_tool_calls=True,\n+    markdown=True,\n+)\n+\n+# Example 1: Basic Academic Paper Search\n+agent.print_response(\n+    \"What are the latest safety mechanisms and mitigation strategies for CRISPR off-target effects?\",\n+    markdown=True,\n+)\n+\n+# Example 2: Focused ArXiv Search with Date Filtering\n+agent.print_response(\n+    \"Search for transformer architecture papers published between June 2023 and January 2024, focusing on attention mechanisms\",\n+    markdown=True,\n+)\n+\n+# Example 3: Search Within Specific Paper\n+agent.print_response(\n+    \"Search within the paper https://arxiv.org/abs/1706.03762 for details about the multi-head attention mechanism architecture\",\n+    markdown=True,\n+)\n+\n+# Example 4: Search Web\n+agent.print_response(\n+    \"What are the main developments in large language model reasoning capabilities published in 2024?\",\n+    markdown=True,\n+)\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "cookbook/workflows/workflows_playground.py",
            "diff": "diff --git a/cookbook/workflows/workflows_playground.py b/cookbook/workflows/workflows_playground.py\nindex aac842b74..dfc6f18d8 100644\n--- a/cookbook/workflows/workflows_playground.py\n+++ b/cookbook/workflows/workflows_playground.py\n@@ -3,7 +3,7 @@\n 2. Run the script using: `python cookbook/workflows/workflows_playground.py`\n \"\"\"\n \n-from agno.playground import Playground, serve_playground_app\n+from agno.playground import Playground\n from agno.storage.sqlite import SqliteStorage\n \n # Import the workflows\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/agent/agent.py",
            "diff": "diff --git a/libs/agno/agno/agent/agent.py b/libs/agno/agno/agent/agent.py\nindex 09ce91570..42320ae53 100644\n--- a/libs/agno/agno/agent/agent.py\n+++ b/libs/agno/agno/agent/agent.py\n@@ -53,6 +53,8 @@ from agno.tools.toolkit import Toolkit\n from agno.utils.events import (\n     create_memory_update_completed_event,\n     create_memory_update_started_event,\n+    create_parser_model_response_completed_event,\n+    create_parser_model_response_started_event,\n     create_reasoning_completed_event,\n     create_reasoning_started_event,\n     create_reasoning_step_event,\n@@ -322,6 +324,9 @@ class Agent:\n     # --- Debug & Monitoring ---\n     # Enable debug logs\n     debug_mode: bool = False\n+    # Debug level: 1 = basic, 2 = detailed\n+    debug_level: Literal[1, 2] = 1\n+\n     # monitoring=True logs Agent information to agno.com for monitoring\n     monitoring: bool = False\n     # telemetry=True logs minimal telemetry for analytics\n@@ -415,6 +420,7 @@ class Agent:\n         add_transfer_instructions: bool = True,\n         team_response_separator: str = \"\\n\",\n         debug_mode: bool = False,\n+        debug_level: Literal[1, 2] = 1,\n         monitoring: bool = False,\n         telemetry: bool = True,\n     ):\n@@ -525,6 +531,10 @@ class Agent:\n         self.team_response_separator = team_response_separator\n \n         self.debug_mode = debug_mode\n+        if debug_level not in [1, 2]:\n+            log_warning(f\"Invalid debug level: {debug_level}. Setting to 1.\")\n+            debug_level = 1\n+        self.debug_level = debug_level\n         self.monitoring = monitoring\n         self.telemetry = telemetry\n \n@@ -562,7 +572,7 @@ class Agent:\n     def set_debug(self) -> None:\n         if self.debug_mode or getenv(\"AGNO_DEBUG\", \"false\").lower() == \"true\":\n             self.debug_mode = True\n-            set_log_level_to_debug()\n+            set_log_level_to_debug(level=self.debug_level)\n         else:\n             set_log_level_to_info()\n \n@@ -642,7 +652,7 @@ class Agent:\n             from copy import deepcopy\n \n             # We store a copy of memory to ensure different team instances reference unique memory copy\n-            if isinstance(self.memory, Memory):\n+            if isinstance(self.memory, Memory) and self.team_id is not None:\n                 self.memory = deepcopy(self.memory)\n             self._memory_deepcopy_done = True\n \n@@ -666,7 +676,7 @@ class Agent:\n \n     @property\n     def should_parse_structured_output(self) -> bool:\n-        return self.response_model is not None and self.parse_response\n+        return self.response_model is not None and self.parse_response and self.parser_model is None\n \n     def add_tool(self, tool: Union[Toolkit, Callable, Function, Dict]):\n         if not self.tools:\n@@ -718,27 +728,7 @@ class Agent:\n         )\n \n         # If a parser model is provided, structure the response separately\n-        if self.parser_model is not None:\n-            if self.response_model is not None:\n-                parser_response_format = self._get_response_format(self.parser_model)\n-                messages_for_parser_model = self.get_messages_for_parser_model(model_response, parser_response_format)\n-                parser_model_response: ModelResponse = self.parser_model.response(\n-                    messages=messages_for_parser_model,\n-                    response_format=parser_response_format,\n-                )\n-                parser_model_response_message: Optional[Message] = None\n-                for message in reversed(messages_for_parser_model):\n-                    if message.role == \"assistant\":\n-                        parser_model_response_message = message\n-                        break\n-                if parser_model_response_message is not None:\n-                    run_messages.messages.append(parser_model_response_message)\n-                    model_response.parsed = parser_model_response.parsed\n-                    model_response.content = parser_model_response.content\n-                else:\n-                    log_warning(\"Unable to parse response with parser model\")\n-            else:\n-                log_warning(\"A response model is required to parse the response with a parser model\")\n+        self._parse_response_with_parser_model(model_response, run_messages)\n \n         self._update_run_response(model_response=model_response, run_response=run_response, run_messages=run_messages)\n \n@@ -826,6 +816,11 @@ class Agent:\n         ):\n             yield event\n \n+        # If a parser model is provided, structure the response separately\n+        yield from self._parse_response_with_parser_model_stream(\n+            run_response=run_response, stream_intermediate_steps=stream_intermediate_steps\n+        )\n+\n         # 3. Add the run to memory\n         self._add_run_to_memory(\n             run_response=run_response,\n@@ -855,9 +850,6 @@ class Agent:\n         # 6. Save output to file if save_response_to_file is set\n         self.save_run_response_to_file(message=run_messages.user_message, session_id=session_id)\n \n-        # Convert the response to the structured format if needed\n-        self._convert_response_to_structured_format(run_response)\n-\n         if stream_intermediate_steps:\n             yield self._handle_event(create_run_response_completed_event(from_run_response=run_response), run_response)\n \n@@ -1163,27 +1155,7 @@ class Agent:\n         )\n \n         # If a parser model is provided, structure the response separately\n-        if self.parser_model is not None:\n-            if self.response_model is not None:\n-                parser_response_format = self._get_response_format(self.parser_model)\n-                messages_for_parser_model = self.get_messages_for_parser_model(model_response, parser_response_format)\n-                parser_model_response: ModelResponse = await self.parser_model.aresponse(\n-                    messages=messages_for_parser_model,\n-                    response_format=parser_response_format,\n-                )\n-                parser_model_response_message: Optional[Message] = None\n-                for message in reversed(messages_for_parser_model):\n-                    if message.role == \"assistant\":\n-                        parser_model_response_message = message\n-                        break\n-                if parser_model_response_message is not None:\n-                    run_messages.messages.append(parser_model_response_message)\n-                    model_response.parsed = parser_model_response.parsed\n-                    model_response.content = parser_model_response.content\n-                else:\n-                    log_warning(\"Unable to parse response with parser model\")\n-            else:\n-                log_warning(\"A response model is required to parse the response with a parser model\")\n+        await self._aparse_response_with_parser_model(model_response=model_response, run_messages=run_messages)\n \n         self._update_run_response(model_response=model_response, run_response=run_response, run_messages=run_messages)\n \n@@ -1270,6 +1242,12 @@ class Agent:\n         ):\n             yield event\n \n+        # If a parser model is provided, structure the response separately\n+        async for event in self._aparse_response_with_parser_model_stream(\n+            run_response=run_response, stream_intermediate_steps=stream_intermediate_steps\n+        ):\n+            yield event\n+\n         # 3. Add the run to memory\n         self._add_run_to_memory(\n             run_response=run_response,\n@@ -1301,9 +1279,6 @@ class Agent:\n         # 6. Save output to file if save_response_to_file is set\n         self.save_run_response_to_file(message=run_messages.user_message, session_id=session_id)\n \n-        # Convert the response to the structured format if needed\n-        self._convert_response_to_structured_format(run_response)\n-\n         if stream_intermediate_steps:\n             yield self._handle_event(create_run_response_completed_event(from_run_response=run_response), run_response)\n \n@@ -1936,9 +1911,6 @@ class Agent:\n         # 6. Save output to file if save_response_to_file is set\n         self.save_run_response_to_file(message=run_messages.user_message, session_id=session_id)\n \n-        # Convert the response to the structured format if needed\n-        self._convert_response_to_structured_format(run_response)\n-\n         if stream_intermediate_steps:\n             yield self._handle_event(create_run_response_completed_event(run_response), run_response)\n \n@@ -2338,9 +2310,6 @@ class Agent:\n         # 6. Save output to file if save_response_to_file is set\n         self.save_run_response_to_file(message=run_messages.user_message, session_id=session_id)\n \n-        # Convert the response to the structured format if needed\n-        self._convert_response_to_structured_format(run_response)\n-\n         if stream_intermediate_steps:\n             yield self._handle_event(create_run_response_completed_event(run_response), run_response)\n \n@@ -2577,7 +2546,7 @@ class Agent:\n                     _t.tool_call_error = True\n                 _t.requires_confirmation = False\n \n-                # Case 2: Handle external execution required tools\n+            # Case 2: Handle external execution required tools\n             elif _t.external_execution_required is not None and _t.external_execution_required is True:\n                 self._handle_external_execution_update(run_messages=run_messages, tool=_t)\n \n@@ -3004,8 +2973,9 @@ class Agent:\n                 run_response=run_response,\n                 model_response=model_response,\n                 model_response_event=model_response_event,\n-                stream_intermediate_steps=stream_intermediate_steps,\n                 reasoning_state=reasoning_state,\n+                parse_structured_output=self.should_parse_structured_output,\n+                stream_intermediate_steps=stream_intermediate_steps,\n             )\n \n         # Determine reasoning completed\n@@ -3072,8 +3042,9 @@ class Agent:\n                 run_response=run_response,\n                 model_response=model_response,\n                 model_response_event=model_response_event,\n-                stream_intermediate_steps=stream_intermediate_steps,\n                 reasoning_state=reasoning_state,\n+                parse_structured_output=self.should_parse_structured_output,\n+                stream_intermediate_steps=stream_intermediate_steps,\n             ):\n                 yield event\n \n@@ -3110,7 +3081,8 @@ class Agent:\n         run_response: RunResponse,\n         model_response: ModelResponse,\n         model_response_event: Union[ModelResponse, RunResponseEvent, TeamRunResponseEvent],\n-        reasoning_state: Dict[str, Any],\n+        reasoning_state: Optional[Dict[str, Any]] = None,\n+        parse_structured_output: bool = False,\n         stream_intermediate_steps: bool = False,\n     ) -> Iterator[RunResponseEvent]:\n         if isinstance(model_response_event, tuple(get_args(RunResponseEvent))) or isinstance(\n@@ -3126,12 +3098,13 @@ class Agent:\n \n                 # Process content and thinking\n                 if model_response_event.content is not None:\n-                    if self.should_parse_structured_output:\n+                    if parse_structured_output:\n                         model_response.content = model_response_event.content\n+                        self._convert_response_to_structured_format(model_response)\n+\n                         content_type = self.response_model.__name__  # type: ignore\n                         run_response.content = model_response.content\n                         run_response.content_type = content_type\n-                        self._convert_response_to_structured_format(model_response)\n                     else:\n                         model_response.content = (model_response.content or \"\") + model_response_event.content\n                         run_response.content = model_response.content\n@@ -3291,7 +3264,7 @@ class Agent:\n                             reasoning_step = self.update_reasoning_content_from_tool_call(tool_name, tool_args)\n \n                             metrics = tool_call.metrics\n-                            if metrics is not None and metrics.time is not None:\n+                            if metrics is not None and metrics.time is not None and reasoning_state is not None:\n                                 reasoning_state[\"reasoning_time_taken\"] = reasoning_state[\n                                     \"reasoning_time_taken\"\n                                 ] + float(metrics.time)\n@@ -3304,7 +3277,7 @@ class Agent:\n \n                 if stream_intermediate_steps:\n                     if reasoning_step is not None:\n-                        if not reasoning_state[\"reasoning_started\"]:\n+                        if reasoning_state and not reasoning_state[\"reasoning_started\"]:\n                             yield self._handle_event(\n                                 create_reasoning_started_event(from_run_response=run_response), run_response\n                             )\n@@ -3769,13 +3742,13 @@ class Agent:\n         )\n \n     def _get_response_format(self, model: Optional[Model] = None) -> Optional[Union[Dict, Type[BaseModel]]]:\n-        self.model = cast(Model, model or self.model)\n+        model = cast(Model, model or self.model)\n         if self.response_model is None:\n             return None\n         else:\n             json_response_format = {\"type\": \"json_object\"}\n \n-            if self.model.supports_native_structured_outputs:\n+            if model.supports_native_structured_outputs:\n                 if not self.use_json_mode or self.structured_outputs:\n                     log_debug(\"Setting Model.response_format to Agent.response_model\")\n                     return self.response_model\n@@ -3785,7 +3758,7 @@ class Agent:\n                     )\n                     return json_response_format\n \n-            elif self.model.supports_json_schema_outputs:\n+            elif model.supports_json_schema_outputs:\n                 if self.use_json_mode or (not self.structured_outputs):\n                     log_debug(\"Setting Model.response_format to JSON response mode\")\n                     return {\n@@ -4888,7 +4861,7 @@ class Agent:\n         system_content = (\n             self.parser_model_prompt\n             if self.parser_model_prompt is not None\n-            else \"You are tasked with creating a structured output from the provided data.\"\n+            else \"You are tasked with creating a structured output from the provided user message.\"\n         )\n \n         if response_format == {\"type\": \"json_object\"} and self.response_model is not None:\n@@ -4899,6 +4872,24 @@ class Agent:\n             Message(role=\"user\", content=model_response.content),\n         ]\n \n+    def get_messages_for_parser_model_stream(\n+        self, run_response: RunResponse, response_format: Optional[Union[Dict, Type[BaseModel]]]\n+    ) -> List[Message]:\n+        \"\"\"Get the messages for the parser model.\"\"\"\n+        system_content = (\n+            self.parser_model_prompt\n+            if self.parser_model_prompt is not None\n+            else \"You are tasked with creating a structured output from the provided data.\"\n+        )\n+\n+        if response_format == {\"type\": \"json_object\"} and self.response_model is not None:\n+            system_content += f\"{get_json_output_prompt(self.response_model)}\"  # type: ignore\n+\n+        return [\n+            Message(role=\"system\", content=system_content),\n+            Message(role=\"user\", content=run_response.content),\n+        ]\n+\n     def get_session_summary(self, session_id: Optional[str] = None, user_id: Optional[str] = None):\n         \"\"\"Get the session summary for the given session ID and user ID.\"\"\"\n         if self.memory is None:\n@@ -4969,11 +4960,11 @@ class Agent:\n         from copy import copy, deepcopy\n \n         # For memory and reasoning_agent, use their deep_copy methods\n-        if field_name in (\"memory\", \"reasoning_agent\"):\n+        if field_name == \"reasoning_agent\":\n             return field_value.deep_copy()\n \n         # For storage, model and reasoning_model, use a deep copy\n-        elif field_name in (\"storage\", \"model\", \"reasoning_model\"):\n+        elif field_name in (\"memory\", \"storage\", \"model\", \"reasoning_model\"):\n             try:\n                 return deepcopy(field_value)\n             except Exception:\n@@ -5662,6 +5653,7 @@ class Agent:\n                 monitoring=self.monitoring,\n                 telemetry=self.telemetry,\n                 debug_mode=self.debug_mode,\n+                debug_level=self.debug_level,\n             )\n             is_deepseek = is_deepseek_reasoning_model(reasoning_model)\n             is_groq = is_groq_reasoning_model(reasoning_model)\n@@ -5750,6 +5742,7 @@ class Agent:\n                     monitoring=self.monitoring,\n                     telemetry=self.telemetry,\n                     debug_mode=self.debug_mode,\n+                    debug_level=self.debug_level,\n                 )\n \n             # Validate reasoning agent\n@@ -5883,6 +5876,7 @@ class Agent:\n                 monitoring=self.monitoring,\n                 telemetry=self.telemetry,\n                 debug_mode=self.debug_mode,\n+                debug_level=self.debug_level,\n             )\n             is_deepseek = is_deepseek_reasoning_model(reasoning_model)\n             is_groq = is_groq_reasoning_model(reasoning_model)\n@@ -5971,6 +5965,7 @@ class Agent:\n                     monitoring=self.monitoring,\n                     telemetry=self.telemetry,\n                     debug_mode=self.debug_mode,\n+                    debug_level=self.debug_level,\n                 )\n \n             # Validate reasoning agent\n@@ -6068,6 +6063,154 @@ class Agent:\n                     self.run_response,\n                 )\n \n+    def _process_parser_response(\n+        self,\n+        model_response: ModelResponse,\n+        run_messages: RunMessages,\n+        parser_model_response: ModelResponse,\n+        messages_for_parser_model: list,\n+    ) -> None:\n+        \"\"\"Common logic for processing parser model response.\"\"\"\n+        parser_model_response_message: Optional[Message] = None\n+        for message in reversed(messages_for_parser_model):\n+            if message.role == \"assistant\":\n+                parser_model_response_message = message\n+                break\n+\n+        if parser_model_response_message is not None:\n+            run_messages.messages.append(parser_model_response_message)\n+            model_response.parsed = parser_model_response.parsed\n+            model_response.content = parser_model_response.content\n+        else:\n+            log_warning(\"Unable to parse response with parser model\")\n+\n+    def _parse_response_with_parser_model(self, model_response: ModelResponse, run_messages: RunMessages) -> None:\n+        \"\"\"Parse the model response using the parser model.\"\"\"\n+        if self.parser_model is None:\n+            return\n+\n+        if self.response_model is not None:\n+            parser_response_format = self._get_response_format(self.parser_model)\n+            messages_for_parser_model = self.get_messages_for_parser_model(model_response, parser_response_format)\n+            parser_model_response: ModelResponse = self.parser_model.response(\n+                messages=messages_for_parser_model,\n+                response_format=parser_response_format,\n+            )\n+            self._process_parser_response(\n+                model_response, run_messages, parser_model_response, messages_for_parser_model\n+            )\n+        else:\n+            log_warning(\"A response model is required to parse the response with a parser model\")\n+\n+    async def _aparse_response_with_parser_model(\n+        self, model_response: ModelResponse, run_messages: RunMessages\n+    ) -> None:\n+        \"\"\"Parse the model response using the parser model.\"\"\"\n+        if self.parser_model is None:\n+            return\n+\n+        if self.response_model is not None:\n+            parser_response_format = self._get_response_format(self.parser_model)\n+            messages_for_parser_model = self.get_messages_for_parser_model(model_response, parser_response_format)\n+            parser_model_response: ModelResponse = await self.parser_model.aresponse(\n+                messages=messages_for_parser_model,\n+                response_format=parser_response_format,\n+            )\n+            self._process_parser_response(\n+                model_response, run_messages, parser_model_response, messages_for_parser_model\n+            )\n+        else:\n+            log_warning(\"A response model is required to parse the response with a parser model\")\n+\n+    def _parse_response_with_parser_model_stream(\n+        self, run_response: RunResponse, stream_intermediate_steps: bool = True\n+    ):\n+        \"\"\"Parse the model response using the parser model\"\"\"\n+        if self.parser_model is not None:\n+            if self.response_model is not None:\n+                if stream_intermediate_steps:\n+                    yield self._handle_event(create_parser_model_response_started_event(run_response), run_response)\n+\n+                parser_model_response = ModelResponse(content=\"\")\n+                parser_response_format = self._get_response_format(self.parser_model)\n+                messages_for_parser_model = self.get_messages_for_parser_model_stream(\n+                    run_response, parser_response_format\n+                )\n+                for model_response_event in self.parser_model.response_stream(\n+                    messages=messages_for_parser_model,\n+                    response_format=parser_response_format,\n+                    stream_model_response=False,\n+                ):\n+                    yield from self._handle_model_response_chunk(\n+                        run_response=run_response,\n+                        model_response=parser_model_response,\n+                        model_response_event=model_response_event,\n+                        parse_structured_output=True,\n+                        stream_intermediate_steps=stream_intermediate_steps,\n+                    )\n+\n+                parser_model_response_message: Optional[Message] = None\n+                for message in reversed(messages_for_parser_model):\n+                    if message.role == \"assistant\":\n+                        parser_model_response_message = message\n+                        break\n+                if parser_model_response_message is not None:\n+                    if run_response.messages is not None:\n+                        run_response.messages.append(parser_model_response_message)\n+                else:\n+                    log_warning(\"Unable to parse response with parser model\")\n+\n+                if stream_intermediate_steps:\n+                    yield self._handle_event(create_parser_model_response_completed_event(run_response), run_response)\n+\n+            else:\n+                log_warning(\"A response model is required to parse the response with a parser model\")\n+\n+    async def _aparse_response_with_parser_model_stream(\n+        self, run_response: RunResponse, stream_intermediate_steps: bool = True\n+    ):\n+        \"\"\"Parse the model response using the parser model stream.\"\"\"\n+        if self.parser_model is not None:\n+            if self.response_model is not None:\n+                if stream_intermediate_steps:\n+                    yield self._handle_event(create_parser_model_response_started_event(run_response), run_response)\n+\n+                parser_model_response = ModelResponse(content=\"\")\n+                parser_response_format = self._get_response_format(self.parser_model)\n+                messages_for_parser_model = self.get_messages_for_parser_model_stream(\n+                    run_response, parser_response_format\n+                )\n+                model_response_stream = self.parser_model.aresponse_stream(\n+                    messages=messages_for_parser_model,\n+                    response_format=parser_response_format,\n+                    stream_model_response=False,\n+                )\n+                async for model_response_event in model_response_stream:  # type: ignore\n+                    for event in self._handle_model_response_chunk(\n+                        run_response=run_response,\n+                        model_response=parser_model_response,\n+                        model_response_event=model_response_event,\n+                        parse_structured_output=True,\n+                        stream_intermediate_steps=stream_intermediate_steps,\n+                    ):\n+                        yield event\n+\n+                parser_model_response_message: Optional[Message] = None\n+                for message in reversed(messages_for_parser_model):\n+                    if message.role == \"assistant\":\n+                        parser_model_response_message = message\n+                        break\n+                if parser_model_response_message is not None:\n+                    if run_response.messages is not None:\n+                        run_response.messages.append(parser_model_response_message)\n+                else:\n+                    log_warning(\"Unable to parse response with parser model\")\n+\n+                if stream_intermediate_steps:\n+                    yield self._handle_event(create_parser_model_response_completed_event(run_response), run_response)\n+            else:\n+                log_warning(\"A response model is required to parse the response with a parser model\")\n+\n     def _handle_event(self, event: RunResponseEvent, run_response: RunResponse):\n         # We only store events that are not run_response_content events\n         events_to_skip = [event.value for event in self.events_to_skip] if self.events_to_skip else []\n@@ -6627,13 +6770,13 @@ class Agent:\n \n         if self.response_model is not None:\n             self.markdown = False\n-            stream = False\n \n         stream_intermediate_steps = stream_intermediate_steps or self.stream_intermediate_steps\n         stream = stream or self.stream or False\n         if stream:\n             _response_content: str = \"\"\n             _response_thinking: str = \"\"\n+            response_content_batch: Union[str, JSON, Markdown] = \"\"\n             reasoning_steps: List[ReasoningStep] = []\n \n             with Live(console=console) as live_log:\n@@ -6686,12 +6829,16 @@ class Agent:\n                                     _response_content += resp.content\n                                 elif self.response_model is not None and isinstance(resp.content, BaseModel):\n                                     try:\n-                                        _response_content += JSON(  # type: ignore\n+                                        response_content_batch = JSON(  # type: ignore\n                                             resp.content.model_dump_json(exclude_none=True), indent=2\n                                         )\n                                     except Exception as e:\n                                         log_warning(f\"Failed to convert response to JSON: {e}\")\n-\n+                                else:\n+                                    try:\n+                                        response_content_batch = JSON(json.dumps(resp.content), indent=4)\n+                                    except Exception as e:\n+                                        log_warning(f\"Failed to convert response to JSON: {e}\")\n                             if hasattr(resp, \"thinking\") and resp.thinking is not None:\n                                 _response_thinking += resp.thinking\n                         if (\n@@ -6701,12 +6848,11 @@ class Agent:\n                         ):\n                             reasoning_steps = resp.extra_data.reasoning_steps\n \n-                    response_content_stream: Union[str, Markdown] = _response_content\n-\n+                    response_content_stream: str = _response_content\n                     # Escape special tags before markdown conversion\n                     if self.markdown:\n                         escaped_content = escape_markdown_tags(_response_content, tags_to_include_in_markdown)\n-                        response_content_stream = Markdown(escaped_content)\n+                        response_content_batch = Markdown(escaped_content)\n                     panels = [status]\n \n                     if message and show_message:\n@@ -6784,18 +6930,30 @@ class Agent:\n                             border_style=\"yellow\",\n                         )\n                         panels.append(tool_calls_panel)\n+                        live_log.update(Group(*panels))\n+\n+                    response_panel = None\n+                    # Check if we have any response content to display\n+                    if response_content_stream and not self.markdown:\n+                        response_content = response_content_stream\n+                    else:\n+                        response_content = response_content_batch  # type: ignore\n+\n+                    # Sanitize empty Markdown content\n+                    if isinstance(response_content, Markdown):\n+                        if not (response_content.markup and response_content.markup.strip()):\n+                            response_content = None  # type: ignore\n \n-                    if len(_response_content) > 0:\n+                    if response_content:\n                         render = True\n-                        # Create panel for response\n                         response_panel = create_panel(\n-                            content=response_content_stream,\n+                            content=response_content,\n                             title=f\"Response ({response_timer.elapsed:.1f}s)\",\n                             border_style=\"blue\",\n                         )\n                         panels.append(response_panel)\n-                    if render:\n-                        live_log.update(Group(*panels))\n+                        if render:\n+                            live_log.update(Group(*panels))\n \n                     if (\n                         isinstance(resp, tuple(get_args(RunResponseEvent)))\n@@ -6948,7 +7106,7 @@ class Agent:\n                     panels.append(tool_calls_panel)\n                     live_log.update(Group(*panels))\n \n-                response_content_batch: Union[str, JSON, Markdown] = \"\"\n+                response_content_batch: Union[str, JSON, Markdown] = \"\"  # type: ignore\n                 if isinstance(run_response, RunResponse):\n                     if isinstance(run_response.content, str):\n                         if self.markdown:\n@@ -7058,7 +7216,6 @@ class Agent:\n \n         if self.response_model is not None:\n             self.markdown = False\n-            stream = False\n \n         stream_intermediate_steps = stream_intermediate_steps or self.stream_intermediate_steps\n         stream = stream or self.stream or False\n@@ -7066,6 +7223,7 @@ class Agent:\n             _response_content: str = \"\"\n             _response_thinking: str = \"\"\n             reasoning_steps: List[ReasoningStep] = []\n+            response_content_batch: Union[str, JSON, Markdown] = \"\"\n \n             with Live(console=console) as live_log:\n                 status = Status(\"Thinking...\", spinner=\"aesthetic\", speed=0.4, refresh_per_second=10)\n@@ -7118,7 +7276,14 @@ class Agent:\n                                 _response_content += resp.content\n                             elif self.response_model is not None and isinstance(resp.content, BaseModel):\n                                 try:\n-                                    _response_content += JSON(resp.content.model_dump_json(exclude_none=True), indent=2)  # type: ignore\n+                                    response_content_batch = JSON(\n+                                        resp.content.model_dump_json(exclude_none=True), indent=2\n+                                    )  # type: ignore\n+                                except Exception as e:\n+                                    log_warning(f\"Failed to convert response to JSON: {e}\")\n+                            else:\n+                                try:\n+                                    response_content_batch = JSON(json.dumps(resp.content), indent=4)\n                                 except Exception as e:\n                                     log_warning(f\"Failed to convert response to JSON: {e}\")\n                             if resp.thinking is not None:\n@@ -7131,11 +7296,11 @@ class Agent:\n                         ):\n                             reasoning_steps = resp.extra_data.reasoning_steps\n \n-                    response_content_stream: Union[str, Markdown] = _response_content\n+                    response_content_stream: str = _response_content\n                     # Escape special tags before markdown conversion\n                     if self.markdown:\n                         escaped_content = escape_markdown_tags(_response_content, tags_to_include_in_markdown)\n-                        response_content_stream = Markdown(escaped_content)\n+                        response_content_batch = Markdown(escaped_content)\n \n                     panels = [status]\n \n@@ -7216,11 +7381,22 @@ class Agent:\n                         panels.append(tool_calls_panel)\n                         live_log.update(Group(*panels))\n \n-                    if len(_response_content) > 0:\n+                    response_panel = None\n+                    # Check if we have any response content to display\n+                    if response_content_stream and not self.markdown:\n+                        response_content = response_content_stream\n+                    else:\n+                        response_content = response_content_batch  # type: ignore\n+\n+                    # Sanitize empty Markdown content\n+                    if isinstance(response_content, Markdown):\n+                        if not (response_content.markup and response_content.markup.strip()):\n+                            response_content = None  # type: ignore\n+\n+                    if response_content:\n                         render = True\n-                        # Create panel for response\n                         response_panel = create_panel(\n-                            content=response_content_stream,\n+                            content=response_content,\n                             title=f\"Response ({response_timer.elapsed:.1f}s)\",\n                             border_style=\"blue\",\n                         )\n@@ -7376,7 +7552,7 @@ class Agent:\n                     panels.append(tool_calls_panel)\n                     live_log.update(Group(*panels))\n \n-                response_content_batch: Union[str, JSON, Markdown] = \"\"\n+                response_content_batch: Union[str, JSON, Markdown] = \"\"  # type: ignore\n                 if isinstance(run_response, RunResponse):\n                     if isinstance(run_response.content, str):\n                         if self.markdown:\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/app/playground/async_router.py",
            "diff": "diff --git a/libs/agno/agno/app/playground/async_router.py b/libs/agno/agno/app/playground/async_router.py\nindex ee540af0e..0e9407249 100644\n--- a/libs/agno/agno/app/playground/async_router.py\n+++ b/libs/agno/agno/app/playground/async_router.py\n@@ -1,5 +1,4 @@\n import json\n-from dataclasses import asdict\n from io import BytesIO\n from typing import Any, AsyncGenerator, Dict, List, Optional, cast\n from uuid import uuid4\n@@ -650,7 +649,7 @@ def get_async_playground_router(\n             else:\n                 # Return as a streaming response\n                 return StreamingResponse(\n-                    (json.dumps(asdict(result)) for result in new_workflow_instance.run(**body.input)),\n+                    (result.to_json() for result in new_workflow_instance.run(**body.input)),\n                     media_type=\"text/event-stream\",\n                 )\n         except Exception as e:\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/app/playground/sync_router.py",
            "diff": "diff --git a/libs/agno/agno/app/playground/sync_router.py b/libs/agno/agno/app/playground/sync_router.py\nindex 4d2b18f12..67e7edaa3 100644\n--- a/libs/agno/agno/app/playground/sync_router.py\n+++ b/libs/agno/agno/app/playground/sync_router.py\n@@ -1,5 +1,4 @@\n import json\n-from dataclasses import asdict\n from io import BytesIO\n from typing import Any, Dict, Generator, List, Optional, cast\n from uuid import uuid4\n@@ -644,7 +643,7 @@ def get_sync_playground_router(\n             else:\n                 # Return as a streaming response\n                 return StreamingResponse(\n-                    (json.dumps(asdict(result)) for result in new_workflow_instance.run(**body.input)),\n+                    (result.to_json() for result in new_workflow_instance.run(**body.input)),\n                     media_type=\"text/event-stream\",\n                 )\n         except Exception as e:\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/embedder/fastembed.py",
            "diff": "diff --git a/libs/agno/agno/embedder/fastembed.py b/libs/agno/agno/embedder/fastembed.py\nindex 5a00a6c84..daf201cf1 100644\n--- a/libs/agno/agno/embedder/fastembed.py\n+++ b/libs/agno/agno/embedder/fastembed.py\n@@ -4,6 +4,13 @@ from typing import Dict, List, Optional, Tuple\n from agno.embedder.base import Embedder\n from agno.utils.log import logger\n \n+try:\n+    import numpy as np\n+\n+except ImportError:\n+    raise ImportError(\"numpy not installed, use `pip install numpy`\")\n+\n+\n try:\n     from fastembed import TextEmbedding  # type: ignore\n \n@@ -22,6 +29,8 @@ class FastEmbedEmbedder(Embedder):\n         model = TextEmbedding(model_name=self.id)\n         embeddings = model.embed(text)\n         embedding_list = list(embeddings)[0]\n+        if isinstance(embedding_list, np.ndarray):\n+            return embedding_list.tolist()\n \n         try:\n             return list(embedding_list)\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/embedder/sentence_transformer.py",
            "diff": "diff --git a/libs/agno/agno/embedder/sentence_transformer.py b/libs/agno/agno/embedder/sentence_transformer.py\nindex ca1a1d622..0f0cc7077 100644\n--- a/libs/agno/agno/embedder/sentence_transformer.py\n+++ b/libs/agno/agno/embedder/sentence_transformer.py\n@@ -10,6 +10,12 @@ try:\n except ImportError:\n     raise ImportError(\"`sentence-transformers` not installed, please run `pip install sentence-transformers`\")\n \n+try:\n+    import numpy as np\n+\n+except ImportError:\n+    raise ImportError(\"numpy not installed, use `pip install numpy`\")\n+\n \n @dataclass\n class SentenceTransformerEmbedder(Embedder):\n@@ -26,6 +32,9 @@ class SentenceTransformerEmbedder(Embedder):\n             model = self.sentence_transformer_client\n         embedding = model.encode(text, prompt=self.prompt, normalize_embeddings=self.normalize_embeddings)\n         try:\n+            if isinstance(embedding, np.ndarray):\n+                return embedding.tolist()\n+\n             return embedding  # type: ignore\n         except Exception as e:\n             logger.warning(e)\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/eval/performance.py",
            "diff": "diff --git a/libs/agno/agno/eval/performance.py b/libs/agno/agno/eval/performance.py\nindex 391b63707..5951858fd 100644\n--- a/libs/agno/agno/eval/performance.py\n+++ b/libs/agno/agno/eval/performance.py\n@@ -8,7 +8,7 @@ from uuid import uuid4\n \n from agno.api.schemas.evals import EvalType\n from agno.eval.utils import async_log_eval_run, log_eval_run, store_result_in_file\n-from agno.utils.log import logger\n+from agno.utils.log import log_debug, set_log_level_to_debug, set_log_level_to_info\n from agno.utils.timer import Timer\n \n if TYPE_CHECKING:\n@@ -96,7 +96,9 @@ class PerformanceResult:\n             self.median_memory_usage = 0\n             self.p95_memory_usage = 0\n \n-    def print_summary(self, console: Optional[\"Console\"] = None):\n+    def print_summary(\n+        self, console: Optional[\"Console\"] = None, measure_memory: bool = True, measure_runtime: bool = True\n+    ):\n         \"\"\"\n         Prints a summary table of the computed stats.\n         \"\"\"\n@@ -109,20 +111,39 @@ class PerformanceResult:\n         # Create performance table\n         perf_table = Table(title=\"Performance Summary\", show_header=True, header_style=\"bold magenta\")\n         perf_table.add_column(\"Metric\", style=\"cyan\")\n-        perf_table.add_column(\"Time (seconds)\", style=\"green\")\n-        perf_table.add_column(\"Memory (MiB)\", style=\"yellow\")\n+        if measure_runtime:\n+            perf_table.add_column(\"Time (seconds)\", style=\"green\")\n+        if measure_memory:\n+            perf_table.add_column(\"Memory (MiB)\", style=\"yellow\")\n \n         # Add rows\n-        perf_table.add_row(\"Average\", f\"{self.avg_run_time:.6f}\", f\"{self.avg_memory_usage:.6f}\")\n-        perf_table.add_row(\"Minimum\", f\"{self.min_run_time:.6f}\", f\"{self.min_memory_usage:.6f}\")\n-        perf_table.add_row(\"Maximum\", f\"{self.max_run_time:.6f}\", f\"{self.max_memory_usage:.6f}\")\n-        perf_table.add_row(\"Std Dev\", f\"{self.std_dev_run_time:.6f}\", f\"{self.std_dev_memory_usage:.6f}\")\n-        perf_table.add_row(\"Median\", f\"{self.median_run_time:.6f}\", f\"{self.median_memory_usage:.6f}\")\n-        perf_table.add_row(\"95th %ile\", f\"{self.p95_run_time:.6f}\", f\"{self.p95_memory_usage:.6f}\")\n+        if measure_runtime and measure_memory:\n+            perf_table.add_row(\"Average\", f\"{self.avg_run_time:.6f}\", f\"{self.avg_memory_usage:.6f}\")\n+            perf_table.add_row(\"Minimum\", f\"{self.min_run_time:.6f}\", f\"{self.min_memory_usage:.6f}\")\n+            perf_table.add_row(\"Maximum\", f\"{self.max_run_time:.6f}\", f\"{self.max_memory_usage:.6f}\")\n+            perf_table.add_row(\"Std Dev\", f\"{self.std_dev_run_time:.6f}\", f\"{self.std_dev_memory_usage:.6f}\")\n+            perf_table.add_row(\"Median\", f\"{self.median_run_time:.6f}\", f\"{self.median_memory_usage:.6f}\")\n+            perf_table.add_row(\"95th %ile\", f\"{self.p95_run_time:.6f}\", f\"{self.p95_memory_usage:.6f}\")\n+        elif measure_runtime:\n+            perf_table.add_row(\"Average\", f\"{self.avg_run_time:.6f}\")\n+            perf_table.add_row(\"Minimum\", f\"{self.min_run_time:.6f}\")\n+            perf_table.add_row(\"Maximum\", f\"{self.max_run_time:.6f}\")\n+            perf_table.add_row(\"Std Dev\", f\"{self.std_dev_run_time:.6f}\")\n+            perf_table.add_row(\"Median\", f\"{self.median_run_time:.6f}\")\n+            perf_table.add_row(\"95th %ile\", f\"{self.p95_run_time:.6f}\")\n+        elif measure_memory:\n+            perf_table.add_row(\"Average\", f\"{self.avg_memory_usage:.6f}\")\n+            perf_table.add_row(\"Minimum\", f\"{self.min_memory_usage:.6f}\")\n+            perf_table.add_row(\"Maximum\", f\"{self.max_memory_usage:.6f}\")\n+            perf_table.add_row(\"Std Dev\", f\"{self.std_dev_memory_usage:.6f}\")\n+            perf_table.add_row(\"Median\", f\"{self.median_memory_usage:.6f}\")\n+            perf_table.add_row(\"95th %ile\", f\"{self.p95_memory_usage:.6f}\")\n \n         console.print(perf_table)\n \n-    def print_results(self, console: Optional[\"Console\"] = None):\n+    def print_results(\n+        self, console: Optional[\"Console\"] = None, measure_memory: bool = True, measure_runtime: bool = True\n+    ):\n         \"\"\"\n         Prints individual run results in tabular form.\n         \"\"\"\n@@ -135,12 +156,21 @@ class PerformanceResult:\n         # Create runs table\n         results_table = Table(title=\"Individual Runs\", show_header=True, header_style=\"bold magenta\")\n         results_table.add_column(\"Run #\", style=\"cyan\")\n-        results_table.add_column(\"Time (seconds)\", style=\"green\")\n-        results_table.add_column(\"Memory (MiB)\", style=\"yellow\")\n+        if measure_runtime:\n+            results_table.add_column(\"Time (seconds)\", style=\"green\")\n+        if measure_memory:\n+            results_table.add_column(\"Memory (MiB)\", style=\"yellow\")\n \n         # Add rows\n-        for i in range(len(self.run_times)):\n-            results_table.add_row(str(i + 1), f\"{self.run_times[i]:.6f}\", f\"{self.memory_usages[i]:.6f}\")\n+        for i in range(len(self.run_times) or len(self.memory_usages)):\n+            if measure_runtime and measure_memory:\n+                results_table.add_row(str(i + 1), f\"{self.run_times[i]:.6f}\", f\"{self.memory_usages[i]:.6f}\")\n+            elif measure_runtime:\n+                results_table.add_row(str(i + 1), f\"{self.run_times[i]:.6f}\")\n+            elif measure_memory:\n+                results_table.add_row(str(i + 1), f\"{self.memory_usages[i]:.6f}\")\n+            else:\n+                results_table.add_row(str(i + 1))\n \n         console.print(results_table)\n \n@@ -175,6 +205,12 @@ class PerformanceEval:\n     print_summary: bool = False\n     # Print detailed results\n     print_results: bool = False\n+    # Print detailed memory growth analysis\n+    with_growth_tracking: bool = False\n+\n+    # Number of memory allocations to track\n+    top_n_memory_allocations: int = 5\n+\n     # If set, results will be saved in the given file path\n     file_path_to_save_results: Optional[str] = None\n     # Enable debug logs\n@@ -189,6 +225,7 @@ class PerformanceEval:\n         timer.start()\n         self.func()\n         timer.stop()\n+        self._set_log_level()  # Set log level incase function changed it\n \n         return timer.elapsed\n \n@@ -199,6 +236,7 @@ class PerformanceEval:\n         timer.start()\n         await self.func()\n         timer.stop()\n+        self._set_log_level()  # Set log level incase function changed it\n \n         return timer.elapsed\n \n@@ -217,15 +255,19 @@ class PerformanceEval:\n \n         # Get peak memory usage\n         current, peak = tracemalloc.get_traced_memory()\n+\n         # Stop tracing memory\n         tracemalloc.stop()\n \n+        self._set_log_level()  # Set log level incase function changed it\n+\n         # Convert to MiB and subtract baseline\n         peak_mib = peak / 1024 / 1024\n         adjusted_usage = max(0, peak_mib - baseline)\n \n         if self.debug_mode:\n-            logger.debug(f\"[DEBUG] Raw peak usage: {peak_mib:.6f} MiB, Adjusted: {adjusted_usage:.6f} MiB\")\n+            log_debug(f\"[DEBUG] Raw peak usage: {peak_mib:.6f} MiB, Adjusted: {adjusted_usage:.6f} MiB\")\n+\n         return adjusted_usage\n \n     def _parse_eval_run_data(self) -> dict:\n@@ -269,15 +311,18 @@ class PerformanceEval:\n \n         # Get peak memory usage\n         current, peak = tracemalloc.get_traced_memory()\n+\n         # Stop tracing memory\n         tracemalloc.stop()\n+        self._set_log_level()  # Set log level incase function changed it\n \n         # Convert to MiB and subtract baseline\n         peak_mib = peak / 1024 / 1024\n         adjusted_usage = max(0, peak_mib - baseline)\n \n         if self.debug_mode:\n-            logger.debug(f\"[DEBUG] Raw peak usage: {peak_mib:.6f} MiB, Adjusted: {adjusted_usage:.6f} MiB\")\n+            log_debug(f\"[DEBUG] Raw peak usage: {peak_mib:.6f} MiB, Adjusted: {adjusted_usage:.6f} MiB\")\n+\n         return adjusted_usage\n \n     def _compute_tracemalloc_baseline(self, samples: int = 3) -> float:\n@@ -300,7 +345,131 @@ class PerformanceEval:\n \n         return sum(results) / len(results) if results else 0\n \n-    def run(self, *, print_summary: bool = False, print_results: bool = False) -> PerformanceResult:\n+    def _set_log_level(self):\n+        if self.debug_mode:\n+            set_log_level_to_debug()\n+        else:\n+            set_log_level_to_info()\n+\n+    def _compare_memory_snapshots(self, snapshot1, snapshot2, top_n: int):\n+        \"\"\"\n+        Compare two memory snapshots to identify what's causing memory growth.\n+        \"\"\"\n+        if self.debug_mode:\n+            log_debug(\"[DEBUG] Memory growth analysis:\")\n+\n+            # Compare snapshots to find new allocations\n+            stats = snapshot2.compare_to(snapshot1, \"lineno\")\n+\n+            log_debug(f\"[DEBUG] Top {top_n} memory growth sources:\")\n+            growth_found = False\n+            for stat in stats[:top_n]:\n+                if stat.size_diff > 0:  # Only show growth\n+                    growth_found = True\n+                    log_debug(f\"  +{stat.size_diff / 1024 / 1024:.1f} MiB: {stat.count_diff} new blocks\")\n+                    log_debug(f\"    {stat.traceback.format()}\")\n+\n+            if not growth_found:\n+                log_debug(\"  No significant memory growth detected between snapshots\")\n+\n+            # Show total growth\n+            total_growth = sum(stat.size_diff for stat in stats if stat.size_diff > 0)\n+            total_shrinkage = sum(abs(stat.size_diff) for stat in stats if stat.size_diff < 0)\n+            log_debug(f\"[DEBUG] Total memory growth: {total_growth / 1024 / 1024:.1f} MiB\")\n+            log_debug(f\"[DEBUG] Total memory freed: {total_shrinkage / 1024 / 1024:.1f} MiB\")\n+            log_debug(f\"[DEBUG] Net memory change: {(total_growth - total_shrinkage) / 1024 / 1024:.1f} MiB\")\n+\n+    def _measure_memory_with_growth_tracking(\n+        self, baseline: float, previous_snapshot=None\n+    ) -> tuple[float, tracemalloc.Snapshot]:\n+        \"\"\"\n+        Enhanced memory measurement that tracks growth between runs.\n+        Returns (adjusted_usage, current_snapshot)\n+        \"\"\"\n+        # Clear memory before measurement\n+        gc.collect()\n+        # Start tracing memory\n+        tracemalloc.start()\n+\n+        self.func()\n+\n+        # Get peak memory usage\n+        current, peak = tracemalloc.get_traced_memory()\n+        # Take snapshot before stopping\n+        current_snapshot = tracemalloc.take_snapshot()\n+        # Stop tracing memory\n+        tracemalloc.stop()\n+\n+        self._set_log_level()  # Set log level incase function changed it\n+\n+        # Convert to MiB and subtract baseline\n+        peak_mib = peak / 1024 / 1024\n+        adjusted_usage = max(0, peak_mib - baseline)\n+\n+        if self.debug_mode and current_snapshot:\n+            log_debug(f\"[DEBUG] Raw peak usage: {peak_mib:.6f} MiB, Adjusted: {adjusted_usage:.6f} MiB\")\n+\n+            # Compare with previous snapshot if available\n+            if previous_snapshot is not None:\n+                self._compare_memory_snapshots(previous_snapshot, current_snapshot, self.top_n_memory_allocations)\n+            else:\n+                # Get detailed memory allocation statistics\n+                top_stats = current_snapshot.statistics(\"lineno\")\n+\n+                log_debug(f\"[DEBUG] Top {self.top_n_memory_allocations} memory allocations:\")\n+                for stat in top_stats[: self.top_n_memory_allocations]:\n+                    log_debug(f\"  {stat.count} blocks: {stat.size / 1024 / 1024:.1f} MiB\")\n+                    log_debug(f\"    {stat.traceback.format()}\")\n+\n+        return adjusted_usage, current_snapshot\n+\n+    async def _async_measure_memory_with_growth_tracking(\n+        self, baseline: float, previous_snapshot=None\n+    ) -> tuple[float, tracemalloc.Snapshot]:\n+        \"\"\"\n+        Enhanced async memory measurement that tracks growth between runs.\n+        Returns (adjusted_usage, current_snapshot)\n+        \"\"\"\n+        # Clear memory before measurement\n+        gc.collect()\n+        # Start tracing memory\n+        tracemalloc.start()\n+\n+        await self.func()\n+\n+        # Get peak memory usage\n+        current, peak = tracemalloc.get_traced_memory()\n+        # Take snapshot before stopping\n+        current_snapshot = tracemalloc.take_snapshot()\n+        # Stop tracing memory\n+        tracemalloc.stop()\n+\n+        self._set_log_level()  # Set log level incase function changed it\n+\n+        # Convert to MiB and subtract baseline\n+        peak_mib = peak / 1024 / 1024\n+        adjusted_usage = max(0, peak_mib - baseline)\n+\n+        if self.debug_mode and current_snapshot:\n+            log_debug(f\"[DEBUG] Raw peak usage: {peak_mib:.6f} MiB, Adjusted: {adjusted_usage:.6f} MiB\")\n+\n+            # Compare with previous snapshot if available\n+            if previous_snapshot is not None:\n+                self._compare_memory_snapshots(previous_snapshot, current_snapshot, self.top_n_memory_allocations)\n+            else:\n+                # Get detailed memory allocation statistics\n+                top_stats = current_snapshot.statistics(\"lineno\")\n+\n+                log_debug(f\"[DEBUG] Top {self.top_n_memory_allocations} memory allocations:\")\n+                for stat in top_stats[: self.top_n_memory_allocations]:\n+                    log_debug(f\"  {stat.count} blocks: {stat.size / 1024 / 1024:.1f} MiB\")\n+                    log_debug(f\"    {stat.traceback.format()}\")\n+\n+        return adjusted_usage, current_snapshot\n+\n+    def run(\n+        self, *, print_summary: bool = False, print_results: bool = False, with_growth_tracking: bool = False\n+    ) -> PerformanceResult:\n         \"\"\"\n         Main method to run the performance evaluation.\n         1. Do optional warm-up runs.\n@@ -317,8 +486,11 @@ class PerformanceEval:\n \n         run_times = []\n         memory_usages = []\n+        previous_snapshot = None\n \n-        logger.debug(f\"************ Evaluation Start: {self.eval_id} ************\")\n+        self._set_log_level()\n+\n+        log_debug(f\"************ Evaluation Start: {self.eval_id} ************\")\n \n         # Add a spinner while running the evaluations\n         console = Console()\n@@ -328,6 +500,7 @@ class PerformanceEval:\n                 status = Status(f\"Warm-up run {i + 1}/{self.warmup_runs}...\", spinner=\"dots\", speed=1.0)\n                 live_log.update(status)\n                 self.func()  # Simply run the function without measuring\n+                self._set_log_level()  # Set log level incase function changed it\n                 status.stop()\n \n             # 2. Measure runtime\n@@ -344,7 +517,7 @@ class PerformanceEval:\n                     elapsed_time = self._measure_time()\n                     run_times.append(elapsed_time)\n \n-                    logger.debug(f\"Run {i + 1} - Time taken: {elapsed_time:.6f} seconds\")\n+                    log_debug(f\"Run {i + 1} - Time taken: {elapsed_time:.6f} seconds\")\n                     status.stop()\n \n             # 3. Measure memory\n@@ -352,7 +525,7 @@ class PerformanceEval:\n                 # 3.1 Compute memory baseline\n                 memory_baseline = 0.0\n                 memory_baseline = self._compute_tracemalloc_baseline()\n-                logger.debug(f\"Computed memory baseline: {memory_baseline:.6f} MiB\")\n+                log_debug(f\"Computed memory baseline: {memory_baseline:.6f} MiB\")\n \n                 for i in range(self.num_iterations):\n                     status = Status(\n@@ -364,9 +537,18 @@ class PerformanceEval:\n                     live_log.update(status)\n \n                     # Measure memory\n-                    usage = self._measure_memory(memory_baseline)\n+                    if self.with_growth_tracking or with_growth_tracking:\n+                        usage, current_snapshot = self._measure_memory_with_growth_tracking(\n+                            memory_baseline, previous_snapshot\n+                        )\n+\n+                        # Update previous snapshot for next iteration\n+                        previous_snapshot = current_snapshot\n+\n+                    else:\n+                        usage = self._measure_memory(memory_baseline)\n                     memory_usages.append(usage)\n-                    logger.debug(f\"Run {i + 1} - Memory usage: {usage:.6f} MiB (adjusted)\")\n+                    log_debug(f\"Run {i + 1} - Memory usage: {usage:.6f} MiB (adjusted)\")\n \n                     status.stop()\n \n@@ -384,9 +566,9 @@ class PerformanceEval:\n \n         # 6. Print results if requested\n         if self.print_results or print_results:\n-            self.result.print_results(console)\n+            self.result.print_results(console, measure_memory=self.measure_memory, measure_runtime=self.measure_runtime)\n         if self.print_summary or print_summary:\n-            self.result.print_summary(console)\n+            self.result.print_summary(console, measure_memory=self.measure_memory, measure_runtime=self.measure_runtime)\n \n         # 7. Log results to the Agno platform if requested\n         if self.monitoring:\n@@ -398,10 +580,12 @@ class PerformanceEval:\n                 evaluated_entity_name=self.func.__name__,\n             )\n \n-        logger.debug(f\"*********** Evaluation End: {self.eval_id} ***********\")\n+        log_debug(f\"*********** Evaluation End: {self.eval_id} ***********\")\n         return self.result\n \n-    async def arun(self, *, print_summary: bool = False, print_results: bool = False) -> PerformanceResult:\n+    async def arun(\n+        self, *, print_summary: bool = False, print_results: bool = False, with_growth_tracking: bool = False\n+    ) -> PerformanceResult:\n         \"\"\"\n         Async method to run the performance evaluation of async functions.\n         1. Do optional warm-up runs.\n@@ -424,8 +608,11 @@ class PerformanceEval:\n \n         run_times = []\n         memory_usages = []\n+        previous_snapshot = None\n+\n+        self._set_log_level()\n \n-        logger.debug(f\"************ Evaluation Start: {self.eval_id} ************\")\n+        log_debug(f\"************ Evaluation Start: {self.eval_id} ************\")\n \n         # Add a spinner while running the evaluations\n         console = Console()\n@@ -435,6 +622,7 @@ class PerformanceEval:\n                 status = Status(f\"Warm-up run {i + 1}/{self.warmup_runs}...\", spinner=\"dots\", speed=1.0)\n                 live_log.update(status)\n                 await self.func()  # Simply run the function without measuring\n+                self._set_log_level()  # Set log level incase function changed it\n                 status.stop()\n \n             # 2. Measure runtime\n@@ -451,7 +639,7 @@ class PerformanceEval:\n                     elapsed_time = await self._async_measure_time()\n                     run_times.append(elapsed_time)\n \n-                    logger.debug(f\"Run {i + 1} - Time taken: {elapsed_time:.6f} seconds\")\n+                    log_debug(f\"Run {i + 1} - Time taken: {elapsed_time:.6f} seconds\")\n                     status.stop()\n \n             # 3. Measure memory\n@@ -459,7 +647,7 @@ class PerformanceEval:\n                 # 3.1 Compute memory baseline\n                 memory_baseline = 0.0\n                 memory_baseline = self._compute_tracemalloc_baseline()\n-                logger.debug(f\"Computed memory baseline: {memory_baseline:.6f} MiB\")\n+                log_debug(f\"Computed memory baseline: {memory_baseline:.6f} MiB\")\n \n                 for i in range(self.num_iterations):\n                     status = Status(\n@@ -471,9 +659,18 @@ class PerformanceEval:\n                     live_log.update(status)\n \n                     # Measure memory\n-                    usage = await self._async_measure_memory(memory_baseline)\n+                    if self.with_growth_tracking or with_growth_tracking:\n+                        usage, current_snapshot = await self._async_measure_memory_with_growth_tracking(\n+                            memory_baseline, previous_snapshot\n+                        )\n+\n+                        # Update previous snapshot for next iteration\n+                        previous_snapshot = current_snapshot\n+\n+                    else:\n+                        usage = await self._async_measure_memory(memory_baseline)\n                     memory_usages.append(usage)\n-                    logger.debug(f\"Run {i + 1} - Memory usage: {usage:.6f} MiB (adjusted)\")\n+                    log_debug(f\"Run {i + 1} - Memory usage: {usage:.6f} MiB (adjusted)\")\n \n                     status.stop()\n \n@@ -491,9 +688,9 @@ class PerformanceEval:\n \n         # 6. Print results if requested\n         if self.print_results or print_results:\n-            self.result.print_results(console)\n+            self.result.print_results(console, measure_memory=self.measure_memory, measure_runtime=self.measure_runtime)\n         if self.print_summary or print_summary:\n-            self.result.print_summary(console)\n+            self.result.print_summary(console, measure_memory=self.measure_memory, measure_runtime=self.measure_runtime)\n \n         # 7. Log results to the Agno platform if requested\n         if self.monitoring:\n@@ -505,5 +702,5 @@ class PerformanceEval:\n                 evaluated_entity_name=self.func.__name__,\n             )\n \n-        logger.debug(f\"*********** Evaluation End: {self.eval_id} ***********\")\n+        log_debug(f\"*********** Evaluation End: {self.eval_id} ***********\")\n         return self.result\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/memory/agent.py",
            "diff": "diff --git a/libs/agno/agno/memory/agent.py b/libs/agno/agno/memory/agent.py\nindex c5ab8f73a..e3192abd0 100644\n--- a/libs/agno/agno/memory/agent.py\n+++ b/libs/agno/agno/memory/agent.py\n@@ -404,24 +404,20 @@ class AgentMemory(BaseModel):\n         self.summary = None\n         self.memories = None\n \n-    def deep_copy(self) -> \"AgentMemory\":\n+    def __deepcopy__(self, memo):\n         from copy import deepcopy\n \n-        # Create a shallow copy of the object\n-        copied_obj = self.__class__(**self.to_dict())\n-\n-        # Manually deepcopy fields that are known to be safe\n-        for field_name, field_value in self.__dict__.items():\n-            if field_name not in [\"db\", \"classifier\", \"manager\", \"summarizer\"]:\n-                try:\n-                    setattr(copied_obj, field_name, deepcopy(field_value))\n-                except Exception as e:\n-                    logger.warning(f\"Failed to deepcopy field: {field_name} - {e}\")\n-                    setattr(copied_obj, field_name, field_value)\n-\n-        copied_obj.db = self.db\n-        copied_obj.classifier = self.classifier\n-        copied_obj.manager = self.manager\n-        copied_obj.summarizer = self.summarizer\n+        # Create a new instance without calling __init__\n+        cls = self.__class__\n+        copied_obj = cls.__new__(cls)\n+        memo[id(self)] = copied_obj\n+\n+        # Deep copy attributes\n+        for k, v in self.__dict__.items():\n+            # Reuse db\n+            if k in {\"db\", \"classifier\", \"manager\", \"summarizer\"}:\n+                setattr(copied_obj, k, v)\n+            else:\n+                setattr(copied_obj, k, deepcopy(v, memo))\n \n         return copied_obj\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/memory/v2/memory.py",
            "diff": "diff --git a/libs/agno/agno/memory/v2/memory.py b/libs/agno/agno/memory/v2/memory.py\nindex ebf5ee991..13229a398 100644\n--- a/libs/agno/agno/memory/v2/memory.py\n+++ b/libs/agno/agno/memory/v2/memory.py\n@@ -190,6 +190,7 @@ class Memory:\n                 all_memories = self.db.read_memories()\n             else:\n                 all_memories = self.db.read_memories(user_id=user_id)\n+\n             # Reset the memories\n             self.memories = {}\n             for memory in all_memories:\n@@ -238,13 +239,12 @@ class Memory:\n         return _memory_dict\n \n     # -*- Public Functions\n-    def get_user_memories(self, user_id: Optional[str] = None, refresh_from_db: bool = True) -> List[UserMemory]:\n+    def get_user_memories(self, user_id: Optional[str] = None) -> List[UserMemory]:\n         \"\"\"Get the user memories for a given user id\"\"\"\n         if user_id is None:\n             user_id = \"default\"\n-        # Refresh from the DB\n-        if refresh_from_db:\n-            self.refresh_from_db(user_id=user_id)\n+\n+        self.refresh_from_db(user_id=user_id)\n \n         if self.memories is None:\n             return []\n@@ -254,19 +254,16 @@ class Memory:\n         \"\"\"Get the session summaries for a given user id\"\"\"\n         if user_id is None:\n             user_id = \"default\"\n+        self.refresh_from_db(user_id=user_id)\n         if self.summaries is None:\n             return []\n         return list(self.summaries.get(user_id, {}).values())\n \n-    def get_user_memory(\n-        self, memory_id: str, user_id: Optional[str] = None, refresh_from_db: bool = True\n-    ) -> Optional[UserMemory]:\n+    def get_user_memory(self, memory_id: str, user_id: Optional[str] = None) -> Optional[UserMemory]:\n         \"\"\"Get the user memory for a given user id\"\"\"\n         if user_id is None:\n             user_id = \"default\"\n-        # Refresh from the DB\n-        if refresh_from_db:\n-            self.refresh_from_db(user_id=user_id)\n+        self.refresh_from_db(user_id=user_id)\n         if self.memories is None:\n             return None\n         return self.memories.get(user_id, {}).get(memory_id, None)\n@@ -986,27 +983,6 @@ class Memory:\n         self.summaries = {}\n         self.runs = {}\n \n-    def deep_copy(self) -> \"Memory\":\n-        from copy import deepcopy\n-\n-        # Create a shallow copy of the object\n-        copied_obj = self.__class__(**self.to_dict())\n-\n-        # Manually deepcopy fields that are known to be safe\n-        for field_name, field_value in self.__dict__.items():\n-            if field_name not in [\"db\", \"memory_manager\", \"summary_manager\"]:\n-                try:\n-                    setattr(copied_obj, field_name, deepcopy(field_value))\n-                except Exception as e:\n-                    logger.warning(f\"Failed to deepcopy field: {field_name} - {e}\")\n-                    setattr(copied_obj, field_name, field_value)\n-\n-        copied_obj.db = self.db\n-        copied_obj.memory_manager = self.memory_manager\n-        copied_obj.summary_manager = self.summary_manager\n-\n-        return copied_obj\n-\n     # -*- Team Functions\n     def add_interaction_to_team_context(\n         self, session_id: str, member_name: str, task: str, run_response: Union[RunResponse, TeamRunResponse]\n@@ -1110,16 +1086,12 @@ class Memory:\n         from copy import deepcopy\n \n         # Create a new instance without calling __init__\n-        cls = self.__class__\n-        copied_obj = cls.__new__(cls)\n+        copied_obj = self.__class__.__new__(self.__class__)\n         memo[id(self)] = copied_obj\n \n-        # Deep copy attributes\n+        # Copy attributes, reusing specific objects\n+        shared_objects = {\"db\", \"memory_manager\", \"summary_manager\", \"team_context\"}\n         for k, v in self.__dict__.items():\n-            # Reuse db\n-            if k in {\"db\", \"memory_manager\", \"summary_manager\", \"team_context\"}:\n-                setattr(copied_obj, k, v)\n-            else:\n-                setattr(copied_obj, k, deepcopy(v, memo))\n+            setattr(copied_obj, k, v if k in shared_objects else deepcopy(v, memo))\n \n         return copied_obj\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/models/anthropic/claude.py",
            "diff": "diff --git a/libs/agno/agno/models/anthropic/claude.py b/libs/agno/agno/models/anthropic/claude.py\nindex 63dd8906a..d2414b17c 100644\n--- a/libs/agno/agno/models/anthropic/claude.py\n+++ b/libs/agno/agno/models/anthropic/claude.py\n@@ -149,8 +149,6 @@ class Claude(Model):\n         if self.request_params:\n             _request_params.update(self.request_params)\n \n-        if _request_params:\n-            log_debug(f\"Calling {self.provider} with request parameters: {_request_params}\")\n         return _request_params\n \n     def _prepare_request_kwargs(\n@@ -179,6 +177,9 @@ class Claude(Model):\n \n         if tools:\n             request_kwargs[\"tools\"] = self._format_tools_for_model(tools)\n+\n+        if request_kwargs:\n+            log_debug(f\"Calling {self.provider} with request parameters: {request_kwargs}\", log_level=2)\n         return request_kwargs\n \n     def _format_tools_for_model(self, tools: Optional[List[Dict[str, Any]]] = None) -> Optional[List[Dict[str, Any]]]:\n@@ -426,16 +427,13 @@ class Claude(Model):\n             log_error(f\"Unexpected error calling Claude API: {str(e)}\")\n             raise ModelProviderError(message=str(e), model_name=self.name, model_id=self.id) from e\n \n-    def format_function_call_results(\n-        self, messages: List[Message], function_call_results: List[Message], tool_ids: List[str]\n-    ) -> None:\n+    def format_function_call_results(self, messages: List[Message], function_call_results: List[Message]) -> None:\n         \"\"\"\n         Handle the results of function calls.\n \n         Args:\n             messages (List[Message]): The list of conversation messages.\n             function_call_results (List[Message]): The results of the function calls.\n-            tool_ids (List[str]): The tool ids.\n         \"\"\"\n         if len(function_call_results) > 0:\n             fc_responses: List = []\n@@ -517,7 +515,6 @@ class Claude(Model):\n                         function_def[\"arguments\"] = json.dumps(tool_input)\n \n                     model_response.extra = model_response.extra or {}\n-                    model_response.extra.setdefault(\"tool_ids\", []).append(block.id)\n                     model_response.tool_calls.append(\n                         {\n                             \"id\": block.id,\n@@ -584,7 +581,6 @@ class Claude(Model):\n                     function_def[\"arguments\"] = json.dumps(tool_input)\n \n                 model_response.extra = model_response.extra or {}\n-                model_response.extra.setdefault(\"tool_ids\", []).append(tool_use.id)\n \n                 model_response.tool_calls = [\n                     {\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/models/aws/bedrock.py",
            "diff": "diff --git a/libs/agno/agno/models/aws/bedrock.py b/libs/agno/agno/models/aws/bedrock.py\nindex 80815d101..9b9f376a7 100644\n--- a/libs/agno/agno/models/aws/bedrock.py\n+++ b/libs/agno/agno/models/aws/bedrock.py\n@@ -266,7 +266,7 @@ class AwsBedrock(Model):\n             body = {k: v for k, v in body.items() if v is not None}\n \n             if self.request_params:\n-                log_debug(f\"Calling {self.provider} with request parameters: {self.request_params}\")\n+                log_debug(f\"Calling {self.provider} with request parameters: {self.request_params}\", log_level=2)\n                 body.update(**self.request_params)\n \n             return self.get_client().converse(modelId=self.id, messages=formatted_messages, **body)\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/models/aws/claude.py",
            "diff": "diff --git a/libs/agno/agno/models/aws/claude.py b/libs/agno/agno/models/aws/claude.py\nindex c7c42319e..53bed5590 100644\n--- a/libs/agno/agno/models/aws/claude.py\n+++ b/libs/agno/agno/models/aws/claude.py\n@@ -161,7 +161,7 @@ class Claude(AnthropicClaude):\n             _request_params.update(self.request_params)\n \n         if _request_params:\n-            log_debug(f\"Calling {self.provider} with request parameters: {_request_params}\")\n+            log_debug(f\"Calling {self.provider} with request parameters: {_request_params}\", log_level=2)\n         return _request_params\n \n     def invoke(\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/models/azure/ai_foundry.py",
            "diff": "diff --git a/libs/agno/agno/models/azure/ai_foundry.py b/libs/agno/agno/models/azure/ai_foundry.py\nindex bb565eb20..d6730f75b 100644\n--- a/libs/agno/agno/models/azure/ai_foundry.py\n+++ b/libs/agno/agno/models/azure/ai_foundry.py\n@@ -123,7 +123,7 @@ class AzureAIFoundry(Model):\n             request_params.update(self.request_params)\n \n         if request_params:\n-            log_debug(f\"Calling {self.provider} with request parameters: {request_params}\")\n+            log_debug(f\"Calling {self.provider} with request parameters: {request_params}\", log_level=2)\n         return request_params\n \n     def _get_client_params(self) -> Dict[str, Any]:\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/models/cerebras/cerebras.py",
            "diff": "diff --git a/libs/agno/agno/models/cerebras/cerebras.py b/libs/agno/agno/models/cerebras/cerebras.py\nindex d46535913..3094e69cc 100644\n--- a/libs/agno/agno/models/cerebras/cerebras.py\n+++ b/libs/agno/agno/models/cerebras/cerebras.py\n@@ -192,7 +192,7 @@ class Cerebras(Model):\n             request_params.update(self.request_params)\n \n         if request_params:\n-            log_debug(f\"Calling {self.provider} with request parameters: {request_params}\")\n+            log_debug(f\"Calling {self.provider} with request parameters: {request_params}\", log_level=2)\n         return request_params\n \n     def invoke(\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/models/cerebras/cerebras_openai.py",
            "diff": "diff --git a/libs/agno/agno/models/cerebras/cerebras_openai.py b/libs/agno/agno/models/cerebras/cerebras_openai.py\nindex 20fc3531f..376a52b75 100644\n--- a/libs/agno/agno/models/cerebras/cerebras_openai.py\n+++ b/libs/agno/agno/models/cerebras/cerebras_openai.py\n@@ -55,7 +55,7 @@ class CerebrasOpenAI(OpenAILike):\n             request_params[\"parallel_tool_calls\"] = self.parallel_tool_calls\n \n         if request_params:\n-            log_debug(f\"Calling {self.provider} with request parameters: {request_params}\")\n+            log_debug(f\"Calling {self.provider} with request parameters: {request_params}\", log_level=2)\n         return request_params\n \n     def _format_message(self, message: Message) -> Dict[str, Any]:\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/models/cohere/chat.py",
            "diff": "diff --git a/libs/agno/agno/models/cohere/chat.py b/libs/agno/agno/models/cohere/chat.py\nindex 107146aee..619f69d48 100644\n--- a/libs/agno/agno/models/cohere/chat.py\n+++ b/libs/agno/agno/models/cohere/chat.py\n@@ -142,7 +142,7 @@ class Cohere(Model):\n             _request_params.update(self.request_params)\n \n         if _request_params:\n-            log_debug(f\"Calling {self.provider} with request parameters: {_request_params}\")\n+            log_debug(f\"Calling {self.provider} with request parameters: {_request_params}\", log_level=2)\n         return _request_params\n \n     def invoke(\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/models/google/gemini.py",
            "diff": "diff --git a/libs/agno/agno/models/google/gemini.py b/libs/agno/agno/models/google/gemini.py\nindex 2e6146a61..83b876988 100644\n--- a/libs/agno/agno/models/google/gemini.py\n+++ b/libs/agno/agno/models/google/gemini.py\n@@ -30,6 +30,7 @@ try:\n         GoogleSearch,\n         GoogleSearchRetrieval,\n         Part,\n+        ThinkingConfig,\n         Tool,\n     )\n     from google.genai.types import (\n@@ -80,6 +81,8 @@ class Gemini(Model):\n     response_modalities: Optional[list[str]] = None  # \"Text\" and/or \"Image\"\n     speech_config: Optional[dict[str, Any]] = None\n     cached_content: Optional[Any] = None\n+    thinking_budget: Optional[int] = None  # Thinking budget for Gemini 2.5 models\n+    include_thoughts: Optional[bool] = None  # Include thought summaries in response\n     request_params: Optional[Dict[str, Any]] = None\n \n     # Client parameters\n@@ -187,6 +190,15 @@ class Gemini(Model):\n             gemini_schema = convert_schema(normalized_schema)\n             config[\"response_schema\"] = gemini_schema\n \n+        # Add thinking configuration\n+        thinking_config_params = {}\n+        if self.thinking_budget is not None:\n+            thinking_config_params[\"thinking_budget\"] = self.thinking_budget\n+        if self.include_thoughts is not None:\n+            thinking_config_params[\"include_thoughts\"] = self.include_thoughts\n+        if thinking_config_params:\n+            config[\"thinking_config\"] = ThinkingConfig(**thinking_config_params)\n+\n         if self.grounding and self.search:\n             log_info(\"Both grounding and search are enabled. Grounding will take precedence.\")\n             self.search = False\n@@ -220,7 +232,7 @@ class Gemini(Model):\n             request_params.update(self.request_params)\n \n         if request_params:\n-            log_debug(f\"Calling {self.provider} with request parameters: {request_params}\")\n+            log_debug(f\"Calling {self.provider} with request parameters: {request_params}\", log_level=2)\n         return request_params\n \n     def invoke(\n@@ -699,9 +711,17 @@ class Gemini(Model):\n                 if hasattr(part, \"text\") and part.text is not None:\n                     text_content: Optional[str] = getattr(part, \"text\")\n                     if isinstance(text_content, str):\n-                        model_response.content = text_content\n+                        # Check if this is a thought summary\n+                        if hasattr(part, \"thought\") and part.thought:\n+                            model_response.reasoning_content = text_content\n+                        else:\n+                            model_response.content = text_content\n                     else:\n-                        model_response.content = str(text_content) if text_content is not None else \"\"\n+                        content_str = str(text_content) if text_content is not None else \"\"\n+                        if hasattr(part, \"thought\") and part.thought:\n+                            model_response.reasoning_content = content_str\n+                        else:\n+                            model_response.content = content_str\n \n                 if hasattr(part, \"inline_data\") and part.inline_data is not None:\n                     model_response.image = ImageArtifact(\n@@ -745,9 +765,14 @@ class Gemini(Model):\n         # Extract usage metadata if present\n         if hasattr(response, \"usage_metadata\") and response.usage_metadata is not None:\n             usage: GenerateContentResponseUsageMetadata = response.usage_metadata\n+\n+            output_tokens = usage.candidates_token_count or 0\n+            if hasattr(usage, \"thoughts_token_count\") and usage.thoughts_token_count is not None:\n+                output_tokens += usage.thoughts_token_count or 0\n+\n             model_response.response_usage = {\n                 \"input_tokens\": usage.prompt_token_count or 0,\n-                \"output_tokens\": usage.candidates_token_count or 0,\n+                \"output_tokens\": output_tokens,\n                 \"total_tokens\": usage.total_token_count or 0,\n                 \"cached_tokens\": usage.cached_content_token_count or 0,\n             }\n@@ -775,7 +800,12 @@ class Gemini(Model):\n                 for part in response_message.parts:\n                     # Extract text if present\n                     if hasattr(part, \"text\") and part.text is not None:\n-                        model_response.content = str(part.text) if part.text is not None else \"\"\n+                        text_content = str(part.text) if part.text is not None else \"\"\n+                        # Check if this is a thought summary\n+                        if hasattr(part, \"thought\") and part.thought:\n+                            model_response.reasoning_content = text_content\n+                        else:\n+                            model_response.content = text_content\n \n                     if hasattr(part, \"inline_data\") and part.inline_data is not None:\n                         model_response.image = ImageArtifact(\n@@ -819,9 +849,14 @@ class Gemini(Model):\n             # Extract usage metadata if present\n             if hasattr(response_delta, \"usage_metadata\") and response_delta.usage_metadata is not None:\n                 usage: GenerateContentResponseUsageMetadata = response_delta.usage_metadata\n+\n+                output_tokens = usage.candidates_token_count or 0\n+                if hasattr(usage, \"thoughts_token_count\") and usage.thoughts_token_count is not None:\n+                    output_tokens += usage.thoughts_token_count or 0\n+\n                 model_response.response_usage = {\n                     \"input_tokens\": usage.prompt_token_count or 0,\n-                    \"output_tokens\": usage.candidates_token_count or 0,\n+                    \"output_tokens\": output_tokens,\n                     \"total_tokens\": usage.total_token_count or 0,\n                     \"cached_tokens\": usage.cached_content_token_count or 0,\n                 }\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/models/groq/groq.py",
            "diff": "diff --git a/libs/agno/agno/models/groq/groq.py b/libs/agno/agno/models/groq/groq.py\nindex 748533b61..0c32819f7 100644\n--- a/libs/agno/agno/models/groq/groq.py\n+++ b/libs/agno/agno/models/groq/groq.py\n@@ -165,7 +165,7 @@ class Groq(Model):\n             request_params.update(self.request_params)\n \n         if request_params:\n-            log_debug(f\"Calling {self.provider} with request parameters: {request_params}\")\n+            log_debug(f\"Calling {self.provider} with request parameters: {request_params}\", log_level=2)\n         return request_params\n \n     def to_dict(self) -> Dict[str, Any]:\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/models/huggingface/huggingface.py",
            "diff": "diff --git a/libs/agno/agno/models/huggingface/huggingface.py b/libs/agno/agno/models/huggingface/huggingface.py\nindex 29566db5f..144c060ab 100644\n--- a/libs/agno/agno/models/huggingface/huggingface.py\n+++ b/libs/agno/agno/models/huggingface/huggingface.py\n@@ -159,7 +159,7 @@ class HuggingFace(Model):\n             _request_params.update(self.request_params)\n \n         if _request_params:\n-            log_debug(f\"Calling {self.provider} with request parameters: {_request_params}\")\n+            log_debug(f\"Calling {self.provider} with request parameters: {_request_params}\", log_level=2)\n         return _request_params\n \n     def to_dict(self) -> Dict[str, Any]:\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/models/ibm/watsonx.py",
            "diff": "diff --git a/libs/agno/agno/models/ibm/watsonx.py b/libs/agno/agno/models/ibm/watsonx.py\nindex cc821391e..8b851ea30 100644\n--- a/libs/agno/agno/models/ibm/watsonx.py\n+++ b/libs/agno/agno/models/ibm/watsonx.py\n@@ -124,7 +124,7 @@ class WatsonX(Model):\n             request_params.update(self.request_params)\n \n         if request_params:\n-            log_debug(f\"Calling {self.provider} with request parameters: {request_params}\")\n+            log_debug(f\"Calling {self.provider} with request parameters: {request_params}\", log_level=2)\n         return request_params\n \n     def _format_message(self, message: Message) -> Dict[str, Any]:\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/models/litellm/chat.py",
            "diff": "diff --git a/libs/agno/agno/models/litellm/chat.py b/libs/agno/agno/models/litellm/chat.py\nindex e06450a92..e30b0591c 100644\n--- a/libs/agno/agno/models/litellm/chat.py\n+++ b/libs/agno/agno/models/litellm/chat.py\n@@ -134,7 +134,7 @@ class LiteLLM(Model):\n             request_params.update(self.request_params)\n \n         if request_params:\n-            log_debug(f\"Calling {self.provider} with request parameters: {request_params}\")\n+            log_debug(f\"Calling {self.provider} with request parameters: {request_params}\", log_level=2)\n         return request_params\n \n     def invoke(\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/models/message.py",
            "diff": "diff --git a/libs/agno/agno/models/message.py b/libs/agno/agno/models/message.py\nindex 51ee6b40b..c636e711d 100644\n--- a/libs/agno/agno/models/message.py\n+++ b/libs/agno/agno/models/message.py\n@@ -73,7 +73,7 @@ class MessageMetrics:\n \n     timer: Optional[Timer] = None\n \n-    def _to_dict(self) -> Dict[str, Any]:\n+    def to_dict(self) -> Dict[str, Any]:\n         metrics_dict = asdict(self)\n         metrics_dict.pop(\"timer\")\n         metrics_dict = {\n@@ -267,7 +267,7 @@ class Message(BaseModel):\n         if self.references:\n             message_dict[\"references\"] = self.references.model_dump()\n         if self.metrics:\n-            message_dict[\"metrics\"] = self.metrics._to_dict()\n+            message_dict[\"metrics\"] = self.metrics.to_dict()\n             if not message_dict[\"metrics\"]:\n                 message_dict.pop(\"metrics\")\n \n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/models/meta/llama.py",
            "diff": "diff --git a/libs/agno/agno/models/meta/llama.py b/libs/agno/agno/models/meta/llama.py\nindex 6db2688e3..e66a2364f 100644\n--- a/libs/agno/agno/models/meta/llama.py\n+++ b/libs/agno/agno/models/meta/llama.py\n@@ -162,7 +162,7 @@ class Llama(Model):\n             request_params.update(self.request_params)\n \n         if request_params:\n-            log_debug(f\"Calling {self.provider} with request parameters: {request_params}\")\n+            log_debug(f\"Calling {self.provider} with request parameters: {request_params}\", log_level=2)\n         return request_params\n \n     def to_dict(self) -> Dict[str, Any]:\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/models/mistral/mistral.py",
            "diff": "diff --git a/libs/agno/agno/models/mistral/mistral.py b/libs/agno/agno/models/mistral/mistral.py\nindex 3ac686a72..2c24ac592 100644\n--- a/libs/agno/agno/models/mistral/mistral.py\n+++ b/libs/agno/agno/models/mistral/mistral.py\n@@ -139,7 +139,7 @@ class MistralChat(Model):\n             _request_params.update(self.request_params)\n \n         if _request_params:\n-            log_debug(f\"Calling {self.provider} with request parameters: {_request_params}\")\n+            log_debug(f\"Calling {self.provider} with request parameters: {_request_params}\", log_level=2)\n         return _request_params\n \n     def to_dict(self) -> Dict[str, Any]:\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/models/ollama/chat.py",
            "diff": "diff --git a/libs/agno/agno/models/ollama/chat.py b/libs/agno/agno/models/ollama/chat.py\nindex 24526a9b7..682f01ed1 100644\n--- a/libs/agno/agno/models/ollama/chat.py\n+++ b/libs/agno/agno/models/ollama/chat.py\n@@ -94,12 +94,7 @@ class Ollama(Model):\n         Returns:\n             Dict[str, Any]: The API kwargs for the model.\n         \"\"\"\n-        base_params = {\n-            \"format\": self.format,\n-            \"options\": self.options,\n-            \"keep_alive\": self.keep_alive,\n-            \"request_params\": self.request_params,\n-        }\n+        base_params = {\"format\": self.format, \"options\": self.options, \"keep_alive\": self.keep_alive}\n         # Filter out None values\n         request_params = {k: v for k, v in base_params.items() if v is not None}\n         # Add tools\n@@ -111,7 +106,7 @@ class Ollama(Model):\n             request_params.update(self.request_params)\n \n         if request_params:\n-            log_debug(f\"Calling {self.provider} with request parameters: {request_params}\")\n+            log_debug(f\"Calling {self.provider} with request parameters: {request_params}\", log_level=2)\n         return request_params\n \n     def to_dict(self) -> Dict[str, Any]:\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/models/ollama/tools.py",
            "diff": "diff --git a/libs/agno/agno/models/ollama/tools.py b/libs/agno/agno/models/ollama/tools.py\nindex f6678ac60..527c35f58 100644\n--- a/libs/agno/agno/models/ollama/tools.py\n+++ b/libs/agno/agno/models/ollama/tools.py\n@@ -58,12 +58,7 @@ class OllamaTools(Ollama):\n         Returns:\n             Dict[str, Any]: The API kwargs for the model.\n         \"\"\"\n-        base_params: Dict[str, Any] = {\n-            \"format\": self.format,\n-            \"options\": self.options,\n-            \"keep_alive\": self.keep_alive,\n-            \"request_params\": self.request_params,\n-        }\n+        base_params: Dict[str, Any] = {\"format\": self.format, \"options\": self.options, \"keep_alive\": self.keep_alive}\n         request_params: Dict[str, Any] = {k: v for k, v in base_params.items() if v is not None}\n         # Add additional request params if provided\n         if self.request_params:\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/models/openai/chat.py",
            "diff": "diff --git a/libs/agno/agno/models/openai/chat.py b/libs/agno/agno/models/openai/chat.py\nindex a8c7007b7..4e71522a8 100644\n--- a/libs/agno/agno/models/openai/chat.py\n+++ b/libs/agno/agno/models/openai/chat.py\n@@ -9,7 +9,7 @@ from pydantic import BaseModel\n from agno.exceptions import ModelProviderError\n from agno.media import AudioResponse\n from agno.models.base import Model\n-from agno.models.message import Message\n+from agno.models.message import Citations, Message, UrlCitation\n from agno.models.response import ModelResponse\n from agno.utils.log import log_debug, log_error, log_warning\n from agno.utils.openai import _format_file_for_message, audio_to_message, images_to_message\n@@ -211,7 +211,7 @@ class OpenAIChat(Model):\n             request_params.update(self.request_params)\n \n         if request_params:\n-            log_debug(f\"Calling {self.provider} with request parameters: {request_params}\")\n+            log_debug(f\"Calling {self.provider} with request parameters: {request_params}\", log_level=2)\n         return request_params\n \n     def to_dict(self) -> Dict[str, Any]:\n@@ -663,6 +663,17 @@ class OpenAIChat(Model):\n         if hasattr(response_message, \"reasoning_content\") and response_message.reasoning_content is not None:\n             model_response.reasoning_content = response_message.reasoning_content\n \n+        # Add citations if present\n+        if hasattr(response, \"citations\") and response.citations:\n+            citations = Citations()\n+            url_citations = []\n+            for citation_url in response.citations:\n+                url_citations.append(UrlCitation(url=str(citation_url)))\n+\n+            citations.urls = url_citations\n+            citations.raw = response.citations\n+            model_response.citations = citations\n+\n         if response.usage is not None:\n             model_response.response_usage = response.usage\n \n@@ -715,6 +726,16 @@ class OpenAIChat(Model):\n                     except Exception as e:\n                         log_warning(f\"Error processing audio: {e}\")\n \n+        if hasattr(response_delta, \"citations\") and response_delta.citations:\n+            citations = Citations()\n+            url_citations = []\n+            for citation_url in response_delta.citations:\n+                url_citations.append(UrlCitation(url=str(citation_url)))\n+\n+            citations.urls = url_citations\n+            citations.raw = response_delta.citations\n+            model_response.citations = citations\n+\n         # Add usage metrics if present\n         if response_delta.usage is not None:\n             model_response.response_usage = response_delta.usage\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/models/openai/responses.py",
            "diff": "diff --git a/libs/agno/agno/models/openai/responses.py b/libs/agno/agno/models/openai/responses.py\nindex 6b4001a5b..bf1761960 100644\n--- a/libs/agno/agno/models/openai/responses.py\n+++ b/libs/agno/agno/models/openai/responses.py\n@@ -224,7 +224,7 @@ class OpenAIResponses(Model):\n             request_params.update(self.request_params)\n \n         if request_params:\n-            log_debug(f\"Calling {self.provider} with request parameters: {request_params}\")\n+            log_debug(f\"Calling {self.provider} with request parameters: {request_params}\", log_level=2)\n         return request_params\n \n     def _upload_file(self, file: File) -> Optional[str]:\n@@ -330,6 +330,7 @@ class OpenAIResponses(Model):\n         Returns:\n             Dict[str, Any]: The formatted message.\n         \"\"\"\n+        print(\"--------------------------------\")\n         formatted_messages: List[Dict[str, Any]] = []\n         for message in messages:\n             if message.role in [\"user\", \"system\"]:\n@@ -357,32 +358,27 @@ class OpenAIResponses(Model):\n \n                 formatted_messages.append(message_dict)\n \n-            if self.id.startswith((\"o3\", \"o4-mini\")):\n-                if message.role == \"tool\":\n-                    if message.tool_call_id and message.content is not None:\n-                        formatted_messages.append(\n-                            {\"type\": \"function_call_output\", \"call_id\": message.tool_call_id, \"output\": message.content}\n-                        )\n-\n-            else:\n-                # OpenAI expects the tool_calls to be None if empty, not an empty list\n-                if message.tool_calls is not None and len(message.tool_calls) > 0:\n-                    for tool_call in message.tool_calls:\n-                        formatted_messages.append(\n-                            {\n-                                \"type\": \"function_call\",\n-                                \"id\": tool_call[\"id\"],\n-                                \"call_id\": tool_call[\"call_id\"],\n-                                \"name\": tool_call[\"function\"][\"name\"],\n-                                \"arguments\": tool_call[\"function\"][\"arguments\"],\n-                                \"status\": \"completed\",\n-                            }\n-                        )\n-\n-                if message.role == \"tool\":\n+            elif message.role == \"tool\":\n+                if message.tool_call_id and message.content is not None:\n                     formatted_messages.append(\n                         {\"type\": \"function_call_output\", \"call_id\": message.tool_call_id, \"output\": message.content}\n                     )\n+            elif message.tool_calls is not None and len(message.tool_calls) > 0:\n+                for tool_call in message.tool_calls:\n+                    formatted_messages.append(\n+                        {\n+                            \"type\": \"function_call\",\n+                            \"id\": tool_call[\"id\"],\n+                            \"call_id\": tool_call[\"call_id\"],\n+                            \"name\": tool_call[\"function\"][\"name\"],\n+                            \"arguments\": tool_call[\"function\"][\"arguments\"],\n+                            \"status\": \"completed\",\n+                        }\n+                    )\n+            elif message.role == \"assistant\":\n+                formatted_messages.append({\"role\": self.role_map[message.role], \"content\": message.content})\n+\n+            print(formatted_messages)\n         return formatted_messages\n \n     def invoke(\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/models/perplexity/perplexity.py",
            "diff": "diff --git a/libs/agno/agno/models/perplexity/perplexity.py b/libs/agno/agno/models/perplexity/perplexity.py\nindex 7422a07a5..34e0fc609 100644\n--- a/libs/agno/agno/models/perplexity/perplexity.py\n+++ b/libs/agno/agno/models/perplexity/perplexity.py\n@@ -77,7 +77,7 @@ class Perplexity(OpenAILike):\n             request_params.update(self.request_params)\n \n         if request_params:\n-            log_debug(f\"Calling {self.provider} with request parameters: {request_params}\")\n+            log_debug(f\"Calling {self.provider} with request parameters: {request_params}\", log_level=2)\n         return request_params\n \n     def parse_provider_response(self, response: Union[ChatCompletion, ParsedChatCompletion], **kwargs) -> ModelResponse:\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/models/response.py",
            "diff": "diff --git a/libs/agno/agno/models/response.py b/libs/agno/agno/models/response.py\nindex 0e080fdcb..2d5294704 100644\n--- a/libs/agno/agno/models/response.py\n+++ b/libs/agno/agno/models/response.py\n@@ -50,7 +50,7 @@ class ToolExecution:\n     def to_dict(self) -> Dict[str, Any]:\n         _dict = asdict(self)\n         if self.metrics is not None:\n-            _dict[\"metrics\"] = self.metrics._to_dict()\n+            _dict[\"metrics\"] = self.metrics.to_dict()\n \n         if self.user_input_schema is not None:\n             _dict[\"user_input_schema\"] = [field.to_dict() for field in self.user_input_schema]\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/models/vllm/vllm.py",
            "diff": "diff --git a/libs/agno/agno/models/vllm/vllm.py b/libs/agno/agno/models/vllm/vllm.py\nindex 1a1c6ce09..081babc94 100644\n--- a/libs/agno/agno/models/vllm/vllm.py\n+++ b/libs/agno/agno/models/vllm/vllm.py\n@@ -73,5 +73,5 @@ class vLLM(OpenAILike):\n             request_kwargs[\"extra_body\"] = {**existing_body, **vllm_body}\n \n         if request_kwargs:\n-            log_debug(f\"Calling {self.provider} with request parameters: {request_kwargs}\")\n+            log_debug(f\"Calling {self.provider} with request parameters: {request_kwargs}\", log_level=2)\n         return request_kwargs\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/models/xai/xai.py",
            "diff": "diff --git a/libs/agno/agno/models/xai/xai.py b/libs/agno/agno/models/xai/xai.py\nindex a41067152..177c6fdf1 100644\n--- a/libs/agno/agno/models/xai/xai.py\n+++ b/libs/agno/agno/models/xai/xai.py\n@@ -1,8 +1,11 @@\n from dataclasses import dataclass\n from os import getenv\n-from typing import Optional\n+from typing import Any, Dict, List, Optional, Type, Union\n+\n+from pydantic import BaseModel\n \n from agno.models.openai.like import OpenAILike\n+from agno.utils.log import log_debug\n \n \n @dataclass\n@@ -16,6 +19,7 @@ class xAI(OpenAILike):\n         provider (str): The provider of the API. Defaults to \"xAI\".\n         api_key (Optional[str]): The API key for the xAI API.\n         base_url (Optional[str]): The base URL for the xAI API. Defaults to \"https://api.x.ai/v1\".\n+        search_parameters (Optional[Dict[str, Any]]): Search parameters for enabling live search.\n     \"\"\"\n \n     id: str = \"grok-beta\"\n@@ -24,3 +28,31 @@ class xAI(OpenAILike):\n \n     api_key: Optional[str] = getenv(\"XAI_API_KEY\")\n     base_url: str = \"https://api.x.ai/v1\"\n+\n+    search_parameters: Optional[Dict[str, Any]] = None\n+\n+    def get_request_params(\n+        self,\n+        response_format: Optional[Union[Dict, Type[BaseModel]]] = None,\n+        tools: Optional[List[Dict[str, Any]]] = None,\n+        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,\n+    ) -> Dict[str, Any]:\n+        \"\"\"\n+        Returns keyword arguments for API requests, including search parameters.\n+\n+        Returns:\n+            Dict[str, Any]: A dictionary of keyword arguments for API requests.\n+        \"\"\"\n+        request_params = super().get_request_params(\n+            response_format=response_format, tools=tools, tool_choice=tool_choice\n+        )\n+\n+        if self.search_parameters:\n+            existing_body = request_params.get(\"extra_body\") or {}\n+            existing_body.update({\"search_parameters\": self.search_parameters})\n+            request_params[\"extra_body\"] = existing_body\n+\n+        if request_params:\n+            log_debug(f\"Calling {self.provider} with request parameters: {request_params}\", log_level=2)\n+\n+        return request_params\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/reasoning/default.py",
            "diff": "diff --git a/libs/agno/agno/reasoning/default.py b/libs/agno/agno/reasoning/default.py\nindex a6be3df63..1cedc33c6 100644\n--- a/libs/agno/agno/reasoning/default.py\n+++ b/libs/agno/agno/reasoning/default.py\n@@ -1,7 +1,7 @@\n from __future__ import annotations\n \n from textwrap import dedent\n-from typing import Callable, Dict, List, Optional, Union\n+from typing import Callable, Dict, List, Literal, Optional, Union\n \n from agno.models.base import Model\n from agno.reasoning.step import ReasoningSteps\n@@ -18,6 +18,7 @@ def get_default_reasoning_agent(\n     monitoring: bool = False,\n     telemetry: bool = True,\n     debug_mode: bool = False,\n+    debug_level: Literal[1, 2] = 1,\n ) -> Optional[\"Agent\"]:  # type: ignore  # noqa: F821\n     from agno.agent import Agent\n \n@@ -85,6 +86,7 @@ def get_default_reasoning_agent(\n         monitoring=monitoring,\n         telemetry=telemetry,\n         debug_mode=debug_mode,\n+        debug_level=debug_level,\n     )\n \n     agent.model.show_tool_calls = False  # type: ignore\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/reasoning/helpers.py",
            "diff": "diff --git a/libs/agno/agno/reasoning/helpers.py b/libs/agno/agno/reasoning/helpers.py\nindex 1f892a94f..e2de6901d 100644\n--- a/libs/agno/agno/reasoning/helpers.py\n+++ b/libs/agno/agno/reasoning/helpers.py\n@@ -1,4 +1,4 @@\n-from typing import List\n+from typing import List, Literal\n \n from agno.models.base import Model\n from agno.models.message import Message\n@@ -8,11 +8,21 @@ from agno.utils.log import logger\n \n \n def get_reasoning_agent(\n-    reasoning_model: Model, monitoring: bool = False, telemetry: bool = False, debug_mode: bool = False\n+    reasoning_model: Model,\n+    monitoring: bool = False,\n+    telemetry: bool = False,\n+    debug_mode: bool = False,\n+    debug_level: Literal[1, 2] = 1,\n ) -> \"Agent\":  # type: ignore  # noqa: F821\n     from agno.agent import Agent\n \n-    return Agent(model=reasoning_model, monitoring=monitoring, telemetry=telemetry, debug_mode=debug_mode)\n+    return Agent(\n+        model=reasoning_model,\n+        monitoring=monitoring,\n+        telemetry=telemetry,\n+        debug_mode=debug_mode,\n+        debug_level=debug_level,\n+    )\n \n \n def get_next_action(reasoning_step: ReasoningStep) -> NextAction:\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/run/response.py",
            "diff": "diff --git a/libs/agno/agno/run/response.py b/libs/agno/agno/run/response.py\nindex 99075a5e7..a4e55c9bb 100644\n--- a/libs/agno/agno/run/response.py\n+++ b/libs/agno/agno/run/response.py\n@@ -34,6 +34,9 @@ class RunEvent(str, Enum):\n     memory_update_started = \"MemoryUpdateStarted\"\n     memory_update_completed = \"MemoryUpdateCompleted\"\n \n+    parser_model_response_started = \"ParserModelResponseStarted\"\n+    parser_model_response_completed = \"ParserModelResponseCompleted\"\n+\n \n @dataclass\n class BaseAgentRunResponseEvent(BaseRunResponseEvent):\n@@ -164,6 +167,16 @@ class ToolCallCompletedEvent(BaseAgentRunResponseEvent):\n     audio: Optional[List[AudioArtifact]] = None  # Audio produced by the tool call\n \n \n+@dataclass\n+class ParserModelResponseStartedEvent(BaseAgentRunResponseEvent):\n+    event: str = RunEvent.parser_model_response_started.value\n+\n+\n+@dataclass\n+class ParserModelResponseCompletedEvent(BaseAgentRunResponseEvent):\n+    event: str = RunEvent.parser_model_response_completed.value\n+\n+\n RunResponseEvent = Union[\n     RunResponseStartedEvent,\n     RunResponseContentEvent,\n@@ -179,6 +192,8 @@ RunResponseEvent = Union[\n     MemoryUpdateCompletedEvent,\n     ToolCallStartedEvent,\n     ToolCallCompletedEvent,\n+    ParserModelResponseStartedEvent,\n+    ParserModelResponseCompletedEvent,\n ]\n \n \n@@ -198,6 +213,8 @@ RUN_EVENT_TYPE_REGISTRY = {\n     RunEvent.memory_update_completed.value: MemoryUpdateCompletedEvent,\n     RunEvent.tool_call_started.value: ToolCallStartedEvent,\n     RunEvent.tool_call_completed.value: ToolCallCompletedEvent,\n+    RunEvent.parser_model_response_started.value: ParserModelResponseStartedEvent,\n+    RunEvent.parser_model_response_completed.value: ParserModelResponseCompletedEvent,\n }\n \n \n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/run/team.py",
            "diff": "diff --git a/libs/agno/agno/run/team.py b/libs/agno/agno/run/team.py\nindex e9558ed3a..06319ad4b 100644\n--- a/libs/agno/agno/run/team.py\n+++ b/libs/agno/agno/run/team.py\n@@ -31,6 +31,9 @@ class TeamRunEvent(str, Enum):\n     memory_update_started = \"TeamMemoryUpdateStarted\"\n     memory_update_completed = \"TeamMemoryUpdateCompleted\"\n \n+    parser_model_response_started = \"TeamParserModelResponseStarted\"\n+    parser_model_response_completed = \"TeamParserModelResponseCompleted\"\n+\n \n @dataclass\n class BaseTeamRunResponseEvent(BaseRunResponseEvent):\n@@ -162,6 +165,16 @@ class ToolCallCompletedEvent(BaseTeamRunResponseEvent):\n     audio: Optional[List[AudioArtifact]] = None  # Audio produced by the tool call\n \n \n+@dataclass\n+class ParserModelResponseStartedEvent(BaseTeamRunResponseEvent):\n+    event: str = TeamRunEvent.parser_model_response_started.value\n+\n+\n+@dataclass\n+class ParserModelResponseCompletedEvent(BaseTeamRunResponseEvent):\n+    event: str = TeamRunEvent.parser_model_response_completed.value\n+\n+\n TeamRunResponseEvent = Union[\n     RunResponseStartedEvent,\n     RunResponseContentEvent,\n@@ -175,6 +188,8 @@ TeamRunResponseEvent = Union[\n     MemoryUpdateCompletedEvent,\n     ToolCallStartedEvent,\n     ToolCallCompletedEvent,\n+    ParserModelResponseStartedEvent,\n+    ParserModelResponseCompletedEvent,\n ]\n \n # Map event string to dataclass for team events\n@@ -191,6 +206,8 @@ TEAM_RUN_EVENT_TYPE_REGISTRY = {\n     TeamRunEvent.memory_update_completed.value: MemoryUpdateCompletedEvent,\n     TeamRunEvent.tool_call_started.value: ToolCallStartedEvent,\n     TeamRunEvent.tool_call_completed.value: ToolCallCompletedEvent,\n+    TeamRunEvent.parser_model_response_started.value: ParserModelResponseStartedEvent,\n+    TeamRunEvent.parser_model_response_completed.value: ParserModelResponseCompletedEvent,\n }\n \n \n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/storage/base.py",
            "diff": "diff --git a/libs/agno/agno/storage/base.py b/libs/agno/agno/storage/base.py\nindex c44ad3c8f..a9d22d91d 100644\n--- a/libs/agno/agno/storage/base.py\n+++ b/libs/agno/agno/storage/base.py\n@@ -27,7 +27,7 @@ class Storage(ABC):\n         raise NotImplementedError\n \n     @abstractmethod\n-    def get_all_session_ids(self, user_id: Optional[str] = None, agent_id: Optional[str] = None) -> List[str]:\n+    def get_all_session_ids(self, user_id: Optional[str] = None, entity_id: Optional[str] = None) -> List[str]:\n         raise NotImplementedError\n \n     @abstractmethod\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/storage/mysql.py",
            "diff": "diff --git a/libs/agno/agno/storage/mysql.py b/libs/agno/agno/storage/mysql.py\nnew file mode 100644\nindex 000000000..cf350236e\n--- /dev/null\n+++ b/libs/agno/agno/storage/mysql.py\n@@ -0,0 +1,644 @@\n+import time\n+from typing import List, Literal, Optional\n+\n+from agno.storage.base import Storage\n+from agno.storage.session import Session\n+from agno.storage.session.agent import AgentSession\n+from agno.storage.session.team import TeamSession\n+from agno.storage.session.workflow import WorkflowSession\n+from agno.utils.log import log_debug, log_info, log_warning, logger\n+\n+try:\n+    from sqlalchemy.dialects import mysql\n+    from sqlalchemy.engine import Engine, create_engine\n+    from sqlalchemy.inspection import inspect\n+    from sqlalchemy.orm import scoped_session, sessionmaker\n+    from sqlalchemy.schema import Column, MetaData, Table\n+    from sqlalchemy.sql.expression import select, text\n+    from sqlalchemy.types import JSON, BigInteger, String\n+except ImportError:\n+    raise ImportError(\"`sqlalchemy` not installed. Please install it using `pip install sqlalchemy pymysql`\")\n+\n+\n+class MySQLStorage(Storage):\n+    def __init__(\n+        self,\n+        table_name: str,\n+        schema: Optional[str] = \"ai\",\n+        db_url: Optional[str] = None,\n+        db_engine: Optional[Engine] = None,\n+        schema_version: int = 1,\n+        auto_upgrade_schema: bool = False,\n+        mode: Optional[Literal[\"agent\", \"team\", \"workflow\"]] = \"agent\",\n+    ):\n+        \"\"\"\n+        This class provides agent storage using a MySQL table.\n+\n+        The following order is used to determine the database connection:\n+            1. Use the db_engine if provided\n+            2. Use the db_url\n+            3. Raise an error if neither is provided\n+\n+        Args:\n+            table_name (str): Name of the table to store Agent sessions.\n+            schema (Optional[str]): The schema to use for the table. Defaults to \"ai\".\n+            db_url (Optional[str]): The database URL to connect to.\n+            db_engine (Optional[Engine]): The SQLAlchemy database engine to use.\n+            schema_version (int): Version of the schema. Defaults to 1.\n+            auto_upgrade_schema (bool): Whether to automatically upgrade the schema.\n+            mode (Optional[Literal[\"agent\", \"team\", \"workflow\"]]): The mode of the storage.\n+        Raises:\n+            ValueError: If neither db_url nor db_engine is provided.\n+        \"\"\"\n+        super().__init__(mode)\n+        _engine: Optional[Engine] = db_engine\n+        if _engine is None and db_url is not None:\n+            _engine = create_engine(db_url)\n+\n+        if _engine is None:\n+            raise ValueError(\"Must provide either db_url or db_engine\")\n+\n+        # Database attributes\n+        self.table_name: str = table_name\n+        self.schema: Optional[str] = schema\n+        self.db_url: Optional[str] = db_url\n+        self.db_engine: Engine = _engine\n+        self.metadata: MetaData = MetaData(schema=self.schema)\n+        self.inspector = inspect(self.db_engine)\n+\n+        # Table schema version\n+        self.schema_version: int = schema_version\n+        # Automatically upgrade schema if True\n+        self.auto_upgrade_schema: bool = auto_upgrade_schema\n+        self._schema_up_to_date: bool = False\n+\n+        # Database session\n+        self.Session: scoped_session = scoped_session(sessionmaker(bind=self.db_engine))\n+        # Database table for storage\n+        self.table: Table = self.get_table()\n+        log_debug(f\"Created MySQLStorage: '{self.schema}.{self.table_name}'\")\n+\n+    @property\n+    def mode(self) -> Literal[\"agent\", \"team\", \"workflow\"]:\n+        \"\"\"Get the mode of the storage.\"\"\"\n+        return super().mode\n+\n+    @mode.setter\n+    def mode(self, value: Optional[Literal[\"agent\", \"team\", \"workflow\"]]) -> None:\n+        \"\"\"Set the mode and refresh the table if mode changes.\"\"\"\n+        super(MySQLStorage, type(self)).mode.fset(self, value)  # type: ignore\n+        if value is not None:\n+            self.table = self.get_table()\n+\n+    def get_table_v1(self) -> Table:\n+        \"\"\"\n+        Define the table schema for version 1.\n+\n+        Returns:\n+            Table: SQLAlchemy Table object representing the schema.\n+        \"\"\"\n+        # Common columns for both agent and workflow modes\n+        common_columns = [\n+            Column(\"session_id\", String(255), primary_key=True),\n+            Column(\"user_id\", String(255), index=True),\n+            Column(\"memory\", JSON),\n+            Column(\"session_data\", JSON),\n+            Column(\"extra_data\", JSON),\n+            Column(\"created_at\", BigInteger, server_default=text(\"UNIX_TIMESTAMP()\")),\n+            Column(\"updated_at\", BigInteger, server_onupdate=text(\"UNIX_TIMESTAMP()\")),\n+        ]\n+\n+        # Mode-specific columns\n+        specific_columns = []\n+        if self.mode == \"agent\":\n+            specific_columns = [\n+                Column(\"agent_id\", String(255), index=True),\n+                Column(\"team_session_id\", String(255), index=True, nullable=True),\n+                Column(\"agent_data\", JSON),\n+            ]\n+        elif self.mode == \"team\":\n+            specific_columns = [\n+                Column(\"team_id\", String(255), index=True),\n+                Column(\"team_session_id\", String(255), index=True, nullable=True),\n+                Column(\"team_data\", JSON),\n+            ]\n+        elif self.mode == \"workflow\":\n+            specific_columns = [\n+                Column(\"workflow_id\", String(255), index=True),\n+                Column(\"workflow_data\", JSON),\n+            ]\n+\n+        # Create table with all columns\n+        table = Table(\n+            self.table_name,\n+            self.metadata,\n+            *common_columns,\n+            *specific_columns,\n+            extend_existing=True,\n+            schema=self.schema,  # type: ignore\n+        )\n+\n+        return table\n+\n+    def get_table(self) -> Table:\n+        \"\"\"\n+        Get the table schema based on the schema version.\n+\n+        Returns:\n+            Table: SQLAlchemy Table object for the current schema version.\n+\n+        Raises:\n+            ValueError: If an unsupported schema version is specified.\n+        \"\"\"\n+        if self.schema_version == 1:\n+            return self.get_table_v1()\n+        else:\n+            raise ValueError(f\"Unsupported schema version: {self.schema_version}\")\n+\n+    def table_exists(self) -> bool:\n+        \"\"\"\n+        Check if the table exists in the database.\n+\n+        Returns:\n+            bool: True if the table exists, False otherwise.\n+        \"\"\"\n+        try:\n+            # Use a direct SQL query to check if the table exists\n+            with self.Session() as sess:\n+                if self.schema is not None:\n+                    exists_query = text(\n+                        \"SELECT 1 FROM information_schema.tables WHERE table_schema = :schema AND table_name = :table\"\n+                    )\n+                    exists = (\n+                        sess.execute(exists_query, {\"schema\": self.schema, \"table\": self.table_name}).scalar()\n+                        is not None\n+                    )\n+                else:\n+                    exists_query = text(\"SELECT 1 FROM information_schema.tables WHERE table_name = :table\")\n+                    exists = sess.execute(exists_query, {\"table\": self.table_name}).scalar() is not None\n+\n+            log_debug(f\"Table '{self.table.fullname}' does{' not ' if not exists else ' '}exist\")\n+            return exists\n+\n+        except Exception as e:\n+            logger.error(f\"Error checking if table exists: {e}\")\n+            return False\n+\n+    def create(self) -> None:\n+        \"\"\"\n+        Create the table if it does not exist.\n+        \"\"\"\n+        self.table = self.get_table()\n+        if not self.table_exists():\n+            try:\n+                with self.Session() as sess, sess.begin():\n+                    if self.schema is not None:\n+                        log_debug(f\"Creating schema: {self.schema}\")\n+                        sess.execute(text(f\"CREATE SCHEMA IF NOT EXISTS `{self.schema}`;\"))\n+\n+                log_debug(f\"Creating table: {self.table_name}\")\n+\n+                # First create the table without indexes\n+                table_without_indexes = Table(\n+                    self.table_name,\n+                    MetaData(schema=self.schema),\n+                    *[c.copy() for c in self.table.columns],\n+                    schema=self.schema,\n+                )\n+                table_without_indexes.create(self.db_engine, checkfirst=True)\n+\n+                # Then create each index individually with error handling\n+                for idx in self.table.indexes:\n+                    try:\n+                        idx_name = idx.name\n+                        log_debug(f\"Creating index: {idx_name}\")\n+\n+                        # Check if index already exists in MySQL\n+                        with self.Session() as sess:\n+                            if self.schema:\n+                                exists_query = text(\n+                                    \"\"\"SELECT 1 FROM information_schema.statistics\n+                                    WHERE table_schema = :schema\n+                                    AND table_name = :table\n+                                    AND index_name = :index_name\"\"\"\n+                                )\n+                                exists = (\n+                                    sess.execute(\n+                                        exists_query,\n+                                        {\"schema\": self.schema, \"table\": self.table_name, \"index_name\": idx_name},\n+                                    ).scalar()\n+                                    is not None\n+                                )\n+                            else:\n+                                exists_query = text(\n+                                    \"\"\"SELECT 1 FROM information_schema.statistics\n+                                    WHERE table_name = :table\n+                                    AND index_name = :index_name\"\"\"\n+                                )\n+                                exists = (\n+                                    sess.execute(\n+                                        exists_query, {\"table\": self.table_name, \"index_name\": idx_name}\n+                                    ).scalar()\n+                                    is not None\n+                                )\n+\n+                        if not exists:\n+                            idx.create(self.db_engine)\n+                        else:\n+                            log_debug(f\"Index {idx_name} already exists, skipping creation\")\n+\n+                    except Exception as e:\n+                        # Log the error but continue with other indexes\n+                        logger.warning(f\"Error creating index {idx.name}: {e}\")\n+\n+            except Exception as e:\n+                logger.error(f\"Could not create table: '{self.table.fullname}': {e}\")\n+                raise\n+\n+    def read(self, session_id: str, user_id: Optional[str] = None) -> Optional[Session]:\n+        \"\"\"\n+        Read an Session from the database.\n+\n+        Args:\n+            session_id (str): ID of the session to read.\n+            user_id (Optional[str]): User ID to filter by. Defaults to None.\n+\n+        Returns:\n+            Optional[Session]: Session object if found, None otherwise.\n+        \"\"\"\n+        try:\n+            with self.Session() as sess:\n+                stmt = select(self.table).where(self.table.c.session_id == session_id)\n+                if user_id:\n+                    stmt = stmt.where(self.table.c.user_id == user_id)\n+                result = sess.execute(stmt).fetchone()\n+                if self.mode == \"agent\":\n+                    return AgentSession.from_dict(result._mapping) if result is not None else None\n+                elif self.mode == \"team\":\n+                    return TeamSession.from_dict(result._mapping) if result is not None else None\n+                elif self.mode == \"workflow\":\n+                    return WorkflowSession.from_dict(result._mapping) if result is not None else None\n+        except Exception as e:\n+            if \"doesn't exist\" in str(e) or \"doesn't exist\" in str(e):\n+                log_debug(f\"Table does not exist: {self.table.name}\")\n+                log_debug(\"Creating table for future transactions\")\n+                self.create()\n+            else:\n+                log_debug(f\"Exception reading from table: {e}\")\n+        return None\n+\n+    def get_all_session_ids(self, user_id: Optional[str] = None, entity_id: Optional[str] = None) -> List[str]:\n+        \"\"\"\n+        Get all session IDs, optionally filtered by user_id and/or entity_id.\n+\n+        Args:\n+            user_id (Optional[str]): The ID of the user to filter by.\n+            entity_id (Optional[str]): The ID of the agent / workflow to filter by.\n+\n+        Returns:\n+            List[str]: List of session IDs matching the criteria.\n+        \"\"\"\n+        try:\n+            with self.Session() as sess, sess.begin():\n+                # get all session_ids\n+                stmt = select(self.table.c.session_id)\n+                if user_id is not None:\n+                    stmt = stmt.where(self.table.c.user_id == user_id)\n+                if entity_id is not None:\n+                    if self.mode == \"agent\":\n+                        stmt = stmt.where(self.table.c.agent_id == entity_id)\n+                    elif self.mode == \"team\":\n+                        stmt = stmt.where(self.table.c.team_id == entity_id)\n+                    elif self.mode == \"workflow\":\n+                        stmt = stmt.where(self.table.c.workflow_id == entity_id)\n+\n+                # order by created_at desc\n+                stmt = stmt.order_by(self.table.c.created_at.desc())\n+                # execute query\n+                rows = sess.execute(stmt).fetchall()\n+                return [row[0] for row in rows] if rows is not None else []\n+        except Exception as e:\n+            log_debug(f\"Exception reading from table: {e}\")\n+            log_debug(f\"Table does not exist: {self.table.name}\")\n+            log_debug(\"Creating table for future transactions\")\n+            self.create()\n+        return []\n+\n+    def get_all_sessions(self, user_id: Optional[str] = None, entity_id: Optional[str] = None) -> List[Session]:\n+        \"\"\"\n+        Get all sessions, optionally filtered by user_id and/or entity_id.\n+\n+        Args:\n+            user_id (Optional[str]): The ID of the user to filter by.\n+            entity_id (Optional[str]): The ID of the agent / workflow to filter by.\n+\n+        Returns:\n+            List[Session]: List of Session objects matching the criteria.\n+        \"\"\"\n+        try:\n+            with self.Session() as sess, sess.begin():\n+                # get all sessions\n+                stmt = select(self.table)\n+                if user_id is not None:\n+                    stmt = stmt.where(self.table.c.user_id == user_id)\n+                if entity_id is not None:\n+                    if self.mode == \"agent\":\n+                        stmt = stmt.where(self.table.c.agent_id == entity_id)\n+                    elif self.mode == \"team\":\n+                        stmt = stmt.where(self.table.c.team_id == entity_id)\n+                    else:\n+                        stmt = stmt.where(self.table.c.workflow_id == entity_id)\n+                # order by created_at desc\n+                stmt = stmt.order_by(self.table.c.created_at.desc())\n+                # execute query\n+                rows = sess.execute(stmt).fetchall()\n+                if rows is not None:\n+                    if self.mode == \"agent\":\n+                        return [AgentSession.from_dict(row._mapping) for row in rows]  # type: ignore\n+                    elif self.mode == \"team\":\n+                        return [TeamSession.from_dict(row._mapping) for row in rows]  # type: ignore\n+                    else:\n+                        return [WorkflowSession.from_dict(row._mapping) for row in rows]  # type: ignore\n+                else:\n+                    return []\n+        except Exception as e:\n+            log_debug(f\"Exception reading from table: {e}\")\n+            log_debug(f\"Table does not exist: {self.table.name}\")\n+            log_debug(\"Creating table for future transactions\")\n+            self.create()\n+        return []\n+\n+    def get_recent_sessions(\n+        self,\n+        user_id: Optional[str] = None,\n+        entity_id: Optional[str] = None,\n+        limit: Optional[int] = 2,\n+    ) -> List[Session]:\n+        \"\"\"Get the last N sessions, ordered by created_at descending.\n+\n+        Args:\n+            num_history_sessions: Number of most recent sessions to return\n+            user_id: Filter by user ID\n+            entity_id: Filter by entity ID (agent_id, team_id, or workflow_id)\n+\n+        Returns:\n+            List[Session]: List of most recent sessions\n+        \"\"\"\n+        try:\n+            with self.Session() as sess, sess.begin():\n+                # Build the base query\n+                stmt = select(self.table)\n+\n+                # Add filters\n+                if user_id is not None:\n+                    stmt = stmt.where(self.table.c.user_id == user_id)\n+                if entity_id is not None:\n+                    if self.mode == \"agent\":\n+                        stmt = stmt.where(self.table.c.agent_id == entity_id)\n+                    elif self.mode == \"team\":\n+                        stmt = stmt.where(self.table.c.team_id == entity_id)\n+                    elif self.mode == \"workflow\":\n+                        stmt = stmt.where(self.table.c.workflow_id == entity_id)\n+\n+                # Order by created_at desc and limit results\n+                stmt = stmt.order_by(self.table.c.created_at.desc())\n+                if limit is not None:\n+                    stmt = stmt.limit(limit)\n+\n+                # Execute query\n+                rows = sess.execute(stmt).fetchall()\n+                if rows is not None:\n+                    sessions: List[Session] = []\n+                    for row in rows:\n+                        session: Optional[Session] = None\n+                        if self.mode == \"agent\":\n+                            session = AgentSession.from_dict(row._mapping)  # type: ignore\n+                        elif self.mode == \"team\":\n+                            session = TeamSession.from_dict(row._mapping)  # type: ignore\n+                        elif self.mode == \"workflow\":\n+                            session = WorkflowSession.from_dict(row._mapping)  # type: ignore\n+\n+                        if session is not None:\n+                            sessions.append(session)\n+                    return sessions\n+                return []\n+\n+        except Exception as e:\n+            if \"doesn't exist\" in str(e) or \"doesn't exist\" in str(e):\n+                log_debug(f\"Table does not exist: {self.table.name}\")\n+                log_debug(\"Creating table for future transactions\")\n+                self.create()\n+            else:\n+                log_debug(f\"Exception reading from table: {e}\")\n+            return []\n+\n+    def upgrade_schema(self) -> None:\n+        \"\"\"\n+        Upgrade the schema to the latest version.\n+        Currently handles adding the team_session_id column for agent mode.\n+        \"\"\"\n+        if not self.auto_upgrade_schema:\n+            log_debug(\"Auto schema upgrade disabled. Skipping upgrade.\")\n+            return\n+\n+        try:\n+            if self.mode == \"agent\" and self.table_exists():\n+                with self.Session() as sess:\n+                    # Check if team_session_id column exists\n+                    column_exists_query = text(\n+                        \"\"\"\n+                        SELECT 1 FROM information_schema.columns\n+                        WHERE table_schema = :schema AND table_name = :table\n+                        AND column_name = 'team_session_id'\n+                        \"\"\"\n+                    )\n+                    column_exists = (\n+                        sess.execute(column_exists_query, {\"schema\": self.schema, \"table\": self.table_name}).scalar()\n+                        is not None\n+                    )\n+\n+                    if not column_exists:\n+                        log_info(f\"Adding 'team_session_id' column to {self.schema}.{self.table_name}\")\n+                        alter_table_query = text(\n+                            f\"ALTER TABLE `{self.schema}`.`{self.table_name}` ADD COLUMN team_session_id VARCHAR(255)\"\n+                        )\n+                        sess.execute(alter_table_query)\n+                        sess.commit()\n+                        self._schema_up_to_date = True\n+                        log_info(\"Schema upgrade completed successfully\")\n+        except Exception as e:\n+            logger.error(f\"Error during schema upgrade: {e}\")\n+            raise\n+\n+    def upsert(self, session: Session, create_and_retry: bool = True) -> Optional[Session]:\n+        \"\"\"\n+        Insert or update an Session in the database.\n+\n+        Args:\n+            session (Session): The session data to upsert.\n+            create_and_retry (bool): Retry upsert if table does not exist.\n+\n+        Returns:\n+            Optional[Session]: The upserted Session, or None if operation failed.\n+        \"\"\"\n+        # Perform schema upgrade if auto_upgrade_schema is enabled\n+        if self.auto_upgrade_schema and not self._schema_up_to_date:\n+            self.upgrade_schema()\n+\n+        try:\n+            with self.Session() as sess, sess.begin():\n+                # Create an insert statement\n+                if self.mode == \"agent\":\n+                    stmt = mysql.insert(self.table).values(\n+                        session_id=session.session_id,\n+                        agent_id=session.agent_id,  # type: ignore\n+                        team_session_id=session.team_session_id,  # type: ignore\n+                        user_id=session.user_id,\n+                        memory=session.memory,\n+                        agent_data=session.agent_data,  # type: ignore\n+                        session_data=session.session_data,\n+                        extra_data=session.extra_data,\n+                    )\n+                    # Define the upsert if the session_id already exists\n+                    # See: https://docs.sqlalchemy.org/en/20/dialects/mysql.html#insert-on-duplicate-key-update\n+                    stmt = stmt.on_duplicate_key_update(\n+                        agent_id=session.agent_id,  # type: ignore\n+                        team_session_id=session.team_session_id,  # type: ignore\n+                        user_id=session.user_id,\n+                        memory=session.memory,\n+                        agent_data=session.agent_data,  # type: ignore\n+                        session_data=session.session_data,\n+                        extra_data=session.extra_data,\n+                        updated_at=int(time.time()),\n+                    )\n+                elif self.mode == \"team\":\n+                    stmt = mysql.insert(self.table).values(\n+                        session_id=session.session_id,\n+                        team_id=session.team_id,  # type: ignore\n+                        user_id=session.user_id,\n+                        team_session_id=session.team_session_id,  # type: ignore\n+                        memory=session.memory,\n+                        team_data=session.team_data,  # type: ignore\n+                        session_data=session.session_data,\n+                        extra_data=session.extra_data,\n+                    )\n+                    # Define the upsert if the session_id already exists\n+                    # See: https://docs.sqlalchemy.org/en/20/dialects/mysql.html#insert-on-duplicate-key-update\n+                    stmt = stmt.on_duplicate_key_update(\n+                        team_id=session.team_id,  # type: ignore\n+                        user_id=session.user_id,\n+                        team_session_id=session.team_session_id,  # type: ignore\n+                        memory=session.memory,\n+                        team_data=session.team_data,  # type: ignore\n+                        session_data=session.session_data,\n+                        extra_data=session.extra_data,\n+                        updated_at=int(time.time()),\n+                    )\n+                else:\n+                    stmt = mysql.insert(self.table).values(\n+                        session_id=session.session_id,\n+                        workflow_id=session.workflow_id,  # type: ignore\n+                        user_id=session.user_id,\n+                        memory=session.memory,\n+                        workflow_data=session.workflow_data,  # type: ignore\n+                        session_data=session.session_data,\n+                        extra_data=session.extra_data,\n+                    )\n+                    # Define the upsert if the session_id already exists\n+                    # See: https://docs.sqlalchemy.org/en/20/dialects/mysql.html#insert-on-duplicate-key-update\n+                    stmt = stmt.on_duplicate_key_update(\n+                        workflow_id=session.workflow_id,  # type: ignore\n+                        user_id=session.user_id,\n+                        memory=session.memory,\n+                        workflow_data=session.workflow_data,  # type: ignore\n+                        session_data=session.session_data,\n+                        extra_data=session.extra_data,\n+                        updated_at=int(time.time()),\n+                    )\n+\n+                sess.execute(stmt)\n+        except Exception as e:\n+            if create_and_retry and not self.table_exists():\n+                log_debug(f\"Table does not exist: {self.table.name}\")\n+                log_debug(\"Creating table and retrying upsert\")\n+                self.create()\n+                return self.upsert(session, create_and_retry=False)\n+            else:\n+                log_warning(f\"Exception upserting into table: {e}\")\n+                log_warning(\n+                    \"A table upgrade might be required, please review these docs for more information: https://agno.link/upgrade-schema\"\n+                )\n+                return None\n+        return self.read(session_id=session.session_id)\n+\n+    def delete_session(self, session_id: Optional[str] = None):\n+        \"\"\"\n+        Delete a session from the database.\n+\n+        Args:\n+            session_id (Optional[str], optional): ID of the session to delete. Defaults to None.\n+\n+        Raises:\n+            Exception: If an error occurs during deletion.\n+        \"\"\"\n+        if session_id is None:\n+            logger.warning(\"No session_id provided for deletion.\")\n+            return\n+\n+        try:\n+            with self.Session() as sess, sess.begin():\n+                # Delete the session with the given session_id\n+                delete_stmt = self.table.delete().where(self.table.c.session_id == session_id)\n+                result = sess.execute(delete_stmt)\n+                if result.rowcount == 0:\n+                    log_debug(f\"No session found with session_id: {session_id}\")\n+                else:\n+                    log_debug(f\"Successfully deleted session with session_id: {session_id}\")\n+        except Exception as e:\n+            logger.error(f\"Error deleting session: {e}\")\n+\n+    def drop(self) -> None:\n+        \"\"\"\n+        Drop the table from the database if it exists.\n+        \"\"\"\n+        if self.table_exists():\n+            log_debug(f\"Deleting table: {self.table_name}\")\n+            # Drop with checkfirst=True to avoid errors if the table doesn't exist\n+            self.table.drop(self.db_engine, checkfirst=True)\n+            # Clear metadata to ensure indexes are recreated properly\n+            self.metadata = MetaData(schema=self.schema)\n+            self.table = self.get_table()\n+\n+    def __deepcopy__(self, memo):\n+        \"\"\"\n+        Create a deep copy of the MySQLStorage instance, handling unpickleable attributes.\n+\n+        Args:\n+            memo (dict): A dictionary of objects already copied during the current copying pass.\n+\n+        Returns:\n+            MySQLStorage: A deep-copied instance of MySQLStorage.\n+        \"\"\"\n+        from copy import deepcopy\n+\n+        # Create a new instance without calling __init__\n+        cls = self.__class__\n+        copied_obj = cls.__new__(cls)\n+        memo[id(self)] = copied_obj\n+\n+        # Deep copy attributes\n+        for k, v in self.__dict__.items():\n+            if k in {\"metadata\", \"table\", \"inspector\"}:\n+                continue\n+            # Reuse db_engine and Session without copying\n+            elif k in {\"db_engine\", \"SqlSession\"}:\n+                setattr(copied_obj, k, v)\n+            else:\n+                setattr(copied_obj, k, deepcopy(v, memo))\n+\n+        # Recreate metadata and table for the copied instance\n+        copied_obj.metadata = MetaData(schema=copied_obj.schema)\n+        copied_obj.inspector = inspect(copied_obj.db_engine)\n+        copied_obj.table = copied_obj.get_table()\n+\n+        return copied_obj\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/team/team.py",
            "diff": "diff --git a/libs/agno/agno/team/team.py b/libs/agno/agno/team/team.py\nindex 80994ff31..f01ff1dca 100644\n--- a/libs/agno/agno/team/team.py\n+++ b/libs/agno/agno/team/team.py\n@@ -50,6 +50,8 @@ from agno.tools.toolkit import Toolkit\n from agno.utils.events import (\n     create_team_memory_update_completed_event,\n     create_team_memory_update_started_event,\n+    create_team_parser_model_response_completed_event,\n+    create_team_parser_model_response_started_event,\n     create_team_reasoning_completed_event,\n     create_team_reasoning_started_event,\n     create_team_reasoning_step_event,\n@@ -217,6 +219,10 @@ class Team:\n     # --- Structured output ---\n     # Response model for the team response\n     response_model: Optional[Type[BaseModel]] = None\n+    # Provide a secondary model to parse the response from the primary model\n+    parser_model: Optional[Model] = None\n+    # Provide a prompt for the parser model\n+    parser_model_prompt: Optional[str] = None\n     # If `response_model` is set, sets the response mode of the model, i.e. if the model should explicitly respond with a JSON object instead of a Pydantic model\n     use_json_mode: bool = False\n     # If True, parse the response\n@@ -276,6 +282,8 @@ class Team:\n     # --- Debug & Monitoring ---\n     # Enable debug logs\n     debug_mode: bool = False\n+    # Debug level: 1 = basic, 2 = detailed\n+    debug_level: Literal[1, 2] = 1\n     # Enable member logs - Sets the debug_mode for team and members\n     show_members_responses: bool = False\n     # monitoring=True logs Team information to agno.com for monitoring\n@@ -327,6 +335,8 @@ class Team:\n         tool_choice: Optional[Union[str, Dict[str, Any]]] = None,\n         tool_hooks: Optional[List[Callable]] = None,\n         response_model: Optional[Type[BaseModel]] = None,\n+        parser_model: Optional[Model] = None,\n+        parser_model_prompt: Optional[str] = None,\n         use_json_mode: bool = False,\n         parse_response: bool = True,\n         memory: Optional[Union[TeamMemory, Memory]] = None,\n@@ -352,6 +362,7 @@ class Team:\n         events_to_skip: Optional[List[Union[RunEvent, TeamRunEvent]]] = None,\n         stream_member_events: bool = True,\n         debug_mode: bool = False,\n+        debug_level: Literal[1, 2] = 1,\n         show_members_responses: bool = False,\n         monitoring: bool = False,\n         telemetry: bool = True,\n@@ -407,6 +418,8 @@ class Team:\n         self.tool_hooks = tool_hooks\n \n         self.response_model = response_model\n+        self.parser_model = parser_model\n+        self.parser_model_prompt = parser_model_prompt\n         self.use_json_mode = use_json_mode\n         self.parse_response = parse_response\n \n@@ -445,6 +458,10 @@ class Team:\n         self.stream_member_events = stream_member_events\n \n         self.debug_mode = debug_mode\n+        if debug_level not in [1, 2]:\n+            log_warning(f\"Invalid debug level: {debug_level}. Setting to 1.\")\n+            debug_level = 1\n+        self.debug_level = debug_level\n         self.show_members_responses = show_members_responses\n \n         self.monitoring = monitoring\n@@ -482,7 +499,7 @@ class Team:\n \n     @property\n     def should_parse_structured_output(self) -> bool:\n-        return self.response_model is not None and self.parse_response\n+        return self.response_model is not None and self.parse_response and self.parser_model is None\n \n     def _set_team_id(self) -> str:\n         if self.team_id is None:\n@@ -492,7 +509,7 @@ class Team:\n     def _set_debug(self) -> None:\n         if self.debug_mode or getenv(\"AGNO_DEBUG\", \"false\").lower() == \"true\":\n             self.debug_mode = True\n-            set_log_level_to_debug(source_type=\"team\")\n+            set_log_level_to_debug(source_type=\"team\", level=self.debug_level)\n         else:\n             set_log_level_to_info(source_type=\"team\")\n \n@@ -516,6 +533,7 @@ class Team:\n         # Set debug mode for all members\n         if self.debug_mode:\n             member.debug_mode = True\n+            member.debug_level = self.debug_level\n         if self.show_tool_calls:\n             member.show_tool_calls = True\n         if self.markdown:\n@@ -540,8 +558,6 @@ class Team:\n             member.parent_team_id = self.team_id\n             for sub_member in member.members:\n                 self._initialize_member(sub_member, session_id)\n-        if member.name is None:\n-            log_warning(\"Team member name is undefined.\")\n \n     def _set_default_model(self) -> None:\n         # Set the default model\n@@ -607,7 +623,7 @@ class Team:\n             self.memory = Memory()\n         elif not self._memory_deepcopy_done:\n             # We store a copy of memory to ensure different team instances reference unique memory copy\n-            if isinstance(self.memory, Memory):\n+            if isinstance(self.memory, Memory) and self.parent_team_id is not None:\n                 self.memory = deepcopy(self.memory)\n             self._memory_deepcopy_done = True\n \n@@ -708,6 +724,8 @@ class Team:\n                 # Generate a new session_id and store it in the agent\n                 session_id = str(uuid4())\n                 self.session_id = session_id\n+        else:\n+            self.session_id = session_id\n \n         session_id = cast(str, session_id)\n \n@@ -763,7 +781,9 @@ class Team:\n \n         # Configure the model for runs\n         self._set_default_model()\n-        response_format: Optional[Union[Dict, Type[BaseModel]]] = self._get_response_format()\n+        response_format: Optional[Union[Dict, Type[BaseModel]]] = (\n+            self._get_response_format() if self.parser_model is None else None\n+        )\n \n         self.model = cast(Model, self.model)\n         self.determine_tools_for_model(\n@@ -940,6 +960,9 @@ class Team:\n             tool_call_limit=self.tool_call_limit,\n         )\n \n+        # If a parser model is provided, structure the response separately\n+        self._parse_response_with_parser_model(model_response, run_messages)\n+\n         #  Update TeamRunResponse\n         self._update_run_response(model_response=model_response, run_response=run_response, run_messages=run_messages)\n \n@@ -1013,6 +1036,11 @@ class Team:\n             stream_intermediate_steps=stream_intermediate_steps,\n         )\n \n+        # If a parser model is provided, structure the response separately\n+        yield from self._parse_response_with_parser_model_stream(\n+            run_response=run_response, stream_intermediate_steps=stream_intermediate_steps\n+        )\n+\n         # 3. Add the run to memory\n         self._add_run_to_memory(\n             run_response=run_response,\n@@ -1029,9 +1057,6 @@ class Team:\n             user_id=user_id,\n         )\n \n-        # 5. Parse team response model\n-        self._convert_response_to_structured_format(run_response=run_response)\n-\n         if stream_intermediate_steps:\n             yield self._handle_event(\n                 create_team_run_response_completed_event(\n@@ -1040,10 +1065,10 @@ class Team:\n                 run_response,\n             )\n \n-        # 6. Save session to storage\n+        # 5. Save session to storage\n         self.write_to_storage(session_id=session_id, user_id=user_id)\n \n-        # 7. Log Team Run\n+        # 6. Log Team Run\n         self._log_team_run(session_id=session_id, user_id=user_id)\n \n         log_debug(f\"Team Run End: {self.run_id}\", center=True, symbol=\"*\")\n@@ -1120,6 +1145,8 @@ class Team:\n                 # Generate a new session_id and store it in the team\n                 session_id = str(uuid4())\n                 self.session_id = session_id\n+        else:\n+            self.session_id = session_id\n \n         session_id = cast(str, session_id)\n \n@@ -1168,7 +1195,9 @@ class Team:\n \n         # Configure the model for runs\n         self._set_default_model()\n-        response_format = self._get_response_format()\n+        response_format: Optional[Union[Dict, Type[BaseModel]]] = (\n+            self._get_response_format() if self.parser_model is None else None\n+        )\n \n         self.model = cast(Model, self.model)\n         self.determine_tools_for_model(\n@@ -1343,6 +1372,9 @@ class Team:\n             tool_call_limit=self.tool_call_limit,\n         )  # type: ignore\n \n+        # If a parser model is provided, structure the response separately\n+        await self._aparse_response_with_parser_model(model_response=model_response, run_messages=run_messages)\n+\n         # Update TeamRunResponse\n         self._update_run_response(model_response=model_response, run_response=run_response, run_messages=run_messages)\n \n@@ -1416,6 +1448,12 @@ class Team:\n         ):\n             yield event\n \n+        # If a parser model is provided, structure the response separately\n+        async for event in self._aparse_response_with_parser_model_stream(\n+            run_response=run_response, stream_intermediate_steps=stream_intermediate_steps\n+        ):\n+            yield event\n+\n         # 3. Add the run to memory\n         self._add_run_to_memory(\n             run_response=run_response,\n@@ -1433,18 +1471,15 @@ class Team:\n         ):\n             yield event\n \n-        # 5. Parse team response model\n-        self._convert_response_to_structured_format(run_response=run_response)\n-\n         if stream_intermediate_steps:\n             yield self._handle_event(\n                 create_team_run_response_completed_event(from_run_response=run_response), run_response\n             )\n \n-        # 6. Save session to storage\n+        # 5. Save session to storage\n         self.write_to_storage(session_id=session_id, user_id=user_id)\n \n-        # 7. Log Team Run\n+        # 6. Log Team Run\n         await self._alog_team_run(session_id=session_id, user_id=user_id)\n \n         log_debug(f\"Team Run End: {self.run_id}\", center=True, symbol=\"*\")\n@@ -1596,6 +1631,7 @@ class Team:\n \n             # 10. Calculate session metrics\n             self.session_metrics = self._calculate_session_metrics(session_messages)\n+            self.full_team_session_metrics = self._calculate_full_team_session_metrics(session_messages)\n \n     async def _aupdate_memory(\n         self,\n@@ -1641,6 +1677,7 @@ class Team:\n \n             # 10. Calculate session metrics\n             self.session_metrics = self._calculate_session_metrics(session_messages)\n+            self.full_team_session_metrics = self._calculate_full_team_session_metrics(session_messages)\n \n     def _handle_model_response_stream(\n         self,\n@@ -1675,8 +1712,9 @@ class Team:\n                 run_response=run_response,\n                 full_model_response=full_model_response,\n                 model_response_event=model_response_event,\n-                stream_intermediate_steps=stream_intermediate_steps,\n                 reasoning_state=reasoning_state,\n+                stream_intermediate_steps=stream_intermediate_steps,\n+                parse_structured_output=self.should_parse_structured_output,\n             )\n \n         # 3. Update TeamRunResponse\n@@ -1755,8 +1793,9 @@ class Team:\n                 run_response=run_response,\n                 full_model_response=full_model_response,\n                 model_response_event=model_response_event,\n-                stream_intermediate_steps=stream_intermediate_steps,\n                 reasoning_state=reasoning_state,\n+                stream_intermediate_steps=stream_intermediate_steps,\n+                parse_structured_output=self.should_parse_structured_output,\n             ):\n                 yield chunk\n \n@@ -1808,8 +1847,9 @@ class Team:\n         run_response: TeamRunResponse,\n         full_model_response: ModelResponse,\n         model_response_event: Union[ModelResponse, TeamRunResponseEvent, RunResponseEvent],\n-        reasoning_state: Dict[str, Any],\n+        reasoning_state: Optional[Dict[str, Any]] = None,\n         stream_intermediate_steps: bool = False,\n+        parse_structured_output: bool = False,\n     ) -> Iterator[Union[TeamRunResponseEvent, RunResponseEvent]]:\n         if isinstance(model_response_event, tuple(get_args(RunResponseEvent))) or isinstance(\n             model_response_event, tuple(get_args(TeamRunResponseEvent))\n@@ -1829,12 +1869,17 @@ class Team:\n                 should_yield = False\n                 # Process content and thinking\n                 if model_response_event.content is not None:\n-                    if self.should_parse_structured_output:\n+                    if parse_structured_output:\n                         full_model_response.content = model_response_event.content\n+                        self._convert_response_to_structured_format(full_model_response)\n                         content_type = self.response_model.__name__  # type: ignore\n                         run_response.content_type = content_type\n+                    elif self._member_response_model is not None:\n+                        full_model_response.content = model_response_event.content\n                         self._convert_response_to_structured_format(full_model_response)\n-                    else:\n+                        content_type = self._member_response_model.__name__  # type: ignore\n+                        run_response.content_type = content_type\n+                    elif isinstance(model_response_event.content, str):\n                         full_model_response.content = (full_model_response.content or \"\") + model_response_event.content\n                     should_yield = True\n \n@@ -1957,7 +2002,7 @@ class Team:\n                             )\n \n                             metrics = tool_call.metrics\n-                            if metrics is not None and metrics.time is not None:\n+                            if metrics is not None and metrics.time is not None and reasoning_state is not None:\n                                 reasoning_state[\"reasoning_time_taken\"] = reasoning_state[\n                                     \"reasoning_time_taken\"\n                                 ] + float(metrics.time)\n@@ -1973,7 +2018,7 @@ class Team:\n \n                 if stream_intermediate_steps:\n                     if reasoning_step is not None:\n-                        if not reasoning_state[\"reasoning_started\"]:\n+                        if reasoning_state is not None and not reasoning_state[\"reasoning_started\"]:\n                             yield self._handle_event(\n                                 create_team_reasoning_started_event(\n                                     from_run_response=run_response,\n@@ -2120,14 +2165,14 @@ class Team:\n                     create_team_memory_update_completed_event(from_run_response=self.run_response), self.run_response\n                 )\n \n-    def _get_response_format(self) -> Optional[Union[Dict, Type[BaseModel]]]:\n-        self.model = cast(Model, self.model)\n+    def _get_response_format(self, model: Optional[Model] = None) -> Optional[Union[Dict, Type[BaseModel]]]:\n+        model = cast(Model, model or self.model)\n         if self.response_model is None:\n             return None\n         else:\n             json_response_format = {\"type\": \"json_object\"}\n \n-            if self.model.supports_native_structured_outputs:\n+            if model.supports_native_structured_outputs:\n                 if not self.use_json_mode:\n                     log_debug(\"Setting Model.response_format to Agent.response_model\")\n                     return self.response_model\n@@ -2137,7 +2182,7 @@ class Team:\n                     )\n                     return json_response_format\n \n-            elif self.model.supports_json_schema_outputs:\n+            elif model.supports_json_schema_outputs:\n                 if self.use_json_mode:\n                     log_debug(\"Setting Model.response_format to JSON response mode\")\n                     return {\n@@ -2154,6 +2199,166 @@ class Team:\n                 log_debug(\"Model does not support structured or JSON schema outputs.\")\n                 return json_response_format\n \n+    def _process_parser_response(\n+        self,\n+        model_response: ModelResponse,\n+        run_messages: RunMessages,\n+        parser_model_response: ModelResponse,\n+        messages_for_parser_model: list,\n+    ) -> None:\n+        \"\"\"Common logic for processing parser model response.\"\"\"\n+        parser_model_response_message: Optional[Message] = None\n+        for message in reversed(messages_for_parser_model):\n+            if message.role == \"assistant\":\n+                parser_model_response_message = message\n+                break\n+\n+        if parser_model_response_message is not None:\n+            run_messages.messages.append(parser_model_response_message)\n+            model_response.parsed = parser_model_response.parsed\n+            model_response.content = parser_model_response.content\n+        else:\n+            log_warning(\"Unable to parse response with parser model\")\n+\n+    def _parse_response_with_parser_model(self, model_response: ModelResponse, run_messages: RunMessages) -> None:\n+        \"\"\"Parse the model response using the parser model.\"\"\"\n+        if self.parser_model is None:\n+            return\n+\n+        if self.response_model is not None:\n+            parser_response_format = self._get_response_format(self.parser_model)\n+            messages_for_parser_model = self.get_messages_for_parser_model(model_response, parser_response_format)\n+            parser_model_response: ModelResponse = self.parser_model.response(\n+                messages=messages_for_parser_model,\n+                response_format=parser_response_format,\n+            )\n+            self._process_parser_response(\n+                model_response, run_messages, parser_model_response, messages_for_parser_model\n+            )\n+        else:\n+            log_warning(\"A response model is required to parse the response with a parser model\")\n+\n+    async def _aparse_response_with_parser_model(\n+        self, model_response: ModelResponse, run_messages: RunMessages\n+    ) -> None:\n+        \"\"\"Parse the model response using the parser model.\"\"\"\n+        if self.parser_model is None:\n+            return\n+\n+        if self.response_model is not None:\n+            parser_response_format = self._get_response_format(self.parser_model)\n+            messages_for_parser_model = self.get_messages_for_parser_model(model_response, parser_response_format)\n+            parser_model_response: ModelResponse = await self.parser_model.aresponse(\n+                messages=messages_for_parser_model,\n+                response_format=parser_response_format,\n+            )\n+            self._process_parser_response(\n+                model_response, run_messages, parser_model_response, messages_for_parser_model\n+            )\n+        else:\n+            log_warning(\"A response model is required to parse the response with a parser model\")\n+\n+    def _parse_response_with_parser_model_stream(\n+        self, run_response: TeamRunResponse, stream_intermediate_steps: bool = True\n+    ):\n+        \"\"\"Parse the model response using the parser model\"\"\"\n+        if self.parser_model is not None:\n+            if self.response_model is not None:\n+                if stream_intermediate_steps:\n+                    yield self._handle_event(\n+                        create_team_parser_model_response_started_event(run_response), run_response\n+                    )\n+\n+                parser_model_response = ModelResponse(content=\"\")\n+                parser_response_format = self._get_response_format(self.parser_model)\n+                messages_for_parser_model = self.get_messages_for_parser_model_stream(\n+                    run_response, parser_response_format\n+                )\n+                for model_response_event in self.parser_model.response_stream(\n+                    messages=messages_for_parser_model,\n+                    response_format=parser_response_format,\n+                    stream_model_response=False,\n+                ):\n+                    yield from self._handle_model_response_chunk(\n+                        run_response=run_response,\n+                        full_model_response=parser_model_response,\n+                        model_response_event=model_response_event,\n+                        parse_structured_output=True,\n+                        stream_intermediate_steps=stream_intermediate_steps,\n+                    )\n+\n+                run_response.content = parser_model_response.content\n+\n+                parser_model_response_message: Optional[Message] = None\n+                for message in reversed(messages_for_parser_model):\n+                    if message.role == \"assistant\":\n+                        parser_model_response_message = message\n+                        break\n+                if parser_model_response_message is not None:\n+                    if run_response.messages is not None:\n+                        run_response.messages.append(parser_model_response_message)\n+                else:\n+                    log_warning(\"Unable to parse response with parser model\")\n+\n+                if stream_intermediate_steps:\n+                    yield self._handle_event(\n+                        create_team_parser_model_response_completed_event(run_response), run_response\n+                    )\n+\n+            else:\n+                log_warning(\"A response model is required to parse the response with a parser model\")\n+\n+    async def _aparse_response_with_parser_model_stream(\n+        self, run_response: TeamRunResponse, stream_intermediate_steps: bool = True\n+    ):\n+        \"\"\"Parse the model response using the parser model stream.\"\"\"\n+        if self.parser_model is not None:\n+            if self.response_model is not None:\n+                if stream_intermediate_steps:\n+                    yield self._handle_event(\n+                        create_team_parser_model_response_started_event(run_response), run_response\n+                    )\n+\n+                parser_model_response = ModelResponse(content=\"\")\n+                parser_response_format = self._get_response_format(self.parser_model)\n+                messages_for_parser_model = self.get_messages_for_parser_model_stream(\n+                    run_response, parser_response_format\n+                )\n+                model_response_stream = self.parser_model.aresponse_stream(\n+                    messages=messages_for_parser_model,\n+                    response_format=parser_response_format,\n+                    stream_model_response=False,\n+                )\n+                async for model_response_event in model_response_stream:  # type: ignore\n+                    for event in self._handle_model_response_chunk(\n+                        run_response=run_response,\n+                        full_model_response=parser_model_response,\n+                        model_response_event=model_response_event,\n+                        parse_structured_output=True,\n+                        stream_intermediate_steps=stream_intermediate_steps,\n+                    ):\n+                        yield event\n+\n+                run_response.content = parser_model_response.content\n+\n+                parser_model_response_message: Optional[Message] = None\n+                for message in reversed(messages_for_parser_model):\n+                    if message.role == \"assistant\":\n+                        parser_model_response_message = message\n+                        break\n+                if parser_model_response_message is not None:\n+                    if run_response.messages is not None:\n+                        run_response.messages.append(parser_model_response_message)\n+                else:\n+                    log_warning(\"Unable to parse response with parser model\")\n+\n+                if stream_intermediate_steps:\n+                    yield self._handle_event(\n+                        create_team_parser_model_response_completed_event(run_response), run_response\n+                    )\n+            else:\n+                log_warning(\"A response model is required to parse the response with a parser model\")\n+\n     def _handle_event(self, event: Union[RunResponseEvent, TeamRunResponseEvent], run_response: TeamRunResponse):\n         # We only store events that are not run_response_content events\n         events_to_skip = [event.value for event in self.events_to_skip] if self.events_to_skip else []\n@@ -2624,7 +2829,6 @@ class Team:\n                             try:\n                                 _response_content = JSON(resp.content.model_dump_json(exclude_none=True), indent=2)  # type: ignore\n                             except Exception as e:\n-                                print(_response_content)\n                                 log_warning(f\"Failed to convert response to JSON: {e}\")\n                         if resp.thinking is not None:\n                             _response_thinking += resp.thinking\n@@ -3963,12 +4167,23 @@ class Team:\n \n         # Get metrics of the team-agent's messages\n         for member in self.members:\n-            # Only members that ran has memory\n+            # Only members with memory\n             if member.memory is not None:\n+                # Handle instances with AgentMemory\n                 if isinstance(member.memory, AgentMemory):\n                     for m in member.memory.messages:\n                         if m.role == assistant_message_role and m.metrics is not None:\n                             current_session_metrics += m.metrics\n+                # Handle instances with Memory v2\n+                elif isinstance(member.memory, Memory):\n+                    if member.memory.runs is not None:\n+                        for runs in member.memory.runs.values():\n+                            for run in runs:\n+                                if run is not None and run.messages is not None:\n+                                    for m in run.messages:\n+                                        if m.role == assistant_message_role and m.metrics is not None:\n+                                            current_session_metrics += m.metrics\n+\n         return current_session_metrics\n \n     def _aggregate_metrics_from_messages(self, messages: List[Message]) -> Dict[str, Any]:\n@@ -3987,7 +4202,11 @@ class Team:\n \n     def _get_reasoning_agent(self, reasoning_model: Model) -> Optional[Agent]:\n         return Agent(\n-            model=reasoning_model, monitoring=self.monitoring, telemetry=self.telemetry, debug_mode=self.debug_mode\n+            model=reasoning_model,\n+            monitoring=self.monitoring,\n+            telemetry=self.telemetry,\n+            debug_mode=self.debug_mode,\n+            debug_level=self.debug_level,\n         )\n \n     def _format_reasoning_step_content(self, run_response: TeamRunResponse, reasoning_step: ReasoningStep) -> str:\n@@ -4135,6 +4354,7 @@ class Team:\n                     monitoring=self.monitoring,\n                     telemetry=self.telemetry,\n                     debug_mode=self.debug_mode,\n+                    debug_level=self.debug_level,\n                     use_json_mode=use_json_mode,\n                 )\n \n@@ -4355,6 +4575,7 @@ class Team:\n                     monitoring=self.monitoring,\n                     telemetry=self.telemetry,\n                     debug_mode=self.debug_mode,\n+                    debug_level=self.debug_level,\n                     use_json_mode=use_json_mode,\n                 )\n \n@@ -4745,8 +4966,9 @@ class Team:\n                     system_message_content += member.get_members_system_message_content(indent=indent + 2)\n             else:\n                 system_message_content += f\"{indent * ' '} - Agent {idx + 1}:\\n\"\n-                if member.name is not None:\n+                if url_safe_member_id is not None:\n                     system_message_content += f\"{indent * ' '}   - ID: {url_safe_member_id}\\n\"\n+                if member.name is not None:\n                     system_message_content += f\"{indent * ' '}   - Name: {member.name}\\n\"\n                 if member.role is not None:\n                     system_message_content += f\"{indent * ' '}   - Role: {member.role}\\n\"\n@@ -5235,6 +5457,46 @@ class Team:\n             except Exception as e:\n                 log_warning(f\"Failed to validate message: {e}\")\n \n+    def get_messages_for_parser_model(\n+        self, model_response: ModelResponse, response_format: Optional[Union[Dict, Type[BaseModel]]]\n+    ) -> List[Message]:\n+        from agno.utils.prompts import get_json_output_prompt\n+\n+        \"\"\"Get the messages for the parser model.\"\"\"\n+        system_content = (\n+            self.parser_model_prompt\n+            if self.parser_model_prompt is not None\n+            else \"You are tasked with creating a structured output from the provided user message.\"\n+        )\n+\n+        if response_format == {\"type\": \"json_object\"} and self.response_model is not None:\n+            system_content += f\"{get_json_output_prompt(self.response_model)}\"  # type: ignore\n+\n+        return [\n+            Message(role=\"system\", content=system_content),\n+            Message(role=\"user\", content=model_response.content),\n+        ]\n+\n+    def get_messages_for_parser_model_stream(\n+        self, run_response: TeamRunResponse, response_format: Optional[Union[Dict, Type[BaseModel]]]\n+    ) -> List[Message]:\n+        \"\"\"Get the messages for the parser model.\"\"\"\n+        from agno.utils.prompts import get_json_output_prompt\n+\n+        system_content = (\n+            self.parser_model_prompt\n+            if self.parser_model_prompt is not None\n+            else \"You are tasked with creating a structured output from the provided data.\"\n+        )\n+\n+        if response_format == {\"type\": \"json_object\"} and self.response_model is not None:\n+            system_content += f\"{get_json_output_prompt(self.response_model)}\"  # type: ignore\n+\n+        return [\n+            Message(role=\"system\", content=system_content),\n+            Message(role=\"user\", content=run_response.content),\n+        ]\n+\n     def _format_message_with_state_variables(self, message: Any, user_id: Optional[str] = None) -> Any:\n         \"\"\"Format a message with the session state variables.\"\"\"\n         import re\n@@ -6123,6 +6385,11 @@ class Team:\n     def _get_member_id(self, member: Union[Agent, \"Team\"]) -> str:\n         \"\"\"\n         Get the ID of a member\n+\n+        If the member has an agent_id or team_id, use that if it is not a valid UUID.\n+        Then if the member has a name, convert that to a URL safe string.\n+        Then if the member has the default UUID ID, use that.\n+        Otherwise, return None.\n         \"\"\"\n         if isinstance(member, Agent) and member.agent_id is not None and (not is_valid_uuid(member.agent_id)):\n             url_safe_member_id = url_safe_string(member.agent_id)\n@@ -6130,6 +6397,10 @@ class Team:\n             url_safe_member_id = url_safe_string(member.team_id)\n         elif member.name is not None:\n             url_safe_member_id = url_safe_string(member.name)\n+        elif isinstance(member, Agent) and member.agent_id is not None:\n+            url_safe_member_id = member.agent_id\n+        elif isinstance(member, Team) and member.team_id is not None:\n+            url_safe_member_id = member.team_id\n         else:\n             url_safe_member_id = None\n         return url_safe_member_id\n@@ -6148,10 +6419,9 @@ class Team:\n         \"\"\"\n         # First check direct members\n         for i, member in enumerate(self.members):\n-            if member.name is not None:\n-                url_safe_member_id = self._get_member_id(member)\n-                if url_safe_member_id == member_id:\n-                    return i, member\n+            url_safe_member_id = self._get_member_id(member)\n+            if url_safe_member_id == member_id:\n+                return i, member\n \n             # If this member is a team, search its members recursively\n             if isinstance(member, Team):\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/tools/csv_toolkit.py",
            "diff": "diff --git a/libs/agno/agno/tools/csv_toolkit.py b/libs/agno/agno/tools/csv_toolkit.py\nindex 1c07f3941..669dc7da4 100644\n--- a/libs/agno/agno/tools/csv_toolkit.py\n+++ b/libs/agno/agno/tools/csv_toolkit.py\n@@ -149,7 +149,9 @@ class CsvTools(Toolkit):\n                 return \"Error connecting to DuckDB, please check the connection.\"\n \n             # Create a table from the csv file\n-            con.execute(f\"CREATE TABLE {csv_name} AS SELECT * FROM read_csv_auto('{file_path}')\")\n+            con.execute(\n+                f\"CREATE TABLE {csv_name} AS SELECT * FROM read_csv('{file_path}', ignore_errors=false, auto_detect=true)\"\n+            )\n \n             # -*- Format the SQL Query\n             # Remove backticks\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/tools/duckdb.py",
            "diff": "diff --git a/libs/agno/agno/tools/duckdb.py b/libs/agno/agno/tools/duckdb.py\nindex ccda52aca..43a0ce28b 100644\n--- a/libs/agno/agno/tools/duckdb.py\n+++ b/libs/agno/agno/tools/duckdb.py\n@@ -33,7 +33,6 @@ class DuckDbTools(Toolkit):\n         tools: List[Any] = []\n         tools.append(self.show_tables)\n         tools.append(self.describe_table)\n-\n         if inspect_queries:\n             tools.append(self.inspect_query)\n         if run_queries:\n@@ -45,6 +44,13 @@ class DuckDbTools(Toolkit):\n         if export_tables:\n             tools.append(self.export_table_to_path)\n \n+        tools.append(self.load_local_path_to_table)\n+        tools.append(self.load_local_csv_to_table)\n+        tools.append(self.load_s3_path_to_table)\n+        tools.append(self.load_s3_csv_to_table)\n+        tools.append(self.create_fts_index)\n+        tools.append(self.full_text_search)\n+\n         super().__init__(name=\"duckdb_tools\", tools=tools, **kwargs)\n \n     @property\n@@ -199,7 +205,12 @@ class DuckDbTools(Toolkit):\n         if replace:\n             create_statement = \"CREATE OR REPLACE TABLE\"\n \n-        create_statement += f\" '{table}' AS SELECT * FROM '{path}';\"\n+        # Check if the file is a CSV\n+        if path.lower().endswith(\".csv\"):\n+            create_statement += f\" {table} AS SELECT * FROM read_csv('{path}', ignore_errors=false, auto_detect=true);\"\n+        else:\n+            create_statement += f\" {table} AS SELECT * FROM '{path}';\"\n+\n         self.run_query(create_statement)\n         log_debug(f\"Created table {table} from {path}\")\n         return table\n@@ -247,7 +258,7 @@ class DuckDbTools(Toolkit):\n             # If the table isn't a valid SQL identifier, we'll need to use something else\n             table = table.replace(\"-\", \"_\").replace(\".\", \"_\").replace(\" \", \"_\").replace(\"/\", \"_\")\n \n-        create_statement = f\"CREATE OR REPLACE TABLE '{table}' AS SELECT * FROM '{path}';\"\n+        create_statement = f\"CREATE OR REPLACE TABLE {table} AS SELECT * FROM '{path}';\"\n         self.run_query(create_statement)\n \n         log_debug(f\"Loaded {path} into duckdb as {table}\")\n@@ -275,13 +286,13 @@ class DuckDbTools(Toolkit):\n             # If the table isn't a valid SQL identifier, we'll need to use something else\n             table = table.replace(\"-\", \"_\").replace(\".\", \"_\").replace(\" \", \"_\").replace(\"/\", \"_\")\n \n-        select_statement = f\"SELECT * FROM read_csv('{path}'\"\n+        select_statement = f\"SELECT * FROM read_csv('{path}', ignore_errors=false, auto_detect=true\"\n         if delimiter is not None:\n             select_statement += f\", delim='{delimiter}')\"\n         else:\n             select_statement += \")\"\n \n-        create_statement = f\"CREATE OR REPLACE TABLE '{table}' AS {select_statement};\"\n+        create_statement = f\"CREATE OR REPLACE TABLE {table} AS {select_statement};\"\n         self.run_query(create_statement)\n \n         log_debug(f\"Loaded CSV {path} into duckdb as {table}\")\n@@ -306,7 +317,7 @@ class DuckDbTools(Toolkit):\n             # If the table isn't a valid SQL identifier, we'll need to use something else\n             table = table.replace(\"-\", \"_\").replace(\".\", \"_\").replace(\" \", \"_\").replace(\"/\", \"_\")\n \n-        create_statement = f\"CREATE OR REPLACE TABLE '{table}' AS SELECT * FROM '{path}';\"\n+        create_statement = f\"CREATE OR REPLACE TABLE {table} AS SELECT * FROM '{path}';\"\n         self.run_query(create_statement)\n \n         log_debug(f\"Loaded {path} into duckdb as {table}\")\n@@ -333,13 +344,13 @@ class DuckDbTools(Toolkit):\n             # If the table isn't a valid SQL identifier, we'll need to use something else\n             table = table.replace(\"-\", \"_\").replace(\".\", \"_\").replace(\" \", \"_\").replace(\"/\", \"_\")\n \n-        select_statement = f\"SELECT * FROM read_csv('{path}'\"\n+        select_statement = f\"SELECT * FROM read_csv('{path}', ignore_errors=false, auto_detect=true\"\n         if delimiter is not None:\n             select_statement += f\", delim='{delimiter}')\"\n         else:\n             select_statement += \")\"\n \n-        create_statement = f\"CREATE OR REPLACE TABLE '{table}' AS {select_statement};\"\n+        create_statement = f\"CREATE OR REPLACE TABLE {table} AS {select_statement};\"\n         self.run_query(create_statement)\n \n         log_debug(f\"Loaded CSV {path} into duckdb as {table}\")\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/tools/exa.py",
            "diff": "diff --git a/libs/agno/agno/tools/exa.py b/libs/agno/agno/tools/exa.py\nindex 89c85eef3..72de94769 100644\n--- a/libs/agno/agno/tools/exa.py\n+++ b/libs/agno/agno/tools/exa.py\n@@ -143,7 +143,7 @@ class ExaTools(Toolkit):\n                     log_debug(f\"Failed to get highlights {e}\")\n                     result_dict[\"highlights\"] = f\"Failed to get highlights {e}\"\n             exa_results_parsed.append(result_dict)\n-        return json.dumps(exa_results_parsed, indent=4)\n+        return json.dumps(exa_results_parsed, indent=4, ensure_ascii=False)\n \n     def search_exa(self, query: str, num_results: int = 5, category: Optional[str] = None) -> str:\n         \"\"\"Use this function to search Exa (a web search engine) for a query.\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/tools/function.py",
            "diff": "diff --git a/libs/agno/agno/tools/function.py b/libs/agno/agno/tools/function.py\nindex 358321628..721b876ca 100644\n--- a/libs/agno/agno/tools/function.py\n+++ b/libs/agno/agno/tools/function.py\n@@ -4,7 +4,6 @@ from typing import Any, Callable, Dict, List, Literal, Optional, Type, TypeVar,\n \n from docstring_parser import parse\n from pydantic import BaseModel, Field, validate_call\n-from pydantic._internal._validate_call import ValidateCallWrapper\n \n from agno.exceptions import AgentRunException\n from agno.utils.log import log_debug, log_error, log_exception, log_warning\n@@ -327,12 +326,14 @@ class Function(BaseModel):\n         # Don't wrap async generator with validate_call\n         if isasyncgenfunction(func):\n             return func\n-        # Don't wrap ValidateCallWrapper with validate_call\n-        elif isinstance(func, ValidateCallWrapper):\n+        # Don't wrap callables that are already wrapped with validate_call\n+        elif getattr(func, \"_wrapped_for_validation\", False):\n             return func\n         # Wrap the callable with validate_call\n         else:\n-            return validate_call(func, config=dict(arbitrary_types_allowed=True))  # type: ignore\n+            wrapped = validate_call(func, config=dict(arbitrary_types_allowed=True))  # type: ignore\n+            wrapped._wrapped_for_validation = True  # Mark as wrapped to avoid infinite recursion\n+            return wrapped\n \n     def process_schema_for_strict(self):\n         self.parameters[\"additionalProperties\"] = False\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/tools/gmail.py",
            "diff": "diff --git a/libs/agno/agno/tools/gmail.py b/libs/agno/agno/tools/gmail.py\nindex 0a495b00c..62a94455a 100644\n--- a/libs/agno/agno/tools/gmail.py\n+++ b/libs/agno/agno/tools/gmail.py\n@@ -116,6 +116,7 @@ class GmailTools(Toolkit):\n         credentials_path: Optional[str] = None,\n         token_path: Optional[str] = None,\n         scopes: Optional[List[str]] = None,\n+        port: Optional[int] = None,\n         **kwargs,\n     ):\n         \"\"\"Initialize GmailTools and authenticate with Gmail API\n@@ -136,12 +137,14 @@ class GmailTools(Toolkit):\n             credentials_path (Optional[str]): Path to credentials file. Defaults to None.\n             token_path (Optional[str]): Path to token file. Defaults to None.\n             scopes (Optional[List[str]]): Custom OAuth scopes. If None, uses DEFAULT_SCOPES.\n+            port (Optional[int]): Port to use for OAuth authentication. Defaults to None.\n         \"\"\"\n         self.creds = creds\n         self.credentials_path = credentials_path\n         self.token_path = token_path\n         self.service = None\n         self.scopes = scopes or self.DEFAULT_SCOPES\n+        self.port = port\n \n         # Validate that required scopes are present for requested operations\n         if (create_draft_email or send_email) and \"https://www.googleapis.com/auth/gmail.compose\" not in self.scopes:\n@@ -219,7 +222,7 @@ class GmailTools(Toolkit):\n                     flow = InstalledAppFlow.from_client_secrets_file(str(creds_file), self.scopes)\n                 else:\n                     flow = InstalledAppFlow.from_client_config(client_config, self.scopes)\n-                self.creds = flow.run_local_server(port=0)\n+                self.creds = flow.run_local_server(port=self.port)\n \n             # Save the credentials for future use\n             if self.creds and self.creds.valid:\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/tools/mcp.py",
            "diff": "diff --git a/libs/agno/agno/tools/mcp.py b/libs/agno/agno/tools/mcp.py\nindex 2aed2aeb3..d09c9ce2d 100644\n--- a/libs/agno/agno/tools/mcp.py\n+++ b/libs/agno/agno/tools/mcp.py\n@@ -83,7 +83,7 @@ class MCPTools(Toolkit):\n         super().__init__(name=\"MCPTools\", **kwargs)\n \n         # Set these after `__init__` to bypass the `_check_tools_filters`\n-        # beacuse tools are not available until `initialize()` is called.\n+        # because tools are not available until `initialize()` is called.\n         self.include_tools = include_tools\n         self.exclude_tools = exclude_tools\n \n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/tools/openai.py",
            "diff": "diff --git a/libs/agno/agno/tools/openai.py b/libs/agno/agno/tools/openai.py\nindex 48b97d115..d0f9d9111 100644\n--- a/libs/agno/agno/tools/openai.py\n+++ b/libs/agno/agno/tools/openai.py\n@@ -1,9 +1,10 @@\n from os import getenv\n-from typing import Any, List, Literal, Optional\n+from typing import Any, List, Literal, Optional, Union\n from uuid import uuid4\n \n from agno.agent import Agent\n from agno.media import AudioArtifact, ImageArtifact\n+from agno.team.team import Team\n from agno.tools import Toolkit\n from agno.utils.log import log_debug, log_error, log_warning\n \n@@ -84,7 +85,7 @@ class OpenAITools(Toolkit):\n \n     def generate_image(\n         self,\n-        agent: Agent,\n+        agent: Union[Agent, Team],\n         prompt: str,\n     ) -> str:\n         \"\"\"Generate images based on a text prompt.\n@@ -139,7 +140,7 @@ class OpenAITools(Toolkit):\n \n     def generate_speech(\n         self,\n-        agent: Agent,\n+        agent: Union[Agent, Team],\n         text_input: str,\n     ) -> str:\n         \"\"\"Generate speech from text using OpenAI's Text-to-Speech API.\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/tools/oxylabs.py",
            "diff": "diff --git a/libs/agno/agno/tools/oxylabs.py b/libs/agno/agno/tools/oxylabs.py\nnew file mode 100644\nindex 000000000..586667feb\n--- /dev/null\n+++ b/libs/agno/agno/tools/oxylabs.py\n@@ -0,0 +1,385 @@\n+import json\n+from os import getenv\n+from typing import Any, Callable, Dict, List, Optional\n+from urllib.parse import urlparse\n+\n+from agno.tools import Toolkit\n+from agno.utils.log import log_debug, log_error, log_info\n+\n+try:\n+    from oxylabs import RealtimeClient\n+    from oxylabs.sources.response import Response\n+    from oxylabs.utils.types import render\n+except ImportError:\n+    raise ImportError(\"Oxylabs SDK not found. Please install it with: pip install oxylabs\")\n+\n+\n+class OxylabsTools(Toolkit):\n+    def __init__(\n+        self,\n+        username: Optional[str] = None,\n+        password: Optional[str] = None,\n+        **kwargs,\n+    ):\n+        self.username = username or getenv(\"OXYLABS_USERNAME\")\n+        self.password = password or getenv(\"OXYLABS_PASSWORD\")\n+\n+        if not self.username or not self.password:\n+            raise ValueError(\n+                \"No Oxylabs credentials provided. Please set the OXYLABS_USERNAME and OXYLABS_PASSWORD environment variables or pass them to the OxylabsTools constructor.\"\n+            )\n+\n+        try:\n+            log_debug(f\"Initializing Oxylabs client with username: {self.username[:5]}...\")\n+            self.client = RealtimeClient(self.username, self.password)\n+            log_debug(\"Oxylabs client initialized successfully\")\n+        except Exception as e:\n+            log_debug(f\"Failed to initialize Oxylabs client: {e}\")\n+            raise\n+\n+        tools: List[Callable[..., str]] = [\n+            self.search_google,\n+            self.get_amazon_product,\n+            self.search_amazon_products,\n+            self.scrape_website,\n+        ]\n+\n+        super().__init__(name=\"oxylabs_web_scraping\", tools=tools, **kwargs)\n+\n+    def search_google(self, query: str, domain_code: str = \"com\") -> str:\n+        \"\"\"Search Google for a query.\n+\n+        Args:\n+            query: Search query\n+            domain_code: Google domain to search (e.g., \"com\", \"co.uk\", \"de\", default: \"com\")\n+\n+        Returns:\n+            JSON of search results\n+        \"\"\"\n+        try:\n+            if not query or not isinstance(query, str) or len(query.strip()) == 0:\n+                return self._error_response(\"search_google\", \"Query cannot be empty\", {\"query\": query})\n+\n+            if not isinstance(domain_code, str) or len(domain_code) > 10:\n+                return self._error_response(\"search_google\", \"Domain must be a valid string (e.g., 'com', 'co.uk')\")\n+\n+            query = query.strip()\n+            log_debug(f\"Google search: '{query}' on google.{domain_code}\")\n+\n+            response: Response = self.client.google.scrape_search(query=query, domain=domain_code, parse=True)\n+\n+            # Extract search results\n+            search_results = []\n+\n+            if response.results and len(response.results) > 0:\n+                result = response.results[0]\n+\n+                # Try parsed content first\n+                if hasattr(result, \"content_parsed\") and result.content_parsed:\n+                    content = result.content_parsed\n+                    if hasattr(content, \"results\") and content.results:\n+                        raw_results = content.results.raw if hasattr(content.results, \"raw\") else {}\n+                        organic_results = raw_results.get(\"organic\", [])\n+\n+                        for item in organic_results:\n+                            search_results.append(\n+                                {\n+                                    \"title\": item.get(\"title\", \"\").strip(),\n+                                    \"url\": item.get(\"url\", \"\").strip(),\n+                                    \"description\": item.get(\"desc\", \"\").strip(),\n+                                    \"position\": item.get(\"pos\", 0),\n+                                }\n+                            )\n+\n+                if not search_results and hasattr(result, \"content\"):\n+                    raw_content = result.content\n+                    if isinstance(raw_content, dict) and \"results\" in raw_content:\n+                        organic_results = raw_content[\"results\"].get(\"organic\", [])\n+                        for item in organic_results:\n+                            search_results.append(\n+                                {\n+                                    \"title\": item.get(\"title\", \"\").strip(),\n+                                    \"url\": item.get(\"url\", \"\").strip(),\n+                                    \"description\": item.get(\"desc\", \"\").strip(),\n+                                    \"position\": item.get(\"pos\", 0),\n+                                }\n+                            )\n+\n+            response_data = {\n+                \"tool\": \"search_google\",\n+                \"query\": query,\n+                \"results\": search_results,\n+            }\n+\n+            log_info(f\"Google search completed. Found {len(search_results)} results\")\n+            return json.dumps(response_data, indent=2)\n+\n+        except Exception as e:\n+            error_msg = f\"Google search failed: {str(e)}\"\n+            log_error(error_msg)\n+            return self._error_response(\"search_google\", error_msg, {\"query\": query})\n+\n+    def get_amazon_product(self, asin: str, domain_code: str = \"com\") -> str:\n+        \"\"\"Get detailed information about an Amazon product by ASIN.\n+\n+        Args:\n+            asin: Amazon Standard Identification Number (10 alphanumeric characters, e.g., \"B07FZ8S74R\")\n+            domain_code: Amazon domain (e.g., \"com\", \"co.uk\", \"de\", default: \"com\")\n+\n+        Returns:\n+            JSON of product details\n+        \"\"\"\n+        try:\n+            if not asin or not isinstance(asin, str):\n+                return self._error_response(\"get_amazon_product\", \"ASIN is required and must be a string\")\n+\n+            asin = asin.strip().upper()\n+            if len(asin) != 10 or not asin.isalnum():\n+                return self._error_response(\n+                    \"get_amazon_product\",\n+                    f\"Invalid ASIN format: {asin}. Must be 10 alphanumeric characters (e.g., 'B07FZ8S74R')\",\n+                )\n+\n+            if not isinstance(domain_code, str) or len(domain_code) > 10:\n+                return self._error_response(\n+                    \"get_amazon_product\", \"Domain must be a valid string (e.g., 'com', 'co.uk')\"\n+                )\n+\n+            log_debug(f\"Amazon product lookup: ASIN {asin} on amazon.{domain_code}\")\n+\n+            response: Response = self.client.amazon.scrape_product(query=asin, domain=domain_code, parse=True)\n+\n+            product_info = {\"found\": False, \"asin\": asin, \"domain\": f\"amazon.{domain_code}\"}\n+\n+            if response.results and len(response.results) > 0:\n+                result = response.results[0]\n+\n+                if hasattr(result, \"content\") and result.content:\n+                    content = result.content\n+                    if isinstance(content, dict):\n+                        product_info.update(\n+                            {\n+                                \"found\": True,\n+                                \"title\": content.get(\"title\", \"\").strip(),\n+                                \"price\": content.get(\"price\", 0),\n+                                \"currency\": content.get(\"currency\", \"\"),\n+                                \"rating\": content.get(\"rating\", 0),\n+                                \"reviews_count\": content.get(\"reviews_count\", 0),\n+                                \"url\": content.get(\"url\", \"\"),\n+                                \"description\": content.get(\"description\", \"\").strip(),\n+                                \"stock_status\": content.get(\"stock\", \"\").strip(),\n+                                \"brand\": content.get(\"brand\", \"\").strip(),\n+                                \"images\": content.get(\"images\", [])[:3],\n+                                \"bullet_points\": content.get(\"bullet_points\", [])[:5]\n+                                if content.get(\"bullet_points\")\n+                                else [],\n+                            }\n+                        )\n+\n+                elif hasattr(result, \"content_parsed\") and result.content_parsed:\n+                    content = result.content_parsed\n+                    product_info.update(\n+                        {\n+                            \"found\": True,\n+                            \"title\": getattr(content, \"title\", \"\").strip(),\n+                            \"price\": getattr(content, \"price\", 0),\n+                            \"currency\": getattr(content, \"currency\", \"\"),\n+                            \"rating\": getattr(content, \"rating\", 0),\n+                            \"reviews_count\": getattr(content, \"reviews_count\", 0),\n+                            \"url\": getattr(content, \"url\", \"\"),\n+                            \"description\": getattr(content, \"description\", \"\").strip(),\n+                            \"stock_status\": getattr(content, \"stock\", \"\").strip(),\n+                            \"brand\": getattr(content, \"brand\", \"\").strip(),\n+                            \"images\": getattr(content, \"images\", [])[:3],\n+                            \"bullet_points\": getattr(content, \"bullet_points\", [])[:5]\n+                            if getattr(content, \"bullet_points\", None)\n+                            else [],\n+                        }\n+                    )\n+\n+            response_data = {\n+                \"tool\": \"get_amazon_product\",\n+                \"asin\": asin,\n+                \"product_info\": product_info,\n+            }\n+\n+            log_info(f\"Amazon product lookup completed for ASIN {asin}\")\n+            return json.dumps(response_data, indent=2)\n+\n+        except Exception as e:\n+            error_msg = f\"Amazon product lookup failed: {str(e)}\"\n+            log_error(error_msg)\n+            return self._error_response(\"get_amazon_product\", error_msg, {\"asin\": asin})\n+\n+    def search_amazon_products(self, query: str, domain_code: str = \"com\") -> str:\n+        \"\"\"Search Amazon for products and return search results.\n+\n+        Args:\n+            query: Product search query\n+            domain_code: Amazon domain (e.g., \"com\", \"co.uk\", \"de\", default: \"com\")\n+\n+        Returns:\n+            JSON string with search results containing:\n+            - success: boolean indicating if search was successful\n+            - query: the original search query\n+            - total_products: number of products found\n+            - products: list of product results with title, asin, price, rating, etc.\n+        \"\"\"\n+        try:\n+            if not query or not isinstance(query, str) or len(query.strip()) == 0:\n+                return self._error_response(\"search_amazon_products\", \"Query cannot be empty\")\n+\n+            if not isinstance(domain_code, str) or len(domain_code) > 10:\n+                return self._error_response(\n+                    \"search_amazon_products\", \"Domain must be a valid string (e.g., 'com', 'co.uk')\"\n+                )\n+\n+            query = query.strip()\n+            log_info(f\"Amazon search: '{query}' on amazon.{domain_code}\")\n+\n+            response: Response = self.client.amazon.scrape_search(query=query, domain=domain_code, parse=True)\n+\n+            # Extract search results\n+            products = []\n+\n+            if response.results and len(response.results) > 0:\n+                result = response.results[0]\n+\n+                if hasattr(result, \"content\") and result.content:\n+                    content = result.content\n+                    if isinstance(content, dict) and \"results\" in content:\n+                        organic_results = content[\"results\"].get(\"organic\", [])\n+\n+                        for item in organic_results:\n+                            products.append(\n+                                {\n+                                    \"title\": item.get(\"title\", \"\").strip(),\n+                                    \"asin\": item.get(\"asin\", \"\").strip(),\n+                                    \"price\": item.get(\"price\", 0),\n+                                    \"currency\": item.get(\"currency\", \"\"),\n+                                    \"rating\": item.get(\"rating\", 0),\n+                                    \"reviews_count\": item.get(\"reviews_count\", 0),\n+                                    \"url\": item.get(\"url\", \"\").strip(),\n+                                    \"position\": item.get(\"pos\", 0),\n+                                    \"image\": item.get(\"image\", \"\").strip(),\n+                                }\n+                            )\n+\n+                elif hasattr(result, \"content_parsed\") and result.content_parsed:\n+                    content = result.content_parsed\n+                    if hasattr(content, \"results\") and content.results:\n+                        if hasattr(content.results, \"organic\"):\n+                            organic_results = content.results.organic\n+                            for item in organic_results:\n+                                products.append(\n+                                    {\n+                                        \"title\": getattr(item, \"title\", \"\").strip(),\n+                                        \"asin\": getattr(item, \"asin\", \"\").strip(),\n+                                        \"price\": getattr(item, \"price\", 0),\n+                                        \"currency\": getattr(item, \"currency\", \"\"),\n+                                        \"rating\": getattr(item, \"rating\", 0),\n+                                        \"reviews_count\": getattr(item, \"reviews_count\", 0),\n+                                        \"url\": getattr(item, \"url\", \"\").strip(),\n+                                        \"position\": getattr(item, \"pos\", 0),\n+                                        \"image\": getattr(item, \"image\", \"\").strip(),\n+                                    }\n+                                )\n+\n+            response_data = {\n+                \"tool\": \"search_amazon_products\",\n+                \"query\": query,\n+                \"products\": products,\n+            }\n+\n+            log_debug(f\"Amazon search completed. Found {len(products)} products\")\n+            return json.dumps(response_data, indent=2)\n+\n+        except Exception as e:\n+            error_msg = f\"Amazon search failed: {str(e)}\"\n+            log_error(error_msg)\n+            return self._error_response(\"search_amazon_products\", error_msg, {\"query\": query})\n+\n+    def scrape_website(self, url: str, render_javascript: bool = False) -> str:\n+        \"\"\"Scrape content from any website URL.\n+\n+        Args:\n+            url: Website URL to scrape (must start with http:// or ht   ps://)\n+            render_javascript: Whether to enable JavaScript rendering for dynamic content (default: False)\n+\n+        Returns:\n+            JSON of results\n+        \"\"\"\n+        try:\n+            if not url or not isinstance(url, str):\n+                return self._error_response(\"scrape_website\", \"URL is required and must be a string\")\n+\n+            url = url.strip()\n+            if not url.startswith((\"http://\", \"https://\")):\n+                return self._error_response(\n+                    \"scrape_website\", f\"Invalid URL format: {url}. Must start with http:// or https://\"\n+                )\n+\n+            try:\n+                parsed_url = urlparse(url)\n+                if not parsed_url.netloc:\n+                    return self._error_response(\"scrape_website\", f\"Invalid URL format: {url}. Missing domain name\")\n+            except Exception:\n+                return self._error_response(\"scrape_website\", f\"Invalid URL format: {url}\")\n+\n+            if not isinstance(render_javascript, bool):\n+                return self._error_response(\"scrape_website\", \"render_javascript must be a boolean (True/False)\")\n+\n+            log_debug(f\"Website scraping: {url} (JS rendering: {render_javascript})\")\n+\n+            response: Response = self.client.universal.scrape_url(\n+                url=url, render=render.HTML if render_javascript else None, parse=True\n+            )\n+\n+            content_info = {\"url\": url, \"javascript_rendered\": render_javascript}\n+\n+            if response.results and len(response.results) > 0:\n+                result = response.results[0]\n+                content = result.content\n+                status_code = getattr(result, \"status_code\", None)\n+\n+                content_preview = \"\"\n+                content_length = 0\n+\n+                if content:\n+                    try:\n+                        content_str = str(content)\n+                        content_length = len(content_str)\n+                        content_preview = content_str[:1000] if content_length > 1000 else content_str\n+                        content_info[\"scraped\"] = True\n+                    except Exception as e:\n+                        log_debug(f\"Could not process content: {e}\")\n+                        content_preview = \"Content available but processing failed\"\n+                        content_info[\"scraped\"] = False\n+\n+                content_info.update(\n+                    {\n+                        \"status_code\": status_code,\n+                        \"content_length\": content_length,\n+                        \"content_preview\": content_preview.strip(),\n+                        \"has_content\": content_length > 0,\n+                    }\n+                )\n+\n+            response_data = {\n+                \"tool\": \"scrape_website\",\n+                \"url\": url,\n+                \"content_info\": content_info,\n+            }\n+\n+            log_debug(f\"Website scraping completed for {url}\")\n+            return json.dumps(response_data, indent=2)\n+\n+        except Exception as e:\n+            error_msg = f\"Website scraping failed: {str(e)}\"\n+            log_error(error_msg)\n+            return self._error_response(\"scrape_website\", error_msg, {\"url\": url})\n+\n+    def _error_response(self, tool_name: str, error_message: str, context: Optional[Dict[str, Any]] = None) -> str:\n+        \"\"\"Generate a standardized error response.\"\"\"\n+        error_data = {\"tool\": tool_name, \"error\": error_message, \"context\": context or {}}\n+        return json.dumps(error_data, indent=2)\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/tools/scrapegraph.py",
            "diff": "diff --git a/libs/agno/agno/tools/scrapegraph.py b/libs/agno/agno/tools/scrapegraph.py\nindex 5a5e44c17..3eae8f573 100644\n--- a/libs/agno/agno/tools/scrapegraph.py\n+++ b/libs/agno/agno/tools/scrapegraph.py\n@@ -6,9 +6,13 @@ from agno.tools import Toolkit\n \n try:\n     from scrapegraph_py import Client\n+    from scrapegraph_py.logger import sgai_logger\n except ImportError:\n     raise ImportError(\"`scrapegraph-py` not installed. Please install using `pip install scrapegraph-py`\")\n \n+# Set logging level\n+sgai_logger.set_logging(level=\"INFO\")\n+\n \n class ScrapeGraphTools(Toolkit):\n     def __init__(\n@@ -16,6 +20,8 @@ class ScrapeGraphTools(Toolkit):\n         api_key: Optional[str] = None,\n         smartscraper: bool = True,\n         markdownify: bool = False,\n+        crawl: bool = False,\n+        searchscraper: bool = False,\n         **kwargs,\n     ):\n         self.api_key: Optional[str] = api_key or os.getenv(\"SGAI_API_KEY\")\n@@ -31,6 +37,10 @@ class ScrapeGraphTools(Toolkit):\n             tools.append(self.smartscraper)\n         if markdownify:\n             tools.append(self.markdownify)\n+        if crawl:\n+            tools.append(self.crawl)\n+        if searchscraper:\n+            tools.append(self.searchscraper)\n \n         super().__init__(name=\"scrapegraph_tools\", tools=tools, **kwargs)\n \n@@ -42,7 +52,6 @@ class ScrapeGraphTools(Toolkit):\n         Returns:\n             The structured data extracted from the webpage\n         \"\"\"\n-\n         try:\n             response = self.client.smartscraper(website_url=url, user_prompt=prompt)\n             return json.dumps(response[\"result\"])\n@@ -56,9 +65,67 @@ class ScrapeGraphTools(Toolkit):\n         Returns:\n             The markdown version of the webpage\n         \"\"\"\n-\n         try:\n             response = self.client.markdownify(website_url=url)\n             return response[\"result\"]\n         except Exception as e:\n             return f\"Error converting to markdown: {str(e)}\"\n+\n+    def crawl(\n+        self,\n+        url: str,\n+        prompt: str,\n+        schema: dict,\n+        cache_website: bool = True,\n+        depth: int = 2,\n+        max_pages: int = 2,\n+        same_domain_only: bool = True,\n+        batch_size: int = 1,\n+    ) -> str:\n+        \"\"\"Use this function to crawl a website and extract structured data using a schema.\n+        Args:\n+            url (str): The URL to crawl\n+            prompt (str): Natural language prompt describing what to extract\n+            schema (dict): JSON schema for extraction\n+            cache_website (bool): Whether to cache the website\n+            depth (int): Crawl depth\n+            max_pages (int): Max number of pages to crawl\n+            same_domain_only (bool): Restrict to same domain\n+            batch_size (int): Batch size for crawling\n+        Returns:\n+            The structured data extracted from the website\n+        \"\"\"\n+        try:\n+            response = self.client.crawl(\n+                url=url,\n+                prompt=prompt,\n+                schema=schema,\n+                cache_website=cache_website,\n+                depth=depth,\n+                max_pages=max_pages,\n+                same_domain_only=same_domain_only,\n+                batch_size=batch_size,\n+            )\n+            return json.dumps(response, indent=2)\n+        except Exception as e:\n+            return json.dumps({\"error\": str(e)})\n+\n+    def searchscraper(self, url: str, prompt: str) -> str:\n+        \"\"\"Use this function to search and extract information from a webpage using LLM.\n+        Args:\n+            url (str): The URL to search\n+            prompt (str): Natural language prompt describing what to search for\n+        Returns:\n+            The search results extracted from the webpage\n+        \"\"\"\n+        try:\n+            response = self.client.searchscraper(website_url=url, user_prompt=prompt)\n+            # If response has a 'result' attribute, return it, else return the whole response\n+            if hasattr(response, \"result\"):\n+                return json.dumps(response.result)\n+            elif isinstance(response, dict) and \"result\" in response:\n+                return json.dumps(response[\"result\"])\n+            else:\n+                return json.dumps(response)\n+        except Exception as e:\n+            return json.dumps({\"error\": str(e)})\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/tools/serper.py",
            "diff": "diff --git a/libs/agno/agno/tools/serper.py b/libs/agno/agno/tools/serper.py\nindex 622089c1b..d49b27963 100644\n--- a/libs/agno/agno/tools/serper.py\n+++ b/libs/agno/agno/tools/serper.py\n@@ -1,80 +1,246 @@\n import json\n from os import getenv\n-from typing import Optional\n+from typing import Any, Dict, List, Optional\n \n import requests\n \n from agno.tools import Toolkit\n-from agno.utils.log import log_debug\n+from agno.utils.log import log_debug, log_error, log_warning\n \n \n class SerperTools(Toolkit):\n-    \"\"\"\n-    A class to interact with the Serper API for Google search functionality. Go to serper.dev for more information.\n-\n-    Attributes:\n-        api_key (str): The API key for accessing the Serper API.\n-        location (str): The Google search location to be used for the search (default is \"us\").\n-        num_results (int): The number of search results to return (default is 10).\n-\n-    Methods:\n-        search_google(query: str, gl: Optional[str] = None) -> str:\n-            Performs a Google search using the Serper API and returns the results.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         api_key: Optional[str] = None,\n         location: str = \"us\",\n+        language: str = \"en\",\n         num_results: int = 10,\n+        date_range: Optional[str] = None,\n+        **kwargs,\n     ):\n         \"\"\"\n-        Initializes the SerperTools instance.\n+        Initialize the SerperTools.\n \n         Args:\n-            api_key (str, optional): The Serper API key. If not provided, will be fetched from the environment variable \"SERPER_API_KEY\".\n-            gl (str, optional): The Google location code for search results (default is \"us\").\n-            num_results (int, optional): The number of search results to retrieve (default is 10).\n+            api_key Optional[str]: The Serper API key.\n+            location Optional[str]: The Google location code for search results.\n+            language Optional[str]: The language code for search results.\n+            num_results Optional[int]: The number of search results to retrieve.\n+            date_range Optional[str]: Default date range filter for searches.\n         \"\"\"\n-        super().__init__(name=\"serper_api_tools\")\n-\n         self.api_key = api_key or getenv(\"SERPER_API_KEY\")\n         if not self.api_key:\n             log_debug(\"No Serper API key provided\")\n \n         self.location = location\n+        self.language = language\n         self.num_results = num_results\n-        self.register(self.search_google)\n+        self.date_range = date_range\n+\n+        tools: List[Any] = []\n+        tools.append(self.search)\n+        tools.append(self.search_news)\n+        tools.append(self.search_scholar)\n+        tools.append(self.scrape_webpage)\n \n-    def search_google(self, query: str, location: Optional[str] = None) -> str:\n+        super().__init__(name=\"serper_tools\", tools=tools, **kwargs)\n+\n+    def _make_request(self, endpoint: str, params: Dict[str, Any]) -> Dict[str, Any]:\n         \"\"\"\n-        Searches Google for the provided query using the Serper API.\n+        Makes a request to the Serper API.\n \n         Args:\n-            query (str): The search query to search for on Google.\n-            location (str, optional): The Google location code for search results. If not provided, the default class attribute is used.\n+            endpoint (str): The API endpoint\n+            params (Dict[str, Any]): Request parameters\n \n         Returns:\n-            str: The search results in JSON format or an error message if the search fails.\n+            Dict[str, Any]: Search response\n         \"\"\"\n         try:\n             if not self.api_key:\n-                return \"Please provide an API key\"\n-            if not query:\n-                return \"Please provide a query to search for\"\n+                log_error(\"No Serper API key provided\")\n+                return {\"success\": False, \"error\": \"Please provide a Serper API key\"}\n \n-            log_debug(f\"Searching Google for: {query}\")\n+            url = f\"https://google.serper.dev/{endpoint}\"\n+            if endpoint == \"scrape\":\n+                url = \"https://scrape.serper.dev\"\n \n-            url = \"https://google.serper.dev/search\"\n             headers = {\"X-API-KEY\": self.api_key, \"Content-Type\": \"application/json\"}\n-            # Use the gl parameter from the method if provided, otherwise use the class attribute\n-            search_gl = location if location is not None else self.location\n-            params = {\"q\": query, \"num\": self.num_results, \"gl\": search_gl}\n+\n+            # Add optional parameters\n+            if self.date_range:\n+                params[\"tbs\"] = self.date_range\n+            if self.location:\n+                params[\"gl\"] = self.location\n+\n+            if self.language:\n+                params[\"hl\"] = self.language\n+\n             payload = json.dumps(params)\n+\n+            log_debug(f\"Making request to {url} with params: {params}\")\n             response = requests.request(\"POST\", url, headers=headers, data=payload)\n-            results = response.text\n+            response.raise_for_status()\n+\n+            log_debug(f\"Successfully received response from {endpoint} endpoint\")\n+            return {\"success\": True, \"data\": response.json(), \"raw_response\": response.text}\n+        except Exception as e:\n+            log_error(f\"Serper API error: {str(e)}\")\n+            return {\"success\": False, \"error\": str(e)}\n+\n+    def search(\n+        self,\n+        query: str,\n+        num_results: Optional[int] = None,\n+    ) -> str:\n+        \"\"\"\n+        Searches Google for the provided query using the Serper API.\n+\n+        Args:\n+            query (str): The search query to search for on Google.\n+            num_results (int, optional): Number of search results to retrieve.\n+\n+        Returns:\n+            str: A JSON-formatted string containing the search results or an error message if the search fails.\n+        \"\"\"\n+        try:\n+            if not query:\n+                return json.dumps({\"error\": \"Please provide a query to search for\"}, indent=2)\n+\n+            log_debug(f\"Searching Google for: {query}\")\n+\n+            params = {\n+                \"q\": query,\n+                \"num\": num_results or self.num_results,\n+            }\n+\n+            result = self._make_request(\"search\", params)\n+\n+            if result[\"success\"]:\n+                log_debug(f\"Successfully found Google search results for query: {query}\")\n+                return result[\"raw_response\"]\n+            else:\n+                log_error(f\"Error searching Google for query {query}: {result['error']}\")\n+                return json.dumps({\"error\": result[\"error\"]}, indent=2)\n+\n+        except Exception as e:\n+            log_error(f\"Unexpected error searching Google for query {query}: {e}\")\n+            return json.dumps({\"error\": f\"An unexpected error occurred: {str(e)}\"}, indent=2)\n+\n+    def search_news(\n+        self,\n+        query: str,\n+        num_results: Optional[int] = None,\n+    ) -> str:\n+        \"\"\"\n+        Searches for news articles using the Serper News API.\n+\n+        Args:\n+            query (str): The search query for news articles.\n+            num_results (int, optional): Number of news results to retrieve.\n+\n+        Returns:\n+            str: A JSON-formatted string containing the news search results or an error message.\n+        \"\"\"\n+        try:\n+            if not query:\n+                return json.dumps({\"error\": \"Please provide a query to search for news\"}, indent=2)\n+\n+            log_debug(f\"Searching news for: {query}\")\n+\n+            params = {\n+                \"q\": query,\n+                \"num\": num_results or self.num_results,\n+            }\n+\n+            result = self._make_request(\"news\", params)\n+\n+            if result[\"success\"]:\n+                log_debug(f\"Successfully found {num_results or self.num_results} news articles for query: {query}\")\n+                return result[\"raw_response\"]\n+            else:\n+                log_error(f\"Error searching news for query {query}: {result['error']}\")\n+                return json.dumps({\"error\": result[\"error\"]}, indent=2)\n+\n+        except Exception as e:\n+            log_error(f\"Unexpected error searching news for query {query}: {e}\")\n+            return json.dumps({\"error\": f\"An unexpected error occurred: {str(e)}\"}, indent=2)\n+\n+    def search_scholar(\n+        self,\n+        query: str,\n+        num_results: Optional[int] = None,\n+    ) -> str:\n+        \"\"\"\n+        Searches for academic papers using Google Scholar via Serper API.\n+\n+        Args:\n+            query (str): The search query for academic papers.\n+            num_results (int, optional): Number of academic papers to retrieve.\n+\n+        Returns:\n+            str: A JSON-formatted string containing the scholar search results or an error message.\n+        \"\"\"\n+        try:\n+            if not query:\n+                return json.dumps({\"error\": \"Please provide a query to search for academic papers\"}, indent=2)\n+\n+            log_debug(f\"Searching scholar for: {query}\")\n+\n+            params = {\n+                \"q\": query,\n+                \"num\": num_results or self.num_results,\n+            }\n+\n+            result = self._make_request(\"scholar\", params)\n+\n+            if result[\"success\"]:\n+                log_debug(f\"Successfully found academic papers for query: {query}\")\n+                return result[\"raw_response\"]\n+            else:\n+                log_error(f\"Error searching scholar for query {query}: {result['error']}\")\n+                return json.dumps({\"error\": result[\"error\"]}, indent=2)\n+\n+        except Exception as e:\n+            log_error(f\"Unexpected error searching scholar for query {query}: {e}\")\n+            return json.dumps({\"error\": f\"An unexpected error occurred: {str(e)}\"}, indent=2)\n+\n+    def scrape_webpage(\n+        self,\n+        url: str,\n+        markdown: bool = False,\n+    ) -> str:\n+        \"\"\"\n+        Scrapes and extracts content from a webpage using the Serper scraping API.\n+\n+        Args:\n+            url (str): The URL of the webpage to scrape.\n+            markdown (bool, optional): Return content in markdown format (default: False).\n+\n+        Returns:\n+            str: A JSON-formatted string containing the scraped webpage content or an error message.\n+        \"\"\"\n+        try:\n+            if not url:\n+                log_warning(\"No URL provided to scrape\")\n+                return json.dumps({\"error\": \"Please provide a URL to scrape\"}, indent=2)\n+\n+            log_debug(f\"Scraping webpage: {url}\")\n+\n+            params = {\n+                \"url\": url,\n+                \"includeMarkdown\": markdown,\n+            }\n+\n+            result = self._make_request(\"scrape\", params)\n \n-            return results\n+            if result[\"success\"]:\n+                log_debug(f\"Successfully scraped webpage: {url}\")\n+                return result[\"raw_response\"]\n+            else:\n+                log_error(f\"Error scraping webpage {url}: {result['error']}\")\n+                return json.dumps({\"error\": result[\"error\"]}, indent=2)\n \n         except Exception as e:\n-            return f\"Error searching for the query {query}: {e}\"\n+            log_error(f\"Unexpected error scraping webpage {url}: {e}\")\n+            return json.dumps({\"error\": f\"An unexpected error occurred: {str(e)}\"}, indent=2)\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/tools/valyu.py",
            "diff": "diff --git a/libs/agno/agno/tools/valyu.py b/libs/agno/agno/tools/valyu.py\nnew file mode 100644\nindex 000000000..a203d9121\n--- /dev/null\n+++ b/libs/agno/agno/tools/valyu.py\n@@ -0,0 +1,199 @@\n+import json\n+from os import getenv\n+from typing import Any, List, Optional\n+\n+from agno.tools import Toolkit\n+from agno.utils.log import log_debug, log_error, log_warning\n+\n+try:\n+    from valyu import Valyu\n+except ImportError:\n+    raise ImportError(\"`valyu` not installed. Please install using `pip install valyu`\")\n+\n+\n+class ValyuTools(Toolkit):\n+    def __init__(\n+        self,\n+        api_key: Optional[str] = None,\n+        text_length: int = 1000,\n+        max_results: int = 10,\n+        relevance_threshold: float = 0.5,\n+        content_category: Optional[str] = None,\n+        search_start_date: Optional[str] = None,\n+        search_end_date: Optional[str] = None,\n+        search_domains: Optional[List[str]] = None,\n+        sources: Optional[List[str]] = None,\n+        max_price: float = 30.0,\n+        tool_call_mode: bool = False,\n+        **kwargs,\n+    ):\n+        self.api_key = api_key or getenv(\"VALYU_API_KEY\")\n+        if not self.api_key:\n+            raise ValueError(\"VALYU_API_KEY not set. Please set the VALYU_API_KEY environment variable.\")\n+\n+        self.valyu = Valyu(api_key=self.api_key)\n+        self.text_length = text_length\n+        self.max_results = max_results\n+        self.relevance_threshold = relevance_threshold\n+        self.max_price = max_price\n+        self.content_category = content_category\n+        self.search_start_date = search_start_date\n+        self.search_end_date = search_end_date\n+        self.search_domains = search_domains\n+        self.sources = sources\n+        self.tool_call_mode = tool_call_mode\n+\n+        super().__init__(\n+            name=\"valyu_search\",\n+            tools=[self.search_academic_sources, self.search_web, self.search_within_paper],\n+            **kwargs,\n+        )\n+\n+    def _parse_results(self, results: List[Any]) -> str:\n+        parsed_results = []\n+        for result in results:\n+            result_dict = {}\n+\n+            # Essential fields\n+            if hasattr(result, \"url\") and result.url:\n+                result_dict[\"url\"] = result.url\n+            if hasattr(result, \"title\") and result.title:\n+                result_dict[\"title\"] = result.title\n+            if hasattr(result, \"source\") and result.source:\n+                result_dict[\"source\"] = result.source\n+            if hasattr(result, \"relevance_score\"):\n+                result_dict[\"relevance_score\"] = result.relevance_score\n+\n+            # Content with length limiting\n+            if hasattr(result, \"content\") and result.content:\n+                content = result.content\n+                if self.text_length and len(content) > self.text_length:\n+                    content = content[: self.text_length] + \"...\"\n+                result_dict[\"content\"] = content\n+\n+            # Additional metadata\n+            if hasattr(result, \"description\") and result.description:\n+                result_dict[\"description\"] = result.description\n+\n+            parsed_results.append(result_dict)\n+\n+        return json.dumps(parsed_results, indent=2)\n+\n+    def _valyu_search(\n+        self,\n+        query: str,\n+        search_type: str,\n+        content_category: Optional[str] = None,\n+        sources: Optional[List[str]] = None,\n+        start_date: Optional[str] = None,\n+        end_date: Optional[str] = None,\n+    ) -> str:\n+        try:\n+            search_params = {\n+                \"query\": query,\n+                \"search_type\": search_type,\n+                \"max_num_results\": self.max_results,\n+                \"is_tool_call\": self.tool_call_mode,\n+                \"relevance_threshold\": self.relevance_threshold,\n+                \"max_price\": self.max_price,\n+            }\n+\n+            # Add optional parameters\n+            if sources or self.sources:\n+                search_params[\"included_sources\"] = sources or self.sources\n+            if content_category or self.content_category:\n+                search_params[\"category\"] = content_category or self.content_category\n+            if start_date or self.search_start_date:\n+                search_params[\"start_date\"] = start_date or self.search_start_date\n+            if end_date or self.search_end_date:\n+                search_params[\"end_date\"] = end_date or self.search_end_date\n+\n+            log_debug(f\"Valyu search parameters: {search_params}\")\n+            response = self.valyu.search(**search_params)\n+\n+            if not response.success:\n+                log_error(f\"Valyu search API error: {response.error}\")\n+                return f\"Error: {response.error or 'Search request failed'}\"\n+\n+            return self._parse_results(response.results or [])\n+\n+        except Exception as e:\n+            error_msg = f\"Valyu search failed: {str(e)}\"\n+            log_error(error_msg)\n+            return f\"Error: {error_msg}\"\n+\n+    def search_academic_sources(\n+        self,\n+        query: str,\n+        start_date: Optional[str] = None,\n+        end_date: Optional[str] = None,\n+    ) -> str:\n+        \"\"\"Search academic sources (ArXiv, PubMed, academic publishers).\n+\n+        Args:\n+            query: Research question or topic\n+            start_date: Filter papers after this date (YYYY-MM-DD)\n+            end_date: Filter papers before this date (YYYY-MM-DD)\n+\n+        Returns:\n+            JSON array of academic papers\n+        \"\"\"\n+        sources = [\"valyu/valyu-arxiv\", \"valyu/valyu-pubmed\", \"wiley/wiley-finance-papers\", \"wiley/wiley-finance-books\"]\n+\n+        return self._valyu_search(\n+            query=query,\n+            search_type=\"proprietary\",\n+            sources=sources,\n+            start_date=start_date,\n+            end_date=end_date,\n+        )\n+\n+    def search_web(\n+        self,\n+        query: str,\n+        start_date: Optional[str] = None,\n+        end_date: Optional[str] = None,\n+        content_category: Optional[str] = None,\n+    ) -> str:\n+        \"\"\"Search web sources for real-time information.\n+\n+        Args:\n+            query: Search query\n+            start_date: Filter content after this date (YYYY-MM-DD)\n+            end_date: Filter content before this date (YYYY-MM-DD)\n+            content_category: Description of the category of the query\n+\n+        Returns:\n+            JSON array of web search results\n+        \"\"\"\n+        return self._valyu_search(\n+            query=query,\n+            search_type=\"web\",\n+            content_category=content_category,\n+            start_date=start_date,\n+            end_date=end_date,\n+        )\n+\n+    def search_within_paper(\n+        self,\n+        paper_url: str,\n+        query: str,\n+    ) -> str:\n+        \"\"\"Search within a specific ArXiv paper.\n+\n+        Args:\n+            paper_url: ArXiv paper URL\n+            query: Search query\n+        Returns:\n+            JSON array of relevant sections from the paper\n+        \"\"\"\n+        # Validate ArXiv URL\n+        if not paper_url.startswith(\"https:/\"):\n+            log_warning(f\"Invalid paper URL: {paper_url}\")\n+            return \"Error: Invalid paper URL format\"\n+\n+        return self._valyu_search(\n+            query=query,\n+            search_type=\"proprietary\",\n+            sources=[paper_url],\n+        )\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/utils/events.py",
            "diff": "diff --git a/libs/agno/agno/utils/events.py b/libs/agno/agno/utils/events.py\nindex c281429c7..1eaaf047b 100644\n--- a/libs/agno/agno/utils/events.py\n+++ b/libs/agno/agno/utils/events.py\n@@ -7,6 +7,8 @@ from agno.reasoning.step import ReasoningStep\n from agno.run.response import (\n     MemoryUpdateCompletedEvent,\n     MemoryUpdateStartedEvent,\n+    ParserModelResponseCompletedEvent,\n+    ParserModelResponseStartedEvent,\n     ReasoningCompletedEvent,\n     ReasoningStartedEvent,\n     ReasoningStepEvent,\n@@ -23,6 +25,8 @@ from agno.run.response import (\n )\n from agno.run.team import MemoryUpdateCompletedEvent as TeamMemoryUpdateCompletedEvent\n from agno.run.team import MemoryUpdateStartedEvent as TeamMemoryUpdateStartedEvent\n+from agno.run.team import ParserModelResponseCompletedEvent as TeamParserModelResponseCompletedEvent\n+from agno.run.team import ParserModelResponseStartedEvent as TeamParserModelResponseStartedEvent\n from agno.run.team import ReasoningCompletedEvent as TeamReasoningCompletedEvent\n from agno.run.team import ReasoningStartedEvent as TeamReasoningStartedEvent\n from agno.run.team import ReasoningStepEvent as TeamReasoningStepEvent\n@@ -400,3 +404,51 @@ def create_team_run_response_content_event(\n         image=image,\n         extra_data=from_run_response.extra_data,\n     )\n+\n+\n+def create_parser_model_response_started_event(\n+    from_run_response: RunResponse,\n+) -> ParserModelResponseStartedEvent:\n+    return ParserModelResponseStartedEvent(\n+        session_id=from_run_response.session_id,\n+        agent_id=from_run_response.agent_id,  # type: ignore\n+        agent_name=from_run_response.agent_name,  # type: ignore\n+        team_session_id=from_run_response.team_session_id,  # type: ignore\n+        run_id=from_run_response.run_id,\n+    )\n+\n+\n+def create_parser_model_response_completed_event(\n+    from_run_response: RunResponse,\n+) -> ParserModelResponseCompletedEvent:\n+    return ParserModelResponseCompletedEvent(\n+        session_id=from_run_response.session_id,\n+        agent_id=from_run_response.agent_id,  # type: ignore\n+        agent_name=from_run_response.agent_name,  # type: ignore\n+        team_session_id=from_run_response.team_session_id,  # type: ignore\n+        run_id=from_run_response.run_id,\n+    )\n+\n+\n+def create_team_parser_model_response_started_event(\n+    from_run_response: TeamRunResponse,\n+) -> TeamParserModelResponseStartedEvent:\n+    return TeamParserModelResponseStartedEvent(\n+        session_id=from_run_response.session_id,\n+        team_id=from_run_response.team_id,  # type: ignore\n+        team_name=from_run_response.team_name,  # type: ignore\n+        team_session_id=from_run_response.team_session_id,  # type: ignore\n+        run_id=from_run_response.run_id,\n+    )\n+\n+\n+def create_team_parser_model_response_completed_event(\n+    from_run_response: TeamRunResponse,\n+) -> TeamParserModelResponseCompletedEvent:\n+    return TeamParserModelResponseCompletedEvent(\n+        session_id=from_run_response.session_id,\n+        team_id=from_run_response.team_id,  # type: ignore\n+        team_name=from_run_response.team_name,  # type: ignore\n+        team_session_id=from_run_response.team_session_id,  # type: ignore\n+        run_id=from_run_response.run_id,\n+    )\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/utils/gemini.py",
            "diff": "diff --git a/libs/agno/agno/utils/gemini.py b/libs/agno/agno/utils/gemini.py\nindex c8ee10199..43875298d 100644\n--- a/libs/agno/agno/utils/gemini.py\n+++ b/libs/agno/agno/utils/gemini.py\n@@ -156,7 +156,7 @@ def convert_schema(schema_dict: Dict[str, Any], root_schema: Optional[Dict[str,\n                     )\n                 }\n                 if value_type == \"ARRAY\":\n-                    placeholder_properties[\"example_key\"].items = {}\n+                    placeholder_properties[\"example_key\"].items = {}  # type: ignore\n \n                 return Schema(\n                     type=Type.OBJECT,\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/utils/log.py",
            "diff": "diff --git a/libs/agno/agno/utils/log.py b/libs/agno/agno/utils/log.py\nindex 41624b173..92f2bcbb7 100644\n--- a/libs/agno/agno/utils/log.py\n+++ b/libs/agno/agno/utils/log.py\n@@ -1,6 +1,6 @@\n import logging\n from os import getenv\n-from typing import Any, Optional\n+from typing import Any, Literal, Optional\n \n from rich.logging import RichHandler\n from rich.text import Text\n@@ -36,6 +36,10 @@ class ColoredRichHandler(RichHandler):\n             if level_name in LOG_STYLES[self.source_type]:\n                 color = LOG_STYLES[self.source_type][level_name]\n                 return Text(record.levelname, style=color)\n+        else:\n+            if level_name in LOG_STYLES[\"agent\"]:\n+                color = LOG_STYLES[\"agent\"][level_name]\n+                return Text(record.levelname, style=color)\n         return super().get_level_text(record)\n \n \n@@ -93,15 +97,22 @@ team_logger: AgnoLogger = build_logger(TEAM_LOGGER_NAME, source_type=\"team\")\n logger: AgnoLogger = agent_logger\n \n debug_on: bool = False\n+debug_level: Literal[1, 2] = 1\n+\n \n+def set_log_level_to_debug(source_type: Optional[str] = None, level: Literal[1, 2] = 1):\n+    if source_type is None:\n+        use_agent_logger()\n \n-def set_log_level_to_debug(source_type: Optional[str] = None):\n     _logger = logging.getLogger(LOGGER_NAME if source_type is None else f\"{LOGGER_NAME}-{source_type}\")\n     _logger.setLevel(logging.DEBUG)\n \n     global debug_on\n     debug_on = True\n \n+    global debug_level\n+    debug_level = level\n+\n \n def set_log_level_to_info(source_type: Optional[str] = None):\n     _logger = logging.getLogger(LOGGER_NAME if source_type is None else f\"{LOGGER_NAME}-{source_type}\")\n@@ -135,11 +146,14 @@ def use_agent_logger():\n     logger = agent_logger\n \n \n-def log_debug(msg, center: bool = False, symbol: str = \"*\", *args, **kwargs):\n+def log_debug(msg, center: bool = False, symbol: str = \"*\", log_level: Literal[1, 2] = 1, *args, **kwargs):\n     global logger\n     global debug_on\n+    global debug_level\n+\n     if debug_on:\n-        logger.debug(msg, center, symbol, *args, **kwargs)\n+        if debug_level >= log_level:\n+            logger.debug(msg, center, symbol, *args, **kwargs)\n \n \n def log_info(msg, center: bool = False, symbol: str = \"*\", *args, **kwargs):\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/utils/string.py",
            "diff": "diff --git a/libs/agno/agno/utils/string.py b/libs/agno/agno/utils/string.py\nindex b40a0d525..523fb1ba2 100644\n--- a/libs/agno/agno/utils/string.py\n+++ b/libs/agno/agno/utils/string.py\n@@ -90,8 +90,8 @@ def _clean_json_content(content: str) -> str:\n     elif \"```\" in content:\n         content = content.split(\"```\")[1].strip()\n \n-    # Remove markdown formatting\n-    content = re.sub(r\"[*`#]\", \"\", content)\n+    # Replace markdown formatting like *\"name\"* or `\"name\"` with \"name\"\n+    content = re.sub(r'[*`#]?\"([A-Za-z0-9_]+)\"[*`#]?', r'\"\\1\"', content)\n \n     # Handle newlines and control characters\n     content = content.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/utils/timer.py",
            "diff": "diff --git a/libs/agno/agno/utils/timer.py b/libs/agno/agno/utils/timer.py\nindex 66dfec275..c7d50866d 100644\n--- a/libs/agno/agno/utils/timer.py\n+++ b/libs/agno/agno/utils/timer.py\n@@ -32,3 +32,10 @@ class Timer:\n         self.end_time = perf_counter()\n         if self.start_time is not None:\n             self.elapsed_time = self.end_time - self.start_time\n+\n+    def to_dict(self):\n+        return {\n+            \"start_time\": str(self.start_time) if self.start_time is not None else None,\n+            \"end_time\": str(self.end_time) if self.end_time is not None else None,\n+            \"elapsed\": self.elapsed,\n+        }\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/vectordb/qdrant/qdrant.py",
            "diff": "diff --git a/libs/agno/agno/vectordb/qdrant/qdrant.py b/libs/agno/agno/vectordb/qdrant/qdrant.py\nindex e6dcf5df9..46e79020d 100644\n--- a/libs/agno/agno/vectordb/qdrant/qdrant.py\n+++ b/libs/agno/agno/vectordb/qdrant/qdrant.py\n@@ -451,7 +451,7 @@ class Qdrant(VectorDb):\n             limit (int): Number of search results to return\n             filters (Optional[Dict[str, Any]]): Filters to apply while searching\n         \"\"\"\n-        filters = self._format_filters(filters or {})\n+        filters = self._format_filters(filters or {})  # type: ignore\n         if self.search_type == SearchType.vector:\n             results = self._run_vector_search_sync(query, limit, filters)\n         elif self.search_type == SearchType.keyword:\n@@ -466,7 +466,7 @@ class Qdrant(VectorDb):\n     async def async_search(\n         self, query: str, limit: int = 5, filters: Optional[Dict[str, Any]] = None\n     ) -> List[Document]:\n-        filters = self._format_filters(filters or {})\n+        filters = self._format_filters(filters or {})  # type: ignore\n         if self.search_type == SearchType.vector:\n             results = await self._run_vector_search_async(query, limit, filters)\n         elif self.search_type == SearchType.keyword:\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/agno/vectordb/weaviate/weaviate.py",
            "diff": "diff --git a/libs/agno/agno/vectordb/weaviate/weaviate.py b/libs/agno/agno/vectordb/weaviate/weaviate.py\nindex 3f4d6f0ae..623cf0fea 100644\n--- a/libs/agno/agno/vectordb/weaviate/weaviate.py\n+++ b/libs/agno/agno/vectordb/weaviate/weaviate.py\n@@ -83,20 +83,22 @@ class Weaviate(VectorDb):\n         Returns:\n             weaviate.WeaviateClient: An initialized Weaviate client instance.\n         \"\"\"\n-        if self.client is not None:\n-            return self.client\n+        if self.client is None:\n+            if self.wcd_url and self.wcd_api_key and not self.local:\n+                log_info(\"Initializing Weaviate Cloud client\")\n+                self.client = weaviate.connect_to_weaviate_cloud(\n+                    cluster_url=self.wcd_url, auth_credentials=Auth.api_key(self.wcd_api_key)\n+                )\n+            else:\n+                log_info(\"Initializing local Weaviate client\")\n+                self.client = weaviate.connect_to_local()\n \n-        if self.wcd_url and self.wcd_api_key and not self.local:\n-            log_info(\"Initializing Weaviate Cloud client\")\n-            self.client = weaviate.connect_to_weaviate_cloud(\n-                cluster_url=self.wcd_url, auth_credentials=Auth.api_key(self.wcd_api_key)\n-            )\n-        else:\n-            log_info(\"Initializing local Weaviate client\")\n-            self.client = weaviate.connect_to_local()\n+        if not self.client.is_connected():  # type: ignore\n+            self.client.connect()  # type: ignore\n+\n+        if not self.client.is_ready():  # type: ignore\n+            raise Exception(\"Weaviate client is not ready\")\n \n-        # Verify connection\n-        self.client.is_ready()\n         return self.client\n \n     async def get_async_client(self) -> WeaviateAsyncClient:\n@@ -453,13 +455,15 @@ class Weaviate(VectorDb):\n \n             log_info(f\"Found {len(search_results)} documents\")\n \n-            self.get_client().close()\n             return search_results\n \n         except Exception as e:\n             logger.error(f\"Error searching for documents: {e}\")\n             return []\n \n+        finally:\n+            self.get_client().close()\n+\n     async def async_vector_search(\n         self, query: str, limit: int = 5, filters: Optional[Dict[str, Any]] = None\n     ) -> List[Document]:\n@@ -527,13 +531,15 @@ class Weaviate(VectorDb):\n \n             log_info(f\"Found {len(search_results)} documents\")\n \n-            self.get_client().close()\n             return search_results\n \n         except Exception as e:\n             logger.error(f\"Error searching for documents: {e}\")\n             return []\n \n+        finally:\n+            self.get_client().close()\n+\n     async def async_keyword_search(\n         self, query: str, limit: int = 5, filters: Optional[Dict[str, Any]] = None\n     ) -> List[Document]:\n@@ -604,13 +610,15 @@ class Weaviate(VectorDb):\n \n             log_info(f\"Found {len(search_results)} documents\")\n \n-            self.get_client().close()\n             return search_results\n \n         except Exception as e:\n             logger.error(f\"Error searching for documents: {e}\")\n             return []\n \n+        finally:\n+            self.get_client().close()\n+\n     async def async_hybrid_search(\n         self, query: str, limit: int = 5, filters: Optional[Dict[str, Any]] = None\n     ) -> List[Document]:\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/scripts/format.bat",
            "diff": "diff --git a/libs/agno/scripts/format.bat b/libs/agno/scripts/format.bat\nindex 5fbf650f5..3fcb60c07 100644\n--- a/libs/agno/scripts/format.bat\n+++ b/libs/agno/scripts/format.bat\n@@ -1,6 +1,6 @@\n @echo off\n REM ###########################################################################\n-REM # Format the agno library\n+REM # Format the agno library using ruff\n REM # Usage: scripts\\format.bat\n REM ###########################################################################\n \n@@ -10,27 +10,34 @@ REM Get current directory\n SET \"CURR_DIR=%~dp0\"\n SET \"AGNO_DIR=%CURR_DIR%\\..\"\n \n-ECHO [INFO] Formatting Python code in %AGNO_DIR%\n+ECHO.\n+ECHO ##################################################\n+ECHO # Formatting agno\n+ECHO ##################################################\n+ECHO.\n \n-REM Check if black and isort are installed\n-python -c \"import black\" 2>nul\n+REM Check if ruff is installed\n+python -c \"import ruff\" 2>nul\n IF %ERRORLEVEL% NEQ 0 (\n-    ECHO [ERROR] black is not installed. Please install it with: pip install black\n+    ECHO [ERROR] ruff is not installed. Please install it with: pip install ruff\n     EXIT /B 1\n )\n \n-python -c \"import isort\" 2>nul\n-IF %ERRORLEVEL% NEQ 0 (\n-    ECHO [ERROR] isort is not installed. Please install it with: pip install isort\n-    EXIT /B 1\n-)\n+ECHO.\n+ECHO ##################################################\n+ECHO # Running: ruff format %AGNO_DIR%\n+ECHO ##################################################\n+ECHO.\n+\n+python -m ruff format \"%AGNO_DIR%\"\n \n-REM Format Python code with black and isort\n-ECHO [INFO] Running black...\n-python -m black \"%AGNO_DIR%\"\n+ECHO.\n+ECHO ##################################################\n+ECHO # Running: ruff check --select I --fix %AGNO_DIR%\n+ECHO ##################################################\n+ECHO.\n \n-ECHO [INFO] Running isort...\n-python -m isort \"%AGNO_DIR%\"\n+python -m ruff check --select I --fix \"%AGNO_DIR%\"\n \n-ECHO [INFO] Agno code formatting complete.\n+ECHO [INFO] agno formatting complete.\n EXIT /B \n\\ No newline at end of file\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/tests/integration/agent/test_agent_print_response.py",
            "diff": "diff --git a/libs/agno/tests/integration/agent/test_agent_print_response.py b/libs/agno/tests/integration/agent/test_agent_print_response.py\nnew file mode 100644\nindex 000000000..d3aa9fd9b\n--- /dev/null\n+++ b/libs/agno/tests/integration/agent/test_agent_print_response.py\n@@ -0,0 +1,439 @@\n+from unittest.mock import Mock, patch\n+\n+import pytest\n+from rich.console import Console\n+from rich.text import Text\n+\n+from agno.agent import Agent\n+from agno.models.openai import OpenAIChat\n+\n+\n+def test_print_response_with_message_panel():\n+    \"\"\"Test that print_response creates a message panel when show_message=True\"\"\"\n+\n+    def get_the_weather():\n+        return \"It is currently 70 degrees and cloudy in Tokyo\"\n+\n+    agent = Agent(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        tools=[get_the_weather],\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    mock_console = Mock(spec=Console)\n+\n+    with patch(\"rich.live.Live\") as mock_live_class:\n+        with patch(\"agno.agent.agent.create_panel\") as mock_create_panel:\n+            # Configure the Live mock to work as a context manager\n+            mock_live = Mock()\n+            mock_live_class.return_value = mock_live\n+            mock_live.__enter__ = Mock(return_value=mock_live)\n+            mock_live.__exit__ = Mock(return_value=None)\n+\n+            # Mock a successful run response\n+            with patch.object(agent, \"run\") as mock_run:\n+                mock_response = Mock()\n+                mock_response.content = \"It is currently 70 degrees and cloudy in Tokyo\"\n+                mock_response.thinking = None\n+                mock_response.formatted_tool_calls = []\n+                mock_response.citations = None\n+                mock_response.is_paused = False\n+                mock_response.extra_data = None\n+                mock_response.get_content_as_string = Mock(\n+                    return_value=\"It is currently 70 degrees and cloudy in Tokyo\"\n+                )\n+                mock_run.return_value = mock_response\n+\n+                # Run print_response with a message\n+                agent.print_response(\n+                    message=\"What is the weather in Tokyo?\", show_message=True, console=mock_console, stream=False\n+                )\n+\n+                # More specific verification - check exact call arguments\n+                message_panel_calls = [\n+                    call\n+                    for call in mock_create_panel.call_args_list\n+                    if len(call) > 1 and call[1].get(\"title\") == \"Message\"\n+                ]\n+                assert len(message_panel_calls) > 0, \"Message panel should be created when show_message=True\"\n+\n+                # Verify the message content and styling\n+                message_call = message_panel_calls[0]\n+                content_arg = message_call[1][\"content\"]\n+\n+                # Check that the content is a Text object with the right text\n+                if isinstance(content_arg, Text):\n+                    assert \"What is the weather in Tokyo?\" in content_arg.plain\n+                else:\n+                    assert \"What is the weather in Tokyo?\" in str(content_arg)\n+\n+                # Verify border style is correct\n+                assert message_call[1].get(\"border_style\") == \"cyan\"\n+\n+\n+def test_panel_creation_and_structure():\n+    \"\"\"Test that the right panels are created with the right structure\"\"\"\n+\n+    agent = Agent(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        markdown=False,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    mock_console = Mock(spec=Console)\n+\n+    with patch(\"rich.live.Live\") as mock_live_class:\n+        with patch(\"agno.agent.agent.create_panel\") as mock_create_panel:\n+            mock_live = Mock()\n+            mock_live_class.return_value = mock_live\n+            mock_live.__enter__ = Mock(return_value=mock_live)\n+            mock_live.__exit__ = Mock(return_value=None)\n+\n+            with patch.object(agent, \"run\") as mock_run:\n+                mock_response = Mock()\n+                mock_response.content = \"Test response content\"\n+                mock_response.thinking = None\n+                mock_response.formatted_tool_calls = []\n+                mock_response.citations = None\n+                mock_response.is_paused = False\n+                mock_response.extra_data = None\n+                mock_response.get_content_as_string.return_value = \"Test response content\"\n+                mock_run.return_value = mock_response\n+\n+                agent.print_response(message=\"Test message\", show_message=True, console=mock_console, stream=False)\n+\n+                # Verify the structure of what was created\n+                calls = mock_create_panel.call_args_list\n+\n+                # Should have at least 2 calls: message panel and response panel\n+                assert len(calls) >= 2, f\"Expected at least 2 panel calls, got {len(calls)}\"\n+\n+                # First call should be message panel\n+                message_call = calls[0]\n+                assert len(message_call) > 1, \"Call should have keyword arguments\"\n+                assert message_call[1][\"title\"] == \"Message\", \"First panel should be Message\"\n+                assert message_call[1][\"border_style\"] == \"cyan\", \"Message panel should have cyan border\"\n+\n+                # Last call should be response panel\n+                response_call = calls[-1]\n+                assert \"Response\" in response_call[1][\"title\"], \"Last panel should be Response\"\n+                assert response_call[1][\"border_style\"] == \"blue\", \"Response panel should have blue border\"\n+                assert \"0.0s\" in response_call[1][\"title\"], \"Response title should include timing\"\n+\n+\n+def test_print_response_content_verification():\n+    \"\"\"Test that the actual response content makes it into the panel\"\"\"\n+\n+    agent = Agent(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        markdown=False,  # Test without markdown first\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    mock_console = Mock(spec=Console)\n+    expected_response = \"The weather is sunny and 75 degrees\"\n+\n+    with patch(\"rich.live.Live\") as mock_live_class:\n+        with patch(\"agno.agent.agent.create_panel\") as mock_create_panel:\n+            mock_live = Mock()\n+            mock_live_class.return_value = mock_live\n+            mock_live.__enter__ = Mock(return_value=mock_live)\n+            mock_live.__exit__ = Mock(return_value=None)\n+\n+            with patch.object(agent, \"run\") as mock_run:\n+                mock_response = Mock()\n+                mock_response.content = expected_response\n+                mock_response.thinking = None\n+                mock_response.formatted_tool_calls = []\n+                mock_response.citations = None\n+                mock_response.is_paused = False\n+                mock_response.extra_data = None\n+                # Based on the debug output, get_content_as_string is called, so let's make sure it works\n+                mock_response.get_content_as_string.return_value = expected_response\n+                mock_run.return_value = mock_response\n+\n+                agent.print_response(message=\"What's the weather?\", console=mock_console, stream=False)\n+\n+                # Find the response panel call\n+                response_panel_calls = [\n+                    call\n+                    for call in mock_create_panel.call_args_list\n+                    if len(call) > 1 and \"Response\" in str(call[1].get(\"title\", \"\"))\n+                ]\n+\n+                assert len(response_panel_calls) > 0, \"Should create a response panel\"\n+\n+                # Verify the response panel was created (content might be processed differently)\n+                response_call = response_panel_calls[0]\n+                assert response_call[1][\"title\"].startswith(\"Response\"), \"Should have Response title\"\n+                assert response_call[1][\"border_style\"] == \"blue\", \"Should have blue border\"\n+\n+                # The key test: verify that run() was called and returned our mock response\n+                assert mock_run.called, \"run() should be called\"\n+                assert mock_run.return_value.content == expected_response, \"Response should have our content\"\n+\n+\n+def test_markdown_content_type():\n+    \"\"\"Test that markdown=True processes content differently than markdown=False\"\"\"\n+\n+    agent = Agent(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        markdown=True,\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    mock_console = Mock(spec=Console)\n+    markdown_content = \"**Bold** and *italic* text\"\n+\n+    with patch(\"rich.live.Live\") as mock_live_class:\n+        with patch(\"agno.agent.agent.create_panel\") as mock_create_panel:\n+            mock_live = Mock()\n+            mock_live_class.return_value = mock_live\n+            mock_live.__enter__ = Mock(return_value=mock_live)\n+            mock_live.__exit__ = Mock(return_value=None)\n+\n+            with patch.object(agent, \"run\") as mock_run:\n+                mock_response = Mock()\n+                mock_response.content = markdown_content\n+                mock_response.thinking = None\n+                mock_response.formatted_tool_calls = []\n+                mock_response.citations = None\n+                mock_response.is_paused = False\n+                mock_response.extra_data = None\n+                mock_run.return_value = mock_response\n+\n+                agent.print_response(message=\"Test markdown\", console=mock_console, stream=False)\n+\n+                # Just verify that agent.markdown is True and panels were created\n+                assert agent.markdown, \"Agent should have markdown=True\"\n+\n+                # Verify panels were created\n+                assert mock_create_panel.called, \"create_panel should have been called\"\n+\n+                # Check if any panel content looks like it was processed for markdown\n+                panel_calls = mock_create_panel.call_args_list\n+                response_panels = [\n+                    call for call in panel_calls if len(call) > 1 and \"Response\" in str(call[1].get(\"title\", \"\"))\n+                ]\n+\n+                assert len(response_panels) > 0, \"Should create response panels even with markdown\"\n+\n+\n+def test_tool_calls_panel_creation():\n+    \"\"\"Test that tool calls are handled properly\"\"\"\n+\n+    agent = Agent(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        show_tool_calls=True,  # Enable tool call display\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    mock_console = Mock(spec=Console)\n+\n+    with patch(\"rich.live.Live\") as mock_live_class:\n+        with patch(\"agno.agent.agent.create_panel\") as mock_create_panel:\n+            mock_live = Mock()\n+            mock_live_class.return_value = mock_live\n+            mock_live.__enter__ = Mock(return_value=mock_live)\n+            mock_live.__exit__ = Mock(return_value=None)\n+\n+            with patch.object(agent, \"run\") as mock_run:\n+                mock_response = Mock()\n+                mock_response.content = \"Response with tool calls\"\n+                mock_response.thinking = None\n+                mock_response.formatted_tool_calls = [\"get_weather(location='Tokyo')\", \"get_temperature()\"]\n+                mock_response.citations = None\n+                mock_response.is_paused = False\n+                mock_response.extra_data = None\n+                mock_response.get_content_as_string = Mock(return_value=\"Response with tool calls\")\n+                mock_run.return_value = mock_response\n+\n+                agent.print_response(message=\"What's the weather?\", console=mock_console, stream=False)\n+\n+                # Debug: Print all create_panel calls\n+                print(\"All create_panel calls for tool test:\")\n+                for i, call in enumerate(mock_create_panel.call_args_list):\n+                    print(f\"Call {i}: {call}\")\n+\n+                # Check if any panel was created with tool-related content\n+                all_panel_calls = mock_create_panel.call_args_list\n+\n+                # Look for tool calls panel specifically, or check if tools are mentioned anywhere\n+                for call in all_panel_calls:\n+                    if len(call) > 1:\n+                        title = call[1].get(\"title\", \"\")\n+                        content = str(call[1].get(\"content\", \"\"))\n+                        if \"Tool\" in title or \"get_weather\" in content or \"get_temperature\" in content:\n+                            break\n+\n+                # The test should verify that show_tool_calls=True was set and response has tool calls\n+                assert agent.show_tool_calls, \"Agent should have show_tool_calls=True\"\n+                assert mock_response.formatted_tool_calls, \"Response should have formatted_tool_calls\"\n+\n+                # If no tool panel was created, maybe tool calls are shown differently\n+                # Let's just verify the basic functionality works\n+                assert len(all_panel_calls) > 0, \"Some panels should be created\"\n+\n+\n+def test_live_update_calls():\n+    \"\"\"Test that Live.update is called the right number of times\"\"\"\n+\n+    agent = Agent(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    mock_console = Mock(spec=Console)\n+\n+    with patch(\"rich.live.Live\") as mock_live_class:\n+        with patch(\"agno.agent.agent.create_panel\"):\n+            mock_live = Mock()\n+            mock_live_class.return_value = mock_live\n+            mock_live.__enter__ = Mock(return_value=mock_live)\n+            mock_live.__exit__ = Mock(return_value=None)\n+\n+            with patch.object(agent, \"run\") as mock_run:\n+                mock_response = Mock()\n+                mock_response.content = \"Simple response\"\n+                mock_response.thinking = None\n+                mock_response.formatted_tool_calls = []\n+                mock_response.citations = None\n+                mock_response.is_paused = False\n+                mock_response.extra_data = None\n+                mock_response.get_content_as_string = Mock(return_value=\"Simple response\")\n+                mock_run.return_value = mock_response\n+\n+                agent.print_response(message=\"Test\", show_message=True, console=mock_console, stream=False)\n+\n+                # Live.update should be called multiple times as panels are added\n+                assert mock_live.update.call_count >= 2, \"Live.update should be called multiple times\"\n+\n+\n+def test_simple_functionality():\n+    \"\"\"Basic test to understand what print_response actually does\"\"\"\n+\n+    agent = Agent(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    mock_console = Mock(spec=Console)\n+\n+    with patch(\"rich.live.Live\") as mock_live_class:\n+        with patch(\"agno.agent.agent.create_panel\") as mock_create_panel:\n+            mock_live = Mock()\n+            mock_live_class.return_value = mock_live\n+            mock_live.__enter__ = Mock(return_value=mock_live)\n+            mock_live.__exit__ = Mock(return_value=None)\n+\n+            with patch.object(agent, \"run\") as mock_run:\n+                mock_response = Mock()\n+                mock_response.content = \"Simple test response\"\n+                mock_response.thinking = None\n+                mock_response.formatted_tool_calls = []\n+                mock_response.citations = None\n+                mock_response.is_paused = False\n+                mock_response.extra_data = None\n+                mock_response.get_content_as_string = Mock(return_value=\"Simple test response\")\n+                mock_run.return_value = mock_response\n+\n+                # Call print_response\n+                agent.print_response(message=\"Test message\", console=mock_console, stream=False)\n+\n+                # Basic verifications that should always pass\n+                assert mock_run.called, \"run() should be called\"\n+                assert mock_live_class.called, \"Live should be created\"\n+                assert mock_create_panel.called, \"create_panel should be called\"\n+\n+                # Print debug info\n+                print(f\"Number of create_panel calls: {len(mock_create_panel.call_args_list)}\")\n+                for i, call in enumerate(mock_create_panel.call_args_list):\n+                    if len(call) > 1:\n+                        print(f\"Panel {i}: title='{call[1].get('title')}', content type={type(call[1].get('content'))}\")\n+\n+\n+def test_error_handling():\n+    \"\"\"Test that print_response behavior when run() fails\"\"\"\n+\n+    agent = Agent(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    mock_console = Mock(spec=Console)\n+\n+    with patch(\"rich.live.Live\") as mock_live_class:\n+        mock_live = Mock()\n+        mock_live_class.return_value = mock_live\n+        mock_live.__enter__ = Mock(return_value=mock_live)\n+        mock_live.__exit__ = Mock(return_value=None)\n+\n+        with patch.object(agent, \"run\") as mock_run:\n+            # Simulate an exception in the run method\n+            mock_run.side_effect = Exception(\"Test error\")\n+\n+            # Check that the exception is propagated (which seems to be the current behavior)\n+            with pytest.raises(Exception) as exc_info:\n+                agent.print_response(message=\"Test error handling\", console=mock_console, stream=False)\n+\n+            # Verify it's our test exception\n+            assert \"Test error\" in str(exc_info.value)\n+\n+            # The test shows that print_response doesn't handle run() exceptions,\n+            # which is actually useful behavior - errors should bubble up\n+\n+\n+def test_stream_vs_non_stream_behavior():\n+    \"\"\"Test that streaming and non-streaming modes behave differently\"\"\"\n+\n+    agent = Agent(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    mock_console = Mock(spec=Console)\n+\n+    with patch(\"rich.live.Live\") as mock_live_class:\n+        with patch(\"agno.agent.agent.create_panel\") as mock_create_panel:\n+            mock_live = Mock()\n+            mock_live_class.return_value = mock_live\n+            mock_live.__enter__ = Mock(return_value=mock_live)\n+            mock_live.__exit__ = Mock(return_value=None)\n+\n+            # Test non-streaming first\n+            with patch.object(agent, \"run\") as mock_run:\n+                mock_response = Mock()\n+                mock_response.content = \"Non-streaming response\"\n+                mock_response.thinking = None\n+                mock_response.formatted_tool_calls = []\n+                mock_response.citations = None\n+                mock_response.is_paused = False\n+                mock_response.extra_data = None\n+                mock_response.get_content_as_string = Mock(return_value=\"Non-streaming response\")\n+                mock_run.return_value = mock_response\n+\n+                agent.print_response(message=\"Test\", console=mock_console, stream=False)\n+\n+                # Reset mocks\n+                mock_run.reset_mock()\n+                mock_create_panel.reset_mock()\n+\n+                # Test streaming\n+                mock_run.return_value = [mock_response]  # Return iterable for streaming\n+\n+                agent.print_response(message=\"Test\", console=mock_console, stream=True)\n+\n+                # Verify run was called with stream=True\n+                assert any(call.kwargs.get(\"stream\") for call in mock_run.call_args_list), (\n+                    \"run() should be called with stream=True in streaming mode\"\n+                )\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/tests/integration/agent/test_event_streaming.py",
            "diff": "diff --git a/libs/agno/tests/integration/agent/test_event_streaming.py b/libs/agno/tests/integration/agent/test_event_streaming.py\nindex 0bf240895..e65d0cef8 100644\n--- a/libs/agno/tests/integration/agent/test_event_streaming.py\n+++ b/libs/agno/tests/integration/agent/test_event_streaming.py\n@@ -398,3 +398,63 @@ def test_intermediate_steps_with_structured_output(agent_storage):\n     assert events[RunEvent.run_completed][0].content_type == \"Person\"\n     assert events[RunEvent.run_completed][0].content.name == \"Elon Musk\"\n     assert len(events[RunEvent.run_completed][0].content.description) > 1\n+\n+    assert agent.run_response.content is not None\n+    assert agent.run_response.content_type == \"Person\"\n+    assert agent.run_response.content.name == \"Elon Musk\"\n+\n+\n+def test_intermediate_steps_with_parser_model(agent_storage):\n+    \"\"\"Test that the agent streams events.\"\"\"\n+\n+    class Person(BaseModel):\n+        name: str\n+        description: str\n+        age: int\n+\n+    agent = Agent(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        storage=agent_storage,\n+        response_model=Person,\n+        parser_model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response_generator = agent.run(\"Describe Elon Musk\", stream=True, stream_intermediate_steps=True)\n+\n+    events = {}\n+    for run_response_delta in response_generator:\n+        if run_response_delta.event not in events:\n+            events[run_response_delta.event] = []\n+        events[run_response_delta.event].append(run_response_delta)\n+\n+    assert events.keys() == {\n+        RunEvent.run_started,\n+        RunEvent.parser_model_response_started,\n+        RunEvent.parser_model_response_completed,\n+        RunEvent.run_response_content,\n+        RunEvent.run_completed,\n+    }\n+\n+    assert len(events[RunEvent.run_started]) == 1\n+    assert len(events[RunEvent.parser_model_response_started]) == 1\n+    assert len(events[RunEvent.parser_model_response_completed]) == 1\n+    assert (\n+        len(events[RunEvent.run_response_content]) >= 2\n+    )  # The first model streams, then the parser model has a single content event\n+    assert len(events[RunEvent.run_completed]) == 1\n+\n+    assert events[RunEvent.run_response_content][-1].content is not None\n+    assert events[RunEvent.run_response_content][-1].content_type == \"Person\"\n+    assert events[RunEvent.run_response_content][-1].content.name == \"Elon Musk\"\n+    assert len(events[RunEvent.run_response_content][-1].content.description) > 1\n+\n+    assert events[RunEvent.run_completed][0].content is not None\n+    assert events[RunEvent.run_completed][0].content_type == \"Person\"\n+    assert events[RunEvent.run_completed][0].content.name == \"Elon Musk\"\n+    assert len(events[RunEvent.run_completed][0].content.description) > 1\n+\n+    assert agent.run_response.content is not None\n+    assert agent.run_response.content_type == \"Person\"\n+    assert agent.run_response.content.name == \"Elon Musk\"\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/tests/integration/agent/test_memory_impact.py",
            "diff": "diff --git a/libs/agno/tests/integration/agent/test_memory_impact.py b/libs/agno/tests/integration/agent/test_memory_impact.py\nnew file mode 100644\nindex 000000000..97face235\n--- /dev/null\n+++ b/libs/agno/tests/integration/agent/test_memory_impact.py\n@@ -0,0 +1,271 @@\n+import gc\n+import tracemalloc\n+from typing import List, Tuple\n+\n+from agno.agent.agent import Agent\n+from agno.models.openai.chat import OpenAIChat\n+\n+\n+class MemoryMonitor:\n+    \"\"\"Utility class to monitor memory usage during agent operations using tracemalloc.\"\"\"\n+\n+    def __init__(self):\n+        self.memory_readings: List[Tuple[int, float]] = []\n+        self.tracemalloc_snapshots: List[tracemalloc.Snapshot] = []\n+        self.baseline_memory = 0.0\n+\n+    def start_monitoring(self):\n+        \"\"\"Start memory monitoring.\"\"\"\n+        tracemalloc.start()\n+        self._take_reading(\"start\")\n+\n+    def stop_monitoring(self):\n+        \"\"\"Stop memory monitoring.\"\"\"\n+        self._take_reading(\"stop\")\n+        tracemalloc.stop()\n+\n+    def _take_reading(self, label: str):\n+        \"\"\"Take a memory reading using tracemalloc.\"\"\"\n+        # Get current memory usage from tracemalloc\n+        current, peak = tracemalloc.get_traced_memory()\n+        current_memory_mb = current / 1024 / 1024  # Convert to MB\n+        peak_memory_mb = peak / 1024 / 1024  # Convert to MB\n+\n+        # Get tracemalloc snapshot\n+        current_snapshot = tracemalloc.take_snapshot()\n+\n+        self.memory_readings.append((len(self.memory_readings), current_memory_mb))\n+        self.tracemalloc_snapshots.append(current_snapshot)\n+\n+        print(f\"Memory reading {label}: {current_memory_mb:.2f} MB (peak: {peak_memory_mb:.2f} MB)\")\n+\n+    def take_reading(self, label: str = \"\"):\n+        \"\"\"Take a memory reading with optional label.\"\"\"\n+        self._take_reading(label)\n+\n+    def force_gc(self):\n+        \"\"\"Force garbage collection and take a reading.\"\"\"\n+        gc.collect()\n+        self._take_reading(\"after_gc\")\n+\n+    def get_memory_growth(self) -> List[float]:\n+        \"\"\"Calculate memory growth between readings.\"\"\"\n+        if len(self.memory_readings) < 2:\n+            return []\n+\n+        growth = []\n+        for i in range(1, len(self.memory_readings)):\n+            prev_memory = self.memory_readings[i - 1][1]\n+            curr_memory = self.memory_readings[i][1]\n+            growth.append(curr_memory - prev_memory)\n+        return growth\n+\n+    def get_peak_memory(self) -> float:\n+        \"\"\"Get peak memory usage.\"\"\"\n+        if not self.memory_readings:\n+            return 0.0\n+        return max(reading[1] for reading in self.memory_readings)\n+\n+    def get_final_memory(self) -> float:\n+        \"\"\"Get final memory usage.\"\"\"\n+        if not self.memory_readings:\n+            return 0.0\n+        return self.memory_readings[-1][1]\n+\n+    def analyze_tracemalloc(self) -> dict:\n+        \"\"\"Analyze tracemalloc snapshots for memory leaks.\"\"\"\n+        if len(self.tracemalloc_snapshots) < 2:\n+            return {}\n+\n+        first_snapshot = self.tracemalloc_snapshots[0]\n+        last_snapshot = self.tracemalloc_snapshots[-1]\n+\n+        # Compare snapshots\n+        stats = last_snapshot.compare_to(first_snapshot, \"lineno\")\n+\n+        # Get top memory allocations\n+        top_stats = stats[:10]\n+\n+        return {\n+            \"top_allocations\": [\n+                {\"file\": stat.traceback.format()[-1], \"size_diff\": stat.size_diff, \"count_diff\": stat.count_diff}\n+                for stat in top_stats\n+            ]\n+        }\n+\n+\n+def test_agent_memory_impact_with_gc_monitoring(agent_storage, memory):\n+    \"\"\"\n+    Test that creates an agent with memory and storage, runs a series of prompts,\n+    and monitors memory usage to verify garbage collection is working correctly.\n+    \"\"\"\n+    # Create agent with memory and storage\n+    agent = Agent(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        storage=agent_storage,\n+        memory=memory,\n+        enable_user_memories=True,\n+    )\n+\n+    # Initialize memory monitor\n+    monitor = MemoryMonitor()\n+    monitor.start_monitoring()\n+\n+    session_id = \"memory_test_session\"\n+    user_id = \"test_user\"\n+\n+    # Series of prompts to test memory usage\n+    prompts = [\n+        \"Hello, my name is Alice and I like programming.\",\n+        \"I work as a software engineer at a tech company.\",\n+        \"My favorite programming language is Python.\",\n+        \"I enjoy reading science fiction books in my free time.\",\n+        \"I have a cat named Whiskers who is very playful.\",\n+        \"I'm planning to learn machine learning this year.\",\n+        \"What do you remember about me?\",\n+        \"Can you summarize our conversation so far?\",\n+        \"Tell me something interesting about Python programming.\",\n+        \"What are the best practices for memory management in Python?\",\n+    ]\n+\n+    try:\n+        # Run each prompt and monitor memory\n+        for i, prompt in enumerate(prompts):\n+            print(f\"\\n--- Running prompt {i + 1}/{len(prompts)} ---\")\n+            monitor.take_reading(f\"before_prompt_{i + 1}\")\n+\n+            # Run the agent\n+            response = agent.run(prompt, session_id=session_id, user_id=user_id)\n+\n+            assert response is not None\n+            assert response.content is not None\n+            assert response.run_id is not None\n+\n+            monitor.take_reading(f\"after_prompt_{i + 1}\")\n+\n+            # Force garbage collection every few prompts to test GC effectiveness\n+            if (i + 1) % 3 == 0:\n+                print(f\"--- Forcing garbage collection after prompt {i + 1} ---\")\n+                monitor.force_gc()\n+\n+        # Final memory analysis\n+        monitor.take_reading(\"final\")\n+\n+        # Get memory statistics\n+        memory_growth = monitor.get_memory_growth()\n+        peak_memory = monitor.get_peak_memory()\n+        final_memory = monitor.get_final_memory()\n+        monitor.analyze_tracemalloc()\n+\n+        print(\"\\n=== Memory Analysis ===\")\n+        print(f\"Peak memory usage: {peak_memory:.2f} MB\")\n+        print(f\"Final memory usage: {final_memory:.2f} MB\")\n+        print(f\"Number of memory readings: {len(monitor.memory_readings)}\")\n+\n+        if memory_growth:\n+            print(f\"Average memory growth per operation: {sum(memory_growth) / len(memory_growth):.2f} MB\")\n+            print(f\"Max memory growth in single operation: {max(memory_growth):.2f} MB\")\n+\n+        # Verify that memory usage is reasonable\n+        # The agent should not leak excessive memory\n+        assert final_memory < 10, f\"Final memory usage too high: {final_memory:.2f} MB\"\n+\n+        # Verify that garbage collection is working\n+        # After GC, memory should not be significantly higher than before\n+        gc_readings = [i for i, (_, memory) in enumerate(monitor.memory_readings) if \"after_gc\" in str(i)]\n+        if len(gc_readings) > 1:\n+            # Check that memory after GC is not growing excessively\n+            for i in range(1, len(gc_readings)):\n+                prev_gc_memory = monitor.memory_readings[gc_readings[i - 1]][1]\n+                curr_gc_memory = monitor.memory_readings[gc_readings[i]][1]\n+                memory_increase = curr_gc_memory - prev_gc_memory\n+\n+                # Allow some memory growth but not excessive\n+                assert memory_increase < 1, f\"Memory leak detected: {memory_increase:.2f} MB increase after GC\"\n+\n+        # Verify that the agent's memory and storage are working correctly\n+        # Check that memories were created\n+        user_memories = memory.get_user_memories(user_id=user_id)\n+        assert len(user_memories) > 0, \"No user memories were created\"\n+\n+        # Check that sessions were stored\n+        session_from_storage = agent_storage.read(session_id=session_id)\n+        assert session_from_storage is not None, \"Session was not stored\"\n+\n+        # Check that runs are in memory\n+        assert session_id in memory.runs, \"Session runs not found in memory\"\n+        assert len(memory.runs[session_id]) == len(prompts), (\n+            f\"Expected {len(prompts)} runs, got {len(memory.runs[session_id])}\"\n+        )\n+\n+        print(\"\u2705 Memory impact test completed successfully\")\n+        print(f\"\u2705 Created {len(user_memories)} user memories\")\n+        print(f\"\u2705 Stored {len(memory.runs[session_id])} runs in memory\")\n+\n+    finally:\n+        monitor.stop_monitoring()\n+\n+\n+def test_agent_memory_cleanup_after_session_switch(agent_storage, memory):\n+    \"\"\"\n+    Test that verifies memory is properly cleaned up when switching between sessions.\n+    \"\"\"\n+    agent = Agent(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        storage=agent_storage,\n+        memory=memory,\n+        enable_user_memories=True,\n+    )\n+\n+    monitor = MemoryMonitor()\n+    monitor.start_monitoring()\n+\n+    user_id = \"test_user_cleanup\"\n+\n+    try:\n+        # Create multiple sessions and run prompts\n+        sessions = [\"session_1\", \"session_2\", \"session_3\"]\n+\n+        for session_idx, session_id in enumerate(sessions):\n+            print(f\"\\n--- Testing session {session_id} ---\")\n+            monitor.take_reading(f\"before_session_{session_idx + 1}\")\n+\n+            # Run a few prompts in this session\n+            for prompt_idx in range(3):\n+                prompt = f\"This is prompt {prompt_idx + 1} in session {session_id}\"\n+                response = agent.run(prompt, session_id=session_id, user_id=user_id)\n+\n+                assert response is not None\n+                assert response.content is not None\n+\n+            monitor.take_reading(f\"after_session_{session_idx + 1}\")\n+\n+            # Force GC after each session\n+            monitor.force_gc()\n+\n+        # Switch back to first session and verify memory doesn't grow excessively\n+        print(\"\\n--- Switching back to first session ---\")\n+        monitor.take_reading(\"before_switch_back\")\n+\n+        response = agent.run(\n+            \"What do you remember from our previous conversation?\", session_id=sessions[0], user_id=user_id\n+        )\n+\n+        assert response is not None\n+        monitor.take_reading(\"after_switch_back\")\n+        monitor.force_gc()\n+\n+        # Verify memory usage is reasonable\n+        final_memory = monitor.get_final_memory()\n+        assert final_memory < 500, f\"Final memory usage too high: {final_memory:.2f} MB\"\n+\n+        # Verify all sessions are properly stored\n+        for session_id in sessions:\n+            session_from_storage = agent_storage.read(session_id=session_id)\n+            assert session_from_storage is not None, f\"Session {session_id} was not stored\"\n+            assert session_id in memory.runs, f\"Session {session_id} runs not found in memory\"\n+\n+        print(\"\u2705 Session switching memory test completed successfully\")\n+\n+    finally:\n+        monitor.stop_monitoring()\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/tests/integration/agent/test_parser_model.py",
            "diff": "diff --git a/libs/agno/tests/integration/agent/test_parser_model.py b/libs/agno/tests/integration/agent/test_parser_model.py\nindex d02eeb508..ec07c1868 100644\n--- a/libs/agno/tests/integration/agent/test_parser_model.py\n+++ b/libs/agno/tests/integration/agent/test_parser_model.py\n@@ -91,3 +91,28 @@ def test_gemini_with_openai_parser_model():\n \n     assert isinstance(response.content.best_season_to_visit, str)\n     assert len(response.content.best_season_to_visit) > 0\n+\n+\n+def test_parser_model_stream():\n+    park_agent = Agent(\n+        model=OpenAIChat(id=\"gpt-4o\"),  # Main model to generate the content\n+        description=\"You are an expert on national parks and provide concise guides.\",\n+        response_model=ParkGuide,\n+        parser_model=Claude(id=\"claude-sonnet-4-20250514\"),  # Model to parse the output\n+    )\n+\n+    response = park_agent.run(\"Tell me about Yosemite National Park.\", stream=True)\n+\n+    for event in response:\n+        print(event)\n+\n+    assert park_agent.run_response.content is not None\n+    assert isinstance(park_agent.run_response.content, ParkGuide)\n+    assert isinstance(park_agent.run_response.content.park_name, str)\n+    assert len(park_agent.run_response.content.park_name) > 0\n+\n+    assert isinstance(park_agent.run_response.content.activities, list)\n+    assert len(park_agent.run_response.content.activities) >= 2\n+    for activity in park_agent.run_response.content.activities:\n+        assert isinstance(activity, str)\n+        assert len(activity) > 0\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/tests/integration/agent/test_user_confirmation_flows.py",
            "diff": "diff --git a/libs/agno/tests/integration/agent/test_user_confirmation_flows.py b/libs/agno/tests/integration/agent/test_user_confirmation_flows.py\nindex 7ce149eb2..55908ddef 100644\n--- a/libs/agno/tests/integration/agent/test_user_confirmation_flows.py\n+++ b/libs/agno/tests/integration/agent/test_user_confirmation_flows.py\n@@ -286,7 +286,6 @@ async def test_tool_call_requires_confirmation_async():\n     response.tools[0].confirmed = True\n \n     response = await agent.acontinue_run(response)\n-    assert response.is_paused is False\n     assert response.tools[0].result == \"It is currently 70 degrees and cloudy in Tokyo\"\n \n \n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/tests/integration/memory/test_memory.py",
            "diff": "diff --git a/libs/agno/tests/integration/memory/test_memory.py b/libs/agno/tests/integration/memory/test_memory.py\nindex e2342b547..358dfe405 100644\n--- a/libs/agno/tests/integration/memory/test_memory.py\n+++ b/libs/agno/tests/integration/memory/test_memory.py\n@@ -500,3 +500,52 @@ async def test_aupdate_memory_task_with_db(memory_with_db):\n     memories = memory_with_db.get_user_memories(\"test_user\")\n     assert len(memories) > 0\n     assert any(\"John Doe\" not in memory.memory for memory in memories)\n+\n+\n+def test_memory_deepcopy(memory_with_db):\n+    \"\"\"Test that deepcopy works correctly for Memory instances.\"\"\"\n+    from copy import deepcopy\n+\n+    # Add some data to the original memory\n+    user_memory = UserMemory(memory=\"The user's name is John Doe\", topics=[\"name\", \"user\"], last_updated=datetime.now())\n+    memory_id = memory_with_db.add_user_memory(memory=user_memory, user_id=\"test_user\")\n+\n+    # Add a run\n+    session_id = \"test_session\"\n+    run_response = RunResponse(\n+        content=\"Sample response\",\n+        messages=[\n+            Message(role=\"user\", content=\"Hello, how are you?\"),\n+            Message(role=\"assistant\", content=\"I'm doing well, thank you for asking!\"),\n+        ],\n+    )\n+    memory_with_db.add_run(session_id, run_response)\n+\n+    # Create a deep copy\n+    copied_memory = deepcopy(memory_with_db)\n+\n+    # Verify the copy is a different object\n+    assert copied_memory is not memory_with_db\n+\n+    # Verify shared objects are reused (not copied)\n+    assert copied_memory.db is memory_with_db.db\n+    assert copied_memory.memory_manager is memory_with_db.memory_manager\n+    assert copied_memory.summary_manager is memory_with_db.summary_manager\n+\n+    # Verify memories are deep copied\n+    assert copied_memory.memories is not memory_with_db.memories\n+    assert copied_memory.memories[\"test_user\"] is not memory_with_db.memories[\"test_user\"]\n+    assert copied_memory.memories[\"test_user\"][memory_id] is not memory_with_db.memories[\"test_user\"][memory_id]\n+    assert (\n+        copied_memory.memories[\"test_user\"][memory_id].memory == memory_with_db.memories[\"test_user\"][memory_id].memory\n+    )\n+\n+    # Verify runs are deep copied\n+    assert copied_memory.runs is not memory_with_db.runs\n+    assert copied_memory.runs[session_id] is not memory_with_db.runs[session_id]\n+    assert len(copied_memory.runs[session_id]) == len(memory_with_db.runs[session_id])\n+\n+    # Verify modifying the copy doesn't affect the original\n+    copied_memory.memories[\"test_user\"][memory_id].memory = \"Modified memory\"\n+    assert memory_with_db.memories[\"test_user\"][memory_id].memory == \"The user's name is John Doe\"\n+    assert copied_memory.memories[\"test_user\"][memory_id].memory == \"Modified memory\"\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/tests/integration/teams/test_event_streaming.py",
            "diff": "diff --git a/libs/agno/tests/integration/teams/test_event_streaming.py b/libs/agno/tests/integration/teams/test_event_streaming.py\nindex ea1afd0fb..7d635a53e 100644\n--- a/libs/agno/tests/integration/teams/test_event_streaming.py\n+++ b/libs/agno/tests/integration/teams/test_event_streaming.py\n@@ -386,6 +386,64 @@ def test_intermediate_steps_with_structured_output(team_storage):\n     assert len(events[TeamRunEvent.run_completed][0].content.description) > 1\n \n \n+def test_intermediate_steps_with_parser_model(team_storage):\n+    \"\"\"Test that the agent streams events.\"\"\"\n+\n+    class Person(BaseModel):\n+        name: str\n+        description: str\n+        age: int\n+\n+    team = Team(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        members=[],\n+        storage=team_storage,\n+        response_model=Person,\n+        parser_model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        instructions=\"You have no members, answer directly\",\n+        telemetry=False,\n+        monitoring=False,\n+    )\n+\n+    response_generator = team.run(\"Describe Elon Musk\", stream=True, stream_intermediate_steps=True)\n+\n+    events = {}\n+    for run_response_delta in response_generator:\n+        if run_response_delta.event not in events:\n+            events[run_response_delta.event] = []\n+        events[run_response_delta.event].append(run_response_delta)\n+\n+    assert events.keys() == {\n+        TeamRunEvent.run_started,\n+        TeamRunEvent.parser_model_response_started,\n+        TeamRunEvent.parser_model_response_completed,\n+        TeamRunEvent.run_response_content,\n+        TeamRunEvent.run_completed,\n+    }\n+\n+    assert len(events[TeamRunEvent.run_started]) == 1\n+    assert len(events[TeamRunEvent.parser_model_response_started]) == 1\n+    assert len(events[TeamRunEvent.parser_model_response_completed]) == 1\n+    assert (\n+        len(events[TeamRunEvent.run_response_content]) >= 2\n+    )  # The first model streams, then the parser model has a single content event\n+    assert len(events[TeamRunEvent.run_completed]) == 1\n+\n+    assert events[TeamRunEvent.run_response_content][-1].content is not None\n+    assert events[TeamRunEvent.run_response_content][-1].content_type == \"Person\"\n+    assert events[TeamRunEvent.run_response_content][-1].content.name == \"Elon Musk\"\n+    assert len(events[TeamRunEvent.run_response_content][-1].content.description) > 1\n+\n+    assert events[TeamRunEvent.run_completed][0].content is not None\n+    assert events[TeamRunEvent.run_completed][0].content_type == \"Person\"\n+    assert events[TeamRunEvent.run_completed][0].content.name == \"Elon Musk\"\n+    assert len(events[TeamRunEvent.run_completed][0].content.description) > 1\n+\n+    assert team.run_response.content is not None\n+    assert team.run_response.content_type == \"Person\"\n+    assert team.run_response.content.name == \"Elon Musk\"\n+\n+\n def test_intermediate_steps_with_member_agents():\n     agent_1 = Agent(\n         name=\"Analyst\",\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/tests/integration/teams/test_memory_impact.py",
            "diff": "diff --git a/libs/agno/tests/integration/teams/test_memory_impact.py b/libs/agno/tests/integration/teams/test_memory_impact.py\nnew file mode 100644\nindex 000000000..c4f9e8cc9\n--- /dev/null\n+++ b/libs/agno/tests/integration/teams/test_memory_impact.py\n@@ -0,0 +1,557 @@\n+import asyncio\n+import gc\n+import tracemalloc\n+from time import time\n+from typing import List, Tuple\n+\n+import pytest\n+\n+from agno.agent.agent import Agent\n+from agno.models.openai.chat import OpenAIChat\n+from agno.team.team import Team\n+\n+\n+class MemoryMonitor:\n+    \"\"\"Utility class to monitor memory usage during team operations using tracemalloc.\"\"\"\n+\n+    def __init__(self):\n+        self.memory_readings: List[Tuple[int, float]] = []\n+        self.tracemalloc_snapshots: List[tracemalloc.Snapshot] = []\n+        self.baseline_memory = 0.0\n+\n+    def start_monitoring(self):\n+        \"\"\"Start memory monitoring.\"\"\"\n+        tracemalloc.start()\n+        self._take_reading(\"start\")\n+\n+    def stop_monitoring(self):\n+        \"\"\"Stop memory monitoring.\"\"\"\n+        self._take_reading(\"stop\")\n+        tracemalloc.stop()\n+\n+    def _take_reading(self, label: str):\n+        \"\"\"Take a memory reading using tracemalloc.\"\"\"\n+        # Get current memory usage from tracemalloc\n+        current, peak = tracemalloc.get_traced_memory()\n+        current_memory_mb = current / 1024 / 1024  # Convert to MB\n+        peak_memory_mb = peak / 1024 / 1024  # Convert to MB\n+\n+        # Get tracemalloc snapshot\n+        current_snapshot = tracemalloc.take_snapshot()\n+\n+        self.memory_readings.append((len(self.memory_readings), current_memory_mb))\n+        self.tracemalloc_snapshots.append(current_snapshot)\n+\n+        print(f\"Memory reading {label}: {current_memory_mb:.2f} MB (peak: {peak_memory_mb:.2f} MB)\")\n+\n+    def take_reading(self, label: str = \"\"):\n+        \"\"\"Take a memory reading with optional label.\"\"\"\n+        self._take_reading(label)\n+\n+    def force_gc(self):\n+        \"\"\"Force garbage collection and take a reading.\"\"\"\n+        gc.collect()\n+        self._take_reading(\"after_gc\")\n+\n+    def get_memory_growth(self) -> List[float]:\n+        \"\"\"Calculate memory growth between readings.\"\"\"\n+        if len(self.memory_readings) < 2:\n+            return []\n+\n+        growth = []\n+        for i in range(1, len(self.memory_readings)):\n+            prev_memory = self.memory_readings[i - 1][1]\n+            curr_memory = self.memory_readings[i][1]\n+            growth.append(curr_memory - prev_memory)\n+        return growth\n+\n+    def get_peak_memory(self) -> float:\n+        \"\"\"Get peak memory usage.\"\"\"\n+        if not self.memory_readings:\n+            return 0.0\n+        return max(reading[1] for reading in self.memory_readings)\n+\n+    def get_final_memory(self) -> float:\n+        \"\"\"Get final memory usage.\"\"\"\n+        if not self.memory_readings:\n+            return 0.0\n+        return self.memory_readings[-1][1]\n+\n+    def analyze_tracemalloc(self) -> dict:\n+        \"\"\"Analyze tracemalloc snapshots for memory leaks.\"\"\"\n+        if len(self.tracemalloc_snapshots) < 2:\n+            return {}\n+\n+        first_snapshot = self.tracemalloc_snapshots[0]\n+        last_snapshot = self.tracemalloc_snapshots[-1]\n+\n+        # Compare snapshots\n+        stats = last_snapshot.compare_to(first_snapshot, \"lineno\")\n+\n+        # Get top memory allocations\n+        top_stats = stats[:10]\n+\n+        return {\n+            \"top_allocations\": [\n+                {\"file\": stat.traceback.format()[-1], \"size_diff\": stat.size_diff, \"count_diff\": stat.count_diff}\n+                for stat in top_stats\n+            ]\n+        }\n+\n+\n+def test_team_memory_impact_with_gc_monitoring(agent_storage, team_storage, memory):\n+    \"\"\"\n+    Test that creates a team with memory and storage, runs a series of prompts,\n+    and monitors memory usage to verify garbage collection is working correctly.\n+    \"\"\"\n+\n+    # Create simple agents for the team\n+    def simple_calculator(operation: str, a: float, b: float) -> str:\n+        \"\"\"Simple calculator function.\"\"\"\n+        if operation == \"add\":\n+            return f\"{a} + {b} = {a + b}\"\n+        elif operation == \"subtract\":\n+            return f\"{a} - {b} = {a - b}\"\n+        elif operation == \"multiply\":\n+            return f\"{a} * {b} = {a * b}\"\n+        elif operation == \"divide\":\n+            return f\"{a} / {b} = {a / b}\"\n+        else:\n+            return f\"Unknown operation: {operation}\"\n+\n+    def text_processor(text: str, operation: str) -> str:\n+        \"\"\"Simple text processing function.\"\"\"\n+        if operation == \"uppercase\":\n+            return text.upper()\n+        elif operation == \"lowercase\":\n+            return text.lower()\n+        elif operation == \"length\":\n+            return f\"Length: {len(text)} characters\"\n+        else:\n+            return f\"Unknown operation: {operation}\"\n+\n+    # Create team members\n+    calculator_agent = Agent(\n+        name=\"Calculator Agent\",\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        role=\"Perform mathematical calculations\",\n+        tools=[simple_calculator],\n+        storage=agent_storage,\n+        memory=memory,\n+        enable_user_memories=True,\n+    )\n+\n+    text_agent = Agent(\n+        name=\"Text Processor Agent\",\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        role=\"Process and analyze text\",\n+        tools=[text_processor],\n+        storage=agent_storage,\n+        memory=memory,\n+        enable_user_memories=True,\n+    )\n+\n+    # Create team with memory and storage\n+    team = Team(\n+        name=\"Memory Test Team\",\n+        mode=\"route\",\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        members=[calculator_agent, text_agent],\n+        storage=team_storage,\n+        memory=memory,\n+        enable_user_memories=True,\n+        instructions=\"Route mathematical questions to the calculator agent and text processing questions to the text processor agent.\",\n+    )\n+\n+    # Initialize memory monitor\n+    monitor = MemoryMonitor()\n+    monitor.start_monitoring()\n+\n+    session_id = \"team_memory_test_session\"\n+    user_id = \"test_user\"\n+\n+    # Series of prompts to test memory usage\n+    prompts = [\n+        \"Calculate 15 + 27\",\n+        \"What is 42 - 18?\",\n+        \"Process the text 'Hello World' to uppercase\",\n+        \"Calculate 7 * 8\",\n+        \"What is the length of 'Python Programming'?\",\n+        \"Calculate 100 / 4\",\n+        \"Convert 'MEMORY TEST' to lowercase\",\n+        \"What is 3 + 5 + 7?\",\n+        \"Process 'Team Memory Impact Test' to uppercase\",\n+        \"Calculate 25 * 4\",\n+    ]\n+\n+    try:\n+        # Run each prompt and monitor memory\n+        for i, prompt in enumerate(prompts):\n+            print(f\"\\n--- Running team prompt {i + 1}/{len(prompts)} ---\")\n+            monitor.take_reading(f\"before_prompt_{i + 1}\")\n+\n+            # Run the team\n+            response = team.run(prompt, session_id=session_id, user_id=user_id)\n+\n+            assert response is not None\n+            assert response.content is not None\n+            assert response.run_id is not None\n+\n+            monitor.take_reading(f\"after_prompt_{i + 1}\")\n+\n+            # Force garbage collection every few prompts to test GC effectiveness\n+            if (i + 1) % 3 == 0:\n+                print(f\"--- Forcing garbage collection after prompt {i + 1} ---\")\n+                monitor.force_gc()\n+\n+        # Final memory analysis\n+        monitor.take_reading(\"final\")\n+\n+        # Get memory statistics\n+        memory_growth = monitor.get_memory_growth()\n+        peak_memory = monitor.get_peak_memory()\n+        final_memory = monitor.get_final_memory()\n+\n+        print(\"\\n=== Team Memory Analysis ===\")\n+        print(f\"Peak memory usage: {peak_memory:.2f} MB\")\n+        print(f\"Final memory usage: {final_memory:.2f} MB\")\n+        print(f\"Number of memory readings: {len(monitor.memory_readings)}\")\n+\n+        if memory_growth:\n+            print(f\"Average memory growth per operation: {sum(memory_growth) / len(memory_growth):.2f} MB\")\n+            print(f\"Max memory growth in single operation: {max(memory_growth):.2f} MB\")\n+\n+        # STRICT MEMORY LIMITS: Final memory must be under 20MB\n+        assert final_memory < 20, f\"Final memory usage too high: {final_memory:.2f} MB (limit: 20MB)\"\n+\n+        # Verify that garbage collection is working\n+        # After GC, memory should not be significantly higher than before\n+        gc_readings = [i for i, (_, memory) in enumerate(monitor.memory_readings) if \"after_gc\" in str(i)]\n+        if len(gc_readings) > 1:\n+            # Check that memory after GC is not growing excessively\n+            for i in range(1, len(gc_readings)):\n+                prev_gc_memory = monitor.memory_readings[gc_readings[i - 1]][1]\n+                curr_gc_memory = monitor.memory_readings[gc_readings[i]][1]\n+                memory_increase = curr_gc_memory - prev_gc_memory\n+\n+                # Allow minimal memory growth but not excessive\n+                assert memory_increase < 0.5, f\"Memory leak detected: {memory_increase:.2f} MB increase after GC\"\n+\n+        # Check that sessions were stored\n+        session_from_storage = team_storage.read(session_id=session_id)\n+        assert session_from_storage is not None, \"Session was not stored\"\n+\n+        # Check that runs are in memory\n+        assert session_id in memory.runs, \"Session runs not found in memory\"\n+        assert len(memory.runs[session_id]) == len(prompts), (\n+            f\"Expected {len(prompts)} runs, got {len(memory.runs[session_id])}\"\n+        )\n+\n+        print(\"\u2705 Team memory impact test completed successfully\")\n+        print(f\"\u2705 Stored {len(memory.runs[session_id])} runs in memory\")\n+        print(f\"\u2705 Peak memory: {peak_memory:.2f} MB, Final memory: {final_memory:.2f} MB\")\n+\n+    finally:\n+        monitor.stop_monitoring()\n+\n+\n+def test_team_memory_cleanup_after_session_switch(agent_storage, team_storage, memory):\n+    \"\"\"\n+    Test that verifies team memory is properly cleaned up when switching between sessions.\n+    \"\"\"\n+\n+    # Create simple team with basic agents\n+    def simple_function(input_text: str) -> str:\n+        return f\"Processed: {input_text}\"\n+\n+    agent = Agent(\n+        name=\"Simple Agent\",\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        role=\"Process simple requests\",\n+        tools=[simple_function],\n+        storage=agent_storage,\n+        memory=memory,\n+        enable_user_memories=True,\n+    )\n+\n+    team = Team(\n+        name=\"Session Switch Team\",\n+        mode=\"route\",\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        members=[agent],\n+        storage=agent_storage,\n+        memory=memory,\n+        enable_user_memories=True,\n+    )\n+\n+    monitor = MemoryMonitor()\n+    monitor.start_monitoring()\n+\n+    user_id = \"test_user_cleanup\"\n+\n+    try:\n+        # Create multiple sessions and run prompts\n+        sessions = [\"session_1\", \"session_2\", \"session_3\"]\n+\n+        for session_idx, session_id in enumerate(sessions):\n+            print(f\"\\n--- Testing team session {session_id} ---\")\n+            monitor.take_reading(f\"before_session_{session_idx + 1}\")\n+\n+            # Run a few prompts in this session\n+            for prompt_idx in range(3):\n+                prompt = f\"Process this text: session {session_id} prompt {prompt_idx + 1}\"\n+                response = team.run(prompt, session_id=session_id, user_id=user_id)\n+\n+                assert response is not None\n+                assert response.content is not None\n+\n+            monitor.take_reading(f\"after_session_{session_idx + 1}\")\n+\n+            # Force GC after each session\n+            monitor.force_gc()\n+\n+        # Switch back to first session and verify memory doesn't grow excessively\n+        print(\"\\n--- Switching back to first session ---\")\n+        monitor.take_reading(\"before_switch_back\")\n+\n+        response = team.run(\n+            \"What do you remember from our previous conversation?\", session_id=sessions[0], user_id=user_id\n+        )\n+\n+        assert response is not None\n+        monitor.take_reading(\"after_switch_back\")\n+        monitor.force_gc()\n+\n+        # STRICT MEMORY LIMITS: Final memory must be under 20MB\n+        final_memory = monitor.get_final_memory()\n+        assert final_memory < 20, f\"Final memory usage too high: {final_memory:.2f} MB (limit: 20MB)\"\n+\n+        # Verify all sessions are properly stored\n+        for session_id in sessions:\n+            session_from_storage = team_storage.read(session_id=session_id)\n+            assert session_from_storage is not None, f\"Session {session_id} was not stored\"\n+            assert session_id in memory.runs, f\"Session {session_id} runs not found in memory\"\n+\n+        print(\"\u2705 Team session switching memory test completed successfully\")\n+        print(f\"\u2705 Final memory: {final_memory:.2f} MB\")\n+\n+    finally:\n+        monitor.stop_monitoring()\n+\n+\n+@pytest.mark.asyncio\n+async def test_team_memory_with_multiple_members(agent_storage, team_storage, memory):\n+    \"\"\"\n+    Test memory usage with multiple team members to ensure scalability.\n+    \"\"\"\n+\n+    # Create multiple agents with realistic functions that would generate memories\n+    def calculate_budget(income: float, expenses: float, savings_goal: float) -> str:\n+        \"\"\"Calculate budget and provide financial advice.\"\"\"\n+        disposable_income = income - expenses\n+        months_to_goal = savings_goal / disposable_income if disposable_income > 0 else float(\"inf\")\n+\n+        if disposable_income <= 0:\n+            return f\"\u26a0\ufe0f Your expenses (${expenses:.2f}) exceed your income (${income:.2f}). Consider reducing expenses or increasing income.\"\n+        elif months_to_goal <= 12:\n+            return f\"\u2705 Great! You can reach your ${savings_goal:.2f} goal in {months_to_goal:.1f} months with ${disposable_income:.2f} disposable income.\"\n+        else:\n+            return f\"\ud83d\udcca You'll reach your ${savings_goal:.2f} goal in {months_to_goal:.1f} months. Consider increasing savings or adjusting your goal.\"\n+\n+    def analyze_health_data(age: int, weight: float, height: float, activity_level: str) -> str:\n+        \"\"\"Analyze health data and provide recommendations.\"\"\"\n+        bmi = weight / ((height / 100) ** 2)\n+\n+        if bmi < 18.5:\n+            category = \"underweight\"\n+            recommendation = \"Consider increasing caloric intake with healthy foods.\"\n+        elif bmi < 25:\n+            category = \"normal weight\"\n+            recommendation = \"Maintain your healthy weight with balanced nutrition and exercise.\"\n+        elif bmi < 30:\n+            category = \"overweight\"\n+            recommendation = \"Focus on portion control and regular physical activity.\"\n+        else:\n+            category = \"obese\"\n+            recommendation = \"Consult with a healthcare provider for a personalized weight management plan.\"\n+\n+        return f\"\ud83d\udcca BMI: {bmi:.1f} ({category}). {recommendation} Activity level: {activity_level}\"\n+\n+    def schedule_meeting(duration_minutes: int, priority: str) -> str:\n+        \"\"\"Schedule a meeting and provide coordination details.\"\"\"\n+        priority_emoji = {\"high\": \"\ud83d\udd34\", \"medium\": \"\ud83d\udfe1\", \"low\": \"\ud83d\udfe2\"}\n+        emoji = priority_emoji.get(priority, \"\u26aa\")\n+\n+        return f\"{emoji} Meeting scheduled: {duration_minutes} minutes ({priority} priority)\"\n+\n+    # Mock the model to avoid API calls\n+    class MockModel:\n+        def __init__(self, id: str):\n+            self.id = id\n+            self.name = \"MockModel\"\n+            self.provider = \"MockProvider\"\n+            self.assistant_message_role = \"assistant\"\n+\n+        def get_instructions_for_model(self, tools: List):\n+            return \"\"\n+\n+        def get_system_message_for_model(self, tools: List):\n+            return \"\"\n+\n+        def to_dict(self):\n+            return {}\n+\n+        async def aresponse(self, messages, **kwargs):\n+            # Return a mock response\n+            return type(\n+                \"MockResponse\",\n+                (),\n+                {\n+                    \"content\": f\"Mock response for {messages[-1].content[:50]}...\",\n+                    \"run_id\": f\"mock_run_{hash(str(messages))}\",\n+                    \"model\": self.name,\n+                    \"thinking\": None,\n+                    \"citations\": None,\n+                    \"tool_executions\": None,\n+                    \"tool_calls\": [],\n+                    \"audio\": None,\n+                    \"created_at\": int(time()),\n+                    \"usage\": type(\n+                        \"MockUsage\", (), {\"total_tokens\": 100, \"prompt_tokens\": 50, \"completion_tokens\": 50}\n+                    )(),\n+                    \"finish_reason\": \"stop\",\n+                },\n+            )()\n+\n+    agent1 = Agent(\n+        name=\"Financial Advisor\",\n+        model=MockModel(id=\"gpt-4o-mini\"),\n+        role=\"Provide financial planning and budget analysis\",\n+        tools=[calculate_budget],\n+        storage=agent_storage,\n+        memory=memory,\n+        enable_user_memories=True,\n+        add_history_to_messages=True,\n+    )\n+\n+    agent2 = Agent(\n+        name=\"Health Coach\",\n+        model=MockModel(id=\"gpt-4o-mini\"),\n+        role=\"Analyze health data and provide wellness recommendations\",\n+        tools=[analyze_health_data],\n+        storage=agent_storage,\n+        memory=memory,\n+        enable_user_memories=True,\n+        add_history_to_messages=True,\n+    )\n+\n+    agent3 = Agent(\n+        name=\"Meeting Coordinator\",\n+        model=MockModel(id=\"gpt-4o-mini\"),\n+        role=\"Help schedule meetings and coordinate team activities\",\n+        tools=[schedule_meeting],\n+        storage=agent_storage,\n+        memory=memory,\n+        enable_user_memories=True,\n+        add_history_to_messages=True,\n+    )\n+\n+    team = Team(\n+        name=\"Personal Assistant Team\",\n+        mode=\"route\",\n+        model=MockModel(id=\"gpt-4o-mini\"),\n+        members=[agent1, agent2, agent3],\n+        storage=team_storage,\n+        memory=memory,\n+        enable_user_memories=True,\n+        add_history_to_messages=True,\n+        instructions=\"Route financial questions to the Financial Advisor, health-related questions to the Health Coach, and meeting/scheduling requests to the Meeting Coordinator. Remember user preferences and past interactions to provide personalized assistance.\",\n+    )\n+\n+    monitor = MemoryMonitor()\n+    monitor.start_monitoring()\n+\n+    users = [f\"test_user_{i}\" for i in range(10)]\n+    try:\n+        # Create realistic prompts that would generate meaningful user memories\n+        realistic_prompts = [\n+            \"I make $5000 per month and spend $3500 on expenses. I want to save $10000 for a vacation. Can you help me plan this?\",\n+            \"I'm 28 years old, weigh 70kg, am 175cm tall, and exercise 3 times per week. How's my health looking?\",\n+            \"I need to schedule a team meeting with 4 people for 1 hour. It's a high priority project kickoff.\",\n+            \"My expenses went up to $4000 this month due to car repairs. How does this affect my vacation savings goal?\",\n+            \"I've been working out more and now exercise 5 times per week. Can you update my health assessment?\",\n+            \"I need to schedule a follow-up meeting with the same team from last time, but this time it's medium priority and only 30 minutes.\",\n+            \"I got a raise to $6000 per month! How much faster can I reach my vacation savings goal now?\",\n+            \"I've lost 3kg since our last conversation. Can you recalculate my health metrics?\",\n+            \"The team meeting went well. I need to schedule a presentation meeting with 8 stakeholders for 2 hours, high priority.\",\n+            \"I'm thinking of buying a house and need to save $50000 for a down payment. How long will this take with my current budget?\",\n+        ]\n+\n+        print(\"--- Running realistic multi-member team test (concurrent) ---\")\n+\n+        async def run_prompt(i, prompt):\n+            response = await team.arun(prompt, session_id=f\"{users[i]}_session\", user_id=users[i])\n+            assert response is not None\n+            assert response.content is not None\n+            assert len(response.content) > 10, f\"Response too short: {response.content}\"\n+            return response\n+\n+        for _ in range(10):\n+            tasks = []\n+            monitor.take_reading(\"before_concurrent_prompts\")\n+            for i, prompt in enumerate(realistic_prompts):\n+                tasks.append(run_prompt(i, prompt))\n+            await asyncio.gather(*tasks)\n+            monitor.take_reading(\"after_concurrent_prompts\")\n+\n+        monitor.force_gc()\n+\n+        # Comprehensive memory growth analysis\n+        memory_growth = monitor.get_memory_growth()\n+        peak_memory = monitor.get_peak_memory()\n+        final_memory = monitor.get_final_memory()\n+        initial_memory = monitor.memory_readings[0][1] if monitor.memory_readings else 0.0\n+\n+        print(\"\\n=== Memory Growth Analysis ===\")\n+        print(f\"Initial memory: {initial_memory:.2f} MB\")\n+        print(f\"Peak memory: {peak_memory:.2f} MB\")\n+        print(f\"Final memory: {final_memory:.2f} MB\")\n+        print(f\"Total memory growth: {final_memory - initial_memory:.2f} MB\")\n+        print(f\"Peak memory growth: {peak_memory - initial_memory:.2f} MB\")\n+\n+        if memory_growth:\n+            avg_growth = sum(memory_growth) / len(memory_growth)\n+            max_growth = max(memory_growth)\n+\n+            print(f\"Average memory growth per operation: {avg_growth:.2f} MB\")\n+\n+        # STRICT MEMORY LIMITS: Final memory must be under 20MB\n+        assert final_memory < 20, f\"Memory usage too high with multiple members: {final_memory:.2f} MB (limit: 20MB)\"\n+\n+        # Verify memory growth patterns are reasonable\n+        if memory_growth:\n+            # Check that average growth per operation is reasonable (should be small)\n+            avg_growth = sum(memory_growth) / len(memory_growth)\n+            assert avg_growth < 2.0, f\"Average memory growth too high: {avg_growth:.2f} MB per operation\"\n+\n+            # Check that no single operation causes excessive memory growth\n+            assert max_growth < 10.0, f\"Single operation memory growth too high: {max_growth:.2f} MB\"\n+\n+            # Verify that garbage collection is effective\n+            # After GC, memory should not be significantly higher than before\n+            gc_readings = [i for i, (_, memory) in enumerate(monitor.memory_readings) if \"after_gc\" in str(i)]\n+            if len(gc_readings) > 1:\n+                for i in range(1, len(gc_readings)):\n+                    prev_gc_memory = monitor.memory_readings[gc_readings[i - 1]][1]\n+                    curr_gc_memory = monitor.memory_readings[gc_readings[i]][1]\n+                    memory_increase = curr_gc_memory - prev_gc_memory\n+\n+                    # Allow minimal memory growth but not excessive\n+                    assert memory_increase < 0.5, f\"Memory leak detected: {memory_increase:.2f} MB increase after GC\"\n+\n+        print(\"\u2705 Realistic multi-member team test completed successfully\")\n+        print(f\"\u2705 Processed {len(realistic_prompts)} realistic prompts\")\n+        print(f\"\u2705 Final memory: {final_memory:.2f} MB\")\n+\n+    finally:\n+        monitor.stop_monitoring()\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/tests/integration/teams/test_team_metrics.py",
            "diff": "diff --git a/libs/agno/tests/integration/teams/test_team_metrics.py b/libs/agno/tests/integration/teams/test_team_metrics.py\nindex 8843a6df5..e76f88420 100644\n--- a/libs/agno/tests/integration/teams/test_team_metrics.py\n+++ b/libs/agno/tests/integration/teams/test_team_metrics.py\n@@ -3,8 +3,11 @@ from typing import Iterator\n \n from agno.agent import Agent\n from agno.agent.metrics import SessionMetrics\n+from agno.memory.v2.db.sqlite import SqliteMemoryDb\n+from agno.memory.v2.memory import Memory\n from agno.models.openai import OpenAIChat\n from agno.team.team import Team\n+from agno.tools.hackernews import HackerNewsTools\n from agno.tools.yfinance import YFinanceTools\n \n \n@@ -117,3 +120,74 @@ def test_team_metrics_multiple_runs():\n \n     # Verify metrics have been updated after second run\n     assert team.session_metrics.total_tokens > metrics_run1.total_tokens\n+\n+\n+def test_member_metrics_aggregation():\n+    \"\"\"Test the metrics of all members' are aggregated correctly.\"\"\"\n+\n+    memory_db = SqliteMemoryDb(table_name=\"memory\", db_file=\"tmp/memory.db\")\n+    memory = Memory(db=memory_db)\n+\n+    stock_agent = Agent(\n+        session_id=\"session-1\",\n+        name=\"Stock Agent\",\n+        model=OpenAIChat(\"gpt-4o\"),\n+        memory=memory,\n+        role=\"Get stock information\",\n+        tools=[YFinanceTools(stock_price=True)],\n+    )\n+\n+    company_info_agent = Agent(\n+        session_id=\"session-1\",\n+        name=\"Company Info Agent\",\n+        model=OpenAIChat(\"gpt-4o\"),\n+        memory=memory,\n+        role=\"Get company information from HackerNews\",\n+        tools=[HackerNewsTools()],\n+    )\n+\n+    team = Team(\n+        session_id=\"session-1\",\n+        name=\"Company Research Team\",\n+        mode=\"collaborate\",\n+        model=OpenAIChat(\"gpt-4o\"),\n+        members=[stock_agent, company_info_agent],\n+    )\n+\n+    # Running the team twice to make sure the metrics are aggregated correctly for multiple runs\n+    team.run(\n+        \"I need information on NVIDIA. Let me know if there are any active Hackernews thread about it, and what is its current stock price.\"\n+    )\n+    team.run(\n+        \"I need information on TSLA. Let me know if there are any active Hackernews thread about it, and what is its current stock price.\"\n+    )\n+\n+    # Aggregating metrics for all team members' runs\n+    members_metrics = {\"input_tokens\": 0, \"output_tokens\": 0, \"total_tokens\": 0}\n+    for member in team.members:\n+        assert isinstance(member.memory, Memory)\n+        if member.memory.runs is not None:\n+            for runs in member.memory.runs.values():\n+                for run in runs:\n+                    if run is not None and run.messages is not None:\n+                        for m in run.messages:\n+                            if m.role == \"assistant\" and m.metrics is not None:\n+                                members_metrics[\"input_tokens\"] += m.metrics.input_tokens\n+                                members_metrics[\"output_tokens\"] += m.metrics.output_tokens\n+                                members_metrics[\"total_tokens\"] += m.metrics.total_tokens\n+\n+    # Asserting team.full_team_session_metrics coincides with our aggregated metrics\n+    assert team.full_team_session_metrics is not None\n+    assert team.session_metrics is not None\n+    assert (\n+        team.full_team_session_metrics.input_tokens\n+        == members_metrics[\"input_tokens\"] + team.session_metrics.input_tokens\n+    )\n+    assert (\n+        team.full_team_session_metrics.output_tokens\n+        == members_metrics[\"output_tokens\"] + team.session_metrics.output_tokens\n+    )\n+    assert (\n+        team.full_team_session_metrics.total_tokens\n+        == members_metrics[\"total_tokens\"] + team.session_metrics.total_tokens\n+    )\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/tests/integration/teams/test_team_with_member_with_parser_model.py",
            "diff": "diff --git a/libs/agno/tests/integration/teams/test_team_with_member_with_parser_model.py b/libs/agno/tests/integration/teams/test_team_with_member_with_parser_model.py\nnew file mode 100644\nindex 000000000..66cbb9665\n--- /dev/null\n+++ b/libs/agno/tests/integration/teams/test_team_with_member_with_parser_model.py\n@@ -0,0 +1,56 @@\n+from typing import List\n+\n+from pydantic import BaseModel, Field\n+\n+from agno.agent import Agent\n+from agno.models.anthropic import Claude\n+from agno.models.openai import OpenAIChat\n+from agno.team import Team\n+\n+\n+class ParkGuide(BaseModel):\n+    park_name: str = Field(..., description=\"The official name of the national park.\")\n+    activities: List[str] = Field(\n+        ..., description=\"A list of popular activities to do in the park. Provide at least three.\"\n+    )\n+    best_season_to_visit: str = Field(\n+        ..., description=\"The best season to visit the park (e.g., Spring, Summer, Autumn, Winter).\"\n+    )\n+\n+\n+agent = Agent(\n+    name=\"National Park Expert\",\n+    model=OpenAIChat(id=\"gpt-4o\"),\n+    response_model=ParkGuide,\n+    parser_model=Claude(id=\"claude-sonnet-4-20250514\"),\n+    description=\"You are an expert on national parks and provide concise guides.\",\n+)\n+\n+team = Team(\n+    name=\"National Park Expert\",\n+    mode=\"route\",\n+    members=[agent],\n+    telemetry=False,\n+    monitoring=False,\n+)\n+\n+\n+def test_team_with_parser_model():\n+    response = team.run(\"Tell me about Yosemite National Park.\")\n+    print(response.content)\n+\n+    assert response.content is not None\n+    assert isinstance(response.content, ParkGuide)\n+    assert isinstance(response.content.park_name, str)\n+    assert len(response.content.park_name) > 0\n+\n+\n+def test_team_with_parser_model_stream():\n+    response = team.run(\"Tell me about Yosemite National Park.\", stream=True)\n+    for event in response:\n+        print(event.event)\n+\n+    assert team.run_response.content is not None\n+    assert isinstance(team.run_response.content, ParkGuide)\n+    assert isinstance(team.run_response.content.park_name, str)\n+    assert len(team.run_response.content.park_name) > 0\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/tests/integration/teams/test_team_with_parser_model.py",
            "diff": "diff --git a/libs/agno/tests/integration/teams/test_team_with_parser_model.py b/libs/agno/tests/integration/teams/test_team_with_parser_model.py\nnew file mode 100644\nindex 000000000..9b52e7576\n--- /dev/null\n+++ b/libs/agno/tests/integration/teams/test_team_with_parser_model.py\n@@ -0,0 +1,49 @@\n+from typing import List\n+\n+from pydantic import BaseModel, Field\n+\n+from agno.models.anthropic import Claude\n+from agno.team import Team\n+\n+\n+class ParkGuide(BaseModel):\n+    park_name: str = Field(..., description=\"The official name of the national park.\")\n+    activities: List[str] = Field(\n+        ..., description=\"A list of popular activities to do in the park. Provide at least three.\"\n+    )\n+    best_season_to_visit: str = Field(\n+        ..., description=\"The best season to visit the park (e.g., Spring, Summer, Autumn, Winter).\"\n+    )\n+\n+\n+team = Team(\n+    name=\"National Park Expert\",\n+    members=[],\n+    response_model=ParkGuide,\n+    parser_model=Claude(id=\"claude-sonnet-4-20250514\"),\n+    instructions=\"You have no members, answer directly\",\n+    description=\"You are an expert on national parks and provide concise guides.\",\n+    telemetry=False,\n+    monitoring=False,\n+)\n+\n+\n+def test_team_with_parser_model():\n+    response = team.run(\"Tell me about Yosemite National Park.\")\n+    print(response.content)\n+\n+    assert response.content is not None\n+    assert isinstance(response.content, ParkGuide)\n+    assert isinstance(response.content.park_name, str)\n+    assert len(response.content.park_name) > 0\n+\n+\n+def test_team_with_parser_model_stream():\n+    response = team.run(\"Tell me about Yosemite National Park.\", stream=True)\n+    for event in response:\n+        print(event.event)\n+\n+    assert team.run_response.content is not None\n+    assert isinstance(team.run_response.content, ParkGuide)\n+    assert isinstance(team.run_response.content.park_name, str)\n+    assert len(team.run_response.content.park_name) > 0\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/tests/integration/teams/test_team_with_storage_and_memory.py",
            "diff": "diff --git a/libs/agno/tests/integration/teams/test_team_with_storage_and_memory.py b/libs/agno/tests/integration/teams/test_team_with_storage_and_memory.py\nindex 0b5d3d5a7..39b126e19 100644\n--- a/libs/agno/tests/integration/teams/test_team_with_storage_and_memory.py\n+++ b/libs/agno/tests/integration/teams/test_team_with_storage_and_memory.py\n@@ -8,6 +8,7 @@ from agno.agent.agent import Agent\n from agno.memory.v2.db.sqlite import SqliteMemoryDb\n from agno.memory.v2.memory import Memory\n from agno.models.anthropic.claude import Claude\n+from agno.models.google.gemini import Gemini\n from agno.models.openai.chat import OpenAIChat\n from agno.storage.sqlite import SqliteStorage\n from agno.team.team import Team\n@@ -77,6 +78,37 @@ def route_team(team_storage, memory):\n     )\n \n \n+@pytest.fixture\n+def route_team_with_members(team_storage, agent_storage, memory):\n+    \"\"\"Create a route team with storage and memory for testing.\"\"\"\n+\n+    def get_weather(city: str) -> str:\n+        return f\"The weather in {city} is sunny.\"\n+\n+    def get_open_restaurants(city: str) -> str:\n+        return f\"The open restaurants in {city} are: {', '.join(['Restaurant 1', 'Restaurant 2', 'Restaurant 3'])}\"\n+\n+    travel_agent = Agent(\n+        name=\"Travel Agent\",\n+        model=Gemini(id=\"gemini-2.0-flash-001\"),\n+        storage=agent_storage,\n+        memory=memory,\n+        add_history_to_messages=True,\n+        role=\"Search the web for travel information. Don't call multiple tools at once. First get weather, then restaurants.\",\n+        tools=[get_weather, get_open_restaurants],\n+    )\n+    return Team(\n+        name=\"Route Team\",\n+        mode=\"route\",\n+        model=Gemini(id=\"gemini-2.0-flash-001\"),\n+        members=[travel_agent],\n+        storage=team_storage,\n+        memory=memory,\n+        instructions=\"Route a single question to the travel agent. Don't make multiple requests.\",\n+        enable_user_memories=True,\n+    )\n+\n+\n @pytest.mark.asyncio\n async def test_run_history_persistence(route_team, team_storage, memory):\n     \"\"\"Test that all runs within a session are persisted in storage.\"\"\"\n@@ -142,6 +174,65 @@ async def test_run_session_summary(route_team, team_storage, memory):\n     assert len(team_session.memory[\"summaries\"][user_id][session_id]) > 0\n \n \n+@pytest.mark.asyncio\n+async def test_member_run_history_persistence(route_team_with_members, agent_storage, memory):\n+    \"\"\"Test that all runs within a member's session are persisted in storage.\"\"\"\n+    user_id = \"john@example.com\"\n+    session_id = \"session_123\"\n+\n+    # Clear memory for this specific test case\n+    memory.clear()\n+\n+    # First request\n+    await route_team_with_members.arun(\n+        \"I'm traveling to Tokyo, what is the weather and open restaurants?\", user_id=user_id, session_id=session_id\n+    )\n+\n+    agent_session = agent_storage.read(session_id=session_id)\n+\n+    stored_memory_data = agent_session.memory\n+    assert stored_memory_data is not None, \"Memory data not found in stored session.\"\n+\n+    agent_runs = stored_memory_data[\"runs\"]\n+\n+    assert len(agent_runs[-1][\"messages\"]) == 7, (\n+        \"Only system message, user message, two tool calls (and results), and response\"\n+    )\n+\n+    first_user_message_content = agent_runs[0][\"messages\"][1][\"content\"]\n+    assert \"I'm traveling to Tokyo, what is the weather and open restaurants?\" in first_user_message_content\n+\n+    # Second request\n+    await route_team_with_members.arun(\n+        \"I'm traveling to Munich, what is the weather and open restaurants?\", user_id=user_id, session_id=session_id\n+    )\n+\n+    agent_session = agent_storage.read(session_id=session_id)\n+\n+    stored_memory_data = agent_session.memory\n+    assert stored_memory_data is not None, \"Memory data not found in stored session.\"\n+\n+    agent_runs = stored_memory_data[\"runs\"]\n+\n+    assert len(agent_runs[-1][\"messages\"]) == 13, \"Full history of messages\"\n+\n+    # Third request (to the member directly)\n+    await route_team_with_members.members[0].arun(\n+        \"Write me a report about all the places I have requested information about\",\n+        user_id=user_id,\n+        session_id=session_id,\n+    )\n+\n+    agent_session = agent_storage.read(session_id=session_id)\n+\n+    stored_memory_data = agent_session.memory\n+    assert stored_memory_data is not None, \"Memory data not found in stored session.\"\n+\n+    agent_runs = stored_memory_data[\"runs\"]\n+\n+    assert len(agent_runs[-1][\"messages\"]) == 15, \"Full history of messages\"\n+\n+\n @pytest.mark.asyncio\n async def test_multi_user_multi_session_route_team(route_team, team_storage, memory):\n     \"\"\"Test multi-user multi-session route team with storage and memory.\"\"\"\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/tests/unit/response.py",
            "diff": "diff --git a/libs/agno/tests/unit/response.py b/libs/agno/tests/unit/response.py\nnew file mode 100644\nindex 000000000..f4da9464d\n--- /dev/null\n+++ b/libs/agno/tests/unit/response.py\n@@ -0,0 +1,16 @@\n+import json\n+\n+from agno.models.message import Message, MessageMetrics\n+from agno.run.response import RunResponse\n+\n+\n+def test_timer_serialization():\n+    message_1 = Message(role=\"user\", content=\"Hello, world!\")\n+    message_2 = Message(role=\"assistant\", metrics=MessageMetrics())\n+\n+    message_2.metrics.start_timer()\n+    message_2.metrics.stop_timer()\n+\n+    run_response = RunResponse(messages=[message_1, message_2])\n+\n+    assert json.dumps(run_response.to_dict()) is not None\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/tests/unit/storage/test_mysql_storage.py",
            "diff": "diff --git a/libs/agno/tests/unit/storage/test_mysql_storage.py b/libs/agno/tests/unit/storage/test_mysql_storage.py\nnew file mode 100644\nindex 000000000..a0e1b2d18\n--- /dev/null\n+++ b/libs/agno/tests/unit/storage/test_mysql_storage.py\n@@ -0,0 +1,531 @@\n+from unittest.mock import MagicMock, patch\n+\n+import pytest\n+\n+from agno.storage.mysql import MySQLStorage\n+from agno.storage.session.agent import AgentSession\n+from agno.storage.session.team import TeamSession\n+from agno.storage.session.workflow import WorkflowSession\n+\n+\n+@pytest.fixture\n+def mock_engine():\n+    \"\"\"Create a mock SQLAlchemy engine.\"\"\"\n+    engine = MagicMock()\n+    return engine\n+\n+\n+@pytest.fixture\n+def mock_session():\n+    \"\"\"Create a mock SQLAlchemy session.\"\"\"\n+    session = MagicMock()\n+    session_instance = MagicMock()\n+    session.return_value.__enter__.return_value = session_instance\n+    return session, session_instance\n+\n+\n+@pytest.fixture\n+def agent_storage(mock_engine, mock_session):\n+    \"\"\"Create a MySQLStorage instance for agent mode with mocked components.\"\"\"\n+    with patch(\"agno.storage.mysql.scoped_session\", return_value=mock_session[0]):\n+        with patch(\"agno.storage.mysql.inspect\", return_value=MagicMock()):\n+            storage = MySQLStorage(table_name=\"agent_sessions\", schema=\"ai\", db_engine=mock_engine, mode=\"agent\")\n+            # Mock table_exists to return True\n+            storage.table_exists = MagicMock(return_value=True)\n+            return storage, mock_session[1]\n+\n+\n+@pytest.fixture\n+def team_storage(mock_engine, mock_session):\n+    \"\"\"Create a MySQLStorage instance for team mode with mocked components.\"\"\"\n+    with patch(\"agno.storage.mysql.scoped_session\", return_value=mock_session[0]):\n+        with patch(\"agno.storage.mysql.inspect\", return_value=MagicMock()):\n+            storage = MySQLStorage(table_name=\"team_sessions\", schema=\"ai\", db_engine=mock_engine, mode=\"team\")\n+            # Mock table_exists to return True\n+            storage.table_exists = MagicMock(return_value=True)\n+            return storage, mock_session[1]\n+\n+\n+@pytest.fixture\n+def workflow_storage(mock_engine, mock_session):\n+    \"\"\"Create a MySQLStorage instance for workflow mode with mocked components.\"\"\"\n+    with patch(\"agno.storage.mysql.scoped_session\", return_value=mock_session[0]):\n+        with patch(\"agno.storage.mysql.inspect\", return_value=MagicMock()):\n+            storage = MySQLStorage(table_name=\"workflow_sessions\", schema=\"ai\", db_engine=mock_engine, mode=\"workflow\")\n+            # Mock table_exists to return True\n+            storage.table_exists = MagicMock(return_value=True)\n+            return storage, mock_session[1]\n+\n+\n+def test_mysql_storage_initialization():\n+    \"\"\"Test MySQLStorage initialization with different parameters.\"\"\"\n+    # Test with db_url\n+    with patch(\"agno.storage.mysql.create_engine\") as mock_create_engine:\n+        with patch(\"agno.storage.mysql.scoped_session\"):\n+            with patch(\"agno.storage.mysql.inspect\"):\n+                mock_engine = MagicMock()\n+                mock_create_engine.return_value = mock_engine\n+\n+                storage = MySQLStorage(table_name=\"test_table\", db_url=\"mysql+pymysql://user:pass@localhost/db\")\n+\n+                mock_create_engine.assert_called_once_with(\"mysql+pymysql://user:pass@localhost/db\")\n+                assert storage.table_name == \"test_table\"\n+                assert storage.schema == \"ai\"  # Default value\n+                assert storage.mode == \"agent\"  # Default value\n+\n+    # Test with missing db_url and db_engine\n+    with pytest.raises(ValueError, match=\"Must provide either db_url or db_engine\"):\n+        MySQLStorage(table_name=\"test_table\")\n+\n+\n+def test_agent_storage_crud(agent_storage):\n+    \"\"\"Test CRUD operations for agent storage.\"\"\"\n+    storage, mock_session = agent_storage\n+\n+    # Create a test session\n+    session = AgentSession(\n+        session_id=\"test-session\",\n+        agent_id=\"test-agent\",\n+        user_id=\"test-user\",\n+        memory={\"key\": \"value\"},\n+        agent_data={\"name\": \"Test Agent\"},\n+        session_data={\"state\": \"active\"},\n+        extra_data={\"custom\": \"data\"},\n+        team_session_id=\"team-123\",\n+    )\n+\n+    # Mock the read method for initial check\n+    original_read = storage.read\n+    storage.read = MagicMock(return_value=None)\n+\n+    # Mock upsert to return the session directly\n+    original_upsert = storage.upsert\n+    storage.upsert = MagicMock(return_value=session)\n+\n+    # Test upsert\n+    result = storage.upsert(session)\n+    assert result == session\n+\n+    # Restore original methods for other tests\n+    storage.read = original_read\n+    storage.upsert = original_upsert\n+\n+    # Now test read with a direct mock\n+    storage.read = MagicMock(return_value=session)\n+    read_result = storage.read(\"test-session\")\n+    assert read_result == session\n+\n+    # Test delete\n+    storage.delete_session(\"test-session\")\n+    mock_session.execute.assert_called()\n+\n+\n+def test_team_storage_crud(team_storage):\n+    \"\"\"Test CRUD operations for team storage.\"\"\"\n+    storage, mock_session = team_storage\n+\n+    # Create a test session\n+    session = TeamSession(\n+        session_id=\"test-session\",\n+        team_id=\"test-team\",\n+        user_id=\"test-user\",\n+        memory={\"key\": \"value\"},\n+        team_data={\"name\": \"Test Team\"},\n+        session_data={\"state\": \"active\"},\n+        extra_data={\"custom\": \"data\"},\n+        team_session_id=\"team-session-123\",\n+    )\n+\n+    # Mock the read method for initial check\n+    original_read = storage.read\n+    storage.read = MagicMock(return_value=None)\n+\n+    # Mock upsert to return the session directly\n+    original_upsert = storage.upsert\n+    storage.upsert = MagicMock(return_value=session)\n+\n+    # Test upsert\n+    result = storage.upsert(session)\n+    assert result == session\n+\n+    # Restore original methods for other tests\n+    storage.read = original_read\n+    storage.upsert = original_upsert\n+\n+    # Now test read with a direct mock\n+    storage.read = MagicMock(return_value=session)\n+    read_result = storage.read(\"test-session\")\n+    assert read_result == session\n+\n+    # Test delete\n+    storage.delete_session(\"test-session\")\n+    mock_session.execute.assert_called()\n+\n+\n+def test_workflow_storage_crud(workflow_storage):\n+    \"\"\"Test CRUD operations for workflow storage.\"\"\"\n+    storage, mock_session = workflow_storage\n+\n+    # Create a test session\n+    session = WorkflowSession(\n+        session_id=\"test-session\",\n+        workflow_id=\"test-workflow\",\n+        user_id=\"test-user\",\n+        memory={\"key\": \"value\"},\n+        workflow_data={\"name\": \"Test Workflow\"},\n+        session_data={\"state\": \"active\"},\n+        extra_data={\"custom\": \"data\"},\n+    )\n+\n+    # Mock the read method for initial check\n+    original_read = storage.read\n+    storage.read = MagicMock(return_value=None)\n+\n+    # Mock upsert to return the session directly\n+    original_upsert = storage.upsert\n+    storage.upsert = MagicMock(return_value=session)\n+\n+    # Test upsert\n+    result = storage.upsert(session)\n+    assert result == session\n+\n+    # Restore original methods for other tests\n+    storage.read = original_read\n+    storage.upsert = original_upsert\n+\n+    # Now test read with a direct mock\n+    storage.read = MagicMock(return_value=session)\n+    read_result = storage.read(\"test-session\")\n+    assert read_result == session\n+\n+    # Test delete\n+    storage.delete_session(\"test-session\")\n+    mock_session.execute.assert_called()\n+\n+\n+def test_get_all_sessions(agent_storage):\n+    \"\"\"Test retrieving all sessions.\"\"\"\n+    storage, mock_session = agent_storage\n+\n+    # Create mock sessions\n+    sessions = [\n+        AgentSession(\n+            session_id=f\"session-{i}\",\n+            agent_id=f\"agent-{i % 2 + 1}\",\n+            user_id=f\"user-{i % 2 + 1}\",\n+        )\n+        for i in range(4)\n+    ]\n+\n+    # Mock the fetchall result\n+    mock_result = MagicMock()\n+    mock_result.fetchall.return_value = [MagicMock(_mapping=session.to_dict()) for session in sessions]\n+    mock_session.execute.return_value = mock_result\n+\n+    # Test get_all_sessions\n+    result = storage.get_all_sessions()\n+    assert len(result) == 4\n+\n+    # Test filtering by user_id\n+    mock_session.execute.reset_mock()\n+    mock_result.fetchall.return_value = [\n+        MagicMock(_mapping=session.to_dict()) for session in sessions if session.user_id == \"user-1\"\n+    ]\n+    mock_session.execute.return_value = mock_result\n+\n+    result = storage.get_all_sessions(user_id=\"user-1\")\n+    assert len(result) == 2\n+    assert all(s.user_id == \"user-1\" for s in result)\n+\n+    # Test filtering by agent_id\n+    mock_session.execute.reset_mock()\n+    mock_result.fetchall.return_value = [\n+        MagicMock(_mapping=session.to_dict()) for session in sessions if session.agent_id == \"agent-1\"\n+    ]\n+    mock_session.execute.return_value = mock_result\n+\n+    result = storage.get_all_sessions(entity_id=\"agent-1\")\n+    assert len(result) == 2\n+    assert all(s.agent_id == \"agent-1\" for s in result)\n+\n+\n+def test_get_all_session_ids(agent_storage):\n+    \"\"\"Test retrieving all session IDs.\"\"\"\n+    storage, mock_session = agent_storage\n+\n+    # Mock the fetchall result\n+    mock_result = MagicMock()\n+    mock_result.fetchall.return_value = [(\"session-1\",), (\"session-2\",), (\"session-3\",)]\n+    mock_session.execute.return_value = mock_result\n+\n+    # Test get_all_session_ids\n+    result = storage.get_all_session_ids()\n+    assert result == [\"session-1\", \"session-2\", \"session-3\"]\n+\n+\n+def test_get_recent_sessions(agent_storage):\n+    \"\"\"Test retrieving recent sessions with limit.\"\"\"\n+    storage, mock_session = agent_storage\n+\n+    # Create mock sessions\n+    sessions = [\n+        AgentSession(\n+            session_id=f\"session-{i}\",\n+            agent_id=\"agent-1\",\n+            user_id=\"user-1\",\n+        )\n+        for i in range(5)\n+    ]\n+\n+    # Mock the fetchall result for recent sessions\n+    mock_result = MagicMock()\n+    mock_result.fetchall.return_value = [MagicMock(_mapping=sessions[i].to_dict()) for i in range(2)]\n+    mock_session.execute.return_value = mock_result\n+\n+    # Test get_recent_sessions with limit\n+    result = storage.get_recent_sessions(user_id=\"user-1\", entity_id=\"agent-1\", limit=2)\n+    assert len(result) == 2\n+\n+\n+def test_table_exists(agent_storage):\n+    \"\"\"Test the table_exists method.\"\"\"\n+    storage, mock_session = agent_storage\n+\n+    # Test when table exists\n+    mock_scalar = MagicMock(return_value=1)\n+    mock_session.execute.return_value.scalar = mock_scalar\n+\n+    # Reset the mocked table_exists\n+    storage.table_exists = MySQLStorage.table_exists.__get__(storage)\n+\n+    assert storage.table_exists() is True\n+\n+    # Test when table doesn't exist\n+    mock_scalar = MagicMock(return_value=None)\n+    mock_session.execute.return_value.scalar = mock_scalar\n+\n+    assert storage.table_exists() is False\n+\n+\n+def test_create_table(agent_storage):\n+    \"\"\"Test table creation.\"\"\"\n+    storage, mock_session = agent_storage\n+\n+    # Reset the mocked table_exists\n+    storage.table_exists = MagicMock(return_value=False)\n+\n+    # Mock the create method\n+    with patch.object(storage.table, \"create\"):\n+        storage.create()\n+        mock_session.execute.assert_called()  # For schema creation\n+\n+\n+def test_drop_table(agent_storage):\n+    \"\"\"Test dropping a table.\"\"\"\n+    storage, mock_session = agent_storage\n+\n+    # Mock table_exists to return True\n+    storage.table_exists = MagicMock(return_value=True)\n+\n+    # Mock the drop method\n+    with patch.object(storage.table, \"drop\") as mock_drop:\n+        storage.drop()\n+        mock_drop.assert_called_once_with(storage.db_engine, checkfirst=True)\n+\n+\n+def test_mode_switching():\n+    \"\"\"Test switching between agent, team, and workflow modes.\"\"\"\n+    with patch(\"agno.storage.mysql.scoped_session\"):\n+        with patch(\"agno.storage.mysql.inspect\"):\n+            with patch(\"agno.storage.mysql.create_engine\"):\n+                # Create storage in agent mode\n+                storage = MySQLStorage(table_name=\"test_table\", db_url=\"mysql+pymysql://user:pass@localhost/db\")\n+                assert storage.mode == \"agent\"\n+\n+                # Switch to workflow mode\n+                with patch.object(storage, \"get_table\") as mock_get_table:\n+                    storage.mode = \"workflow\"\n+                    assert storage.mode == \"workflow\"\n+                    mock_get_table.assert_called_once()\n+\n+                # Switch to team mode\n+                with patch.object(storage, \"get_table\") as mock_get_table:\n+                    storage.mode = \"team\"\n+                    assert storage.mode == \"team\"\n+                    mock_get_table.assert_called_once()\n+\n+\n+def test_schema_upgrade(agent_storage):\n+    \"\"\"Test schema upgrade functionality.\"\"\"\n+    storage, mock_session = agent_storage\n+    storage.auto_upgrade_schema = True\n+    storage._schema_up_to_date = False\n+\n+    # Mock column exists check to return False (column doesn't exist)\n+    mock_scalar = MagicMock(return_value=None)\n+    mock_session.execute.return_value.scalar = mock_scalar\n+\n+    # Test upgrade_schema\n+    storage.upgrade_schema()\n+\n+    # Verify ALTER TABLE was called\n+    calls = mock_session.execute.call_args_list\n+    alter_table_called = any(\"ALTER TABLE\" in str(call) for call in calls)\n+    assert alter_table_called or storage._schema_up_to_date\n+\n+\n+def test_mysql_specific_features(agent_storage):\n+    \"\"\"Test MySQL-specific features like backtick quoting.\"\"\"\n+    storage, mock_session = agent_storage\n+\n+    # Reset table_exists to test actual implementation\n+    storage.table_exists = MySQLStorage.table_exists.__get__(storage)\n+\n+    # Mock execute to capture SQL queries\n+    executed_queries = []\n+\n+    def capture_execute(query, *args, **kwargs):\n+        executed_queries.append(str(query))\n+        result = MagicMock()\n+        result.scalar = MagicMock(return_value=1)\n+        return result\n+\n+    mock_session.execute = MagicMock(side_effect=capture_execute)\n+\n+    # Call table_exists to trigger query\n+    storage.table_exists()\n+\n+    # Verify information_schema query is used (MySQL style)\n+    assert any(\"information_schema.tables\" in query for query in executed_queries)\n+\n+\n+def test_deepcopy(agent_storage):\n+    \"\"\"Test deep copying of MySQLStorage instance.\"\"\"\n+    storage, _ = agent_storage\n+\n+    import copy\n+\n+    # Test deepcopy\n+    copied_storage = copy.deepcopy(storage)\n+\n+    # Verify essential attributes are preserved\n+    assert copied_storage.table_name == storage.table_name\n+    assert copied_storage.schema == storage.schema\n+    assert copied_storage.mode == storage.mode\n+    assert copied_storage.schema_version == storage.schema_version\n+    assert copied_storage.auto_upgrade_schema == storage.auto_upgrade_schema\n+\n+    # Verify db_engine is the same (not copied)\n+    assert copied_storage.db_engine is storage.db_engine\n+\n+\n+def test_error_handling(agent_storage):\n+    \"\"\"Test error handling for various scenarios.\"\"\"\n+    storage, mock_session = agent_storage\n+\n+    # Test 1: Read with table doesn't exist error\n+    mock_session.execute.side_effect = Exception(\"Table 'ai.agent_sessions' doesn't exist\")\n+\n+    # Reset read method\n+    storage.read = MySQLStorage.read.__get__(storage)\n+\n+    # Should handle error gracefully and return None\n+    result = storage.read(\"test-session\")\n+    assert result is None\n+\n+    # Test 2: Verify upsert calls create when table doesn't exist\n+    # We'll test this by directly verifying the logic path\n+    session = AgentSession(\n+        session_id=\"test-session\",\n+        agent_id=\"test-agent\",\n+        user_id=\"test-user\",\n+    )\n+\n+    # Setup the scenario where upsert fails and needs to create table\n+    original_table_exists = storage.table_exists\n+    create_called = False\n+\n+    def track_create():\n+        nonlocal create_called\n+        create_called = True\n+\n+    # Override methods\n+    storage.table_exists = MagicMock(return_value=False)\n+    storage.create = MagicMock(side_effect=track_create)\n+    storage.read = MagicMock(return_value=session)\n+\n+    # Setup mock session to fail initially\n+    def failing_execute(*args, **kwargs):\n+        if not create_called:\n+            raise Exception(\"Table doesn't exist\")\n+        return MagicMock()\n+\n+    mock_session.execute = MagicMock(side_effect=failing_execute)\n+    mock_session.begin.return_value.__enter__.return_value = mock_session\n+\n+    # Get the actual upsert method\n+    storage.upsert = MySQLStorage.upsert.__get__(storage)\n+\n+    # Call upsert - it should fail, check table_exists, call create, then succeed\n+    try:\n+        result = storage.upsert(session, create_and_retry=True)\n+        # If successful, verify create was called\n+        assert create_called\n+    except Exception:\n+        # Even if it fails, create should have been called\n+        assert create_called\n+\n+    # Verify create was actually called\n+    storage.create.assert_called()\n+\n+    # Restore original\n+    storage.table_exists = original_table_exists\n+\n+    # Test 3: Error handling in get methods\n+    mock_session.execute.side_effect = Exception(\"doesn't exist\")\n+\n+    # These should handle errors gracefully\n+    storage.get_all_sessions = MySQLStorage.get_all_sessions.__get__(storage)\n+    assert storage.get_all_sessions() == []\n+\n+    storage.get_all_session_ids = MySQLStorage.get_all_session_ids.__get__(storage)\n+    assert storage.get_all_session_ids() == []\n+\n+    storage.get_recent_sessions = MySQLStorage.get_recent_sessions.__get__(storage)\n+    assert storage.get_recent_sessions() == []\n+\n+\n+def test_all_modes_table_structure():\n+    \"\"\"Test that table structure is correct for all modes.\"\"\"\n+    with patch(\"agno.storage.mysql.scoped_session\"):\n+        with patch(\"agno.storage.mysql.inspect\"):\n+            with patch(\"agno.storage.mysql.create_engine\"):\n+                # Test agent mode columns\n+                agent_storage = MySQLStorage(\n+                    table_name=\"agent_table\", db_url=\"mysql+pymysql://user:pass@localhost/db\", mode=\"agent\"\n+                )\n+                agent_table = agent_storage.get_table()\n+                agent_columns = {c.name for c in agent_table.columns}\n+                assert \"agent_id\" in agent_columns\n+                assert \"team_session_id\" in agent_columns\n+                assert \"agent_data\" in agent_columns\n+\n+                # Test team mode columns\n+                team_storage = MySQLStorage(\n+                    table_name=\"team_table\", db_url=\"mysql+pymysql://user:pass@localhost/db\", mode=\"team\"\n+                )\n+                team_table = team_storage.get_table()\n+                team_columns = {c.name for c in team_table.columns}\n+                assert \"team_id\" in team_columns\n+                assert \"team_session_id\" in team_columns\n+                assert \"team_data\" in team_columns\n+\n+                # Test workflow mode columns\n+                workflow_storage = MySQLStorage(\n+                    table_name=\"workflow_table\", db_url=\"mysql+pymysql://user:pass@localhost/db\", mode=\"workflow\"\n+                )\n+                workflow_table = workflow_storage.get_table()\n+                workflow_columns = {c.name for c in workflow_table.columns}\n+                assert \"workflow_id\" in workflow_columns\n+                assert \"workflow_data\" in workflow_columns\n+                assert \"team_session_id\" not in workflow_columns\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/tests/unit/team/test_team.py",
            "diff": "diff --git a/libs/agno/tests/unit/team/test_team.py b/libs/agno/tests/unit/team/test_team.py\nindex 7f98c40f9..f56d5ecba 100644\n--- a/libs/agno/tests/unit/team/test_team.py\n+++ b/libs/agno/tests/unit/team/test_team.py\n@@ -72,6 +72,8 @@ def test_get_member_id():\n     assert Team(members=[member])._get_member_id(member) == \"123\"\n     member = Agent(name=\"Test Agent\", agent_id=str(uuid.uuid4()))\n     assert Team(members=[member])._get_member_id(member) == \"test-agent\"\n+    member = Agent(agent_id=str(uuid.uuid4()))\n+    assert Team(members=[member])._get_member_id(member) == member.agent_id\n \n     member = Agent(name=\"Test Agent\")\n     inner_team = Team(name=\"Test Team\", members=[member])\n@@ -80,3 +82,5 @@ def test_get_member_id():\n     assert Team(members=[inner_team])._get_member_id(inner_team) == \"123\"\n     inner_team = Team(name=\"Test Team\", team_id=str(uuid.uuid4()), members=[member])\n     assert Team(members=[inner_team])._get_member_id(inner_team) == \"test-team\"\n+    inner_team = Team(team_id=str(uuid.uuid4()), members=[member])\n+    assert Team(members=[inner_team])._get_member_id(inner_team) == inner_team.team_id\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/tests/unit/tools/test_duckdb.py",
            "diff": "diff --git a/libs/agno/tests/unit/tools/test_duckdb.py b/libs/agno/tests/unit/tools/test_duckdb.py\nnew file mode 100644\nindex 000000000..50b495491\n--- /dev/null\n+++ b/libs/agno/tests/unit/tools/test_duckdb.py\n@@ -0,0 +1,346 @@\n+from unittest.mock import MagicMock, patch\n+\n+import pytest\n+\n+from agno.tools.duckdb import DuckDbTools\n+\n+\n+@pytest.fixture\n+def mock_duckdb_connection():\n+    \"\"\"Mock DuckDB connection used by DuckDbTools.\"\"\"\n+    with patch(\"agno.tools.duckdb.duckdb\") as mock_duckdb:\n+        mock_connection = MagicMock()\n+        mock_duckdb.connect.return_value = mock_connection\n+\n+        # Mock the query result\n+        mock_result = MagicMock()\n+        mock_result.fetchall.return_value = [(\"test_table\",)]\n+        mock_result.columns = [\"name\"]\n+        mock_connection.sql.return_value = mock_result\n+\n+        yield mock_connection\n+\n+\n+@pytest.fixture\n+def duckdb_tools_instance(mock_duckdb_connection):\n+    \"\"\"Fixture to instantiate DuckDbTools with mocked connection.\"\"\"\n+    tools = DuckDbTools()\n+    # Override the connection property to use the mock\n+    tools._connection = mock_duckdb_connection\n+    return tools\n+\n+\n+# --- Test Cases for Table Creation Methods ---\n+\n+\n+def test_create_table_from_path_no_quotes_around_table_name(duckdb_tools_instance, mock_duckdb_connection):\n+    \"\"\"Test that create_table_from_path does not wrap table names in single quotes.\"\"\"\n+    path = \"/path/to/test-file.csv\"\n+    expected_table_name = \"test_file\"\n+\n+    result = duckdb_tools_instance.create_table_from_path(path)\n+\n+    # Verify the table name returned\n+    assert result == expected_table_name\n+\n+    # Verify the SQL statement does not contain quoted table name\n+    mock_duckdb_connection.sql.assert_called()\n+    call_args = mock_duckdb_connection.sql.call_args[0][0]\n+    assert f\"CREATE TABLE IF NOT EXISTS {expected_table_name} AS\" in call_args\n+    assert f\"'{expected_table_name}'\" not in call_args  # Should NOT contain quoted table name\n+    assert (\n+        f\"read_csv('{path}', ignore_errors=false, auto_detect=true)\" in call_args\n+    )  # CSV files should use read_csv with parameters\n+\n+\n+def test_create_table_from_path_with_replace(duckdb_tools_instance, mock_duckdb_connection):\n+    \"\"\"Test create_table_from_path with replace=True.\"\"\"\n+    path = \"/path/to/data.json\"\n+    expected_table_name = \"data\"\n+\n+    result = duckdb_tools_instance.create_table_from_path(path, replace=True)\n+\n+    assert result == expected_table_name\n+    call_args = mock_duckdb_connection.sql.call_args[0][0]\n+    assert f\"CREATE OR REPLACE TABLE {expected_table_name} AS\" in call_args\n+    assert f\"'{expected_table_name}'\" not in call_args\n+    assert f\"SELECT * FROM '{path}'\" in call_args  # Non-CSV files should use the old approach\n+\n+\n+def test_create_table_from_path_custom_table_name(duckdb_tools_instance, mock_duckdb_connection):\n+    \"\"\"Test create_table_from_path with custom table name.\"\"\"\n+    path = \"/path/to/file.csv\"\n+    custom_table = \"my_custom_table\"\n+\n+    result = duckdb_tools_instance.create_table_from_path(path, table=custom_table)\n+\n+    assert result == custom_table\n+    call_args = mock_duckdb_connection.sql.call_args[0][0]\n+    assert f\"CREATE TABLE IF NOT EXISTS {custom_table} AS\" in call_args\n+    assert f\"'{custom_table}'\" not in call_args\n+\n+\n+def test_load_local_path_to_table_no_quotes(duckdb_tools_instance, mock_duckdb_connection):\n+    \"\"\"Test that load_local_path_to_table does not wrap table names in single quotes.\"\"\"\n+    path = \"/local/path/jira-backlog.csv\"\n+    expected_table_name = \"jira_backlog\"\n+\n+    table_name, sql_statement = duckdb_tools_instance.load_local_path_to_table(path)\n+\n+    assert table_name == expected_table_name\n+    assert f\"CREATE OR REPLACE TABLE {expected_table_name} AS\" in sql_statement\n+    assert f\"'{expected_table_name}'\" not in sql_statement\n+    # The run_query method removes semicolons, so check for the statement without semicolon\n+    expected_call = sql_statement.rstrip(\";\")\n+    mock_duckdb_connection.sql.assert_called_with(expected_call)\n+\n+\n+def test_load_local_csv_to_table_no_quotes(duckdb_tools_instance, mock_duckdb_connection):\n+    \"\"\"Test that load_local_csv_to_table does not wrap table names in single quotes.\"\"\"\n+    path = \"/local/path/test.data.csv\"\n+    expected_table_name = \"test_data\"\n+\n+    table_name, sql_statement = duckdb_tools_instance.load_local_csv_to_table(path)\n+\n+    assert table_name == expected_table_name\n+    assert f\"CREATE OR REPLACE TABLE {expected_table_name} AS\" in sql_statement\n+    assert f\"'{expected_table_name}'\" not in sql_statement\n+    assert \"read_csv(\" in sql_statement\n+    assert \"ignore_errors=false, auto_detect=true\" in sql_statement\n+\n+\n+def test_load_local_csv_to_table_with_delimiter(duckdb_tools_instance, mock_duckdb_connection):\n+    \"\"\"Test load_local_csv_to_table with custom delimiter.\"\"\"\n+    path = \"/local/path/pipe-separated.csv\"\n+    delimiter = \"|\"\n+    expected_table_name = \"pipe_separated\"\n+\n+    table_name, sql_statement = duckdb_tools_instance.load_local_csv_to_table(path, delimiter=delimiter)\n+\n+    assert table_name == expected_table_name\n+    assert f\"CREATE OR REPLACE TABLE {expected_table_name} AS\" in sql_statement\n+    assert f\"delim='{delimiter}'\" in sql_statement\n+    assert f\"'{expected_table_name}'\" not in sql_statement\n+\n+\n+def test_load_s3_path_to_table_no_quotes(duckdb_tools_instance, mock_duckdb_connection):\n+    \"\"\"Test that load_s3_path_to_table does not wrap table names in single quotes.\"\"\"\n+    path = \"s3://bucket/path/my-data-file.parquet\"\n+    expected_table_name = \"my_data_file\"\n+\n+    table_name, sql_statement = duckdb_tools_instance.load_s3_path_to_table(path)\n+\n+    assert table_name == expected_table_name\n+    assert f\"CREATE OR REPLACE TABLE {expected_table_name} AS\" in sql_statement\n+    assert f\"'{expected_table_name}'\" not in sql_statement\n+\n+\n+def test_load_s3_csv_to_table_no_quotes(duckdb_tools_instance, mock_duckdb_connection):\n+    \"\"\"Test that load_s3_csv_to_table does not wrap table names in single quotes.\"\"\"\n+    path = \"s3://bucket/data/sales-report.csv\"\n+    expected_table_name = \"sales_report\"\n+\n+    table_name, sql_statement = duckdb_tools_instance.load_s3_csv_to_table(path)\n+\n+    assert table_name == expected_table_name\n+    assert f\"CREATE OR REPLACE TABLE {expected_table_name} AS\" in sql_statement\n+    assert f\"'{expected_table_name}'\" not in sql_statement\n+    assert \"read_csv(\" in sql_statement\n+    assert \"ignore_errors=false, auto_detect=true\" in sql_statement\n+\n+\n+# --- Test Cases for Table Name Sanitization ---\n+\n+\n+def test_get_table_name_from_path_special_characters(duckdb_tools_instance):\n+    \"\"\"Test that table names are properly sanitized from paths with special characters.\"\"\"\n+    test_cases = [\n+        (\"/path/to/my-file.csv\", \"my_file\"),\n+        (\"/path/to/data.backup.csv\", \"data_backup\"),\n+        (\"/path/to/file with spaces.json\", \"file_with_spaces\"),\n+        (\"/path/to/complex-file.name.data.csv\", \"complex_file_name_data\"),\n+        (\"s3://bucket/sub/folder/test-data.parquet\", \"test_data\"),\n+    ]\n+\n+    for path, expected_table_name in test_cases:\n+        result = duckdb_tools_instance.get_table_name_from_path(path)\n+        assert result == expected_table_name, f\"Failed for path: {path}\"\n+\n+\n+# --- Test Cases for Query Execution ---\n+\n+\n+def test_run_query_success(duckdb_tools_instance, mock_duckdb_connection):\n+    \"\"\"Test successful query execution.\"\"\"\n+    # Setup mock result\n+    mock_result = MagicMock()\n+    mock_result.fetchall.return_value = [(1, \"issue-1\", \"High\"), (2, \"issue-2\", \"Medium\")]\n+    mock_result.columns = [\"id\", \"issue_id\", \"priority\"]\n+    mock_duckdb_connection.sql.return_value = mock_result\n+\n+    query = \"SELECT id, issue_id, priority FROM test_table\"\n+    result = duckdb_tools_instance.run_query(query)\n+\n+    expected_output = \"id,issue_id,priority\\n1,issue-1,High\\n2,issue-2,Medium\"\n+    assert result == expected_output\n+    mock_duckdb_connection.sql.assert_called_with(query)\n+\n+\n+def test_run_query_removes_backticks(duckdb_tools_instance, mock_duckdb_connection):\n+    \"\"\"Test that run_query removes backticks from queries.\"\"\"\n+    mock_result = MagicMock()\n+    mock_result.fetchall.return_value = [(\"test\",)]\n+    mock_result.columns = [\"col\"]\n+    mock_duckdb_connection.sql.return_value = mock_result\n+\n+    query_with_backticks = \"SELECT `column` FROM `table`\"\n+    expected_cleaned_query = \"SELECT column FROM table\"\n+\n+    duckdb_tools_instance.run_query(query_with_backticks)\n+\n+    mock_duckdb_connection.sql.assert_called_with(expected_cleaned_query)\n+\n+\n+def test_describe_table_success(duckdb_tools_instance, mock_duckdb_connection):\n+    \"\"\"Test successful table description.\"\"\"\n+    # Setup mock result for DESCRIBE query\n+    mock_result = MagicMock()\n+    mock_result.fetchall.return_value = [\n+        (\"issue_id\", \"VARCHAR\", \"YES\", None, None, None),\n+        (\"priority\", \"VARCHAR\", \"YES\", None, None, None),\n+        (\"status\", \"VARCHAR\", \"YES\", None, None, None),\n+    ]\n+    mock_result.columns = [\"column_name\", \"column_type\", \"null\", \"key\", \"default\", \"extra\"]\n+    mock_duckdb_connection.sql.return_value = mock_result\n+\n+    table_name = \"test_table\"\n+    result = duckdb_tools_instance.describe_table(table_name)\n+\n+    expected_output = f\"{table_name}\\ncolumn_name,column_type,null,key,default,extra\\nissue_id,VARCHAR,YES,None,None,None\\npriority,VARCHAR,YES,None,None,None\\nstatus,VARCHAR,YES,None,None,None\"\n+    assert result == expected_output\n+    # The run_query method removes semicolons, so check for the statement without semicolon\n+    mock_duckdb_connection.sql.assert_called_with(f\"DESCRIBE {table_name}\")\n+\n+\n+# --- Integration Test Case ---\n+\n+\n+def test_integration_create_and_query_table(duckdb_tools_instance, mock_duckdb_connection):\n+    \"\"\"Integration test: create table and then query it successfully.\"\"\"\n+    # Test the workflow that was failing in the original issue\n+    path = \"/path/to/jira_backlog.csv\"\n+    expected_table_name = \"jira_backlog\"\n+\n+    # Step 1: Create table\n+    table_name = duckdb_tools_instance.create_table_from_path(path)\n+    assert table_name == expected_table_name\n+\n+    # Verify table creation SQL doesn't have quoted table name\n+    create_call_args = mock_duckdb_connection.sql.call_args[0][0]\n+    assert f\"CREATE TABLE IF NOT EXISTS {expected_table_name} AS\" in create_call_args\n+    assert f\"'{expected_table_name}'\" not in create_call_args\n+    assert f\"read_csv('{path}', ignore_errors=false, auto_detect=true)\" in create_call_args\n+\n+    # Step 2: Setup mock for query execution\n+    mock_result = MagicMock()\n+    mock_result.fetchall.return_value = [(1, \"ISSUE-1\", \"High\"), (2, \"ISSUE-2\", \"Medium\")]\n+    mock_result.columns = [\"rownum\", \"issue_id\", \"priority\"]\n+    mock_duckdb_connection.sql.return_value = mock_result\n+\n+    # Step 3: Query the table (this was failing before the fix)\n+    query = f\"SELECT row_number() OVER () AS rownum, issue_id, priority FROM {expected_table_name}\"\n+    result = duckdb_tools_instance.run_query(query)\n+\n+    # Verify query executed successfully\n+    expected_output = \"rownum,issue_id,priority\\n1,ISSUE-1,High\\n2,ISSUE-2,Medium\"\n+    assert result == expected_output\n+\n+\n+# --- Error Handling Tests ---\n+\n+\n+def test_run_query_duckdb_error(duckdb_tools_instance, mock_duckdb_connection):\n+    \"\"\"Test run_query handles DuckDB errors gracefully.\"\"\"\n+    # Create a proper DuckDB error by using the actual DuckDB module\n+    with patch(\"agno.tools.duckdb.duckdb\") as mock_duckdb_module:\n+        # Setup the error classes to be proper exception classes\n+        class MockDuckDBError(Exception):\n+            pass\n+\n+        class MockProgrammingError(Exception):\n+            pass\n+\n+        mock_duckdb_module.Error = MockDuckDBError\n+        mock_duckdb_module.ProgrammingError = MockProgrammingError\n+\n+        mock_duckdb_connection.sql.side_effect = MockDuckDBError(\"Test error\")\n+\n+        query = \"SELECT * FROM non_existent_table\"\n+        result = duckdb_tools_instance.run_query(query)\n+\n+        assert \"Test error\" in result\n+\n+\n+def test_run_query_programming_error(duckdb_tools_instance, mock_duckdb_connection):\n+    \"\"\"Test run_query handles programming errors gracefully.\"\"\"\n+    # Create a proper DuckDB programming error\n+    with patch(\"agno.tools.duckdb.duckdb\") as mock_duckdb_module:\n+        # Setup the error classes to be proper exception classes\n+        class MockDuckDBError(Exception):\n+            pass\n+\n+        class MockProgrammingError(Exception):\n+            pass\n+\n+        mock_duckdb_module.Error = MockDuckDBError\n+        mock_duckdb_module.ProgrammingError = MockProgrammingError\n+\n+        mock_duckdb_connection.sql.side_effect = MockProgrammingError(\"Syntax error\")\n+\n+        query = \"INVALID SQL SYNTAX\"\n+        result = duckdb_tools_instance.run_query(query)\n+\n+        assert \"Syntax error\" in result\n+\n+\n+# --- Test Cases for Edge Cases ---\n+\n+\n+def test_run_query_single_column_result(duckdb_tools_instance, mock_duckdb_connection):\n+    \"\"\"Test run_query with single column results.\"\"\"\n+    mock_result = MagicMock()\n+    mock_result.fetchall.return_value = [(\"value1\",), (\"value2\",), (\"value3\",)]\n+    mock_result.columns = [\"single_col\"]\n+    mock_duckdb_connection.sql.return_value = mock_result\n+\n+    query = \"SELECT single_col FROM test_table\"\n+    result = duckdb_tools_instance.run_query(query)\n+\n+    expected_output = \"single_col\\nvalue1\\nvalue2\\nvalue3\"\n+    assert result == expected_output\n+\n+\n+def test_run_query_no_results(duckdb_tools_instance, mock_duckdb_connection):\n+    \"\"\"Test run_query with no results.\"\"\"\n+    mock_result = MagicMock()\n+    mock_result.fetchall.return_value = []\n+    mock_result.columns = [\"col1\", \"col2\"]\n+    mock_duckdb_connection.sql.return_value = mock_result\n+\n+    query = \"SELECT * FROM empty_table\"\n+    result = duckdb_tools_instance.run_query(query)\n+\n+    expected_output = \"col1,col2\\n\"\n+    assert result == expected_output\n+\n+\n+def test_custom_table_name_with_special_chars(duckdb_tools_instance, mock_duckdb_connection):\n+    \"\"\"Test that custom table names are used as-is without additional sanitization.\"\"\"\n+    path = \"/path/to/file.csv\"\n+    custom_table = \"my_custom_table_123\"\n+\n+    result = duckdb_tools_instance.create_table_from_path(path, table=custom_table)\n+\n+    assert result == custom_table\n+    call_args = mock_duckdb_connection.sql.call_args[0][0]\n+    assert f\"CREATE TABLE IF NOT EXISTS {custom_table} AS\" in call_args\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/tests/unit/tools/test_functions.py",
            "diff": "diff --git a/libs/agno/tests/unit/tools/test_functions.py b/libs/agno/tests/unit/tools/test_functions.py\nindex 17fa24414..e118c1e3b 100644\n--- a/libs/agno/tests/unit/tools/test_functions.py\n+++ b/libs/agno/tests/unit/tools/test_functions.py\n@@ -1,6 +1,7 @@\n from typing import Any, Callable, Dict\n \n import pytest\n+from pydantic import ValidationError\n \n from agno.tools.decorator import tool\n from agno.tools.function import Function, FunctionCall\n@@ -118,7 +119,7 @@ def test_wrap_callable():\n     \"\"\"Test wrapping a callable.\"\"\"\n \n     @tool\n-    def test_func(param1: str, param2: int = 42) -> str:\n+    def test_func(param1: str, param2: int) -> str:\n         \"\"\"Test function with parameters.\"\"\"\n         return f\"{param1}-{param2}\"\n \n@@ -128,10 +129,18 @@ def test_wrap_callable():\n     test_func.process_entrypoint()\n     assert isinstance(test_func, Function)\n     assert test_func.entrypoint is not None\n+    assert test_func.entrypoint(param1=\"test\", param2=42) == \"test-42\"\n+    with pytest.raises(ValidationError):\n+        test_func.entrypoint(param1=\"test\")\n+    assert test_func.entrypoint._wrapped_for_validation is True\n \n     test_func.process_entrypoint()\n     assert isinstance(test_func, Function)\n     assert test_func.entrypoint is not None\n+    assert test_func.entrypoint(param1=\"test\", param2=42) == \"test-42\"\n+    with pytest.raises(ValidationError):\n+        test_func.entrypoint(param1=\"test\")\n+    assert test_func.entrypoint._wrapped_for_validation is True\n \n \n def test_function_from_callable_strict():\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/tests/unit/tools/test_oxylabs.py",
            "diff": "diff --git a/libs/agno/tests/unit/tools/test_oxylabs.py b/libs/agno/tests/unit/tools/test_oxylabs.py\nnew file mode 100644\nindex 000000000..010e94f3f\n--- /dev/null\n+++ b/libs/agno/tests/unit/tools/test_oxylabs.py\n@@ -0,0 +1,373 @@\n+\"\"\"Unit tests for OxylabsTools class.\"\"\"\n+\n+import json\n+from unittest.mock import Mock, patch\n+\n+import pytest\n+\n+from agno.agent import Agent\n+from agno.tools.oxylabs import OxylabsTools\n+\n+\n+@pytest.fixture\n+def mock_agent():\n+    \"\"\"Create a mock Agent instance.\"\"\"\n+    return Mock(spec=Agent)\n+\n+\n+@pytest.fixture\n+def mock_oxylabs_client():\n+    \"\"\"Create a mocked Oxylabs RealtimeClient with all resource methods stubbed.\"\"\"\n+    with patch(\"agno.tools.oxylabs.RealtimeClient\") as mock_realtime_client:\n+        # Primary client mock returned by the SDK constructor\n+        mock_client = Mock()\n+\n+        # Mock nested resource clients\n+        mock_client.google = Mock()\n+        mock_client.amazon = Mock()\n+        mock_client.universal = Mock()\n+\n+        # Configure the RealtimeClient constructor to return our mock\n+        mock_realtime_client.return_value = mock_client\n+\n+        yield mock_client\n+\n+\n+@pytest.fixture\n+def mock_environment_variables():\n+    \"\"\"Mock environment variables for Oxylabs credentials.\"\"\"\n+    with patch.dict(\"os.environ\", {\"OXYLABS_USERNAME\": \"test_user\", \"OXYLABS_PASSWORD\": \"test_pass\"}):\n+        yield\n+\n+\n+def create_mock_response(results=None, status_code=200):\n+    \"\"\"Helper to create a mock response object matching the SDK structure.\"\"\"\n+    mock_response = Mock()\n+    mock_response.results = []\n+\n+    if results:\n+        for result_data in results:\n+            mock_result = Mock()\n+            mock_result.content = result_data.get(\"content\")\n+            mock_result.status_code = result_data.get(\"status_code\", status_code)\n+            mock_result.content_parsed = result_data.get(\"content_parsed\")\n+            mock_response.results.append(mock_result)\n+\n+    return mock_response\n+\n+\n+class TestOxylabsToolsInitialization:\n+    \"\"\"Test cases for OxylabsTools initialization.\"\"\"\n+\n+    def test_init_with_credentials(self, mock_oxylabs_client):\n+        \"\"\"Test initialization with provided credentials.\"\"\"\n+        tools = OxylabsTools(username=\"test_user\", password=\"test_pass\")\n+\n+        assert tools.username == \"test_user\"\n+        assert tools.password == \"test_pass\"\n+        assert tools.client is not None\n+\n+    def test_init_with_env_variables(self, mock_oxylabs_client, mock_environment_variables):\n+        \"\"\"Test initialization with environment variables.\"\"\"\n+        tools = OxylabsTools()\n+\n+        assert tools.username == \"test_user\"\n+        assert tools.password == \"test_pass\"\n+        assert tools.client is not None\n+\n+    def test_init_without_credentials(self):\n+        \"\"\"Test initialization failure without credentials.\"\"\"\n+        # Ensure no environment variables are set\n+        with patch.dict(\"os.environ\", {}, clear=True):\n+            with pytest.raises(ValueError, match=\"No Oxylabs credentials provided\"):\n+                OxylabsTools()\n+\n+    def test_init_partial_credentials(self):\n+        \"\"\"Test initialization failure with partial credentials.\"\"\"\n+        # Ensure no environment variables are set\n+        with patch.dict(\"os.environ\", {}, clear=True):\n+            with pytest.raises(ValueError, match=\"No Oxylabs credentials provided\"):\n+                OxylabsTools(username=\"test_user\")\n+\n+\n+class TestSearchGoogle:\n+    \"\"\"Test cases for search_google method.\"\"\"\n+\n+    def test_search_google_success(self, mock_oxylabs_client, mock_environment_variables):\n+        \"\"\"Test successful Google search.\"\"\"\n+        # Arrange\n+        mock_response = create_mock_response(\n+            results=[\n+                {\n+                    \"content\": {\n+                        \"results\": {\n+                            \"organic\": [\n+                                {\n+                                    \"title\": \"Test Result\",\n+                                    \"url\": \"https://example.com\",\n+                                    \"desc\": \"Test description\",\n+                                    \"pos\": 1,\n+                                }\n+                            ]\n+                        }\n+                    },\n+                    \"status_code\": 200,\n+                }\n+            ]\n+        )\n+        mock_oxylabs_client.google.scrape_search.return_value = mock_response\n+\n+        tools = OxylabsTools()\n+\n+        # Act\n+        result = tools.search_google(query=\"test query\", domain_code=\"com\")\n+\n+        # Assert\n+        mock_oxylabs_client.google.scrape_search.assert_called_once_with(query=\"test query\", domain=\"com\", parse=True)\n+\n+        result_data = json.loads(result)\n+        assert result_data[\"tool\"] == \"search_google\"\n+        assert result_data[\"query\"] == \"test query\"\n+        assert \"results\" in result_data\n+\n+    def test_search_google_empty_query(self, mock_oxylabs_client, mock_environment_variables):\n+        \"\"\"Test Google search with empty query.\"\"\"\n+        tools = OxylabsTools()\n+\n+        result = tools.search_google(query=\"\")\n+\n+        result_data = json.loads(result)\n+        assert \"error\" in result_data\n+        assert result_data[\"tool\"] == \"search_google\"\n+        assert \"cannot be empty\" in result_data[\"error\"]\n+\n+    def test_search_google_invalid_domain(self, mock_oxylabs_client, mock_environment_variables):\n+        \"\"\"Test Google search with invalid domain.\"\"\"\n+        tools = OxylabsTools()\n+\n+        result = tools.search_google(query=\"test\", domain_code=\"x\" * 15)\n+\n+        result_data = json.loads(result)\n+        assert \"error\" in result_data\n+        assert result_data[\"tool\"] == \"search_google\"\n+        assert \"valid string\" in result_data[\"error\"]\n+\n+\n+class TestGetAmazonProduct:\n+    \"\"\"Test cases for get_amazon_product method.\"\"\"\n+\n+    def test_get_amazon_product_success(self, mock_oxylabs_client, mock_environment_variables):\n+        \"\"\"Test successful Amazon product lookup.\"\"\"\n+        # Arrange\n+        mock_response = create_mock_response(\n+            results=[\n+                {\n+                    \"content\": {\"title\": \"Test Product\", \"price\": 29.99, \"currency\": \"USD\", \"rating\": 4.5},\n+                    \"status_code\": 200,\n+                }\n+            ]\n+        )\n+        mock_oxylabs_client.amazon.scrape_product.return_value = mock_response\n+\n+        tools = OxylabsTools()\n+\n+        # Act\n+        result = tools.get_amazon_product(asin=\"B08N5WRWNW\", domain_code=\"com\")\n+\n+        # Assert\n+        mock_oxylabs_client.amazon.scrape_product.assert_called_once_with(query=\"B08N5WRWNW\", domain=\"com\", parse=True)\n+\n+        result_data = json.loads(result)\n+        assert result_data[\"tool\"] == \"get_amazon_product\"\n+        assert result_data[\"asin\"] == \"B08N5WRWNW\"\n+        assert \"product_info\" in result_data\n+\n+    def test_get_amazon_product_invalid_asin(self, mock_oxylabs_client, mock_environment_variables):\n+        \"\"\"Test Amazon product lookup with invalid ASIN.\"\"\"\n+        tools = OxylabsTools()\n+\n+        result = tools.get_amazon_product(asin=\"INVALID\")\n+\n+        result_data = json.loads(result)\n+        assert \"error\" in result_data\n+        assert result_data[\"tool\"] == \"get_amazon_product\"\n+        assert \"Invalid ASIN format\" in result_data[\"error\"]\n+\n+\n+class TestSearchAmazonProducts:\n+    \"\"\"Test cases for search_amazon_products method.\"\"\"\n+\n+    def test_search_amazon_products_success(self, mock_oxylabs_client, mock_environment_variables):\n+        \"\"\"Test successful Amazon search.\"\"\"\n+        # Arrange\n+        mock_response = create_mock_response(\n+            results=[\n+                {\n+                    \"content\": {\n+                        \"results\": {\"organic\": [{\"title\": \"Test Product\", \"asin\": \"B08N5WRWNW\", \"price\": 29.99}]}\n+                    },\n+                    \"status_code\": 200,\n+                }\n+            ]\n+        )\n+        mock_oxylabs_client.amazon.scrape_search.return_value = mock_response\n+\n+        tools = OxylabsTools()\n+\n+        # Act\n+        result = tools.search_amazon_products(query=\"wireless headphones\", domain_code=\"com\")\n+\n+        # Assert\n+        mock_oxylabs_client.amazon.scrape_search.assert_called_once_with(\n+            query=\"wireless headphones\", domain=\"com\", parse=True\n+        )\n+\n+        result_data = json.loads(result)\n+        assert result_data[\"tool\"] == \"search_amazon_products\"\n+        assert result_data[\"query\"] == \"wireless headphones\"\n+        assert \"products\" in result_data\n+\n+    def test_search_amazon_products_empty_query(self, mock_oxylabs_client, mock_environment_variables):\n+        \"\"\"Test Amazon search with empty query.\"\"\"\n+        tools = OxylabsTools()\n+\n+        result = tools.search_amazon_products(query=\"\")\n+\n+        result_data = json.loads(result)\n+        assert \"error\" in result_data\n+        assert result_data[\"tool\"] == \"search_amazon_products\"\n+        assert \"cannot be empty\" in result_data[\"error\"]\n+\n+\n+class TestScrapeWebsite:\n+    \"\"\"Test cases for scrape_website method.\"\"\"\n+\n+    def test_scrape_website_success(self, mock_oxylabs_client, mock_environment_variables):\n+        \"\"\"Test successful website scraping.\"\"\"\n+        # Arrange\n+        mock_response = create_mock_response(\n+            results=[{\"content\": \"<html><body>Test Content</body></html>\", \"status_code\": 200}]\n+        )\n+        mock_oxylabs_client.universal.scrape_url.return_value = mock_response\n+\n+        tools = OxylabsTools()\n+\n+        # Act\n+        result = tools.scrape_website(url=\"https://example.com\", render_javascript=False)\n+\n+        # Assert\n+        mock_oxylabs_client.universal.scrape_url.assert_called_once_with(\n+            url=\"https://example.com\", render=None, parse=True\n+        )\n+\n+        result_data = json.loads(result)\n+        assert result_data[\"tool\"] == \"scrape_website\"\n+        assert result_data[\"url\"] == \"https://example.com\"\n+        assert \"content_info\" in result_data\n+\n+    def test_scrape_website_invalid_url(self, mock_oxylabs_client, mock_environment_variables):\n+        \"\"\"Test website scraping with invalid URL.\"\"\"\n+        tools = OxylabsTools()\n+\n+        result = tools.scrape_website(url=\"not-a-url\")\n+\n+        result_data = json.loads(result)\n+        assert \"error\" in result_data\n+        assert result_data[\"tool\"] == \"scrape_website\"\n+        assert \"Invalid URL format\" in result_data[\"error\"]\n+\n+    def test_scrape_website_with_javascript(self, mock_oxylabs_client, mock_environment_variables):\n+        \"\"\"Test website scraping with JavaScript rendering.\"\"\"\n+        # Arrange\n+        mock_response = create_mock_response(\n+            results=[{\"content\": \"<html><body>Rendered Content</body></html>\", \"status_code\": 200}]\n+        )\n+        mock_oxylabs_client.universal.scrape_url.return_value = mock_response\n+\n+        tools = OxylabsTools()\n+\n+        # Act\n+        result = tools.scrape_website(url=\"https://example.com\", render_javascript=True)\n+\n+        # Assert\n+        result_data = json.loads(result)\n+        assert result_data[\"tool\"] == \"scrape_website\"\n+        assert result_data[\"content_info\"][\"javascript_rendered\"] is True\n+\n+\n+class TestErrorHandling:\n+    \"\"\"Test cases for error handling.\"\"\"\n+\n+    def test_api_exception_handling(self, mock_oxylabs_client, mock_environment_variables):\n+        \"\"\"Test handling of API exceptions.\"\"\"\n+        # Arrange\n+        mock_oxylabs_client.google.scrape_search.side_effect = Exception(\"API Error\")\n+        tools = OxylabsTools()\n+\n+        # Act\n+        result = tools.search_google(query=\"test\")\n+\n+        # Assert\n+        result_data = json.loads(result)\n+        assert \"error\" in result_data\n+        assert result_data[\"tool\"] == \"search_google\"\n+        assert \"API Error\" in result_data[\"error\"]\n+\n+\n+class TestResponseFormatting:\n+    \"\"\"Test cases for response formatting.\"\"\"\n+\n+    def test_format_response_with_parsed_content(self, mock_oxylabs_client, mock_environment_variables):\n+        \"\"\"Test response formatting with parsed content.\"\"\"\n+        # Arrange\n+        mock_response = create_mock_response(\n+            results=[\n+                {\n+                    \"content_parsed\": Mock(\n+                        results=Mock(\n+                            raw={\n+                                \"organic\": [\n+                                    {\n+                                        \"title\": \"Test\",\n+                                        \"url\": \"https://example.com\",\n+                                        \"desc\": \"Test description\",\n+                                        \"pos\": 1,\n+                                    }\n+                                ]\n+                            }\n+                        )\n+                    ),\n+                    \"status_code\": 200,\n+                }\n+            ]\n+        )\n+        mock_oxylabs_client.google.scrape_search.return_value = mock_response\n+\n+        tools = OxylabsTools()\n+\n+        # Act\n+        result = tools.search_google(query=\"test\")\n+\n+        # Assert\n+        result_data = json.loads(result)\n+        assert result_data[\"tool\"] == \"search_google\"\n+        assert result_data[\"query\"] == \"test\"\n+        assert len(result_data[\"results\"]) == 1\n+        assert result_data[\"results\"][0][\"title\"] == \"Test\"\n+\n+    def test_format_response_empty_results(self, mock_oxylabs_client, mock_environment_variables):\n+        \"\"\"Test response formatting with empty results.\"\"\"\n+        # Arrange\n+        mock_response = create_mock_response(results=[])\n+        mock_oxylabs_client.google.scrape_search.return_value = mock_response\n+\n+        tools = OxylabsTools()\n+\n+        # Act\n+        result = tools.search_google(query=\"test\")\n+\n+        # Assert\n+        result_data = json.loads(result)\n+        assert result_data[\"tool\"] == \"search_google\"\n+        assert result_data[\"query\"] == \"test\"\n+        assert len(result_data[\"results\"]) == 0\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/tests/unit/tools/test_serper.py",
            "diff": "diff --git a/libs/agno/tests/unit/tools/test_serper.py b/libs/agno/tests/unit/tools/test_serper.py\nnew file mode 100644\nindex 000000000..41e31eefa\n--- /dev/null\n+++ b/libs/agno/tests/unit/tools/test_serper.py\n@@ -0,0 +1,359 @@\n+import json\n+from unittest.mock import Mock, patch\n+\n+import pytest\n+import requests\n+\n+from agno.tools.serper import SerperTools\n+\n+\n+@pytest.fixture(autouse=True)\n+def clear_env(monkeypatch):\n+    \"\"\"Ensure SERPER_API_KEY is unset unless explicitly needed.\"\"\"\n+    monkeypatch.delenv(\"SERPER_API_KEY\", raising=False)\n+\n+\n+@pytest.fixture\n+def api_tools():\n+    \"\"\"SerperTools with a known API key and custom settings for testing.\"\"\"\n+    return SerperTools(\n+        api_key=\"test_key\",\n+        location=\"us\",\n+        language=\"en\",\n+        num_results=5,\n+        date_range=\"qdr:d\",  # Last day\n+    )\n+\n+\n+@pytest.fixture\n+def mock_search_response():\n+    \"\"\"Mock a successful Serper API search response.\"\"\"\n+    mock = Mock(spec=requests.Response)\n+    mock.text = '{\"organic\": [{\"title\": \"Test Result\", \"link\": \"http://example.com\"}]}'\n+    mock.json.return_value = {\"organic\": [{\"title\": \"Test Result\", \"link\": \"http://example.com\"}]}\n+    mock.raise_for_status.return_value = None\n+    return mock\n+\n+\n+@pytest.fixture\n+def mock_news_response():\n+    \"\"\"Mock a successful Serper API news response.\"\"\"\n+    mock = Mock(spec=requests.Response)\n+    mock.text = '{\"news\": [{\"title\": \"Breaking News\", \"link\": \"http://news.example.com\", \"date\": \"2 hours ago\"}]}'\n+    mock.json.return_value = {\n+        \"news\": [{\"title\": \"Breaking News\", \"link\": \"http://news.example.com\", \"date\": \"2 hours ago\"}]\n+    }\n+    mock.raise_for_status.return_value = None\n+    return mock\n+\n+\n+@pytest.fixture\n+def mock_scholar_response():\n+    \"\"\"Mock a successful Serper API scholar response.\"\"\"\n+    mock = Mock(spec=requests.Response)\n+    mock.text = (\n+        '{\"organic\": [{\"title\": \"Research Paper\", \"link\": \"http://scholar.example.com\", \"authors\": [\"Dr. Smith\"]}]}'\n+    )\n+    mock.json.return_value = {\n+        \"organic\": [{\"title\": \"Research Paper\", \"link\": \"http://scholar.example.com\", \"authors\": [\"Dr. Smith\"]}]\n+    }\n+    mock.raise_for_status.return_value = None\n+    return mock\n+\n+\n+@pytest.fixture\n+def mock_scrape_response():\n+    \"\"\"Mock a successful Serper API scrape response.\"\"\"\n+    mock = Mock(spec=requests.Response)\n+    mock.text = '{\"text\": \"Scraped content\", \"title\": \"Example Page\"}'\n+    mock.json.return_value = {\"text\": \"Scraped content\", \"title\": \"Example Page\"}\n+    mock.raise_for_status.return_value = None\n+    return mock\n+\n+\n+# Initialization Tests\n+def test_init_without_api_key_and_env(monkeypatch):\n+    \"\"\"If no api_key argument and no SERPER_API_KEY in env, api_key should be None.\"\"\"\n+    monkeypatch.delenv(\"SERPER_API_KEY\", raising=False)\n+    tools = SerperTools()\n+    assert tools.api_key is None\n+\n+\n+def test_init_with_env_var(monkeypatch):\n+    \"\"\"If SERPER_API_KEY is set in the environment, it is picked up.\"\"\"\n+    monkeypatch.setenv(\"SERPER_API_KEY\", \"env_key\")\n+    tools = SerperTools(api_key=None)\n+    assert tools.api_key == \"env_key\"\n+\n+\n+def test_init_with_custom_params():\n+    \"\"\"Test initialization with custom parameters.\"\"\"\n+    tools = SerperTools(\n+        api_key=\"test_key\",\n+        location=\"uk\",\n+        language=\"fr\",\n+        num_results=15,\n+        date_range=\"qdr:w\",\n+    )\n+    assert tools.api_key == \"test_key\"\n+    assert tools.location == \"uk\"\n+    assert tools.language == \"fr\"\n+    assert tools.num_results == 15\n+    assert tools.date_range == \"qdr:w\"\n+\n+\n+# Search Tests\n+def test_search_no_api_key():\n+    \"\"\"Calling search without any API key returns an error message.\"\"\"\n+    tools = SerperTools(api_key=None)\n+    result = tools.search(\"anything\")\n+    result_json = json.loads(result)\n+    assert \"error\" in result_json\n+    assert \"Please provide a Serper API key\" in result_json[\"error\"]\n+\n+\n+def test_search_empty_query(api_tools):\n+    \"\"\"Calling search with an empty query returns an error message.\"\"\"\n+    result = api_tools.search(\"\")\n+    result_json = json.loads(result)\n+    assert \"error\" in result_json\n+    assert \"Please provide a query to search for\" in result_json[\"error\"]\n+\n+\n+def test_search_success(api_tools, mock_search_response):\n+    \"\"\"A successful search should return the raw response.text and call requests.request correctly.\"\"\"\n+    with patch(\"requests.request\", return_value=mock_search_response) as mock_req:\n+        result = api_tools.search(\"pytest testing\")\n+        assert result == mock_search_response.text\n+\n+        mock_req.assert_called_once_with(\n+            \"POST\",\n+            \"https://google.serper.dev/search\",\n+            headers={\"X-API-KEY\": \"test_key\", \"Content-Type\": \"application/json\"},\n+            data=json.dumps({\"q\": \"pytest testing\", \"num\": 5, \"tbs\": \"qdr:d\", \"gl\": \"us\", \"hl\": \"en\"}),\n+        )\n+\n+\n+def test_search_with_custom_num_results(api_tools, mock_search_response):\n+    \"\"\"Overriding the num_results parameter should be respected in the request payload.\"\"\"\n+    with patch(\"requests.request\", return_value=mock_search_response) as mock_req:\n+        result = api_tools.search(\"pytest testing\", num_results=20)\n+        assert result == mock_search_response.text\n+\n+        expected_payload = {\n+            \"q\": \"pytest testing\",\n+            \"num\": 20,\n+            \"tbs\": \"qdr:d\",\n+            \"gl\": \"us\",\n+            \"hl\": \"en\",\n+        }\n+        mock_req.assert_called_once_with(\n+            \"POST\",\n+            \"https://google.serper.dev/search\",\n+            headers={\"X-API-KEY\": \"test_key\", \"Content-Type\": \"application/json\"},\n+            data=json.dumps(expected_payload),\n+        )\n+\n+\n+def test_search_exception(api_tools):\n+    \"\"\"If requests.request raises, search should catch and return an error string.\"\"\"\n+    with patch(\"requests.request\", side_effect=Exception(\"Network failure\")):\n+        result = api_tools.search(\"failure test\")\n+        result_json = json.loads(result)\n+        assert \"error\" in result_json\n+        assert \"Network failure\" in result_json[\"error\"]\n+\n+\n+# News Search Tests\n+def test_search_news_no_api_key():\n+    \"\"\"Calling search_news without any API key returns an error message.\"\"\"\n+    tools = SerperTools(api_key=None)\n+    result = tools.search_news(\"tech news\")\n+    result_json = json.loads(result)\n+    assert \"error\" in result_json\n+    assert \"Please provide a Serper API key\" in result_json[\"error\"]\n+\n+\n+def test_search_news_empty_query(api_tools):\n+    \"\"\"Calling search_news with an empty query returns an error message.\"\"\"\n+    result = api_tools.search_news(\"\")\n+    result_json = json.loads(result)\n+    assert \"error\" in result_json\n+    assert \"Please provide a query to search for news\" in result_json[\"error\"]\n+\n+\n+def test_search_news_success(api_tools, mock_news_response):\n+    \"\"\"A successful news search should return the raw response.text.\"\"\"\n+    with patch(\"requests.request\", return_value=mock_news_response) as mock_req:\n+        result = api_tools.search_news(\"latest tech news\")\n+        assert result == mock_news_response.text\n+\n+        expected_payload = {\n+            \"q\": \"latest tech news\",\n+            \"num\": 5,\n+            \"tbs\": \"qdr:d\",\n+            \"gl\": \"us\",\n+            \"hl\": \"en\",\n+        }\n+        mock_req.assert_called_once_with(\n+            \"POST\",\n+            \"https://google.serper.dev/news\",\n+            headers={\"X-API-KEY\": \"test_key\", \"Content-Type\": \"application/json\"},\n+            data=json.dumps(expected_payload),\n+        )\n+\n+\n+def test_search_news_with_custom_num_results(api_tools, mock_news_response):\n+    \"\"\"Overriding num_results in news search should work.\"\"\"\n+    with patch(\"requests.request\", return_value=mock_news_response) as mock_req:\n+        result = api_tools.search_news(\"tech news\", num_results=15)\n+        assert result == mock_news_response.text\n+\n+        expected_payload = {\"q\": \"tech news\", \"num\": 15, \"tbs\": \"qdr:d\", \"gl\": \"us\", \"hl\": \"en\"}\n+        mock_req.assert_called_once()\n+        call_args = mock_req.call_args\n+        assert json.loads(call_args[1][\"data\"]) == expected_payload\n+\n+\n+def test_search_news_exception(api_tools):\n+    \"\"\"If requests.request raises during news search, should catch and return error.\"\"\"\n+    with patch(\"requests.request\", side_effect=Exception(\"API timeout\")):\n+        result = api_tools.search_news(\"breaking news\")\n+        result_json = json.loads(result)\n+        assert \"error\" in result_json\n+        assert \"API timeout\" in result_json[\"error\"]\n+\n+\n+# Scholar Search Tests\n+def test_search_scholar_no_api_key():\n+    \"\"\"Calling search_scholar without any API key returns an error message.\"\"\"\n+    tools = SerperTools(api_key=None)\n+    result = tools.search_scholar(\"machine learning\")\n+    result_json = json.loads(result)\n+    assert \"error\" in result_json\n+    assert \"Please provide a Serper API key\" in result_json[\"error\"]\n+\n+\n+def test_search_scholar_empty_query(api_tools):\n+    \"\"\"Calling search_scholar with an empty query returns an error message.\"\"\"\n+    result = api_tools.search_scholar(\"\")\n+    result_json = json.loads(result)\n+    assert \"error\" in result_json\n+    assert \"Please provide a query to search for academic papers\" in result_json[\"error\"]\n+\n+\n+def test_search_scholar_success(api_tools, mock_scholar_response):\n+    \"\"\"A successful scholar search should return the raw response.text.\"\"\"\n+    with patch(\"requests.request\", return_value=mock_scholar_response) as mock_req:\n+        result = api_tools.search_scholar(\"artificial intelligence\")\n+        assert result == mock_scholar_response.text\n+\n+        expected_payload = {\n+            \"q\": \"artificial intelligence\",\n+            \"num\": 5,\n+            \"tbs\": \"qdr:d\",\n+            \"gl\": \"us\",\n+            \"hl\": \"en\",\n+        }\n+        mock_req.assert_called_once_with(\n+            \"POST\",\n+            \"https://google.serper.dev/scholar\",\n+            headers={\"X-API-KEY\": \"test_key\", \"Content-Type\": \"application/json\"},\n+            data=json.dumps(expected_payload),\n+        )\n+\n+\n+def test_search_scholar_exception(api_tools):\n+    \"\"\"If requests.request raises during scholar search, should catch and return error.\"\"\"\n+    with patch(\"requests.request\", side_effect=Exception(\"Scholar API error\")):\n+        result = api_tools.search_scholar(\"quantum computing\")\n+        result_json = json.loads(result)\n+        assert \"error\" in result_json\n+        assert \"Scholar API error\" in result_json[\"error\"]\n+\n+\n+# Webpage Scraping Tests\n+def test_scrape_webpage_no_api_key():\n+    \"\"\"Calling scrape_webpage without any API key returns an error message.\"\"\"\n+    tools = SerperTools(api_key=None)\n+    result = tools.scrape_webpage(\"https://example.com\")\n+    result_json = json.loads(result)\n+    assert \"error\" in result_json\n+    assert \"Please provide a Serper API key\" in result_json[\"error\"]\n+\n+\n+def test_scrape_webpage_empty_url(api_tools):\n+    \"\"\"Calling scrape_webpage with an empty URL returns an error message.\"\"\"\n+    result = api_tools.scrape_webpage(\"\")\n+    result_json = json.loads(result)\n+    assert \"error\" in result_json\n+    assert \"Please provide a URL to scrape\" in result_json[\"error\"]\n+\n+\n+def test_scrape_webpage_success(api_tools, mock_scrape_response):\n+    \"\"\"A successful webpage scrape should return the raw response.text.\"\"\"\n+    with patch(\"requests.request\", return_value=mock_scrape_response) as mock_req:\n+        result = api_tools.scrape_webpage(\"https://example.com\")\n+        assert result == mock_scrape_response.text\n+\n+        expected_payload = {\n+            \"url\": \"https://example.com\",\n+            \"includeMarkdown\": False,\n+            \"tbs\": \"qdr:d\",\n+            \"gl\": \"us\",\n+            \"hl\": \"en\",\n+        }\n+        mock_req.assert_called_once_with(\n+            \"POST\",\n+            \"https://scrape.serper.dev\",\n+            headers={\"X-API-KEY\": \"test_key\", \"Content-Type\": \"application/json\"},\n+            data=json.dumps(expected_payload),\n+        )\n+\n+\n+def test_scrape_webpage_with_markdown(api_tools, mock_scrape_response):\n+    \"\"\"Scraping with markdown=True should set includeMarkdown to True.\"\"\"\n+    with patch(\"requests.request\", return_value=mock_scrape_response) as mock_req:\n+        result = api_tools.scrape_webpage(\"https://example.com\", markdown=True)\n+        assert result == mock_scrape_response.text\n+\n+        expected_payload = {\n+            \"url\": \"https://example.com\",\n+            \"includeMarkdown\": True,\n+            \"tbs\": \"qdr:d\",\n+            \"gl\": \"us\",\n+            \"hl\": \"en\",\n+        }\n+        call_args = mock_req.call_args\n+        assert json.loads(call_args[1][\"data\"]) == expected_payload\n+\n+\n+def test_scrape_webpage_exception(api_tools):\n+    \"\"\"If requests.request raises during scraping, should catch and return error.\"\"\"\n+    with patch(\"requests.request\", side_effect=Exception(\"Scraping failed\")):\n+        result = api_tools.scrape_webpage(\"https://example.com\")\n+        result_json = json.loads(result)\n+        assert \"error\" in result_json\n+        assert \"Scraping failed\" in result_json[\"error\"]\n+\n+\n+# Edge Cases and Integration Tests\n+def test_tools_without_optional_params():\n+    \"\"\"Test initialization and usage with minimal parameters.\"\"\"\n+    tools = SerperTools(api_key=\"test_key\")\n+    assert tools.location == \"us\"\n+    assert tools.language == \"en\"\n+    assert tools.num_results == 10\n+    assert tools.date_range is None\n+\n+\n+def test_http_error_handling(api_tools):\n+    \"\"\"Test that HTTP errors are properly handled.\"\"\"\n+    mock_response = Mock()\n+    mock_response.raise_for_status.side_effect = requests.exceptions.HTTPError(\"404 Not Found\")\n+\n+    with patch(\"requests.request\", return_value=mock_response):\n+        result = api_tools.search(\"test query\")\n+        result_json = json.loads(result)\n+        assert \"error\" in result_json\n+        assert \"404 Not Found\" in result_json[\"error\"]\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/tests/unit/tools/test_serperdev.py",
            "diff": "diff --git a/libs/agno/tests/unit/tools/test_serperdev.py b/libs/agno/tests/unit/tools/test_serperdev.py\ndeleted file mode 100644\nindex 960295598..000000000\n--- a/libs/agno/tests/unit/tools/test_serperdev.py\n+++ /dev/null\n@@ -1,88 +0,0 @@\n-import json\n-from unittest.mock import Mock, patch\n-\n-import pytest\n-import requests\n-\n-from agno.tools.serper import SerperTools\n-\n-\n-@pytest.fixture(autouse=True)\n-def clear_env(monkeypatch):\n-    \"\"\"Ensure SERPER_API_KEY is unset unless explicitly needed.\"\"\"\n-    monkeypatch.delenv(\"SERPER_API_KEY\", raising=False)\n-\n-\n-@pytest.fixture\n-def api_tools():\n-    \"\"\"SerperApiTools with a known API key, custom location, and fewer results for testing.\"\"\"\n-    return SerperTools(api_key=\"test_key\", location=\"us\", num_results=5)\n-\n-\n-@pytest.fixture\n-def mock_search_response():\n-    \"\"\"Mock a successful Serper API HTTP response.\"\"\"\n-    mock = Mock(spec=requests.Response)\n-    mock.text = '{\"results\": [{\"title\": \"Test Result\", \"link\": \"http://example.com\"}]}'\n-    return mock\n-\n-\n-def test_init_without_api_key_and_env(monkeypatch):\n-    \"\"\"If no api_key argument and no SERPER_API_KEY in env, api_key should be None.\"\"\"\n-    # Ensure environment has no key\n-    monkeypatch.delenv(\"SERPER_API_KEY\", raising=False)\n-    tools = SerperTools()\n-    assert tools.api_key is None\n-\n-\n-def test_init_with_env_var(monkeypatch):\n-    \"\"\"If SERPER_API_KEY is set in the environment, it is picked up.\"\"\"\n-    monkeypatch.setenv(\"SERPER_API_KEY\", \"env_key\")\n-    tools = SerperTools(api_key=None)\n-    assert tools.api_key == \"env_key\"\n-\n-\n-def test_search_google_no_api_key():\n-    \"\"\"Calling search_google without any API key returns an error message.\"\"\"\n-    tools = SerperTools(api_key=None)\n-    assert tools.search_google(\"anything\") == \"Please provide an API key\"\n-\n-\n-def test_search_google_empty_query(api_tools):\n-    \"\"\"Calling search_google with an empty query returns an error message.\"\"\"\n-    assert api_tools.search_google(\"\") == \"Please provide a query to search for\"\n-\n-\n-def test_search_google_success_default_location(api_tools, mock_search_response):\n-    \"\"\"A successful search should return the raw response.text and call requests.request correctly.\"\"\"\n-    with patch(\"requests.request\", return_value=mock_search_response) as mock_req:\n-        result = api_tools.search_google(\"pytest testing\")\n-        assert result == mock_search_response.text\n-\n-        mock_req.assert_called_once_with(\n-            \"POST\",\n-            \"https://google.serper.dev/search\",\n-            headers={\"X-API-KEY\": \"test_key\", \"Content-Type\": \"application/json\"},\n-            data=json.dumps({\"q\": \"pytest testing\", \"num\": 5, \"gl\": \"us\"}),\n-        )\n-\n-\n-def test_search_google_success_override_location(api_tools, mock_search_response):\n-    \"\"\"Overriding the location parameter should be respected in the request payload.\"\"\"\n-    with patch(\"requests.request\", return_value=mock_search_response) as mock_req:\n-        result = api_tools.search_google(\"pytest testing\", location=\"uk\")\n-        assert result == mock_search_response.text\n-\n-        mock_req.assert_called_once_with(\n-            \"POST\",\n-            \"https://google.serper.dev/search\",\n-            headers={\"X-API-KEY\": \"test_key\", \"Content-Type\": \"application/json\"},\n-            data=json.dumps({\"q\": \"pytest testing\", \"num\": 5, \"gl\": \"uk\"}),\n-        )\n-\n-\n-def test_search_google_exception(api_tools):\n-    \"\"\"If requests.request raises, search_google should catch and return an error string.\"\"\"\n-    with patch(\"requests.request\", side_effect=Exception(\"Network failure\")):\n-        result = api_tools.search_google(\"failure test\")\n-        assert \"Error searching for the query failure test: Network failure\" in result\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/tests/unit/tools/test_valyu.py",
            "diff": "diff --git a/libs/agno/tests/unit/tools/test_valyu.py b/libs/agno/tests/unit/tools/test_valyu.py\nnew file mode 100644\nindex 000000000..626a42f8b\n--- /dev/null\n+++ b/libs/agno/tests/unit/tools/test_valyu.py\n@@ -0,0 +1,223 @@\n+import json\n+from unittest.mock import patch\n+\n+import pytest\n+\n+from agno.tools.valyu import ValyuTools\n+\n+\n+class MockSearchResult:\n+    def __init__(\n+        self,\n+        title=\"Test Paper\",\n+        url=\"https://example.com\",\n+        content=\"Test content\",\n+        source=\"test\",\n+        relevance_score=0.8,\n+        description=\"Test description\",\n+    ):\n+        self.title = title\n+        self.url = url\n+        self.content = content\n+        self.source = source\n+        self.relevance_score = relevance_score\n+        self.description = description\n+\n+\n+class MockSearchResponse:\n+    def __init__(self, success=True, results=None, error=None):\n+        self.success = success\n+        self.results = results or []\n+        self.error = error\n+\n+\n+@pytest.fixture\n+def mock_valyu():\n+    with patch(\"agno.tools.valyu.Valyu\") as mock:\n+        yield mock\n+\n+\n+@pytest.fixture\n+def valyu_tools(mock_valyu):\n+    return ValyuTools(api_key=\"test_key\")\n+\n+\n+class TestValyuTools:\n+    def test_init_with_api_key(self, mock_valyu):\n+        \"\"\"Test initialization with API key.\"\"\"\n+        tools = ValyuTools(api_key=\"test_key\")\n+        assert tools.api_key == \"test_key\"\n+        assert tools.max_price == 30.0\n+        assert tools.text_length == 1000\n+        mock_valyu.assert_called_once_with(api_key=\"test_key\")\n+\n+    def test_init_without_api_key_raises_error(self, mock_valyu):\n+        \"\"\"Test initialization without API key raises ValueError.\"\"\"\n+        with patch.dict(\"os.environ\", {}, clear=True):\n+            with pytest.raises(ValueError, match=\"VALYU_API_KEY not set\"):\n+                ValyuTools()\n+\n+    @patch.dict(\"os.environ\", {\"VALYU_API_KEY\": \"env_key\"})\n+    def test_init_with_env_api_key(self, mock_valyu):\n+        \"\"\"Test initialization with API key from environment.\"\"\"\n+        tools = ValyuTools()\n+        assert tools.api_key == \"env_key\"\n+\n+    def test_parse_results_basic(self, valyu_tools):\n+        \"\"\"Test basic result parsing.\"\"\"\n+        results = [MockSearchResult()]\n+        parsed = valyu_tools._parse_results(results)\n+        data = json.loads(parsed)\n+\n+        assert len(data) == 1\n+        assert data[0][\"title\"] == \"Test Paper\"\n+        assert data[0][\"url\"] == \"https://example.com\"\n+        assert data[0][\"content\"] == \"Test content\"\n+        assert data[0][\"relevance_score\"] == 0.8\n+\n+    def test_parse_results_with_text_truncation(self, valyu_tools):\n+        \"\"\"Test result parsing with text length truncation.\"\"\"\n+        valyu_tools.text_length = 10\n+        long_content = \"A\" * 20\n+        results = [MockSearchResult(content=long_content)]\n+        parsed = valyu_tools._parse_results(results)\n+        data = json.loads(parsed)\n+\n+        assert data[0][\"content\"] == \"A\" * 10 + \"...\"\n+\n+    def test_parse_results_empty(self, valyu_tools):\n+        \"\"\"Test parsing empty results.\"\"\"\n+        parsed = valyu_tools._parse_results([])\n+        data = json.loads(parsed)\n+        assert data == []\n+\n+    def test_search_academic_sources_success(self, valyu_tools):\n+        \"\"\"Test successful academic search.\"\"\"\n+        mock_response = MockSearchResponse(success=True, results=[MockSearchResult(title=\"Academic Paper\")])\n+        valyu_tools.valyu.search.return_value = mock_response\n+\n+        result = valyu_tools.search_academic_sources(\"test query\")\n+        data = json.loads(result)\n+\n+        assert len(data) == 1\n+        assert data[0][\"title\"] == \"Academic Paper\"\n+\n+        # Verify search was called with correct parameters\n+        valyu_tools.valyu.search.assert_called_once()\n+        call_args = valyu_tools.valyu.search.call_args[1]\n+        assert call_args[\"query\"] == \"test query\"\n+        assert call_args[\"search_type\"] == \"proprietary\"\n+        assert \"valyu/valyu-arxiv\" in call_args[\"included_sources\"]\n+        assert \"valyu/valyu-pubmed\" in call_args[\"included_sources\"]\n+\n+    def test_search_academic_sources_with_dates(self, valyu_tools):\n+        \"\"\"Test academic search with date filters.\"\"\"\n+        mock_response = MockSearchResponse(success=True, results=[])\n+        valyu_tools.valyu.search.return_value = mock_response\n+\n+        valyu_tools.search_academic_sources(\"test query\", start_date=\"2023-01-01\", end_date=\"2023-12-31\")\n+\n+        call_args = valyu_tools.valyu.search.call_args[1]\n+        assert call_args[\"start_date\"] == \"2023-01-01\"\n+        assert call_args[\"end_date\"] == \"2023-12-31\"\n+\n+    def test_search_web_success(self, valyu_tools):\n+        \"\"\"Test successful web search.\"\"\"\n+        mock_response = MockSearchResponse(success=True, results=[MockSearchResult(title=\"Web Article\")])\n+        valyu_tools.valyu.search.return_value = mock_response\n+\n+        result = valyu_tools.search_web(\"test query\")\n+        data = json.loads(result)\n+\n+        assert len(data) == 1\n+        assert data[0][\"title\"] == \"Web Article\"\n+\n+        call_args = valyu_tools.valyu.search.call_args[1]\n+        assert call_args[\"search_type\"] == \"web\"\n+\n+    def test_search_web_with_category(self, valyu_tools):\n+        \"\"\"Test web search with category.\"\"\"\n+        mock_response = MockSearchResponse(success=True, results=[])\n+        valyu_tools.valyu.search.return_value = mock_response\n+\n+        valyu_tools.search_web(\"test query\", content_category=\"technology\")\n+\n+        call_args = valyu_tools.valyu.search.call_args[1]\n+        assert call_args[\"category\"] == \"technology\"\n+\n+    def test_search_within_paper_success(self, valyu_tools):\n+        \"\"\"Test successful within-paper search.\"\"\"\n+        mock_response = MockSearchResponse(success=True, results=[MockSearchResult(title=\"Paper Section\")])\n+        valyu_tools.valyu.search.return_value = mock_response\n+\n+        result = valyu_tools.search_within_paper(\"https://arxiv.org/abs/1234.5678\", \"test query\")\n+        data = json.loads(result)\n+\n+        assert len(data) == 1\n+        assert data[0][\"title\"] == \"Paper Section\"\n+\n+        call_args = valyu_tools.valyu.search.call_args[1]\n+        assert call_args[\"included_sources\"] == [\"https://arxiv.org/abs/1234.5678\"]\n+\n+    def test_search_within_paper_invalid_url(self, valyu_tools):\n+        \"\"\"Test within-paper search with invalid URL.\"\"\"\n+        result = valyu_tools.search_within_paper(\"invalid-url\", \"test query\")\n+        assert \"Error: Invalid paper URL format\" in result\n+\n+    def test_search_api_error(self, valyu_tools):\n+        \"\"\"Test handling of API error.\"\"\"\n+        mock_response = MockSearchResponse(success=False, error=\"API Error\")\n+        valyu_tools.valyu.search.return_value = mock_response\n+\n+        result = valyu_tools.search_academic_sources(\"test query\")\n+        assert \"Error: API Error\" in result\n+\n+    def test_search_exception_handling(self, valyu_tools):\n+        \"\"\"Test exception handling during search.\"\"\"\n+        valyu_tools.valyu.search.side_effect = Exception(\"Network error\")\n+\n+        result = valyu_tools.search_academic_sources(\"test query\")\n+        assert \"Error: Valyu search failed: Network error\" in result\n+\n+    def test_constructor_parameters_used_in_search(self, mock_valyu):\n+        \"\"\"Test that constructor parameters are properly used in searches.\"\"\"\n+        tools = ValyuTools(\n+            api_key=\"test_key\",\n+            max_results=5,\n+            relevance_threshold=0.7,\n+            content_category=\"science\",\n+            search_start_date=\"2023-01-01\",\n+        )\n+\n+        mock_response = MockSearchResponse(success=True, results=[])\n+        tools.valyu.search.return_value = mock_response\n+\n+        tools.search_academic_sources(\"test query\")\n+\n+        call_args = tools.valyu.search.call_args[1]\n+        assert call_args[\"max_num_results\"] == 5\n+        assert call_args[\"relevance_threshold\"] == 0.7\n+        assert call_args[\"category\"] == \"science\"\n+        assert call_args[\"start_date\"] == \"2023-01-01\"\n+\n+    def test_method_parameters_override_constructor(self, valyu_tools):\n+        \"\"\"Test that method parameters override constructor defaults.\"\"\"\n+        valyu_tools.content_category = \"default_category\"\n+        valyu_tools.search_start_date = \"2023-01-01\"\n+\n+        mock_response = MockSearchResponse(success=True, results=[])\n+        valyu_tools.valyu.search.return_value = mock_response\n+\n+        valyu_tools.search_web(\"test query\", content_category=\"override_category\", start_date=\"2024-01-01\")\n+\n+        call_args = valyu_tools.valyu.search.call_args[1]\n+        assert call_args[\"category\"] == \"override_category\"\n+        assert call_args[\"start_date\"] == \"2024-01-01\"\n+\n+    def test_tools_registration(self, valyu_tools):\n+        \"\"\"Test that all tools are properly registered.\"\"\"\n+        tool_names = list(valyu_tools.functions.keys())\n+        expected_tools = [\"search_academic_sources\", \"search_web\", \"search_within_paper\"]\n+\n+        for tool in expected_tools:\n+            assert tool in tool_names\n"
        },
        {
            "commit": "7efb1f09096dd41ea09b30ceb17746686931d1cf",
            "file_path": "libs/agno/tests/unit/utils/test_string.py",
            "diff": "diff --git a/libs/agno/tests/unit/utils/test_string.py b/libs/agno/tests/unit/utils/test_string.py\nindex ad7a7cb5c..599d2ce50 100644\n--- a/libs/agno/tests/unit/utils/test_string.py\n+++ b/libs/agno/tests/unit/utils/test_string.py\n@@ -319,3 +319,30 @@ def test_parse_preserves_field_name_case_with_markdown():\n     assert result is not None\n     assert result.Supplier_name == 'test \"quoted\" supplier'\n     assert result.newData == 'some \"quoted\" data'\n+\n+\n+def test_parse_json_with_python_code_in_value():\n+    \"\"\"Test parsing JSON with valid Python code containing # and * characters as a value\"\"\"\n+\n+    class CodeModel(BaseModel):\n+        function_name: str\n+        code: str\n+        description: str\n+\n+    content = \"\"\"```json\n+    {\n+        \"function_name\": \"calculate_factorial\",\n+        \"code\": \"def factorial(n):\\n    # Calculate factorial of n\\n    if n <= 1:\\n        return 1\\n    return n * factorial(n - 1)\",\n+        \"description\": \"A recursive factorial function with comments and multiplication\"\n+    }\n+    ```\"\"\"\n+\n+    result = parse_response_model_str(content, CodeModel)\n+\n+    assert result is not None\n+    assert result.function_name == \"calculate_factorial\"\n+    assert (\n+        result.code\n+        == \"def factorial(n):     # Calculate factorial of n     if n <= 1:         return 1     return n * factorial(n - 1)\"\n+    )\n+    assert result.description == \"A recursive factorial function with comments and multiplication\"\n"
        }
    ]
}